Id,summary,text,gold
"('https://openreview.net/forum?id=B1-Hhnslg',)","One learning class 


-I am not sure the reported results correctly reflect the state of the art for all tasks..I would suggest to start with Chris Burges 2010 tutorial..-*** References ***


-Large Margin Nearest Neighbors.

The paper is an extension of the matching networks by Vinyals et al..The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013)..in NIPS2016.","The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am not fully convinced its advantages.  This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier. Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1). This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely.----------------Although the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode). In its current form the paper a bit vague about this. *** Paper Summary ***----------------This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.----------------*** Review ***----------------The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. ----------------The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example ""the query"" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class ----------------I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?----------------Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.----------------*** References ***----------------Large Margin Nearest Neighbors. Weinberger et al, 2005--------From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010--------A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the relationships of this work to existing work in literature (both in terms of a discussion to clarify the novelty, and in terms of more complete empirical comparisons). Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work."
"('https://openreview.net/forum?id=B1-q5Pqxl',)","However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult..---- Page 6, last paragraph: missing “.”: “… searching This…”





--Summary:
---While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate..---4.

The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text)..-Strengths:
---1..The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps.","The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.----------------Strength:--------- The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)--------- The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.------------------------Weaknesses:--------1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.--------2. Experimental evaluation--------2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.--------2.2. It would be interested if this approach generalizes to other datasets.------------------------Other (minor/discussion points)--------- The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.--------- I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process.--------- Page 6, last paragraph: missing “.”: “… searching This…”--------------------------------Summary:--------While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task. Summary:--------The paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.----------------Strengths:--------1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.--------2. Significant performance boost over the baseline presented in the SQuAD paper.--------3. Some insightful analyses of the results such as performance is better when answers are short, ""why"" questions are difficult to answer.----------------Weaknesses/Questions/Suggestions:--------1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.--------2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.--------3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.--------4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? --------5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.--------6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?----------------Review Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult. SUMMARY.--------This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.--------The proposed model combines two well-know neural network architectures match-lstm and pointer nets.--------First the passage and the questions are encoded with a unidirectional LSTM.--------Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.--------For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.--------The same process is done in the opposite direction with a backward lstm.--------The final representation is a concatenation of the two lstms.--------As a decoded a pointer network is used.--------The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.----------------The proposed model is tested on the Stanford Question Answering Dataset.--------An ensemble of the proposed model achieves performance close to state-of-the-art models.--------------------------------------------------OVERALL JUDGMENT----------------I think the model is interesting mainly because of the use of pointer networks as a decoder.--------One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.--------The analysis of the model is interesting and insightful.--------The sharing of the code is good.","This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so."
"('https://openreview.net/forum?id=B16Jem9xe',)","These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way..In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used..The paper provides an exposition of multiple ways of learning in implicit generative models, of which generative adversarial networks are an example.

I just noticed I submitted my review as a pre-review question - sorry about this..An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa..presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty.","I just noticed I submitted my review as a pre-review question - sorry about this. Here it is again, with a few more thoughts added...----------------The authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.----------------My main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper ""But it has left us unsatisfied since we have not gained the insight needed to choose between them.” summarises my feeling about this paper: this is a nice 'unifying review’ type paper that - for me - lacks a novel insight.----------------In summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I’m not highly confident about this as a conference submission (and hence my low score) I am open to be convinced either way.----------------Detailed comments:----------------I think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.----------------I don’t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.----------------There is a typo in spelling Csiszar divergence----------------Equation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009). A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense. I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.----------------I have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.----------------On likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders). An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa. To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach. Could the authors comment on this?----------------I think the hypothesis testing angle is oversold in the paper.  I’m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used. The paper provides an exposition of multiple ways of learning in implicit generative models, of which generative adversarial networks are an example. The paper is very clear, the exposition is insightful, and the presented material is clearly important.----------------It is hard to assess ""novelty"" of this work, as the individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.----------------I believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning. This has the potential to accelerate progress.----------------I recommend publishing this paper at ICLR, even though it is not the ""typical"" paper that get published at this conference (in that it doesn't offer empirical validation, nor makes a particular claim about relative merits of different methods). Thank you for an interesting read.----------------Given the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.----------------The only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for.","Hello Authors,  Congratulations on the acceptance of the paper.  I've just reread parts of the revised paper and noticed a few things that you might want to consider and change before the camera-ready deadline.  * You now include a reference to KLIEP after Eqn. (16), but this procedure is in fact known as least-squares importance estimation. * in turn, Eqn. (14) is actually more akin to KLIEP, the main difference being the use of the unnormalised form of the KL-divergence. So I think you meant to put the KLIEP reference here.  Further comment on making Eqn. (14) practical: If you read the KLIEP paper, they formulate the procedure as a constrained optimisation problem: maximise subject to the constraint that Compare this constrained optimisation to your solution, it is easy to make the connection: if you introduce a Lagrange multiplier to handle the constraint, one obtains the following unconstrained optimisation problem:  seek stationary points of I do think that solving this unconstrained optimisation problem is actually possible, you can do that via stochastic gradient descent, and it does not include your nasty cross-entropy term.  What am I missing?  Thanks,  Rev1"
