{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/all_reviews_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Results on the VQA task are good for this simp...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This work proposes to approximate the bilinear...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summary:--------This paper proposes to use sur...</td>\n",
       "      <td>Based on the feedback, I'm going to be rejecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper proposes to use previous error sign...</td>\n",
       "      <td>Based on the feedback, I'm going to be rejecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Summary: The paper presents low-rank bilinear ...   \n",
       "1  Results on the VQA task are good for this simp...   \n",
       "2  This work proposes to approximate the bilinear...   \n",
       "3  Summary:--------This paper proposes to use sur...   \n",
       "4  This paper proposes to use previous error sign...   \n",
       "\n",
       "                                                gold  \n",
       "0  The program committee appreciates the authors'...  \n",
       "1  The program committee appreciates the authors'...  \n",
       "2  The program committee appreciates the authors'...  \n",
       "3  Based on the feedback, I'm going to be rejecti...  \n",
       "4  Based on the feedback, I'm going to be rejecti...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'gold']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for each year consist of `['id','text','gold']`\n",
    "- Text: Is the source \n",
    "- Gold: We assume that the area chair's motivations for their decision provide a reasonable comparison (summary)\n",
    "\n",
    "*Note*: For each paper, 3 reviews are extracted, you can notice that the `gold` value is same for all the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by following directories in order and apply some functions\n",
    "\n",
    "We start from the directory `glimpse/baselines` where comparative results are treated\n",
    "\n",
    "1. `generate_llm_summaries.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glimpse.baselines import generate_llm_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model replace 'togethercomputer/Llama-2-7B-32K-Instruct'\n",
    "model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token='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')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model, trust_remote_code=True, torch_dtype=torch.float16, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_llm_summaries.prepare_dataset('reviews_2017', dataset_path='data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-Hhnslg</th>\n",
       "      <td>The paper is an extension of the matching netw...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-q5Pqxl</th>\n",
       "      <td>The paper looks at the problem of locating the...</td>\n",
       "      <td>This paper provides two approaches to question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B16Jem9xe</th>\n",
       "      <td>I just noticed I submitted my review as a pre-...</td>\n",
       "      <td>Hello Authors,  Congratulations on the accepta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        text  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The paper is an extension of the matching netw...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  The paper looks at the problem of locating the...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  I just noticed I submitted my review as a pre-...   \n",
       "\n",
       "                                                                                        gold  \n",
       "id                                                                                            \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The program committee appreciates the authors'...  \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  This paper provides two approaches to question...  \n",
       "https://openreview.net/forum?id=B16Jem9xe  Hello Authors,  Congratulations on the accepta...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_llm_summaries.group_text_by_id(df)\n",
    "\n",
    "# Group text by sample id and concatenate text\n",
    "df.head(3)\n",
    "\n",
    "# Grouped by id, text is concatenated of all reviews, and gold is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take first 10 samples for testing\n",
    "df = df.head(10)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThe paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am not fully convinced its advantages.  This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier. Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1). This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely.----------------Although the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode). In its current form the paper a bit vague about this. *** Paper Summary ***----------------This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.----------------*** Review ***----------------The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. ----------------The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class ----------------I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?----------------Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.----------------*** References ***----------------Large Margin Nearest Neighbors. Weinberger et al, 2005--------From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010--------A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Add pad token\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_llm_summaries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\glimpse\\baselines\\generate_llm_summaries.py:88\u001b[0m, in \u001b[0;36mgenerate_summaries\u001b[1;34m(model, tokenizer, df, batch_size, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m     82\u001b[0m             batch,\n\u001b[0;32m     83\u001b[0m             padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m             truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     85\u001b[0m             return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         )\n\u001b[0;32m     87\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 88\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m         summaries\u001b[38;5;241m.\u001b[39mextend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[0;32m     91\u001b[0m             outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# remove the instruction from the summaries\u001b[39;00m\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1210\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m-> 1210\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "df = generate_llm_summaries.generate_summaries(model, tokenizer, df, batch_size=2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'summary'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextwrap\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m textwrap\u001b[38;5;241m.\u001b[39mwrap(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'summary'"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "textwrap.wrap(df['summary'].iloc[0], width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'gold', 'instruction'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- For each paper (document) we have 3 reviews, these reviews are concatenated\n",
    "- The review (contactenated 3 reviews) are fed to a model to provide a summary for them\n",
    "- In order to do that we use `generate_summaries` function that adds first a column to the `df` where instruction is applied along with `text`.\n",
    "- Then the model provides a summary for each text and df is returned back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `sumy_baselines.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimpse.baselines import sumy_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in [1]:\n",
    "        summaries = []\n",
    "        for text in df.text:\n",
    "            summary = sumy_baselines.summarize('LSA', \"english\", N, \"text\", text)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        df['summary'] = summaries\n",
    "        df[\"metadata/method\"] = 'LSA'\n",
    "        df[\"metadata/sentence_count\"] = N\n",
    "\n",
    "        name = f\"{df}-_-'LSA'-_-sumy_{N}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>instruction</th>\n",
       "      <th>summary</th>\n",
       "      <th>metadata/method</th>\n",
       "      <th>metadata/sentence_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-Hhnslg</th>\n",
       "      <td>The paper is an extension of the matching netw...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>[INST]\\nThe paper is an extension of the match...</td>\n",
       "      <td>On Cub 200, I thought that the state-of-the-ar...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-q5Pqxl</th>\n",
       "      <td>The paper looks at the problem of locating the...</td>\n",
       "      <td>This paper provides two approaches to question...</td>\n",
       "      <td>[INST]\\nThe paper looks at the problem of loca...</td>\n",
       "      <td>The authors might want to consider pointing to...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B16Jem9xe</th>\n",
       "      <td>I just noticed I submitted my review as a pre-...</td>\n",
       "      <td>Hello Authors,  Congratulations on the accepta...</td>\n",
       "      <td>[INST]\\nI just noticed I submitted my review a...</td>\n",
       "      <td>It is well written and a good read, and one I ...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B16dGcqlx</th>\n",
       "      <td>This paper proposed a novel adversarial framew...</td>\n",
       "      <td>pros:  - new problem  - huge number of experim...</td>\n",
       "      <td>[INST]\\nThis paper proposed a novel adversaria...</td>\n",
       "      <td>I will list these concerns in the following (i...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B184E5qee</th>\n",
       "      <td>The authors present a simple method to affix a...</td>\n",
       "      <td>Reviewers agree that this paper is based on a ...</td>\n",
       "      <td>[INST]\\nThe authors present a simple method to...</td>\n",
       "      <td>They demonstrate good improvements on language...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B186cP9gx</th>\n",
       "      <td>This paper investigates the hessian of small d...</td>\n",
       "      <td>This is quite an important topic to understand...</td>\n",
       "      <td>[INST]\\nThis paper investigates the hessian of...</td>\n",
       "      <td>Overall, the results feel preliminary but like...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1E7Pwqgl</th>\n",
       "      <td>The authors proposes an interesting idea of co...</td>\n",
       "      <td>While the paper may have an interesting theore...</td>\n",
       "      <td>[INST]\\nThe authors proposes an interesting id...</td>\n",
       "      <td>On the third in-painting tasks, baselines are ...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1ElR4cgg</th>\n",
       "      <td>This is a parallel work with BiGAN.  The idea ...</td>\n",
       "      <td>The reviewers were positive about this paper a...</td>\n",
       "      <td>[INST]\\nThis is a parallel work with BiGAN.  T...</td>\n",
       "      <td>ALI's objective is to match the joint distribu...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1G9tvcgx</th>\n",
       "      <td>This paper proposes a multimodal neural machin...</td>\n",
       "      <td>The area chair agrees with the reviewers that ...</td>\n",
       "      <td>[INST]\\nThis paper proposes a multimodal neura...</td>\n",
       "      <td>This paper proposes a multimodal neural machin...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1GOWV5eg</th>\n",
       "      <td>This paper shows that extending deep RL algori...</td>\n",
       "      <td>The basic idea of this paper is simple: run RL...</td>\n",
       "      <td>[INST]\\nThis paper shows that extending deep R...</td>\n",
       "      <td>This has become quite standard and would make ...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        text  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The paper is an extension of the matching netw...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  The paper looks at the problem of locating the...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  I just noticed I submitted my review as a pre-...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  This paper proposed a novel adversarial framew...   \n",
       "https://openreview.net/forum?id=B184E5qee  The authors present a simple method to affix a...   \n",
       "https://openreview.net/forum?id=B186cP9gx  This paper investigates the hessian of small d...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  The authors proposes an interesting idea of co...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  This is a parallel work with BiGAN.  The idea ...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  This paper proposes a multimodal neural machin...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  This paper shows that extending deep RL algori...   \n",
       "\n",
       "                                                                                        gold  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The program committee appreciates the authors'...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  This paper provides two approaches to question...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  Hello Authors,  Congratulations on the accepta...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  pros:  - new problem  - huge number of experim...   \n",
       "https://openreview.net/forum?id=B184E5qee  Reviewers agree that this paper is based on a ...   \n",
       "https://openreview.net/forum?id=B186cP9gx  This is quite an important topic to understand...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  While the paper may have an interesting theore...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  The reviewers were positive about this paper a...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  The area chair agrees with the reviewers that ...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  The basic idea of this paper is simple: run RL...   \n",
       "\n",
       "                                                                                 instruction  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  [INST]\\nThe paper is an extension of the match...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  [INST]\\nThe paper looks at the problem of loca...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  [INST]\\nI just noticed I submitted my review a...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  [INST]\\nThis paper proposed a novel adversaria...   \n",
       "https://openreview.net/forum?id=B184E5qee  [INST]\\nThe authors present a simple method to...   \n",
       "https://openreview.net/forum?id=B186cP9gx  [INST]\\nThis paper investigates the hessian of...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  [INST]\\nThe authors proposes an interesting id...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  [INST]\\nThis is a parallel work with BiGAN.  T...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  [INST]\\nThis paper proposes a multimodal neura...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  [INST]\\nThis paper shows that extending deep R...   \n",
       "\n",
       "                                                                                     summary  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  On Cub 200, I thought that the state-of-the-ar...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  The authors might want to consider pointing to...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  It is well written and a good read, and one I ...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  I will list these concerns in the following (i...   \n",
       "https://openreview.net/forum?id=B184E5qee  They demonstrate good improvements on language...   \n",
       "https://openreview.net/forum?id=B186cP9gx  Overall, the results feel preliminary but like...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  On the third in-painting tasks, baselines are ...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  ALI's objective is to match the joint distribu...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  This paper proposes a multimodal neural machin...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  This has become quite standard and would make ...   \n",
       "\n",
       "                                          metadata/method  \\\n",
       "id                                                          \n",
       "https://openreview.net/forum?id=B1-Hhnslg             LSA   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl             LSA   \n",
       "https://openreview.net/forum?id=B16Jem9xe             LSA   \n",
       "https://openreview.net/forum?id=B16dGcqlx             LSA   \n",
       "https://openreview.net/forum?id=B184E5qee             LSA   \n",
       "https://openreview.net/forum?id=B186cP9gx             LSA   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl             LSA   \n",
       "https://openreview.net/forum?id=B1ElR4cgg             LSA   \n",
       "https://openreview.net/forum?id=B1G9tvcgx             LSA   \n",
       "https://openreview.net/forum?id=B1GOWV5eg             LSA   \n",
       "\n",
       "                                           metadata/sentence_count  \n",
       "id                                                                  \n",
       "https://openreview.net/forum?id=B1-Hhnslg                        1  \n",
       "https://openreview.net/forum?id=B1-q5Pqxl                        1  \n",
       "https://openreview.net/forum?id=B16Jem9xe                        1  \n",
       "https://openreview.net/forum?id=B16dGcqlx                        1  \n",
       "https://openreview.net/forum?id=B184E5qee                        1  \n",
       "https://openreview.net/forum?id=B186cP9gx                        1  \n",
       "https://openreview.net/forum?id=B1E7Pwqgl                        1  \n",
       "https://openreview.net/forum?id=B1ElR4cgg                        1  \n",
       "https://openreview.net/forum?id=B1G9tvcgx                        1  \n",
       "https://openreview.net/forum?id=B1GOWV5eg                        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this script, we can produce summaries using 'LSA', 'Text Rank', 'LexRank', 'Edmundson', 'Luhn', 'KL-Sum', 'Random'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "We move now to the second directory `glimpse/data_loading` where we have 3 scripts (one of them can be skipped)\n",
    "\n",
    "1. `generate_abstractive_candidates.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_CONFIGS = {\n",
    "    \"top_p_sampling\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 1.0,\n",
    "        \"num_return_sequences\": 8,\n",
    "        \"num_beams\" : 1,\n",
    "\n",
    "        #\"num_beam_groups\" : 4,\n",
    "    },\n",
    "\n",
    "    **{\n",
    "        f\"sampling_topp_{str(topp).replace('.', '')}\": {\n",
    "            \"max_new_tokens\": 200,\n",
    "            \"do_sample\": True,\n",
    "            \"num_return_sequences\": 8,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "        for topp in [0.5, 0.8, 0.95, 0.99]\n",
    "    },\n",
    "}\n",
    "\n",
    "for key, value in GENERATION_CONFIGS.items():\n",
    "    GENERATION_CONFIGS[key] = {\n",
    "        # \"max_length\": 2048,\n",
    "        \"min_length\": 0,\n",
    "        \"early_stopping\": True,\n",
    "        **value,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_length': 0,\n",
       " 'early_stopping': True,\n",
       " 'max_new_tokens': 200,\n",
       " 'do_sample': True,\n",
       " 'num_return_sequences': 8,\n",
       " 'top_p': 0.95}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERATION_CONFIGS['sampling_topp_05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimpse.data_loading import generate_abstractive_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "dataset = generate_abstractive_candidates.prepare_dataset('data/processed/all_reviews_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]d:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 1/5 [02:27<09:51, 147.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 2/5 [04:54<07:21, 147.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 3/5 [07:21<04:54, 147.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 4/5 [18:13<05:46, 346.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 5/5 [23:49<00:00, 285.93s/it]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 37.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = generate_abstractive_candidates.evaluate_summarizer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    GENERATION_CONFIGS['sampling_topp_05'],\n",
    "    2,\n",
    "    'cuda',\n",
    "    True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'gold', 'summary'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'https://openreview.net/forum?id=r1rhWnZkg',\n",
       " 'text': 'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.',\n",
       " 'gold': \"The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.\",\n",
       " 'summary': ['Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The authors have built a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. However, the paper does not provide sufficient experimental justification for the performance of the proposed model. The paper presents low-rank bilinear pooling with various ablation studies but the performance of the model is statistically insignificant compared to the current state-of-art. The authors argue that low-rank bilinear pooling reduces the number of parameters, but this does not necessarily mean that the model is better. The paper also compares low-rank bilinear pooling with compact bilinear pooling and finds that low-rank bilinear pooling performs worse. Finally, the authors claim that low-rank bilinear pooling outperforms the current state-of-art on VQA by 0.42%. However, the authors do not provide any experimental justification for',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents a new model for Visual Question Answering (VQA) that uses low-rank bilinear pooling. The model outperforms the current state-of-art by 0.42% and is experimentally verified. The authors also present ablation studies of the new model. However, the paper could be improved by addressing the following concerns: the performance of low-rank bilinear pooling on VQA is not statistically significant, the model requires more parameters than compact bilinear pooling, and the performance is not statistically significant when compared to the current state-of-art.',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents low-rank bilinear pooling, a form of bilinear pooling, that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model. However, I have concerns about the statistical significance of the performance (see weaknesses below). The paper also presents a new model for the task of VQA that beats the current state-of-art by 0.42%, but I have concerns about the experimental justification of this performance. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. However, the reduction in number of parameters does not seem to be experimentally justified. The training time of the model and',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents low-rank bilinear pooling, which uses the Hadamard product (element-wise multiplication) to reduce the number of parameters in the model. The authors compare low-rank bilinear pooling with compact bilinear pooling, and found that low-rank bilinear pooling performs worse. They also found that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. The paper implements a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The authors also present various ablation studies of the new VQA model. The paper has several design choices that have been experimentally verified. However, the authors argue that the performance of the proposed model is statistically significantly better than the current state-of-art. This raises concerns about the statistical significance of the performance. The paper concludes that low-rank bilinear pooling is better than compact bilinear pooling, but the authors do not provide any experimental',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents a new model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The model uses low-rank bilinear pooling, which has been shown to be a common operation in VQA literature. The authors experimentally verified the design choices made in the model development and found that low-rank bilinear pooling performs worse than compact bilinear pooling. They also found that low-rank bilinear pooling leads to a reduction in number of parameters. However, the authors did not provide any experimental justification for why low-rank bilinear pooling is better than other forms of pooling. The performance of the proposed model is statistically significantly better than the current state-of-art.',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary of the paper: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The authors implement low-rank bilinear pooling on an existing model (Kim et al., 2016b) and build a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. The authors compare low-rank bilinear pooling with compact bilinear pooling and find that low-rank bilinear pooling performs worse. They also show that the reduction in number of parameters is not experimentally justified. The authors propose a model for VQA that outperforms the current state-of-art by 0.42% and argue that it beats the current state-of-art by 0.42%. However, they explicitly mention that the performance of the proposed model is statistically significantly better than the current state-of-art.',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents low-rank bilinear pooling for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The authors implement low-rank bilinear pooling on an existing model and build a VQA model that achieves this improvement. The paper also presents various ablation studies of the new VQA model. The design choices made in model development have been experimentally verified. The proposed model has been tested on the VQA dataset, and it outperforms the current state-of-art by 0.42%. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling, which could help experimentally. However, the authors claim that the proposed model outperforms the current state-of-art on VQA by 0.42%, which is statistically significant. However, I am skeptical about the statistical significance of the performance. The performance of the proposed model is not clearly justified by the differences',\n",
       "  \"Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary of the paper's conclusions: The paper presents low-rank bilinear pooling for Visual Question Answering (VQA) and finds that it outperforms the current state-of-art by 0.42%. The paper also presents a new model for VQA that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built, showing that the new model is robust to different design choices. The paper concludes that the proposed model is robust to different design choices and outperforms the current state-of-art. I would like to see more experimental justification of the conclusions drawn by the authors. The paper has some strengths, such as presenting new insights into element-wise multiplication and building a new model for VQA. However, it also has some weaknesses, such as the lack of statistical justification of the performance and the potential lack of robustness of the proposed model. The paper could benefit from more experimental results and a more\"]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = dataset.to_pandas()\n",
    "df_dataset = df_dataset.explode('summary')\n",
    "df_dataset = df_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['id_candidate'] = df_dataset.groupby(['index']).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>summary</th>\n",
       "      <th>id_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         id  \\\n",
       "0      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "1      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "2      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "3      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "4      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "\n",
       "                                                text  \\\n",
       "0  Summary: The paper presents low-rank bilinear ...   \n",
       "1  Summary: The paper presents low-rank bilinear ...   \n",
       "2  Summary: The paper presents low-rank bilinear ...   \n",
       "3  Summary: The paper presents low-rank bilinear ...   \n",
       "4  Summary: The paper presents low-rank bilinear ...   \n",
       "\n",
       "                                                gold  \\\n",
       "0  The program committee appreciates the authors'...   \n",
       "1  The program committee appreciates the authors'...   \n",
       "2  The program committee appreciates the authors'...   \n",
       "3  The program committee appreciates the authors'...   \n",
       "4  The program committee appreciates the authors'...   \n",
       "\n",
       "                                             summary  id_candidate  \n",
       "0  Summary: The paper presents low-rank bilinear ...             0  \n",
       "1  Summary: The paper presents low-rank bilinear ...             1  \n",
       "2  Summary: The paper presents low-rank bilinear ...             2  \n",
       "3  Summary: The paper presents low-rank bilinear ...             3  \n",
       "4  Summary: The paper presents low-rank bilinear ...             4  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `generate_extractive_candidates.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as abstractive_candidates while summaries are nothing but the set of sentences.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to discover `glimpse/evaluate` where a set of evaluators is introduced. Here nothing special, dataframe is filtered to get the gold vs summaries, and then some evaluator is called to be applied.\n",
    "\n",
    "1. `evaluate_bartbert_metrics.py`: Computing Bert Score\n",
    "2. `evaluate_commo_metrics_samples.py`: Evaluating Rouge; Rouge1, Rouge2, RougeL and RougeLsum\n",
    "3. ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
