{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/all_reviews_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Results on the VQA task are good for this simp...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This work proposes to approximate the bilinear...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summary:--------This paper proposes to use sur...</td>\n",
       "      <td>Based on the feedback, I'm going to be rejecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper proposes to use previous error sign...</td>\n",
       "      <td>Based on the feedback, I'm going to be rejecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Summary: The paper presents low-rank bilinear ...   \n",
       "1  Results on the VQA task are good for this simp...   \n",
       "2  This work proposes to approximate the bilinear...   \n",
       "3  Summary:--------This paper proposes to use sur...   \n",
       "4  This paper proposes to use previous error sign...   \n",
       "\n",
       "                                                gold  \n",
       "0  The program committee appreciates the authors'...  \n",
       "1  The program committee appreciates the authors'...  \n",
       "2  The program committee appreciates the authors'...  \n",
       "3  Based on the feedback, I'm going to be rejecti...  \n",
       "4  Based on the feedback, I'm going to be rejecti...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'gold']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for each year consist of `['id','text','gold']`\n",
    "- Text: Is the source \n",
    "- Gold: We assume that the area chair's motivations for their decision provide a reasonable comparison (summary)\n",
    "\n",
    "*Note*: For each paper, 3 reviews are extracted, you can notice that the `gold` value is same for all the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by following directories in order and apply some functions\n",
    "\n",
    "We start from the directory `glimpse/baselines` where comparative results are treated\n",
    "\n",
    "1. `generate_llm_summaries.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glimpse.baselines import generate_llm_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model replace 'togethercomputer/Llama-2-7B-32K-Instruct'\n",
    "model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "token = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model, trust_remote_code=True, torch_dtype=torch.float16, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_llm_summaries.prepare_dataset('reviews_2017', dataset_path='data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-Hhnslg</th>\n",
       "      <td>The paper is an extension of the matching netw...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-q5Pqxl</th>\n",
       "      <td>The paper looks at the problem of locating the...</td>\n",
       "      <td>This paper provides two approaches to question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B16Jem9xe</th>\n",
       "      <td>I just noticed I submitted my review as a pre-...</td>\n",
       "      <td>Hello Authors,  Congratulations on the accepta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        text  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The paper is an extension of the matching netw...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  The paper looks at the problem of locating the...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  I just noticed I submitted my review as a pre-...   \n",
       "\n",
       "                                                                                        gold  \n",
       "id                                                                                            \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The program committee appreciates the authors'...  \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  This paper provides two approaches to question...  \n",
       "https://openreview.net/forum?id=B16Jem9xe  Hello Authors,  Congratulations on the accepta...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_llm_summaries.group_text_by_id(df)\n",
    "\n",
    "# Group text by sample id and concatenate text\n",
    "df.head(3)\n",
    "\n",
    "# Grouped by id, text is concatenated of all reviews, and gold is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take first 10 samples for testing\n",
    "df = df.head(10)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThe paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am not fully convinced its advantages.  This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier. Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1). This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely.----------------Although the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode). In its current form the paper a bit vague about this. *** Paper Summary ***----------------This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.----------------*** Review ***----------------The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. ----------------The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class ----------------I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?----------------Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.----------------*** References ***----------------Large Margin Nearest Neighbors. Weinberger et al, 2005--------From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010--------A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:21<03:17, 21.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThe paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.----------------Strength:--------- The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)--------- The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.------------------------Weaknesses:--------1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.--------2. Experimental evaluation--------2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.--------2.2. It would be interested if this approach generalizes to other datasets.------------------------Other (minor/discussion points)--------- The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.--------- I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process.--------- Page 6, last paragraph: missing “.”: “… searching This…”--------------------------------Summary:--------While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task. Summary:--------The paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.----------------Strengths:--------1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.--------2. Significant performance boost over the baseline presented in the SQuAD paper.--------3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.----------------Weaknesses/Questions/Suggestions:--------1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.--------2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.--------3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.--------4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? --------5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.--------6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?----------------Review Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult. SUMMARY.--------This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.--------The proposed model combines two well-know neural network architectures match-lstm and pointer nets.--------First the passage and the questions are encoded with a unidirectional LSTM.--------Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.--------For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.--------The same process is done in the opposite direction with a backward lstm.--------The final representation is a concatenation of the two lstms.--------As a decoded a pointer network is used.--------The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.----------------The proposed model is tested on the Stanford Question Answering Dataset.--------An ensemble of the proposed model achieves performance close to state-of-the-art models.--------------------------------------------------OVERALL JUDGMENT----------------I think the model is interesting mainly because of the use of pointer networks as a decoder.--------One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.--------The analysis of the model is interesting and insightful.--------The sharing of the code is good.\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:35<02:15, 16.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nI just noticed I submitted my review as a pre-review question - sorry about this. Here it is again, with a few more thoughts added...----------------The authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.----------------My main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper \"But it has left us unsatisfied since we have not gained the insight needed to choose between them.” summarises my feeling about this paper: this is a nice \\'unifying review’ type paper that - for me - lacks a novel insight.----------------In summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I’m not highly confident about this as a conference submission (and hence my low score) I am open to be convinced either way.----------------Detailed comments:----------------I think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.----------------I don’t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.----------------There is a typo in spelling Csiszar divergence----------------Equation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009). A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense. I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.----------------I have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.----------------On likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders). An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa. To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach. Could the authors comment on this?----------------I think the hypothesis testing angle is oversold in the paper.  I’m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used. The paper provides an exposition of multiple ways of learning in implicit generative models, of which generative adversarial networks are an example. The paper is very clear, the exposition is insightful, and the presented material is clearly important.----------------It is hard to assess \"novelty\" of this work, as the individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.----------------I believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning. This has the potential to accelerate progress.----------------I recommend publishing this paper at ICLR, even though it is not the \"typical\" paper that get published at this conference (in that it doesn\\'t offer empirical validation, nor makes a particular claim about relative merits of different methods). Thank you for an interesting read.----------------Given the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don\\'t use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.----------------The only reason that I decided to hold back my strong acceptance recommendation is that I don\\'t understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I\\'m not exactly sure what kinds of contributions the ICLR committee is looking for.\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:41<01:22, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThis paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.----------------While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.----------------Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing. The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.----------------I believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable.--------There are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order)--------- The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: \"Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills\"; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.--------  There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: \"we find that this simple approach has been able to solve the problems\"--------- The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why?--------- While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?--------- Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.--------- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step?--------- The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?----------------UPDATE:--------I updated the score. Please see my response to the rebuttal below. The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).----------------The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.----------------I would have expected to see comparison to the following methods added to Figure 3:--------1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.--------2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.--------3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).----------------Including these results would in my view significantly enhance the impact of the paper.\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:56<01:19, 13.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThe authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines.----------------The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.----------------The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.----------------My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication. This paper proposes a simple extension to a neural network language model by adding a cache component. --------The model stores <previous hidden state, word> pairs in memory cells and uses the current hidden state to control the lookup. --------The final probability of a word is a linear interpolation between a standard language model and the cache language model. --------Additionally, an alternative that uses global normalization instead of linear interpolation is also presented. --------Experiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model.----------------There is a lot of similar work on memory-augmented/pointer neural language models, and the main difference is that the proposed method is simple and scales to a large cache size.--------However, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive. --------While it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks). --------A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)----------------Some questions:--------- In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model? Can you show results for both? --------- Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)? Please also show accuracy on this dataset. --------- It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements. Are the results about the same or worse? This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.----------------They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.--------I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.----------------I recommend this interesting and well analyzed paper be accepted.\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:06<00:59, 11.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[INST]\\nThis paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.----------------The overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. “Almost All Learning Machines are Singular”, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further—a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. “Identifying and attacking the saddle point problem in high dimensional non-convex optimization” NIPS 2014 or the work of Amari and others.----------------Experimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out.----------------This paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice. The paper analyzes the properties of the Hessian of the training objective for various neural networks and data distributions. The authors study in particular, the eigenspectrum of the Hessian, which relates to the difficulty and the local convexity of the optimization problem.----------------While there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and Hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.----------------For example, regarding the concentration of eigenvalues to zero in Figure 6, it is unclear whether the concentration effect is really caused by training (e.g. increasing insensitivity to local perturbations), or the consequence of a specific choice of scale for the initial parameters.----------------In Figure 8, the complexity of the data is not defined. It is not clear whether two fully overlapping distributions (the Hessian would then become zero?) is considered as complex or simple data.----------------Some of the plots legends (Fig. 1 and 2) and labels are unreadable in printed format. Plots of Figure 3 don't have the same range for the x-axis. The image of Hessian matrix of Figure 1 does not render properly in printed format. The work presents some empirical observations to support the statement that “the Hessian of the loss functions in deep learning is degenerate”. But what does this statement refer to? To my understanding, there are at least three interpretations:----------------(i) The Hessian of the loss functions in deep learning is degenerate at any point in the parameter space, i.e., any network weight matrices.----------------(ii) The Hessian of the loss functions in deep learning is degenerate at any critical point.----------------(iii) The Hessian of the loss functions in deep learning is degenerate at any local minimum, or any global minimum.----------------None of these interpretations is solidly supported by the observations provided in the paper.----------------More comments are as follows:----------------1) The authors state that “we don’t have much information on what the actual Hessian looks like.” Then I just wonder what Hessian is investigated. Is it the actual one or approximate one? Please clarify and provide the references for computing the actual Hessian.----------------2) It is not clear whether the optimization was done by a batch gradient descent algorithm, i.e., batch back propagation (BP) algorithm, or a stochastic BP algorithm. If the training was done via a stochastic BP algorithm, it is hard to conclude that the the Neural Network has been trained to its local minimum. When it was done by a full-batch BP algorithm, what was the accumulating point? Was it local minimum or global minimum?----------------3) Since the negative log likelihood function was used as at the end of training, it is essentially a joint learning approach in both the Newton weight matrices and the negative log likelihood vector. Certainly, the whole loss function is not convex in these two parameters. But if least squares error function is used at the end, would it make any difference in claiming the degeneracy of the Hessian?----------------4) Finally, the statement “There are still negative eigenvalues even when they are small in magnitude” is very puzzling. Potential reasons are:--------(a) If the training algorithm did converge, the accumulating points were not local minima, i.e., they were saddle points.--------(b) Training algorithms did not converge, or have not converged yet.--------(c) The calculation of the actual Hessian might be inaccurate. Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.----------------Quality:--------It seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:--------1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.--------2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.--------3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.--------4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.----------------Clarity:--------1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.--------2- The texts in the figures are very small and hard to read.\\n Summarize the previous text:[/INST]\\n\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:36<01:13, 18.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[INST]\\nThe authors proposes an interesting idea of connecting the energy-based model (descriptor) and --------the generator network to help each other. The samples from the generator are used as the initialization --------of the descriptor inference. And the revised samples from the descriptor is in turn used to update--------the generator as the target image. ----------------The proposed idea is interesting. However, I think the main flaw is that the advantages of having that --------architecture are not convincingly demonstrated in the experiments. For example, readers will expect --------quantative analysis on how initializing with the samples from the generator helps? Also, the only --------quantative experiment on the reconstruction is also compared to quite old models. Considering that --------the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison --------to that model. ----------------** Minor--------- I'm wondering if the analysis on the convergence is sound when considering the fact that samples --------from SGLD are biased samples (with fixed step size). --------- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G? This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the “descriptor”) with the help of an auxiliary directed bayes net, e.g. “the generator”. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to “cooperative training”.----------------The above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.----------------In a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\\\\tilde{Y}, \\\\hat{X}) instead of ({\\\\tilde{Y}, \\\\tilde{X}) ? Run comparative experiments.----------------The paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (“pioneering work” in reference to closely related, but non-peer reviewed work) and prose (“tale of two nets”). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.----------------PROS:--------+ Interesting and novel idea--------CONS:--------- Improper experimental protocols--------- Missing baselines--------- Missing diagnostic experiments----------------[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts. This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.----------------This is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:--------- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.--------- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.----------------Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.----------------Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\\n Summarize the previous text:[/INST]\\n\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:48<00:48, 16.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[INST]\\nThis is a parallel work with BiGAN.  The idea is using auto encoder to provide extra information for discriminator. This approach seems is promising from reported result. After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.---------------------------------------Initial Review:----------------This paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.----------------There are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.----------------Since ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.----------------The quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.----------------There are two sets of semi-supervised results: --------The first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.----------------The second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above. This paper extends the GAN framework to allow for latent variables. The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x). The joint distribution on x,z is then modeled using a joint generator model p(x,z)=p(z)p(x|z).  Both q and p are then trained by trying to fool a discriminator. This constitutes a worthwhile extension of GANs: giving GANs the ability to do inference opens up many applications that could previously only be addressed by e.g. VAEs.----------------The results are very promising. The CIFAR-10 samples are the best I've seen so far (not counting methods that use class labels). Matching the semi-supervised results from Salimans et al. without feature matching also indicates the proposed method may improve the stability of training GANs.\\n Summarize the previous text:[/INST]\\n\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:52<00:24, 12.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThis paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. ----------------As pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author\\'s also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it. The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences. ----------------The idea seems to be to use a latent variable model and condition it on the image. In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference. That said, the approach appears flawed, because the image is not really used for translation.----------------Experimental results are weak. If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline. In the view of the overall variance of the results, these improvements can not be considered significant. ----------------The qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.----------------Overall, there are major issues with both the approach and the execution of the paper. I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn\\'t demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn\\'t convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. ----------------The dataset is too small to experiment with NMT. I\\'m not sure if it\\'s fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.----------------Besides, I have problems with the presentation of this paper.--------(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. --------(b) The \\' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h\\'_\\\\pi obtained from Eq. (3) which is about h_\\\\pi (yes, I understand what the authors mean, but there can be better ways to present that).--------(c) I\\'m not sure if it\\'s correct in Section 3.2.2 h\\'_z is computed from \\\\mu and \\\\sigma. So how \\\\mu\\' and \\\\sigma\\' are being used ?--------(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there\\'s an ablation test there. Similarly for other symbols.----------------Other things: no explanations for Figure 2 & 3. There\\'s a missing \\\\pi symbol in Appendix A before the KL derivation.\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:56<00:09,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[INST]\\nThis paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.----------------A few comments/questions:--------- Table 1 could be easier to interpret as a figure of histograms.--------- Figure 3 could be easier to interpret as a table.--------- How was the subset of Atari games selected?--------- The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores.--------- It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.--------- FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.----------------Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using.----------------------------------------------------I\\'m increasing my score to 8 based on the rebuttal and the revised paper. This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.----------------Comments:----------------- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it\\'s from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?----------------- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.----------------- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)----------------- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn\\'t the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?----------------- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn\\'t FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? ----------------- Is the \\'name_this_game\\' name in the tables  intentional?----------------- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR). This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).----------------Here are some comments and questions, for improving the paper:----------------The introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments — repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.----------------In the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)----------------Experiments:--------Can you provide error bars on the experimental results? (from running multiple random seeds)----------------It would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.----------------The TRPO evaluation is different from the results reported in Duan et al. ICML ’16. Why not use the same benchmark?----------------Videos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?----------------How many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).----------------Can the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.----------------Minor comments:---------- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.---------- “idea of deciding when necessary” - seems like it would be better to say “idea of only deciding when necessary\"---------- \"spaces.Durugkar et al.” — missing a space.---------- “R={4}” — why 4? Could you use a letter to indicate a constant instead? (or a different notation)\\n Summarize the previous text:[/INST]\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:06<00:00, 12.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "df = generate_llm_summaries.generate_summaries(model, tokenizer, df, batch_size=2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[INST] The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of',\n",
       " 'using all the examples in the support set during test, the method represents each class by the mean',\n",
       " 'of its learned embeddings. The training procedure and experimental setting are very similar to the',\n",
       " 'original matching networks. I am not completely sure about its advantages over the original matching',\n",
       " 'networks. It seems to me when dealing with 1-shot case, these two methods are identical since there',\n",
       " 'is only one example seen in this class, so the mean of the embedding is the embedding itself. When',\n",
       " 'dealing with 5-shot case, original matching networks compute the weighted average of all examples,',\n",
       " 'but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly',\n",
       " 'better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am',\n",
       " 'not fully convinced its advantages.  This paper proposes an improved version of matching networks,',\n",
       " 'with better scalability properties with respect to the support set of a few-shot classifier. Instead',\n",
       " 'of considering each support point individually, they learn an embedding function that aggregates',\n",
       " 'over items of each class within the support set (eq. 1). This is combined with episodic few-shot',\n",
       " 'training with randomly-sampled partitions of the training set classes, so that the training and',\n",
       " 'testing scenarios match closely.----------------Although the idea is quite straightforward, and',\n",
       " 'there are a great many prior works on zero-shot and few-shot learning, the proposed technique is',\n",
       " 'novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One',\n",
       " 'addition that I think would improve the paper is a clearer description of the training algorithm',\n",
       " '(perhaps pseudocode). In its current form the paper a bit vague about this. *** Paper Summary',\n",
       " '***----------------This paper simplify matching network by considering only a single prototype per',\n",
       " 'class which is obtained as the average of the embedding of the training class samples. Empirical',\n",
       " 'comparisons with matching networks are reported.----------------*** Review ***----------------The',\n",
       " 'paper reads well and clearly motivate the work. This work of learning metric learning propose to',\n",
       " 'simplify an earlier work (matching network) which is a great objective. However, I am not sure it',\n",
       " 'achieve better results than matching networks. The space of learning embeddings to optimize nearest',\n",
       " 'neighbor classification has been explored before, but the idea of averaging the propotypes is',\n",
       " 'interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the',\n",
       " 'discussion of related work and to consolidate the results section to help distinguish between the',\n",
       " 'methods you outperform and the one you do not. ----------------The related work section can be',\n",
       " 'extended to include work on learning distance metric to optimize a nearest neighbor classification,',\n",
       " 'see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural',\n",
       " 'networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches',\n",
       " 'pursuing similar goals with a different learning objective, you cite siamese network with pairwise',\n",
       " 'supervision. The learning to rank (for websearch) litterature with triplet supervision or global',\n",
       " 'ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the',\n",
       " 'embedding space need to be such that positive/relevant document are closer to the query than the',\n",
       " 'others. I would suggest to start with Chris Burges 2010 tutorial. One learning class',\n",
       " '----------------I am not sure the reported results correctly reflect the state of the art for all',\n",
       " 'tasks. The results are positive on Omniglot but I feel that you should also report the better',\n",
       " 'results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can',\n",
       " 'be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was',\n",
       " '50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?',\n",
       " '----------------Overall, paper could greatly be improved, both in the discussion of related work and',\n",
       " 'with a less partial reporting of prior empirical results.----------------*** References',\n",
       " '***----------------Large Margin Nearest Neighbors. Weinberger et al, 2005--------From RankNet to',\n",
       " 'LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010--------A Deep Non-linear',\n",
       " 'Feature Mapping for Large-Margin kNN Classification, Min et al, 09  Summarize the previous',\n",
       " 'text:[/INST]  The paper proposes a new method for few-shot learning, which involves representing',\n",
       " 'each class by the mean of its learned embeddings. This method is compared to the original matching',\n",
       " 'networks, which use all examples in the support set during training. The paper claims that the new',\n",
       " 'method is simpler, more scalable, and achieves better results on several benchmark datasets.  The',\n",
       " \"paper's main contribution is the idea of averaging the prototypes of each class to represent the\",\n",
       " 'class in the embedding space. This is done by learning an embedding function that aggregates over',\n",
       " 'items of each class within the support set. The paper also reports empirical comparisons with',\n",
       " \"matching networks.  The paper's strengths are that it is novel and achieves state-of-the-art results\",\n",
       " 'on several benchmark datasets. However, the paper could be improved by providing a clearer',\n",
       " 'description of the training algorithm and consolidating the results section to distinguish between',\n",
       " \"the methods being compared.  The paper's weaknesses are that it does not report the full range of\",\n",
       " 'results for all tasks, and it could be more detailed in its discussion of related work.',\n",
       " 'Additionally, the paper could benefit from a more thorough evaluation of the state-of-the-art',\n",
       " 'results.  Overall, the paper proposes a new and interesting approach to few-shot learning, and it is',\n",
       " 'worth considering for further development.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "textwrap.wrap(df['summary'].iloc[0], width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'gold', 'instruction', 'summary'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- For each paper (document) we have 3 reviews, these reviews are concatenated\n",
    "- The review (contactenated 3 reviews) are fed to a model to provide a summary for them\n",
    "- In order to do that we use `generate_summaries` function that adds first a column to the `df` where instruction is applied along with `text`.\n",
    "- Then the model provides a summary for each text and df is returned back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `sumy_baselines.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimpse.baselines import sumy_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in [1]:\n",
    "        summaries = []\n",
    "        for text in df.text:\n",
    "            summary = sumy_baselines.summarize('LSA', \"english\", N, \"text\", text)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        df['summary'] = summaries\n",
    "        df[\"metadata/method\"] = 'LSA'\n",
    "        df[\"metadata/sentence_count\"] = N\n",
    "\n",
    "        name = f\"{df}-_-'LSA'-_-sumy_{N}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>instruction</th>\n",
       "      <th>summary</th>\n",
       "      <th>metadata/method</th>\n",
       "      <th>metadata/sentence_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-Hhnslg</th>\n",
       "      <td>The paper is an extension of the matching netw...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>[INST]\\nThe paper is an extension of the match...</td>\n",
       "      <td>On Cub 200, I thought that the state-of-the-ar...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1-q5Pqxl</th>\n",
       "      <td>The paper looks at the problem of locating the...</td>\n",
       "      <td>This paper provides two approaches to question...</td>\n",
       "      <td>[INST]\\nThe paper looks at the problem of loca...</td>\n",
       "      <td>The authors might want to consider pointing to...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B16Jem9xe</th>\n",
       "      <td>I just noticed I submitted my review as a pre-...</td>\n",
       "      <td>Hello Authors,  Congratulations on the accepta...</td>\n",
       "      <td>[INST]\\nI just noticed I submitted my review a...</td>\n",
       "      <td>It is well written and a good read, and one I ...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B16dGcqlx</th>\n",
       "      <td>This paper proposed a novel adversarial framew...</td>\n",
       "      <td>pros:  - new problem  - huge number of experim...</td>\n",
       "      <td>[INST]\\nThis paper proposed a novel adversaria...</td>\n",
       "      <td>I will list these concerns in the following (i...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B184E5qee</th>\n",
       "      <td>The authors present a simple method to affix a...</td>\n",
       "      <td>Reviewers agree that this paper is based on a ...</td>\n",
       "      <td>[INST]\\nThe authors present a simple method to...</td>\n",
       "      <td>They demonstrate good improvements on language...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B186cP9gx</th>\n",
       "      <td>This paper investigates the hessian of small d...</td>\n",
       "      <td>This is quite an important topic to understand...</td>\n",
       "      <td>[INST]\\nThis paper investigates the hessian of...</td>\n",
       "      <td>Overall, the results feel preliminary but like...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1E7Pwqgl</th>\n",
       "      <td>The authors proposes an interesting idea of co...</td>\n",
       "      <td>While the paper may have an interesting theore...</td>\n",
       "      <td>[INST]\\nThe authors proposes an interesting id...</td>\n",
       "      <td>On the third in-painting tasks, baselines are ...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1ElR4cgg</th>\n",
       "      <td>This is a parallel work with BiGAN.  The idea ...</td>\n",
       "      <td>The reviewers were positive about this paper a...</td>\n",
       "      <td>[INST]\\nThis is a parallel work with BiGAN.  T...</td>\n",
       "      <td>ALI's objective is to match the joint distribu...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1G9tvcgx</th>\n",
       "      <td>This paper proposes a multimodal neural machin...</td>\n",
       "      <td>The area chair agrees with the reviewers that ...</td>\n",
       "      <td>[INST]\\nThis paper proposes a multimodal neura...</td>\n",
       "      <td>This paper proposes a multimodal neural machin...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://openreview.net/forum?id=B1GOWV5eg</th>\n",
       "      <td>This paper shows that extending deep RL algori...</td>\n",
       "      <td>The basic idea of this paper is simple: run RL...</td>\n",
       "      <td>[INST]\\nThis paper shows that extending deep R...</td>\n",
       "      <td>This has become quite standard and would make ...</td>\n",
       "      <td>LSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        text  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The paper is an extension of the matching netw...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  The paper looks at the problem of locating the...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  I just noticed I submitted my review as a pre-...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  This paper proposed a novel adversarial framew...   \n",
       "https://openreview.net/forum?id=B184E5qee  The authors present a simple method to affix a...   \n",
       "https://openreview.net/forum?id=B186cP9gx  This paper investigates the hessian of small d...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  The authors proposes an interesting idea of co...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  This is a parallel work with BiGAN.  The idea ...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  This paper proposes a multimodal neural machin...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  This paper shows that extending deep RL algori...   \n",
       "\n",
       "                                                                                        gold  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  The program committee appreciates the authors'...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  This paper provides two approaches to question...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  Hello Authors,  Congratulations on the accepta...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  pros:  - new problem  - huge number of experim...   \n",
       "https://openreview.net/forum?id=B184E5qee  Reviewers agree that this paper is based on a ...   \n",
       "https://openreview.net/forum?id=B186cP9gx  This is quite an important topic to understand...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  While the paper may have an interesting theore...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  The reviewers were positive about this paper a...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  The area chair agrees with the reviewers that ...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  The basic idea of this paper is simple: run RL...   \n",
       "\n",
       "                                                                                 instruction  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  [INST]\\nThe paper is an extension of the match...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  [INST]\\nThe paper looks at the problem of loca...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  [INST]\\nI just noticed I submitted my review a...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  [INST]\\nThis paper proposed a novel adversaria...   \n",
       "https://openreview.net/forum?id=B184E5qee  [INST]\\nThe authors present a simple method to...   \n",
       "https://openreview.net/forum?id=B186cP9gx  [INST]\\nThis paper investigates the hessian of...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  [INST]\\nThe authors proposes an interesting id...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  [INST]\\nThis is a parallel work with BiGAN.  T...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  [INST]\\nThis paper proposes a multimodal neura...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  [INST]\\nThis paper shows that extending deep R...   \n",
       "\n",
       "                                                                                     summary  \\\n",
       "id                                                                                             \n",
       "https://openreview.net/forum?id=B1-Hhnslg  On Cub 200, I thought that the state-of-the-ar...   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl  The authors might want to consider pointing to...   \n",
       "https://openreview.net/forum?id=B16Jem9xe  It is well written and a good read, and one I ...   \n",
       "https://openreview.net/forum?id=B16dGcqlx  I will list these concerns in the following (i...   \n",
       "https://openreview.net/forum?id=B184E5qee  They demonstrate good improvements on language...   \n",
       "https://openreview.net/forum?id=B186cP9gx  Overall, the results feel preliminary but like...   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl  On the third in-painting tasks, baselines are ...   \n",
       "https://openreview.net/forum?id=B1ElR4cgg  ALI's objective is to match the joint distribu...   \n",
       "https://openreview.net/forum?id=B1G9tvcgx  This paper proposes a multimodal neural machin...   \n",
       "https://openreview.net/forum?id=B1GOWV5eg  This has become quite standard and would make ...   \n",
       "\n",
       "                                          metadata/method  \\\n",
       "id                                                          \n",
       "https://openreview.net/forum?id=B1-Hhnslg             LSA   \n",
       "https://openreview.net/forum?id=B1-q5Pqxl             LSA   \n",
       "https://openreview.net/forum?id=B16Jem9xe             LSA   \n",
       "https://openreview.net/forum?id=B16dGcqlx             LSA   \n",
       "https://openreview.net/forum?id=B184E5qee             LSA   \n",
       "https://openreview.net/forum?id=B186cP9gx             LSA   \n",
       "https://openreview.net/forum?id=B1E7Pwqgl             LSA   \n",
       "https://openreview.net/forum?id=B1ElR4cgg             LSA   \n",
       "https://openreview.net/forum?id=B1G9tvcgx             LSA   \n",
       "https://openreview.net/forum?id=B1GOWV5eg             LSA   \n",
       "\n",
       "                                           metadata/sentence_count  \n",
       "id                                                                  \n",
       "https://openreview.net/forum?id=B1-Hhnslg                        1  \n",
       "https://openreview.net/forum?id=B1-q5Pqxl                        1  \n",
       "https://openreview.net/forum?id=B16Jem9xe                        1  \n",
       "https://openreview.net/forum?id=B16dGcqlx                        1  \n",
       "https://openreview.net/forum?id=B184E5qee                        1  \n",
       "https://openreview.net/forum?id=B186cP9gx                        1  \n",
       "https://openreview.net/forum?id=B1E7Pwqgl                        1  \n",
       "https://openreview.net/forum?id=B1ElR4cgg                        1  \n",
       "https://openreview.net/forum?id=B1G9tvcgx                        1  \n",
       "https://openreview.net/forum?id=B1GOWV5eg                        1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this script, we can produce summaries using 'LSA', 'Text Rank', 'LexRank', 'Edmundson', 'Luhn', 'KL-Sum', 'Random'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "We move now to the second directory `glimpse/data_loading` where we have 3 scripts (one of them can be skipped)\n",
    "\n",
    "1. `generate_abstractive_candidates.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_CONFIGS = {\n",
    "    \"top_p_sampling\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 1.0,\n",
    "        \"num_return_sequences\": 8,\n",
    "        \"num_beams\" : 1,\n",
    "\n",
    "        #\"num_beam_groups\" : 4,\n",
    "    },\n",
    "\n",
    "    **{\n",
    "        f\"sampling_topp_{str(topp).replace('.', '')}\": {\n",
    "            \"max_new_tokens\": 200,\n",
    "            \"do_sample\": True,\n",
    "            \"num_return_sequences\": 8,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "        for topp in [0.5, 0.8, 0.95, 0.99]\n",
    "    },\n",
    "}\n",
    "\n",
    "for key, value in GENERATION_CONFIGS.items():\n",
    "    GENERATION_CONFIGS[key] = {\n",
    "        # \"max_length\": 2048,\n",
    "        \"min_length\": 0,\n",
    "        \"early_stopping\": True,\n",
    "        **value,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_length': 0,\n",
       " 'early_stopping': True,\n",
       " 'max_new_tokens': 200,\n",
       " 'do_sample': True,\n",
       " 'num_return_sequences': 8,\n",
       " 'top_p': 0.95}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERATION_CONFIGS['sampling_topp_05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimpse.data_loading import generate_abstractive_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "dataset = generate_abstractive_candidates.prepare_dataset('data/processed/all_reviews_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\glimpse-mds\\env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 1/5 [00:21<01:25, 21.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 2/5 [00:42<01:03, 21.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 3/5 [01:02<00:41, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 4/5 [01:23<00:20, 20.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 5/5 [01:44<00:00, 20.95s/it]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 480.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = generate_abstractive_candidates.evaluate_summarizer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    GENERATION_CONFIGS['sampling_topp_05'],\n",
    "    2,\n",
    "    'cuda',\n",
    "    True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'gold', 'summary'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'https://openreview.net/forum?id=r1rhWnZkg',\n",
       " 'text': 'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.',\n",
       " 'gold': \"The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.\",\n",
       " 'summary': ['Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents a model for Visual Question Answering (VQA) that uses low-rank bilinear pooling with Hadamard product (commonly known as element-wise multiplication). The paper builds on an existing model and presents a new model for VQA that outperforms the current state-of-art by 0.42%. The authors present various ablation studies of the new VQA model. However, I have concerns about the statistical significance of the performance, particularly when comparing low-rank bilinear pooling with compact bilinear pooling. The paper also presents design choices made in model development, which have been experimentally verified. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling, which could potentially help experimentally. However, the performance of the proposed model is statistically significantly better than the current state-of-art. The paper could also provide more insights into the differences between MRN, MARN, and MLB. Overall, the paper',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary of the paper: The paper presents a new model for Visual Question Answering (VQA) that uses low-rank bilinear pooling instead of compact bilinear pooling. The authors claim that low-rank bilinear pooling leads to better performance on VQA. However, I have concerns about the statistical significance of the performance. The paper also presents ablation studies of the new model. Overall, the paper has experimental evidence that the new model outperforms the current state-of-art by 0.42%. However, I am skeptical about the significance of the improvement. The authors claim that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling, but I am not sure how this reduction in parameters helps. The paper also mentions that the proposed model outperforms the current state-of-art on VQA by 0.42%, but I am skeptical about the statistical significance of this improvement. I would like the authors to provide more experimental justification of why low-r',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary of the proposed solution: The paper presents a new model for VQA that uses low-rank bilinear pooling. The model has been built upon an existing model (Kim et al., 2016b) and has outperformed the current state-of-art by 0.42%. The authors have also presented various ablation studies of the new VQA model. The proposed solution has been experimentally verified through the design choices made in model development. However, I have concerns about the statistical significance of the performance. I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. I also would like the authors to explicitly mention the differences between MRN, MARN and MLB. Finally, I would like to see the results of the caption for Table 1 fixed as “have no” instead of “have not”. I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. summary: The paper presents low-rank bilinear pooling using Hadamard product (commonly known as element-wise multiplication) and compares it to compact bilinear pooling. The authors show that low-rank bilinear pooling outperforms the current state-of-art by 0.42% on VQA. However, I have several concerns about the statistical significance of the performance. The paper does not explicitly compare low-rank bilinear pooling to other forms of pooling (MRN, MARN, MLB) and I am skeptical about the performance. Additionally, the paper does not mention the differences between these forms of pooling. In the caption for Table 1, the term \"have not\" should be replaced with \"have no\". Finally, the paper could benefit from experimental justification of why low-rank bilinear pooling is better than compact bilinear pooling.',\n",
       "  \"Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary of the main contributions of the paper are:1. The authors present a new model for Visual Question Answering (VQA) that uses low-rank bilinear pooling on an existing model.2. The model has been experimentally verified to outperform the current state-of-art by 0.42% on VQA tasks.3. The authors have implemented a new VQA model that uses low-rank bilinear pooling on the existing model, and this new model outperforms the current state-of-art by 0.42%.4. The authors have conducted various ablation studies to investigate the effect of low-rank bilinear pooling on the model's performance.5. The authors have compared the performance of low-rank bilinear pooling with compact bilinear pooling.6. The authors have conducted experiments to investigate how the number of parameters in the model affects its performance.7. The authors have conducted experiments to investigate how the training time of the model affects its performance.8\",\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents a low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). It implements a VQA model that outperforms the current state-of-art by 0.42%. The paper presents ablation studies of the new VQA model. However, I have concerns about the statistical significance of the performance. The paper also presents a new model for VQA that beats the current state-of-art by 0.42%. However, I have concerns about the experimental justification of the performance. The authors could provide more experimental justification to justify that low-rank bilinear pooling is better than compact bilinear pooling. Moreover, the paper could include more details on the differences between MRN, MARN and MLB. Finally, the authors could provide more justification for why the reduction in number of parameters leads to better performance. Overall, the paper presents a new model for VQA that beats the current state-of-art by 0.42%, but',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary: The paper presents low-rank bilinear pooling using Hadamard product (commonly known as element-wise multiplication) on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature without insights on why it should work. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. The paper presents various design choices made in model development have been experimentally verified. Weaknesses: The paper could not be justified that low-rank bilinear pooling leads to better performance than compact bilinear pooling. The reduction in number of parameters does not help experimentally. The performance of the proposed model is',\n",
       "  'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. Summary of the paper is as follows: The paper presents a new model for Visual Question Answering (VQA) that uses low-rank bilinear pooling with Hadamard product. The model outperforms the current state-of-art by 0.42% on the VQA dataset. The authors implemented low-rank bilinear pooling on an existing model (Kim et al., 2016b) and compared it with compact bilinear pooling. They found that low-rank bilinear pooling performs worse. They then built a new model for VQA that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model. The authors also compare the performance of the new model with MRN, MARN and MLB. The paper concludes that low-rank bilinear pooling is better than other forms of pooling for VQA.']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = dataset.to_pandas()\n",
    "df_dataset = df_dataset.explode('summary')\n",
    "df_dataset = df_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['id_candidate'] = df_dataset.groupby(['index']).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>summary</th>\n",
       "      <th>id_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         id  \\\n",
       "0      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "1      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "2      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "3      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "4      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "\n",
       "                                                text  \\\n",
       "0  Summary: The paper presents low-rank bilinear ...   \n",
       "1  Summary: The paper presents low-rank bilinear ...   \n",
       "2  Summary: The paper presents low-rank bilinear ...   \n",
       "3  Summary: The paper presents low-rank bilinear ...   \n",
       "4  Summary: The paper presents low-rank bilinear ...   \n",
       "\n",
       "                                                gold  \\\n",
       "0  The program committee appreciates the authors'...   \n",
       "1  The program committee appreciates the authors'...   \n",
       "2  The program committee appreciates the authors'...   \n",
       "3  The program committee appreciates the authors'...   \n",
       "4  The program committee appreciates the authors'...   \n",
       "\n",
       "                                             summary  id_candidate  \n",
       "0  Summary: The paper presents low-rank bilinear ...             0  \n",
       "1  Summary: The paper presents low-rank bilinear ...             1  \n",
       "2  Summary: The paper presents low-rank bilinear ...             2  \n",
       "3  Summary: The paper presents low-rank bilinear ...             3  \n",
       "4  Summary: The paper presents low-rank bilinear ...             4  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `generate_extractive_candidates.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as abstractive_candidates while summaries are nothing but the set of sentences.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to discover `glimpse/evaluate` where a set of evaluators is introduced. Here nothing special, dataframe is filtered to get the gold vs summaries, and then some evaluator is called to be applied.\n",
    "\n",
    "1. `evaluate_bartbert_metrics.py`: Computing Bert Score\n",
    "2. `evaluate_common_metrics_samples.py`: Evaluating Rouge; Rouge1, Rouge2, RougeL and RougeLsum\n",
    "3. `evaluate_seahorse_metrics_samples.py`: Computing Seahorse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using',\n",
       " 'all the examples in the support set during test, the method represents each class by the mean of its',\n",
       " 'learned embeddings. The training procedure and experimental setting are very similar to the original',\n",
       " 'matching networks. I am not completely sure about its advantages over the original matching',\n",
       " 'networks. It seems to me when dealing with 1-shot case, these two methods are identical since there',\n",
       " 'is only one example seen in this class, so the mean of the embedding is the embedding itself. When',\n",
       " 'dealing with 5-shot case, original matching networks compute the weighted average of all examples,',\n",
       " 'but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly',\n",
       " 'better than matching networks. I  think it is a simple, straightforward,  novel extension, but I am',\n",
       " 'not fully convinced its advantages.  This paper proposes an improved version of matching networks,',\n",
       " 'with better scalability properties with respect to the support set of a few-shot classifier. Instead',\n",
       " 'of considering each support point individually, they learn an embedding function that aggregates',\n",
       " 'over items of each class within the support set (eq. 1). This is combined with episodic few-shot',\n",
       " 'training with randomly-sampled partitions of the training set classes, so that the training and',\n",
       " 'testing scenarios match closely.----------------Although the idea is quite straightforward, and',\n",
       " 'there are a great many prior works on zero-shot and few-shot learning, the proposed technique is',\n",
       " 'novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets. One',\n",
       " 'addition that I think would improve the paper is a clearer description of the training algorithm',\n",
       " '(perhaps pseudocode). In its current form the paper a bit vague about this. *** Paper Summary',\n",
       " '***----------------This paper simplify matching network by considering only a single prototype per',\n",
       " 'class which is obtained as the average of the embedding of the training class samples. Empirical',\n",
       " 'comparisons with matching networks are reported.----------------*** Review ***----------------The',\n",
       " 'paper reads well and clearly motivate the work. This work of learning metric learning propose to',\n",
       " 'simplify an earlier work (matching network) which is a great objective. However, I am not sure it',\n",
       " 'achieve better results than matching networks. The space of learning embeddings to optimize nearest',\n",
       " 'neighbor classification has been explored before, but the idea of averaging the propotypes is',\n",
       " 'interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the',\n",
       " 'discussion of related work and to consolidate the results section to help distinguish between the',\n",
       " 'methods you outperform and the one you do not. ----------------The related work section can be',\n",
       " 'extended to include work on learning distance metric to optimize a nearest neighbor classification,',\n",
       " 'see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural',\n",
       " 'networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches',\n",
       " 'pursuing similar goals with a different learning objective, you cite siamese network with pairwise',\n",
       " 'supervision. The learning to rank (for websearch) litterature with triplet supervision or global',\n",
       " 'ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the',\n",
       " 'embedding space need to be such that positive/relevant document are closer to the query than the',\n",
       " 'others. I would suggest to start with Chris Burges 2010 tutorial. One learning class',\n",
       " '----------------I am not sure the reported results correctly reflect the state of the art for all',\n",
       " 'tasks. The results are positive on Omniglot but I feel that you should also report the better',\n",
       " 'results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can',\n",
       " 'be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was',\n",
       " '50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?',\n",
       " '----------------Overall, paper could greatly be improved, both in the discussion of related work and',\n",
       " 'with a less partial reporting of prior empirical results.----------------*** References',\n",
       " '***----------------Large Margin Nearest Neighbors. Weinberger et al, 2005--------From RankNet to',\n",
       " 'LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010--------A Deep Non-linear',\n",
       " 'Feature Mapping for Large-Margin kNN Classification, Min et al, 09']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textwrap.wrap(df[['text','summary']].iloc[0]['text'], width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as',\n",
       " 'element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model',\n",
       " '(Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the',\n",
       " 'current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they',\n",
       " 'built.----------------Strengths:----------------1. The paper presents new insights into element-wise',\n",
       " 'multiplication operation which has been previously used in VQA literature (such as Antol et al.,',\n",
       " 'ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model',\n",
       " 'for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the',\n",
       " 'statistical significance of the performance (see weaknesses below).----------------3. The various',\n",
       " 'design choices made in model development have been experimentally verified.',\n",
       " '----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of',\n",
       " 'the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they',\n",
       " 'found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified',\n",
       " 'that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for',\n",
       " 'the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less',\n",
       " 'parameters than compact bilinear pooling. So, could the authors please explain how does the',\n",
       " 'reduction in number of parameters help experimentally? Does the training time of the model reduce',\n",
       " 'significantly? Can we train the model with less data? ----------------3. One of the contributions of',\n",
       " 'the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However,',\n",
       " 'I am skeptical that the performance of the proposed model is statistically significantly better than',\n",
       " 'the current state-of-art.----------------4. I would like the authors to explicitly mention the',\n",
       " 'differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------',\n",
       " '5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review',\n",
       " 'Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise',\n",
       " 'multiplication) presented in the paper. However, it could not be justified that low-rank bilinear',\n",
       " 'pooling leads to better performance than compact biliear pooling. It does lead to reduction in',\n",
       " 'number of parameters but it is not clear how much that helps experimentally. So, to be more',\n",
       " 'convinced I would like the authors to provide experimental justification of why low-rank bilinear',\n",
       " 'pooling is better than other forms of pooling. Summary: The paper presents a model for Visual',\n",
       " 'Question Answering (VQA) that uses low-rank bilinear pooling with Hadamard product (commonly known',\n",
       " 'as element-wise multiplication). The paper builds on an existing model and presents a new model for',\n",
       " 'VQA that outperforms the current state-of-art by 0.42%. The authors present various ablation studies',\n",
       " 'of the new VQA model. However, I have concerns about the statistical significance of the',\n",
       " 'performance, particularly when comparing low-rank bilinear pooling with compact bilinear pooling.',\n",
       " 'The paper also presents design choices made in model development, which have been experimentally',\n",
       " 'verified. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact',\n",
       " 'bilinear pooling, which could potentially help experimentally. However, the performance of the',\n",
       " 'proposed model is statistically significantly better than the current state-of-art. The paper could',\n",
       " 'also provide more insights into the differences between MRN, MARN, and MLB. Overall, the paper']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "textwrap.wrap(df_dataset[['text','summary']].iloc[0]['summary'], width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimpse.evaluate import evaluate_common_metrics_samples\n",
    "\n",
    "# This script is used to evaluate the quality of the generated summaries\n",
    "# Using ROUGE\n",
    "\n",
    "metrics = evaluate_common_metrics_samples.evaluate_rouge(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': [0.22764227642276422,\n",
       "  0.27586206896551724,\n",
       "  0.21513944223107567,\n",
       "  0.29824561403508776,\n",
       "  0.10416666666666666],\n",
       " 'rouge2': [0.049586776859504134,\n",
       "  0.05263157894736842,\n",
       "  0.016064257028112452,\n",
       "  0.017857142857142856,\n",
       "  0.010443864229765011],\n",
       " 'rougeL': [0.14634146341463414,\n",
       "  0.15517241379310348,\n",
       "  0.11155378486055777,\n",
       "  0.17543859649122806,\n",
       "  0.057291666666666664],\n",
       " 'rougeLsum': [0.14634146341463414,\n",
       "  0.15517241379310348,\n",
       "  0.11155378486055777,\n",
       "  0.17543859649122806,\n",
       "  0.057291666666666664]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying SeaHorse for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "100%|██████████| 5/5 [00:19<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "from glimpse.evaluate import evaluate_seahorse_metrics_samples\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = f\"google/seahorse-large-q2\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "quest = \"SHMetric/Repetition\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model= model.to('cuda')\n",
    "metrics =  evaluate_seahorse_metrics_samples.evaluate_classification_task(model, tokenizer, quest, df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SHMetric/Repetition/proba_1': [0.962890625,\n",
       "  0.9873046875,\n",
       "  0.99072265625,\n",
       "  0.9892578125,\n",
       "  0.99169921875,\n",
       "  0.99169921875,\n",
       "  0.9892578125,\n",
       "  0.994140625,\n",
       "  0.99267578125,\n",
       "  0.986328125],\n",
       " 'SHMetric/Repetition/proba_0': [0.036956787109375,\n",
       "  0.01275634765625,\n",
       "  0.009033203125,\n",
       "  0.01068878173828125,\n",
       "  0.00814056396484375,\n",
       "  0.00835418701171875,\n",
       "  0.0105743408203125,\n",
       "  0.0060577392578125,\n",
       "  0.007213592529296875,\n",
       "  0.01345062255859375],\n",
       " 'SHMetric/Repetition/guess': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "Below a small test on *Generating extractive candidates*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\candidates\\extractive_sentences-_-all_reviews_2017_tst-_-none-_-2024-12-18-20-15-06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>summary</th>\n",
       "      <th>id_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper implements low-rank bilinear pooling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper presents various ablation studies of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-Strengths:-1.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper presents new insights into element-w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>8</td>\n",
       "      <td>https://openreview.net/forum?id=BkCPyXm1l</td>\n",
       "      <td>This manuscript tries to tackle neural network...</td>\n",
       "      <td>The reviewers unanimously recommend rejection.</td>\n",
       "      <td>It's unclear what conclusions can be drawn abo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>8</td>\n",
       "      <td>https://openreview.net/forum?id=BkCPyXm1l</td>\n",
       "      <td>This manuscript tries to tackle neural network...</td>\n",
       "      <td>The reviewers unanimously recommend rejection.</td>\n",
       "      <td>-I have remaining reservations about data hygi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>8</td>\n",
       "      <td>https://openreview.net/forum?id=BkCPyXm1l</td>\n",
       "      <td>This manuscript tries to tackle neural network...</td>\n",
       "      <td>The reviewers unanimously recommend rejection.</td>\n",
       "      <td>Relatedly, the regularization potential of ear...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>8</td>\n",
       "      <td>https://openreview.net/forum?id=BkCPyXm1l</td>\n",
       "      <td>This manuscript tries to tackle neural network...</td>\n",
       "      <td>The reviewers unanimously recommend rejection.</td>\n",
       "      <td>See, e.g.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>8</td>\n",
       "      <td>https://openreview.net/forum?id=BkCPyXm1l</td>\n",
       "      <td>This manuscript tries to tackle neural network...</td>\n",
       "      <td>The reviewers unanimously recommend rejection.</td>\n",
       "      <td>the protocol in Goodfellow et al (2013).</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                         id  \\\n",
       "0        0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "1        0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "2        0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "3        0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "4        0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "..     ...                                        ...   \n",
       "150      8  https://openreview.net/forum?id=BkCPyXm1l   \n",
       "151      8  https://openreview.net/forum?id=BkCPyXm1l   \n",
       "152      8  https://openreview.net/forum?id=BkCPyXm1l   \n",
       "153      8  https://openreview.net/forum?id=BkCPyXm1l   \n",
       "154      8  https://openreview.net/forum?id=BkCPyXm1l   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Summary: The paper presents low-rank bilinear ...   \n",
       "1    Summary: The paper presents low-rank bilinear ...   \n",
       "2    Summary: The paper presents low-rank bilinear ...   \n",
       "3    Summary: The paper presents low-rank bilinear ...   \n",
       "4    Summary: The paper presents low-rank bilinear ...   \n",
       "..                                                 ...   \n",
       "150  This manuscript tries to tackle neural network...   \n",
       "151  This manuscript tries to tackle neural network...   \n",
       "152  This manuscript tries to tackle neural network...   \n",
       "153  This manuscript tries to tackle neural network...   \n",
       "154  This manuscript tries to tackle neural network...   \n",
       "\n",
       "                                                  gold  \\\n",
       "0    The program committee appreciates the authors'...   \n",
       "1    The program committee appreciates the authors'...   \n",
       "2    The program committee appreciates the authors'...   \n",
       "3    The program committee appreciates the authors'...   \n",
       "4    The program committee appreciates the authors'...   \n",
       "..                                                 ...   \n",
       "150     The reviewers unanimously recommend rejection.   \n",
       "151     The reviewers unanimously recommend rejection.   \n",
       "152     The reviewers unanimously recommend rejection.   \n",
       "153     The reviewers unanimously recommend rejection.   \n",
       "154     The reviewers unanimously recommend rejection.   \n",
       "\n",
       "                                               summary  id_candidate  \n",
       "0    Summary: The paper presents low-rank bilinear ...             0  \n",
       "1    The paper implements low-rank bilinear pooling...             1  \n",
       "2    The paper presents various ablation studies of...             2  \n",
       "3                                       -Strengths:-1.             3  \n",
       "4    The paper presents new insights into element-w...             4  \n",
       "..                                                 ...           ...  \n",
       "150  It's unclear what conclusions can be drawn abo...             8  \n",
       "151  -I have remaining reservations about data hygi...             9  \n",
       "152  Relatedly, the regularization potential of ear...            10  \n",
       "153                                          See, e.g.            11  \n",
       "154           the protocol in Goodfellow et al (2013).            12  \n",
       "\n",
       "[155 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gold</th>\n",
       "      <th>summary</th>\n",
       "      <th>id_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper implements low-rank bilinear pooling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper presents various ablation studies of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-Strengths:-1.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper presents new insights into element-w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-2.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The paper presents a new model for the task of...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>However, I have concerns about the statistical...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-3.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The various design choices made in model devel...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-Weaknesses/Suggestions:-1.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>When authors explicitly (keeping rest of the m...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Hence, it could not be experimentally verified...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-2.</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>The authors argue that low-rank bilinear pooli...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>So, could the authors please explain how does ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Does the training time of the model reduce sig...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>Can we train the model with less data?</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-3.</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>One of the contributions of the paper is that ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>However, I am skeptical that the performance o...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-4.</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>I would like the authors to explicitly mention...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>It is not very clear from reading the paper.</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>-5.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>In the caption for Table 1, fix the following:...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>However, it could not be justified that low-ra...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>It does lead to reduction in number of paramet...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>https://openreview.net/forum?id=r1rhWnZkg</td>\n",
       "      <td>Summary: The paper presents low-rank bilinear ...</td>\n",
       "      <td>The program committee appreciates the authors'...</td>\n",
       "      <td>So, to be more convinced I would like the auth...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                         id  \\\n",
       "0       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "1       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "2       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "3       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "4       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "5       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "6       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "7       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "8       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "9       0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "10      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "11      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "12      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "13      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "14      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "15      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "16      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "17      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "18      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "19      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "20      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "21      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "22      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "23      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "24      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "25      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "26      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "27      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "28      0  https://openreview.net/forum?id=r1rhWnZkg   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Summary: The paper presents low-rank bilinear ...   \n",
       "1   Summary: The paper presents low-rank bilinear ...   \n",
       "2   Summary: The paper presents low-rank bilinear ...   \n",
       "3   Summary: The paper presents low-rank bilinear ...   \n",
       "4   Summary: The paper presents low-rank bilinear ...   \n",
       "5   Summary: The paper presents low-rank bilinear ...   \n",
       "6   Summary: The paper presents low-rank bilinear ...   \n",
       "7   Summary: The paper presents low-rank bilinear ...   \n",
       "8   Summary: The paper presents low-rank bilinear ...   \n",
       "9   Summary: The paper presents low-rank bilinear ...   \n",
       "10  Summary: The paper presents low-rank bilinear ...   \n",
       "11  Summary: The paper presents low-rank bilinear ...   \n",
       "12  Summary: The paper presents low-rank bilinear ...   \n",
       "13  Summary: The paper presents low-rank bilinear ...   \n",
       "14  Summary: The paper presents low-rank bilinear ...   \n",
       "15  Summary: The paper presents low-rank bilinear ...   \n",
       "16  Summary: The paper presents low-rank bilinear ...   \n",
       "17  Summary: The paper presents low-rank bilinear ...   \n",
       "18  Summary: The paper presents low-rank bilinear ...   \n",
       "19  Summary: The paper presents low-rank bilinear ...   \n",
       "20  Summary: The paper presents low-rank bilinear ...   \n",
       "21  Summary: The paper presents low-rank bilinear ...   \n",
       "22  Summary: The paper presents low-rank bilinear ...   \n",
       "23  Summary: The paper presents low-rank bilinear ...   \n",
       "24  Summary: The paper presents low-rank bilinear ...   \n",
       "25  Summary: The paper presents low-rank bilinear ...   \n",
       "26  Summary: The paper presents low-rank bilinear ...   \n",
       "27  Summary: The paper presents low-rank bilinear ...   \n",
       "28  Summary: The paper presents low-rank bilinear ...   \n",
       "\n",
       "                                                 gold  \\\n",
       "0   The program committee appreciates the authors'...   \n",
       "1   The program committee appreciates the authors'...   \n",
       "2   The program committee appreciates the authors'...   \n",
       "3   The program committee appreciates the authors'...   \n",
       "4   The program committee appreciates the authors'...   \n",
       "5   The program committee appreciates the authors'...   \n",
       "6   The program committee appreciates the authors'...   \n",
       "7   The program committee appreciates the authors'...   \n",
       "8   The program committee appreciates the authors'...   \n",
       "9   The program committee appreciates the authors'...   \n",
       "10  The program committee appreciates the authors'...   \n",
       "11  The program committee appreciates the authors'...   \n",
       "12  The program committee appreciates the authors'...   \n",
       "13  The program committee appreciates the authors'...   \n",
       "14  The program committee appreciates the authors'...   \n",
       "15  The program committee appreciates the authors'...   \n",
       "16  The program committee appreciates the authors'...   \n",
       "17  The program committee appreciates the authors'...   \n",
       "18  The program committee appreciates the authors'...   \n",
       "19  The program committee appreciates the authors'...   \n",
       "20  The program committee appreciates the authors'...   \n",
       "21  The program committee appreciates the authors'...   \n",
       "22  The program committee appreciates the authors'...   \n",
       "23  The program committee appreciates the authors'...   \n",
       "24  The program committee appreciates the authors'...   \n",
       "25  The program committee appreciates the authors'...   \n",
       "26  The program committee appreciates the authors'...   \n",
       "27  The program committee appreciates the authors'...   \n",
       "28  The program committee appreciates the authors'...   \n",
       "\n",
       "                                              summary  id_candidate  \n",
       "0   Summary: The paper presents low-rank bilinear ...             0  \n",
       "1   The paper implements low-rank bilinear pooling...             1  \n",
       "2   The paper presents various ablation studies of...             2  \n",
       "3                                      -Strengths:-1.             3  \n",
       "4   The paper presents new insights into element-w...             4  \n",
       "5                                                 -2.             5  \n",
       "6   The paper presents a new model for the task of...             6  \n",
       "7   However, I have concerns about the statistical...             7  \n",
       "8                                                 -3.             8  \n",
       "9   The various design choices made in model devel...             9  \n",
       "10                        -Weaknesses/Suggestions:-1.            10  \n",
       "11  When authors explicitly (keeping rest of the m...            11  \n",
       "12  Hence, it could not be experimentally verified...            12  \n",
       "13                                                -2.            13  \n",
       "14  The authors argue that low-rank bilinear pooli...            14  \n",
       "15  So, could the authors please explain how does ...            15  \n",
       "16  Does the training time of the model reduce sig...            16  \n",
       "17             Can we train the model with less data?            17  \n",
       "18                                                -3.            18  \n",
       "19  One of the contributions of the paper is that ...            19  \n",
       "20  However, I am skeptical that the performance o...            20  \n",
       "21                                                -4.            21  \n",
       "22  I would like the authors to explicitly mention...            22  \n",
       "23       It is not very clear from reading the paper.            23  \n",
       "24                                                -5.            24  \n",
       "25  In the caption for Table 1, fix the following:...            25  \n",
       "26  However, it could not be justified that low-ra...            26  \n",
       "27  It does lead to reduction in number of paramet...            27  \n",
       "28  So, to be more convinced I would like the auth...            28  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['index'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "text = df['text'][0].replace('-----', '\\n')\n",
    "sentc = nltk.sent_tokenize(text)\n",
    "\n",
    "len(sentc) == len(df[df['index'] == 0]) == 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
