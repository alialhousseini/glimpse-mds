,id,text,gold,summary_LSA,summary_text-rank,summary_lex-rank,summary_edmundson,summary_luhn,summary_kl-sum,summary_random,summary_reduction,rouge1_LSA,rouge2_LSA,rougeL_LSA,rougeLsum_LSA,rouge1_text-rank,rouge2_text-rank,rougeL_text-rank,rougeLsum_text-rank,rouge1_lex-rank,rouge2_lex-rank,rougeL_lex-rank,rougeLsum_lex-rank,rouge1_edmundson,rouge2_edmundson,rougeL_edmundson,rougeLsum_edmundson,rouge1_luhn,rouge2_luhn,rougeL_luhn,rougeLsum_luhn,rouge1_kl-sum,rouge2_kl-sum,rougeL_kl-sum,rougeLsum_kl-sum,rouge1_random,rouge2_random,rougeL_random,rougeLsum_random,rouge1_reduction,rouge2_reduction,rougeL_reduction,rougeLsum_reduction,proba_of_success_kl-sum,proba_of_success_luhn,proba_of_success_edmundson,proba_of_success_text-rank,proba_of_success_LSA,proba_of_success_lex-rank,proba_of_success_reduction,proba_of_success_random,coherence_LSA,consistency_LSA,fluency_LSA,coherence_text-rank,consistency_text-rank,fluency_text-rank,coherence_lex-rank,consistency_lex-rank,fluency_lex-rank,coherence_edmundson,consistency_edmundson,fluency_edmundson,coherence_luhn,consistency_luhn,fluency_luhn,coherence_kl-sum,consistency_kl-sum,fluency_kl-sum,coherence_random,consistency_random,fluency_random,coherence_reduction,consistency_reduction,fluency_reduction
0,https://openreview.net/forum?id=-TwO99rbVRu,"summary:-----this paper introduces a model to improve semantic segmentation by using a limited amount of pixel-labeled data and unlabeled data or image-level labeled data. the authors use a self-attention grad-cam (sgc) and segmenter to generate the pseudo-labels during training. the approach shows good results on pascal voc and coco datasets and is well analyzed.-----reasons for score:-----i do not think the technical contribution is strong enough for iclr. the paper is incremental and the proposed approach is a combination of a lot of existing approaches. but i also want to highlight that the experimental section is strong and detailed.-----pros:-----the idea of using pseudo-labels is interesting because it allows to build larger dataset without increasing the annotation cost.-----the approach is evaluated in the settings of using unlabeled data and using image-level labeled data.-----the ablation study section gives a lot of details about the model. the authors analyzed a lot of things: expected calibration error, hypercolumn feature, soft vs hard label, temperature sharpening, color jittering strength, backbone architecture.-----the approach shows good results on pascal voc and coco datasets.-----the proposed method achieves good performance in the low-data regime-----cons:-----the overall approach seems incremental because it is a combination of a lot of existing approaches and there is not a strong technical contribution. for instance, the model uses several loss functions and all the losses are jointly optimized.-----i think the related work section should be in the main paper instead of the supplementary.-----i feel some parts are a bit difficult to read because of some misleading information. for example, the title of section 3.1 is experiments using unlabeled data but the model still uses some labeled data. summary: this paper focuses on the problem of semi-supervised semantic segmentation, where less pixel-level annotations are used to train the network. a new one-stage training framework is proposed to include the process of localization cue generation, pseudo label refinement and training of semantic segmentation. inspire by recent success in the semi-supervised learning (ssl), a novel calibrated fusion strategy is proposed to incorporate the concept of consistency training with data augmentation into the framework. experiments on pascal voc and mscoco benchmarks validate the effectiveness of the proposed method.-----pro:-----the proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training.-----the new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.-----achieve a new state-of-the-art on both pascal voc and mscoco benchmarks compared with recent semi-supervised semantic segmentation methods.-----questions:-----cct (ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features. i'm wondering if authors can provide some insights about why the proposed method can achieve better performance than cct when they both include the consistency training and data augmentation in the designs.-----in table 3, i suggest to include the segmentation framework used by each method in the table. in early works, old version of deeplab is usually treated as the standard. i understand using deeplab v3 is a fair comparison with cct. it would be good to make this information clear in the table.-----it is also suggested to report the performance on pascal voc test set as it is a common practice in this area (although cct does not do so).-----sine the unlabeled data training branch does not rely on any pixel-level annotations, i'm wondering if the proposed method can also work under weakly-supervised setting, where no pixel-level annotations are available during the training. ** summary this work addresses the task of semi-supervised learning (ssl) in semantic segmentation. following recent sotas in ssl, this work also advocates for the use of pseudo-labels on unlabeled data and heavy data augmentation. the main novelty of this work is the novel way to construct higher-quality pseudo-labels: besides the pixel-wise classifier's probabilistic outputs, the authors leverage as well cam-based activation maps, named as sgc, as an additional pseudo-label source. the final set of pseudo-labels is determined by linear combining the two soft pseudo-label sources with temperature adjustment. the authors conducted extensive experiments with lots of ablation studies to validate the proposed framework.-----** strengths:-----the paper is well-written, easy to follow-----extensive experiments with adequate discussions-----improvements over sotas on the addressed benchmarks.-----** weakness/concerns:-----does ""distribution sharpen operation"" always use temperature < 1? if yes, what is the reason?-----how is the temperature-----t-----is chosen? may the authors produce a performance analysis over t?-----in sec 3.4, it's not clear to me the advantage of proposed method on boundaries. cam-based activations mostly focus on most discriminative areas (usually inner areas). so hardly sgc can find pseudo-labels on boundaries. why does the proposed method have an advantage there?-----more and more segmentation works report results in urban datasets like cityscapes or camvid. it would be interesting to see results on those datasets. one interesting aspect in urban datasets is the natural long-tail class distributions, which severely damages performance on minor classes, especially in low-data regime.","the authors introduce an approach for designing pseudo-labels in semi-supervised segmentation. the approach combines the idea a refining pseudo-labels with self-attention grad-cam (sgc) and a calibrated prediction fusion, and consistency training by enforcing pseudo labels to be robust to strongly-augmented data.-----the reviewers overall like idea and point out the good level of performance obtained by the method in the challenging semi-supervised context. however, they also point out the limited novelty of the approach, and the need for a better positioning with respect to related works. after rebuttal, reviewers were satisfied with authors' answers and paper modifications, and all recommend acceptance.-----the ac considers that the submission is a nice combination of existing techniques and likes the simplicity of the one-stage approach, which reaches good performances. therefore, the ac recommends acceptance.","inspire by recent success in the semi-supervised learning (ssl), a novel calibrated fusion strategy is proposed to incorporate the concept of consistency training with data augmentation into the framework.","experiments on pascal voc and mscoco benchmarks validate the effectiveness of the proposed method.-----pro:-----the proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training.-----the new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.-----achieve a new state-of-the-art on both pascal voc and mscoco benchmarks compared with recent semi-supervised semantic segmentation methods.-----questions:-----cct (ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features.",but i also want to highlight that the experimental section is strong and detailed.-----pros:-----the idea of using pseudo-labels is interesting because it allows to build larger dataset without increasing the annotation cost.-----the approach is evaluated in the settings of using unlabeled data and using image-level labeled data.-----the ablation study section gives a lot of details about the model.,summary:-----this paper introduces a model to improve semantic segmentation by using a limited amount of pixel-labeled data and unlabeled data or image-level labeled data.,"the authors analyzed a lot of things: expected calibration error, hypercolumn feature, soft vs hard label, temperature sharpening, color jittering strength, backbone architecture.-----the approach shows good results on pascal voc and coco datasets.-----the proposed method achieves good performance in the low-data regime-----cons:-----the overall approach seems incremental because it is a combination of a lot of existing approaches and there is not a strong technical contribution.","experiments on pascal voc and mscoco benchmarks validate the effectiveness of the proposed method.-----pro:-----the proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training.-----the new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.-----achieve a new state-of-the-art on both pascal voc and mscoco benchmarks compared with recent semi-supervised semantic segmentation methods.-----questions:-----cct (ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features.",the final set of pseudo-labels is determined by linear combining the two soft pseudo-label sources with temperature adjustment.,"experiments on pascal voc and mscoco benchmarks validate the effectiveness of the proposed method.-----pro:-----the proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training.-----the new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.-----achieve a new state-of-the-art on both pascal voc and mscoco benchmarks compared with recent semi-supervised semantic segmentation methods.-----questions:-----cct (ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features.",0.2130177514792899,0.0359281437125748,0.1301775147928993,0.1301775147928993,0.3320158102766798,0.0557768924302788,0.1581027667984189,0.1581027667984189,0.2561576354679803,0.0597014925373134,0.1576354679802955,0.1576354679802955,0.1686746987951807,0.0,0.0843373493975903,0.0843373493975903,0.2870813397129186,0.0772946859903381,0.1913875598086124,0.1913875598086124,0.3320158102766798,0.0557768924302788,0.1581027667984189,0.1581027667984189,0.1383647798742138,0.0382165605095541,0.10062893081761,0.10062893081761,0.3320158102766798,0.0557768924302788,0.1581027667984189,0.1581027667984189,9.986734390258787,8.633788108825684,11.616464614868164,9.986734390258787,7.078330993652344,9.988052368164062,9.986734390258787,6.305878162384033,0.9420609631194165,0.922156797386812,0.39440838212158685,0.9754774579019617,0.9760623706374454,0.7803464033861928,0.9333040583068049,0.9437286575761252,0.8391916879031173,0.9779343398806294,0.9655913143945307,0.6950489603492073,0.9649814732628237,0.951438492905568,0.820261130134945,0.9754774579019617,0.9760623706374454,0.7803464033861928,0.41675769979634786,0.5740519606443847,0.6741496139618685,0.9754774579019617,0.9760623706374454,0.7803464033861928
1,https://openreview.net/forum?id=1s1T7xHc5l6,"this paper builds the connection between various steerable cnn structures based on group representation theory and filter transformations. using the discrete rotation and reflection group as an example, the paper establishes ways to construct steerable convolutional filters that transform features in trivial, regular, and irreducible representations. in particular, recent works on steerable cnns such as orn, rotdcf, ti-pooling, and roteqnet can all be explained under such framework.-----the paper is generally well written and well organized. however, i reckon the material would be quite dense for readers interested in equivariant cnns but not well-versed in group representation theory, so a bit more explanation either in text or in the appendix would be helpful. the main theoretical results in the paper on the considered discrete group transformation are interesting, and can potentially lead to other development in the community. the numerical experiments seem to be limited (though this phenomenon seems to be the issue for most of the papers in this area...)-----** pros-----well written and technically correct.-----interesting theoretical results that might inspire other works in the field.-----** cons (to be explained in more details in questions)-----some notation and abbreviation are used without proper explanation, which might be confusing.-----limited experimental results-----** questions and comment-----some of the notations and abbreviations are used without proper explanation. for example, ""k"" in equation (12), ocr on page 1, and missing reference on r2conv. i would encourage the authors to make a greater effort to clarify their ideas and results.-----the theory seems to be built only on discrete group. does it generalize to continuous group transformation (such as so(2) before discretization)?-----also, does the theory generalize to non-compact groups such as scaling and shearing?-----the experiments seem to be limited, even though this seems to be a common issue in papers in this field. however, i do recommend the authors to present the ""equivariant loss"" when using their proposed filtra, considering that discretization and interpolation might cause a problem in their setting unlike other means of steerable cnns such as rotdcf and harmonic net this paper studies the connection between steerable cnn and filter transformation. the authors show theoretically that filter transformation can be used to realize steerable filters over different group representations. the authors also empirically show that the filter transformation based steerable cnn performs on par with the implementation based on harmonic bases.-----overall, i think this paper is hard to follow. the presentation and notation depends on that of prior works and is hard to understand without being familiar with steerable cnn. compared with prior works, this paper focuses more on the mathematical derivation but falls short of explaining their implication. the result is therefore hard to understand for those that are not familiar with the theoretical basis. also, the authors do not clearly show why the contribution of this work is significant. the theoretical connection doesnt seem to provide a significant advantage nor solve the problems in existing methods. similarly, the experiment doesnt provide much information and only shows that the proposed implementation works on simple datasets. the authors should try to highlight how this work may benefit other research or applications and verify that the proposed method works on more complicated and realistic data.-----another aspect that can be improved is to discuss how the theory generalizes when multiple convolution layers are applied. in general, the theoretical properties of steerable convolution does not automatically generalize to multiple convolution layers. this is particularly important in real world data, because the transformation may happen at different scales and may need to be accounted for at different layers in the network. therefore, the results derived from a single convolution operation may not be sufficient. equivariant steerable cnns for 2d/3d rotation+reflection+translation groups have generally been implemented as a filter transform/expansion step followed by a standard convolution. the filter expansion step involves taking a linear combination of steerable basis filters. these basis filters are pre-computed before network training by solving a linear system or by sampling the continuous analytical solution (this can take a few minutes). depending on the chosen group representation wrt which the network layer is equivariant, a different filter basis will emerge, but in general one can see that the basis filters come out as rotated and flipped copies of some basis filters, with the occasional sign flip (this has been stated in some earlier works). the precise way in which a basis filter is to be rotated and flipped to obtain the steerable filter basis for the 2d case had not been worked out before, to my knowledge, and this is one of the contributions of this paper. the analysis is done for each (input, output) representation type chosen from {trivial, irreducible, regular} representations. having worked this out, the paper proposes to use this as a way of implementing the filter expansion step, starting from a basis filter and rotating/flipping it to obtain an expanded filter bank.-----the proposed method (filtra) does not require a precomputation step if i understand correctly, which is a significant practical advantage. experiments further show that the method is similar or faster at filter expansion. finally, the method is validated by training networks on benchmark tasks and shown to perform similarly to or better than the steerable cnn implementation of weiler & cesa, which is the best existing implementation.-----the paper briefly mentions that filters cannot be rotated exactly on a discrete grid, but i didn't figure out how the authors propose to deal with this issue. how exactly are the filters rotated?-----i think the method proposed in the paper is useful, as it is both faster and better than existing steerable cnn implementations. the paper itself is fairly well written and technically correct as far as i can tell, but may be challenging to read for those who are not yet knowledgeable about steerable cnns. those readers however are unlikely to be interested in learning about the implementation details of steerable cnns anyway, so perhaps this is fine. the reason i am not giving a higher rating is that i think that although this is a useful contribution to the literature on steerable cnns, which are being used in an increasing number of applications, the paper does not represent a major breakthrough and although the calculations are non-trivial, does not contain highly unexpected or deep theoretical results.-----typos: cadestrian -> cartesian irreduciable -> irreducible equity -> equality-----edit: having read the reviews, rebuttal and updated paper, i have decided to maintain my score of 6.","this paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. especially, the authors rely on the haramonic analysis view of steerable cnns given in weiler & cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters.-----the reviewers finds the paper technically solid but difficult to read and with a limited contribution. the ac carefully reads the paper and discussions. although the connection between steerable cnns and filter transform are interesting, the ac considers that the main contributions of the paper should be consolidated, especially the positioning with respect to weiler & cesa (2019).-----therefore, the ac recommends rejection.","finally, the method is validated by training networks on benchmark tasks and shown to perform similarly to or better than the steerable cnn implementation of weiler & cesa, which is the best existing implementation.-----the paper briefly mentions that filters cannot be rotated exactly on a discrete grid, but i didn't figure out how the authors propose to deal with this issue.","depending on the chosen group representation wrt which the network layer is equivariant, a different filter basis will emerge, but in general one can see that the basis filters come out as rotated and flipped copies of some basis filters, with the occasional sign flip (this has been stated in some earlier works).","does it generalize to continuous group transformation (such as so(2) before discretization)?-----also, does the theory generalize to non-compact groups such as scaling and shearing?-----the experiments seem to be limited, even though this seems to be a common issue in papers in this field.",this paper builds the connection between various steerable cnn structures based on group representation theory and filter transformations.,the theoretical connection doesnt seem to provide a significant advantage nor solve the problems in existing methods.,"however, i do recommend the authors to present the ""equivariant loss"" when using their proposed filtra, considering that discretization and interpolation might cause a problem in their setting unlike other means of steerable cnns such as rotdcf and harmonic net this paper studies the connection between steerable cnn and filter transformation.","similarly, the experiment doesnt provide much information and only shows that the proposed implementation works on simple datasets.","having worked this out, the paper proposes to use this as a way of implementing the filter expansion step, starting from a basis filter and rotating/flipping it to obtain an expanded filter bank.-----the proposed method (filtra) does not require a precomputation step if i understand correctly, which is a significant practical advantage.",0.3157894736842105,0.0473372781065088,0.1637426900584795,0.1637426900584795,0.2469135802469135,0.025,0.1358024691358025,0.1358024691358025,0.1923076923076923,0.0,0.1153846153846153,0.1153846153846153,0.2047244094488189,0.1119999999999999,0.1574803149606299,0.1574803149606299,0.0952380952380952,0.0,0.0634920634920634,0.0634920634920634,0.3125,0.1518987341772152,0.2,0.2,0.0944881889763779,0.016,0.0629921259842519,0.0629921259842519,0.1840490797546012,0.0248447204968944,0.1226993865030674,0.1226993865030674,7.595203399658203,3.899004459381104,11.388761520385742,7.366742610931396,7.646482944488525,5.78055477142334,5.545511722564697,4.512392520904541,0.18938711427798538,0.1984735913941936,0.9464878537806727,0.5346276483410448,0.6273469608908051,0.7948432196935412,0.9572302646137809,0.9701139417752256,0.7849169511040328,0.9375865394943155,0.9450901986155348,0.9077178146611962,0.9147265689535579,0.9178751830241643,0.8360559006678591,0.9490237228095162,0.945936231940726,0.09140111196465947,0.9120377193600143,0.9233393279237874,0.9171927768122904,0.528688015121573,0.7434295382361364,0.9062495850894063
2,https://openreview.net/forum?id=2nm0fGwWBMr,"panrep: universal node embeddings for heterogeneous graphs-----the manuscript proposes panrep, an unsupervised node embedding learning model for heterogeneous graphs. panrep considers four unsupervised tasks of cluster, motif, heterogenous structure, and mutual information (contrastive learning). experiments on two different tasks (i.e., node classification and link prediction) demonstrate that panrep outperforms some baseline methods.-----pros-----the problem is interesting and important. it is meaningful to study heterogeneous graph pre-training.-----the presentation is overall good. the content is clear.-----four interesting unsupervised tasks are proposed.-----cons-----the novelty of panrep is incremental. there are a number of studies [1,2,3,4,5] for graph pre-training. despite the difference in unsupervised task design, i did not find exciting thing from this work.-----experiments should be improved. baseline methods are relatively weak, it is better to compare and discuss more recent work for graph pre-training [1,2,3,4].-----[1] strategies for pre-training graph neural networks, iclr 2020-----[2] gcc: graph contrastive coding for graph neural network pre-training, kdd 2020-----[3] gpt-gnn: generative pre-training of graph neural networks, kdd 2020-----[4] self-supervised auxiliary learning with meta-paths for heterogeneous graphs, neurips 2020-----[5] self-supervised learning on graphs: deep insights and new directions, arxiv-----to summarize, the novelty of this work is incremental and the experiments could be improved. summary: this paper proposes a universal and unsupervised gnn-based representation learning (node embedding pretraining) model named panrep for heterogeneous graphs, which benefits a variety of downstream tasks such as node classification and link prediction. more specifically, employing an encoder similar to r-gcn, panrep utilizes four different types of universal supervision signals for heterogeneous graphs, i.e. cluster and recover, motif, metapath random walk and heterogeneous info maximization, to better characterize the graph structures. panrep can be further fine-tuned in a semi-supervised manner with limited labeled data, known as panrep-ft, for specific applications. experiments on benchmark datasets have shown the effectiveness and performance gain over other unsupervised and some supervised baseline approaches. one example use case is applied to covid-19 drug repurposing to identify potential treatment candidates.-----strong aspects: (1) the motivation, problem formulation, and model illustration are well explained. (2) proposed four types of universal supervision signals are interesting and generalizable for all (heterogeneous) graphs. (3) extensive experiments, ablation study, and case study are supportive.-----weak aspects: (1) some comparable sota baselines may not be considered. (2) the heterogeneous information maximization signal seems not clearly explained. (see detailed comments below)-----detailed comments: (1) it is strongly recommended in related work to explicitly explain the difference from other graph pretraining models such as [1,2] and the reason why they are excluded from baselines. though some work (partially) focus on graph-level application, some node-level pretraining methods are generally comparable and applicable for heterogeneous graph representation learning.-----(2) regarding the drug repurposing task, some baseline approaches (transe, etc) implemented by dlg-ke on dkrg dataset are not reported [4]. it would be more beneficial and convincing to include the results as well by comparing the #hit other than r-gcn. (3) him is an important component from the observations in the ablation study, it requires a clearer explanation for better understanding. some questions are: for the bilinear scoring function, why one node close to its global summary is the ultimate goal for him while other signals are used to learn its structures and the nodes are different from each other even though they are of the same type? why is it reasonable to design contrastive negative samples as introduced in panrep? what is the type of information of the nodes are fully not available (for example, the node type is not defined in the ontology)?-----references: [1] hu, weihua, et al. ""strategies for pre-training graph neural networks."" iclr (2020).-----[2] hu, ziniu, et al. ""heterogeneous graph transformer."" proceedings of the web conference 2020. 2020.-----[3] ioannidis, vassilis n., et al. ""drkg-drug repurposing knowledge graph for covid-19."" (2020). github: https://github.com/gnn4dr/drkg/tree/master/drug_repurpose pros:-----(1) this paper is well-organized and easy to follow, clearly presenting the composition of the proposed model.-----(2) motivation of this paper is strong, and readers may foresee rich applications.-----(3) each model component is with a clear goal, facilitating the interpretation of the proposed approach.-----cons:-----(1) this paper is not technical. all techniques are not originally proposed, and the whole model is simply putting everything together.-----(2) for all ablation studies, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance.-----(3) no performance variance is provided to make the improvement more convincing considering the close performances of some cases.-----detailed comments:-----this paper proposes an approach for unsupervised learning of universal node representations on heterogenous graphs. specifically, four components are presented with individual loss functions to ensure the representations encoding a certain category of information, and the final model is attained by putting everything together. overall, this paper is well-organized and easy to follow, clearly presenting a well-motivated model and how each component handles the information extraction. we can also foresee great application value with the improved fitting ability on universal node representations. however, all techniques are not originally proposed, making this paper less technical. besides, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance in the ablation study. also, no performance variance is provided to make the improvement more convincing considering the close performances of some cases in the experiments. to make the paper more convincing, i would have the following suggestions:-----(1) ablation study should be enriched. for the current ablation study, only one component contributes to the main performance. both the specification of where the performance improvement comes from and how components cooperation facilitates the performance are of little evidence.-----(2) more details should be provided on why larger training datasets (e.g., from 40% to 80%) can degrade the performance of some baselines.-----(3) for experiments, variance or standard deviation should be provided to make the minor improvements in some cases convincing. also, performance on more diverse datasets should also be presented for generalization purposes.-----(4) in drug-repurposing experiments, the conclusion can also be obtained from table 4 that panrep-ft tends to have long-tail effect; however rgcn shows more stable performance. to better evaluate the performance, it is better to present the distribution among all drugs and evaluate the performance of the two models from different perspectives including application consideration.-----(5) there is no information on how to balance different loss terms. for example, at least five loss terms should be delicately tuned for a balance in panrep-ft. however, the author does not present how to balance the weights among loss terms. besides, i would argue the experimental weights among loss terms should be presented in detail to determine how each component contributes to the final performance. this paper proposed introduces a problem formulation of universal unsupervised learning. they develop an unsupervised node representation learning method by combining four signals: (1) cluster and recover supervision, (2) motif supervision, (3) metapath random walk supervision, and (4) heterogeneous information maximization. aside from conducting experiments on node classification and link prediction, they apply their method on drop-repurposing knowledge graph to discover drugs for covid-19.-----i agree that there is contribution to putting these supervision signals together and showing results on drug-repurposing for covid-19. however, the proposed approach is rather a mix of previous works and hence not novel.-----all four supervision signals are based on existing works. what's the novelty in all four signals? recovering cluster assignments, calculating motif frequency vectors, and proximity-based embedding (i.e. rw) are all well-known methods in the graph domain and the objective used in ""heterogenous information maximization"" to me seems nearly identical to the one proposed in deep graph infomax [1]-----the performance improvement can simply a result of loss ensembles. how does each individual supervision signal contribute to the performance? an ablation study discussing that should be conducted.-----for the cluster and recover supervision signal, panrep attempts to recover results obtained from k-means. however,-----k-means perform poorly on many datasets (e.g. graphs where true communities are highly overlapping). i doubt that learning to recover such signals would be beneficial.-----how do you decide on the number of communities-----k-----and does it affect performance a lot?-----equation (2) should be permutation invariant to communities (invariant to the ordering of communities)-----[1] petar velickovi  c, william fedus, william l hamilton, pietro li, yoshua bengio, and r devon  hjelm. deep graph infomax. arxiv preprint arxiv:1809.10341, 2018b.","although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews.-----during the discussion, reviewers agree with that this submission is not ready for publication. in particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing.-----i will therefore reject the paper. for future submission, i strongly recommend the authors to do author response. there are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process.","aside from conducting experiments on node classification and link prediction, they apply their method on drop-repurposing knowledge graph to discover drugs for covid-19.-----i agree that there is contribution to putting these supervision signals together and showing results on drug-repurposing for covid-19.","baseline methods are relatively weak, it is better to compare and discuss more recent work for graph pre-training [1,2,3,4].-----[1] strategies for pre-training graph neural networks, iclr 2020-----[2] gcc: graph contrastive coding for graph neural network pre-training, kdd 2020-----[3] gpt-gnn: generative pre-training of graph neural networks, kdd 2020-----[4] self-supervised auxiliary learning with meta-paths for heterogeneous graphs, neurips 2020-----[5] self-supervised learning on graphs: deep insights and new directions, arxiv-----to summarize, the novelty of this work is incremental and the experiments could be improved.","all techniques are not originally proposed, and the whole model is simply putting everything together.-----(2) for all ablation studies, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance.-----(3) no performance variance is provided to make the improvement more convincing considering the close performances of some cases.-----detailed comments:-----this paper proposes an approach for unsupervised learning of universal node representations on heterogenous graphs.","panrep: universal node embeddings for heterogeneous graphs-----the manuscript proposes panrep, an unsupervised node embedding learning model for heterogeneous graphs.","baseline methods are relatively weak, it is better to compare and discuss more recent work for graph pre-training [1,2,3,4].-----[1] strategies for pre-training graph neural networks, iclr 2020-----[2] gcc: graph contrastive coding for graph neural network pre-training, kdd 2020-----[3] gpt-gnn: generative pre-training of graph neural networks, kdd 2020-----[4] self-supervised auxiliary learning with meta-paths for heterogeneous graphs, neurips 2020-----[5] self-supervised learning on graphs: deep insights and new directions, arxiv-----to summarize, the novelty of this work is incremental and the experiments could be improved.","all techniques are not originally proposed, and the whole model is simply putting everything together.-----(2) for all ablation studies, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance.-----(3) no performance variance is provided to make the improvement more convincing considering the close performances of some cases.-----detailed comments:-----this paper proposes an approach for unsupervised learning of universal node representations on heterogenous graphs.","an ablation study discussing that should be conducted.-----for the cluster and recover supervision signal, panrep attempts to recover results obtained from k-means.","all techniques are not originally proposed, and the whole model is simply putting everything together.-----(2) for all ablation studies, no evidence is shown which component contributes to the performance improvement compared with the best single-component performance.-----(3) no performance variance is provided to make the improvement more convincing considering the close performances of some cases.-----detailed comments:-----this paper proposes an approach for unsupervised learning of universal node representations on heterogenous graphs.",0.1768707482993197,0.0,0.1088435374149659,0.1088435374149659,0.1809045226130653,0.0203045685279187,0.100502512562814,0.100502512562814,0.2514285714285714,0.0231213872832369,0.1257142857142857,0.1257142857142857,0.0495867768595041,0.0,0.0495867768595041,0.0495867768595041,0.1809045226130653,0.0203045685279187,0.100502512562814,0.100502512562814,0.2514285714285714,0.0231213872832369,0.1257142857142857,0.1257142857142857,0.096,0.016260162601626,0.08,0.08,0.2514285714285714,0.0231213872832369,0.1257142857142857,0.1257142857142857,7.896063804626465,8.386072158813477,13.878949165344238,8.386072158813477,5.696771621704102,7.896063804626465,7.896063804626465,5.875319480895996,0.10171794050396274,0.07016711563739055,0.8605578031206492,0.9573645180087652,0.9679307437612934,0.8689373420437747,0.09903355474995686,0.20861822271702385,0.36957726198898583,0.9756567766721334,0.9711268210916721,0.9028842501930382,0.9573645180087652,0.9679307437612934,0.8689374094620304,0.09903355474995686,0.20861822271702385,0.3695765058807691,0.029697282872444462,0.17048977839244575,0.000502667706873105,0.09903355474995686,0.20861822271702385,0.3695765058807691
3,https://openreview.net/forum?id=3InxcRQsYLf,"this paper proposes a generative model to synthesize videos using vq-vaes. the scheme works in latent space by using embeddings for video sequences learnt by the vq-vae. for inference, an autoregressive transformer prior for video sequences is learnt, which upon sampling from and sending to the vq-vae decoder, generates unconditional (or conditional) samples of video. to learn video embeddings, the paper uses a 3d convolutional network, with an extra dimension for time.-----pros:-----simple, principled setup-----architectural novelties for videos (3d cnn, transformer prior)-----cons:-----i feel that the development is slightly incremental, compared with the original vq-vae work.-----not enough analysis of latent space. for example, the original vq-vae work looks at a few experiments where the scene is traversed by moving 'forward' and 'right' (figure 7 in [1]). i would have hoped that we had some experiments that show the virtues of working in latent space.-----codebook collapse: it would be nice to have some more analysis of this component of the model.-----other comments on analysis: there are many components in the setup, many of which need some discussion and analysis for this kind of work such as axial attention, the transformer model, latent spaces, etc.-----how does this model perform on larger image sequences, and larger number of timesteps?-----other tasks: this work looks at the task of video generation. but there are many other areas of practical application where we can benefit from modeling video sequences. how does, for example, the model work with image segmentation, or tracking?-----overall: the work is interesting, but does not seem to have sufficient novelty other than having a different architecture design than used in the original vq-vae work. that being said, there's a lot to learn for practitioners if the authors were to put up a detailed write up on architectures and experiments.-----[1] vq-vae: https://arxiv.org/abs/1711.00937 summary: authors propose to model video by combining a vq-vae encoder-decoder model and a gpt model for the prior.-----the primary contribution as stated by the authors: ""our primary contribution is videogen, a new method to model complex video data in a computationally efficient manner"" ___________ pros:-----an interesting model and an ablation of its components.-----cons:-----the primary contribution is stated but not validated. the claim is a new method to model complex video efficiently.-----there is no experiments and/or benchmarks validating this claim anywhere in the paper.-----there is work on efficiency in the video generation field that is neither cited nor benchmarked against.-----tganv2 (https://link.springer.com/article/10.1007%2fs11263-020-01333-y) and ldvd-gan (https://www.sciencedirect.com/science/article/abs/pii/s0893608020303397) come to mind.-----""computational efficiency is a primary advantage to our method, where we can first use the vq-vae to downsample by space time before learning an autoregressive prior"" - tganv2, ldvd-gan and dvd-gan also do this .-----some questionable highlights:-----""videogen can generate realistic samples that are competitive with existing methods such as dvd-gan""-----a very weak highlight because several existing methods already do this better as shown in table 1. and dvd-gan is not the state-of-the-art for this benchmark as shown in the same table.-----"" videogen can easily be adapted for action conditional video generation""-----this is applicable to every video generation model-----""our results are achievable with a maximum of 8 quadro rtx 6000 gpus (24 gb memory), significantly lower than the resources used in prior methods such as dvd-gan""-----this claim is not experimentally validated. dvd-gan is also trainable on 8 quadro rtx 6000 gpus (24 gb memory). i would go further to argue that dvd-gan would train faster and result in a higher performance than videogen. i would like to see a head to head benchmark or at the very least the wall clock time for training both the gpt prior and the vq-vae encoder-decoders.-----selective citation: the video generation and prediction field has been around for a long time now. it is hard to believe that the authors can manage to find and cite every relevant (un)published paper by google and deepmind authors yet they fail to find work published by other groups in this field. they then go on to talk about the slow progress in the field of video generation without acknowledging all the work being done in this field. the following statements highlight this:-----""however, one notable modality that has not seen the same level of progress in generative modeling is high fidelity natural videos. ""-----"" the complexity of the problem also demands more compute resources which can be considered as one important reason for the slow progress in generative modeling of videos.""-----missing references to published articles (related to the previous point)-----tgan: temporal gan - iccv 2017 (first appeared on arxiv - nov 2016) - https://openaccess.thecvf.com/content_iccv_2017/html/saito_temporal_generative_adversarial_iccv_2017_paper.html-----mocogan - cvpr 2018 (first appeared on arxiv - jul 2017) - https://openaccess.thecvf.com/content_cvpr_2018/html/tulyakov_mocogan_decomposing_motion_cvpr_2018_paper.html-----progressive video gan - masters thesis (first appeared on arxiv - oct 2018) - https://arxiv.org/abs/1810----mdp-gan: markov decision process for video generation - iccv 2019 (first appeared on arxiv - sep 2019) - https://openaccess.thecvf.com/content_iccvw_2019/html/hvu/yushchenko_markov_decision_process_for_video_generation_iccvw_2019_paper.html-----tganv2: train sparsely, generate densely - journal of computer vision 2020 - (first appeared on arxiv - nov 2018) - https://link.springer.com/article/10.1007%2fs11263-020-01333-y-----ldvd-gan: lower dimensional kernels for video discriminators - journal of neural networks 2020 - (first appeared on arxiv - dec 2019) - https://www.sciencedirect.com/science/article/abs/pii/s0893608020303397-----if we were to include unpublished preprints on arxiv in this area, this list would at least double in size.-----specific points:-----""however, one notable modality that has not seen the same level of progress in generative modeling is high fidelity natural videos. the complexity of natural videos requires modeling correlations across both space and time with much higher input dimensions, thereby presenting a natural next challenge for current deep generative models""-----the only natural video dataset benchmarked on is bair, the rest are all synthetic. please benchmark on other datasets of natural video such as ucf101 and kinetics-600 which also have comparative benchmarks at similar spatio-temporal resolutions.-----""can we generate high-fidelity samples from complex video datasets with limited compute?""-----please address and expand on this point. it is currently left unanswered.-----current recommendation: rejection-----all in all, this paper is lacking in novelty and does not do a good job of convincing readers of its primary contributions. the ablation studies provide for the most interesting insights with regard to this work. the bair evaluations show that the proposed model is more expensive and has a lower performance than many existing models. the claims of efficiency are also questionable given that the vqvae prior is notoriously expensive to train for image models, let alone video models and there is no head to head comparison or wall clock benchmark to demonstrate otherwise. lastly, the very selective referencing of work situated around google and deepmind while ignoring related and highly relevant (and famous) work from scientists in other institutes is detrimental to research in this field. i am happy to update my review and score if these issues are addressed. but this work in it's current form is not publishable at any conference.","the paper focuses on the problem of high quality video generation. it approaches the problem by extending vq-vae to videos, where a gpt is used to model the low dimensional representation of the vae. as agreed upon by the authors and the reviewers, the proposed method is simple and produces interesting results.-----based on all the reviews and the subsequent discussions, it seems that the reviewers' comments were mostly not addressed and they maintain their stance with regards to the paper's technical novelty, empirical justification of the paper's claims (specifically the claim on computational efficiency), and the rigorous comparison with prior art. the authors themselves make it clear that technical novelty was not the main driving force in this paper. however, in this case, it would be expected that the major claims of the paper be very clearly justified (especially with experiments and analysis) and comparison with other methods be more thorough. it seems that these latter two points remain in the latest revision of this paper. since the paper shows promise, the authors are recommended to take the reviewers' comments and suggestions into consideration to produce a stronger and more thorough submission in the future.","-----"" the complexity of the problem also demands more compute resources which can be considered as one important reason for the slow progress in generative modeling of videos.""","the claim is a new method to model complex video efficiently.-----there is no experiments and/or benchmarks validating this claim anywhere in the paper.-----there is work on efficiency in the video generation field that is neither cited nor benchmarked against.-----tganv2 (https://link.springer.com/article/10.1007%2fs11263-020-01333-y) and ldvd-gan (https://www.sciencedirect.com/science/article/abs/pii/s0893608020303397) come to mind.-----""computational efficiency is a primary advantage to our method, where we can first use the vq-vae to downsample by space time before learning an autoregressive prior"" - tganv2, ldvd-gan and dvd-gan also do this .-----some questionable highlights:-----""videogen can generate realistic samples that are competitive with existing methods such as dvd-gan""-----a very weak highlight because several existing methods already do this better as shown in table 1. and dvd-gan is not the state-of-the-art for this benchmark as shown in the same table.-----"" videogen can easily be adapted for action conditional video generation""-----this is applicable to every video generation model-----""our results are achievable with a maximum of 8 quadro rtx 6000 gpus (24 gb memory), significantly lower than the resources used in prior methods such as dvd-gan""-----this claim is not experimentally validated.","the claim is a new method to model complex video efficiently.-----there is no experiments and/or benchmarks validating this claim anywhere in the paper.-----there is work on efficiency in the video generation field that is neither cited nor benchmarked against.-----tganv2 (https://link.springer.com/article/10.1007%2fs11263-020-01333-y) and ldvd-gan (https://www.sciencedirect.com/science/article/abs/pii/s0893608020303397) come to mind.-----""computational efficiency is a primary advantage to our method, where we can first use the vq-vae to downsample by space time before learning an autoregressive prior"" - tganv2, ldvd-gan and dvd-gan also do this .-----some questionable highlights:-----""videogen can generate realistic samples that are competitive with existing methods such as dvd-gan""-----a very weak highlight because several existing methods already do this better as shown in table 1. and dvd-gan is not the state-of-the-art for this benchmark as shown in the same table.-----"" videogen can easily be adapted for action conditional video generation""-----this is applicable to every video generation model-----""our results are achievable with a maximum of 8 quadro rtx 6000 gpus (24 gb memory), significantly lower than the resources used in prior methods such as dvd-gan""-----this claim is not experimentally validated.",this paper proposes a generative model to synthesize videos using vq-vaes.,"that being said, there's a lot to learn for practitioners if the authors were to put up a detailed write up on architectures and experiments.-----[1] vq-vae: https://arxiv.org/abs/1711.00937 summary: authors propose to model video by combining a vq-vae encoder-decoder model and a gpt model for the prior.-----the primary contribution as stated by the authors: ""our primary contribution is videogen, a new method to model complex video data in a computationally efficient manner"" ___________ pros:-----an interesting model and an ablation of its components.-----cons:-----the primary contribution is stated but not validated.","-----missing references to published articles (related to the previous point)-----tgan: temporal gan - iccv 2017 (first appeared on arxiv - nov 2016) - https://openaccess.thecvf.com/content_iccv_2017/html/saito_temporal_generative_adversarial_iccv_2017_paper.html-----mocogan - cvpr 2018 (first appeared on arxiv - jul 2017) - https://openaccess.thecvf.com/content_cvpr_2018/html/tulyakov_mocogan_decomposing_motion_cvpr_2018_paper.html-----progressive video gan - masters thesis (first appeared on arxiv - oct 2018) - https://arxiv.org/abs/1810----mdp-gan: markov decision process for video generation - iccv 2019 (first appeared on arxiv - sep 2019) - https://openaccess.thecvf.com/content_iccvw_2019/html/hvu/yushchenko_markov_decision_process_for_video_generation_iccvw_2019_paper.html-----tganv2: train sparsely, generate densely - journal of computer vision 2020 - (first appeared on arxiv - nov 2018) - https://link.springer.com/article/10.1007%2fs11263-020-01333-y-----ldvd-gan: lower dimensional kernels for video discriminators - journal of neural networks 2020 - (first appeared on arxiv - dec 2019) - https://www.sciencedirect.com/science/article/abs/pii/s0893608020303397-----if we were to include unpublished preprints on arxiv in this area, this list would at least double in size.-----specific points:-----""however, one notable modality that has not seen the same level of progress in generative modeling is high fidelity natural videos.","for example, the original vq-vae work looks at a few experiments where the scene is traversed by moving 'forward' and 'right' (figure 7 in [1]).","the claim is a new method to model complex video efficiently.-----there is no experiments and/or benchmarks validating this claim anywhere in the paper.-----there is work on efficiency in the video generation field that is neither cited nor benchmarked against.-----tganv2 (https://link.springer.com/article/10.1007%2fs11263-020-01333-y) and ldvd-gan (https://www.sciencedirect.com/science/article/abs/pii/s0893608020303397) come to mind.-----""computational efficiency is a primary advantage to our method, where we can first use the vq-vae to downsample by space time before learning an autoregressive prior"" - tganv2, ldvd-gan and dvd-gan also do this .-----some questionable highlights:-----""videogen can generate realistic samples that are competitive with existing methods such as dvd-gan""-----a very weak highlight because several existing methods already do this better as shown in table 1. and dvd-gan is not the state-of-the-art for this benchmark as shown in the same table.-----"" videogen can easily be adapted for action conditional video generation""-----this is applicable to every video generation model-----""our results are achievable with a maximum of 8 quadro rtx 6000 gpus (24 gb memory), significantly lower than the resources used in prior methods such as dvd-gan""-----this claim is not experimentally validated.",0.1238938053097345,0.0178571428571428,0.079646017699115,0.079646017699115,0.3017031630170316,0.0586797066014669,0.1362530413625303,0.1362530413625303,0.3017031630170316,0.0586797066014669,0.1362530413625303,0.1362530413625303,0.1042654028436019,0.0191387559808612,0.0568720379146919,0.0568720379146919,0.2933333333333333,0.046979865771812,0.1466666666666666,0.1466666666666666,0.1654501216545012,0.0146699266503667,0.097323600973236,0.097323600973236,0.0977777777777777,0.0089686098654708,0.0711111111111111,0.0711111111111111,0.3017031630170316,0.0586797066014669,0.1362530413625303,0.1362530413625303,2.8261466026306152,11.569408416748049,16.496509552001953,8.315868377685547,11.542346954345703,8.315868377685547,8.315868377685547,5.87346887588501,0.34601876382246904,0.43473047538990955,0.9405219602397007,0.9104616227110253,0.9217628348204058,0.9170961712172024,0.9104616227110253,0.9217628348204058,0.9170962034905537,0.9714275559093307,0.9703873816907781,0.9179896273785493,0.9818020701719886,0.9657587924850078,0.8980755249853802,0.7133790661497803,0.758274654863186,0.913332948830739,0.9276036677607539,0.9318143273065325,0.8765445958016811,0.9104616227110253,0.9217628348204058,0.9170961712172024
4,https://openreview.net/forum?id=3uiR9bkbDjL,"clarity : eqn(5) and (6) introduce symbols-----1-----and-----2-----whose meaning is unclear. eqn(6) requires more clarity as it is important to understand how intermediate embeddings for cold nodes are being generated. overall the problem motivation and related work section is well described-----novelty: the proposed model is created by clubbing together gcn model for node classification and gae model for link prediction. the loss function is a weighted combination of the two. lacks sufficient novelty impact: the paper clubs together two earlier well known models and makes no additional theoretical contributions.-----correctness: the link recommendation test sets have the same no. of positive and negative ratio. this scenario is highly unlikely in a real-world setting. a more skewed dataset would throw off the performance metrics significantly. auc scores are reported. ap/rr/accuracy values should be explored as well. while i agree auc is well prevalent in community, precision captures the sensitivity in ranking much better in link prediction.-----maybe i missed the code, can the reviewer point out to it? ########################################################################## summary: the paper proposes a new task in graph learning. basically, the idea is the following: suppose we have a node classification model trained on a graph g, suppose we have a new node (not present in g) and we want to classify it. given that the new node has no connections with gs other nodes we cannot leverage any structural information to run the classifier. this is an issue that authors present it as cold-start in semi-supervised graph learning. the solution, even if simple, is very effective and for this reason even more interesting. basically, they start with a retrieval step (they call it link prediction) that is trained using a link reconstruction loss and its based on dot-product to make the link prediction phase scalable. after those links are reconstructed they run a node classification step (based on gcn, graphsage, or gat) on g plus the new node and the predicted links. the results obtained in the experiments are really encouraging with improvements ranging from 15 to 25% over a baseline that does not consider the link prediction phase. ########################################################################## reasons for score: overall, i vote for accepting. i consider this as a non-trivial step forward towards using graph learning on recommendation problems and node classification problems. there are many applications of graph learning where the technique presented in this paper can be of help. consider, for instance, all the words relying on gnns to do fake news detection based on a semi-supervised technique. all of those methods fail in the case a new document (node) has no explicit connections to the graph. this method would solve that issue. ##########################################################################pros: -----the paper tackles a problem that is, in my opinion, very important and, so far, overlooked: cold-start node prediction using graph learning.-----the technique presented in the paper is simple, which i consider a plus. it works and its simple.-----the experiments show a great improvement over the baseline----- ########################################################################## cons: -----one big limitation of this work, that is in my opinion under explored is that it is based on the assumption that links depend on the content/features of the nodes. in some cases this assumption might not hold true. i would like authors to discuss on this point.-----i am not sure how the link prediction phase could be made scalable. as it is defined now it is an o(n^2) step. or better, i have some ideas but id like authors to discuss this.-----why you pick a threshold of .5 in equation (8) shouldnt this be an hyperparam?----- ########################################################################## questions during rebuttal period: please address and clarify the cons above ######################################################################### some minor issues (1) there is an inconsistency in the notation for m in equation (4) and m in equation (5). in equation (4) m takes 3 parameters, while in (5) it takes 2. please clarify. (2) why you speak about cold nodes x_c instead of cold node? up until now it looked like you only used one node at a time. please clarify. (3) what is q() in equations (5) and (6)? (4) in figure 3, what is the black line under the blue curve? its not written anywhere. summary:-----this paper presents a model called coldexpand that addresses learning tasks related to attributed graphs. authors argue that this model is the first model able to deal with the cold-start problem, i.e. new nodes with no structure information, which explains its name. however i think they totally miss part of the recent literature on inductive approaches. coldexpand addresses the new node issue by using an architecture that combines a deep auto-encoder and a gnn classifier. experiments show that coldexpand is better for both link prediction and node classification, but (again) it is not compared to more appropriate baselines.-----##########################################################################-----reasons for score:-----i vote for rejection because of two reasons. first, i think the cold-start problem presented in this paper can be solved by using inductive approaches that has already been developed. i guess that ""inductive"" can be understood in different ways, but previous works are able to deal with new nodes that are described by attributes only (see, for instance, g2g [1] and idne [2]). second, the model looks really incremental with a simple concatenation of ae and gnn. the semi-supervised task is not clearly described (it looks as if the whole dataset is labelled). as a minor remark, i find that evaluating the model on a unique data type (here, scientific networks) shed a bad light on the ability to generalize on diverse datasets.-----[1] aleksandar bojchevski, stephan gnnemann (2018), deep gaussian embedding of graphs: unsupervised inductive learning via ranking, iclr 2018-----[2] r. brochier, a. guille and j. velcin (2020), inductive document network embedding with topic-word attention, ecir 2020-----##########################################################################pros:-----the model addresses the more realistic task of inductive prediction, in contrast to transductive approaches.-----the paper is reasonnably written.-----##########################################################################-----cons:-----litterature on inductive approaches is missing (see above).-----model looks really incremental-----it is not compared to inductive models-----##########################################################################-----questions during rebuttal period:-----please clarify the novelty of the cold-start problem addressed in this paper and the usual inductive task as addressed in [1] and [2]. besides i would like to know why it is a semi-supervised setting.-----#########################################################################-----i guess a should be mentioned in eq. (5).-----the authors should check the references (e.g., velickovic et al. has been published at iclr 2018).-----some typos:-----""in a log of variant matrix"" ""different hyperparameter""-----update: after reading the other reviews and author response, i decided to increase my grade to 6.","the reviewers agree that the paper is addressing an interesting problem (cold-start for representation learning on dynamic graphs). however, the proposed methods can be improved by proposing more novel ideas. at the moment, the proposed methods is a combination of gcn model for node classification and gae model for link prediction. in this case, some analysis or theoretical justification may make the paper more interesting. furthermore, the reviewers think the experiments can be improved. for instance, results on more datasets, more comparison methods and a different setup will strengthen the paper.",lacks sufficient novelty impact: the paper clubs together two earlier well known models and makes no additional theoretical contributions.-----correctness: the link recommendation test sets have the same no.,"as a minor remark, i find that evaluating the model on a unique data type (here, scientific networks) shed a bad light on the ability to generalize on diverse datasets.-----[1] aleksandar bojchevski, stephan gnnemann (2018), deep gaussian embedding of graphs: unsupervised inductive learning via ranking, iclr 2018-----[2] r. brochier, a. guille and j. velcin (2020), inductive document network embedding with topic-word attention, ecir 2020-----##########################################################################pros:-----the model addresses the more realistic task of inductive prediction, in contrast to transductive approaches.-----the paper is reasonnably written.-----##########################################################################-----cons:-----litterature on inductive approaches is missing (see above).-----model looks really incremental-----it is not compared to inductive models-----##########################################################################-----questions during rebuttal period:-----please clarify the novelty of the cold-start problem addressed in this paper and the usual inductive task as addressed in [1] and [2].",this is an issue that authors present it as cold-start in semi-supervised graph learning.,clarity : eqn(5) and (6) introduce symbols-----1-----and-----2-----whose meaning is unclear.,summary:-----this paper presents a model called coldexpand that addresses learning tasks related to attributed graphs.,"as a minor remark, i find that evaluating the model on a unique data type (here, scientific networks) shed a bad light on the ability to generalize on diverse datasets.-----[1] aleksandar bojchevski, stephan gnnemann (2018), deep gaussian embedding of graphs: unsupervised inductive learning via ranking, iclr 2018-----[2] r. brochier, a. guille and j. velcin (2020), inductive document network embedding with topic-word attention, ecir 2020-----##########################################################################pros:-----the model addresses the more realistic task of inductive prediction, in contrast to transductive approaches.-----the paper is reasonnably written.-----##########################################################################-----cons:-----litterature on inductive approaches is missing (see above).-----model looks really incremental-----it is not compared to inductive models-----##########################################################################-----questions during rebuttal period:-----please clarify the novelty of the cold-start problem addressed in this paper and the usual inductive task as addressed in [1] and [2].",clarity : eqn(5) and (6) introduce symbols-----1-----and-----2-----whose meaning is unclear.,"as a minor remark, i find that evaluating the model on a unique data type (here, scientific networks) shed a bad light on the ability to generalize on diverse datasets.-----[1] aleksandar bojchevski, stephan gnnemann (2018), deep gaussian embedding of graphs: unsupervised inductive learning via ranking, iclr 2018-----[2] r. brochier, a. guille and j. velcin (2020), inductive document network embedding with topic-word attention, ecir 2020-----##########################################################################pros:-----the model addresses the more realistic task of inductive prediction, in contrast to transductive approaches.-----the paper is reasonnably written.-----##########################################################################-----cons:-----litterature on inductive approaches is missing (see above).-----model looks really incremental-----it is not compared to inductive models-----##########################################################################-----questions during rebuttal period:-----please clarify the novelty of the cold-start problem addressed in this paper and the usual inductive task as addressed in [1] and [2].",0.1487603305785124,0.0168067226890756,0.1157024793388429,0.1157024793388429,0.2894736842105263,0.0442477876106194,0.131578947368421,0.131578947368421,0.1666666666666666,0.0188679245283018,0.0925925925925925,0.0925925925925925,0.0566037735849056,0.0,0.0377358490566037,0.0377358490566037,0.1481481481481481,0.0,0.074074074074074,0.074074074074074,0.2894736842105263,0.0442477876106194,0.131578947368421,0.131578947368421,0.0566037735849056,0.0,0.0377358490566037,0.0377358490566037,0.2894736842105263,0.0442477876106194,0.131578947368421,0.131578947368421,7.586188316345215,10.271551132202148,13.143911361694336,7.586188316345215,7.107095718383789,8.594274520874023,7.586188316345215,13.143911361694336,0.945189946540379,0.9563813628407181,0.054785753655647515,0.6746002110948006,0.6527409520724775,0.9178189547457236,0.9489227886553605,0.9500602566305605,0.15567618752176796,0.9462763031552303,0.9524794459917783,0.08353950707400283,0.6463985839302296,0.6704271793109278,0.9515766885208512,0.6746002110948006,0.6527409520724775,0.9178189547457236,0.9462763031552303,0.9524794459917783,0.08353950707400283,0.6746002110948006,0.6527409520724775,0.9178189547457236
5,https://openreview.net/forum?id=5Spjp0zDYt,"summary: the paper presents a characterization of failure modes of gaussian vaes. it is known that gaussian vaes can fail to produce good models either by failing to match the data distribution or by learning latent variables that are uninformative. the paper builds upon prior work that suggests that the vae objective can cause the inference model to over-regularize the generative model. the paper characterizes the conditions under which this over-regularization occurs with corresponding. furthermore, the paper examines the affect of vae pathologies on downstream tasks a number of unsupervised and supervised downstream tasks.-----the paper is clearly written and the authors have attempted cover all cases of the theoretical reasons they have identified for vae failures with corresponding experiments. the attempt to cover a number of downstream tasks is also a positive since this is a main goal of unsupervised learning.-----as for the core of the paper the authors have identified two types of vae failures: 1) when the generative quality is traded off for simple posteriors and 2) when the learning of output variance is biased. the first case is presented as occuring when the true posterior is difficult to estimate and there is no good likelhood function with a simple posterior. an expression of this is given as theorem 1.-----this brings me to my main concern: theorem 1 seems to say that if 1) the model cannot match the true posterior (gaussian assumption is false) and 2) the model cannot match p(x) without matching the true posterior (decoder is not too powerful) then the model will not match p(x). if this is the case, then i would suggest that the statement does not present any new insight into vae failures since a good vae model of p(x) would either match the true simple posterior or it will model p(x) without matching the posterior.-----another concern is that although the authors mention prior work on posterior collapse, they do not consider this when characterizing the conditions that cause pathological behvaviour in vaes. in particular it seems that the ""no good, simpler alternative"" condition explicitly excludes the case of a strong decoder that is able to ignore the latent code to model p(x) directly. this seems to be at odds with the authors' claim that they consider ""fully flexible generative and inference models"" when comparing with the prior work of yacoby et al. also, the case of strong decoder able to ignore the latent code is difficult to see as a case of the inference model over-regularizing the generative model.-----if my understanding above is correct, then i suggest that the authors revise their claim of considering fully flexible models. in particular, they should state whether or not the case of posterior collapse is handled by their characterization.-----the authors also present theorem 2 which suggests that using the elbo to choose output variance results in a biased estimate. as far as i can tell this is new and of interest.-----the authors present a number of experiments to show vae failures: when the true posterior is far from gaussian, when the true posterior is gaussian and the decoder is weak, when the posterior is far from gaussian and but the decoder is strong enough. they also show experiments to show that the output noise is overestimated when the elbo is used to compute it. one limitation is that the authors have chosen very simple datasets with 1-d latent spaces for illustration. this leads to another question of interest of whether there are problems with vaes that only appear with higher dimensional latent spaces?-----overall i found the paper to be interesting but in light of the questions mentioned above i do not recommend acceptance. this paper exposes the pathologies of vaes and characterizes them with concrete conditions. the synthetic data experiments well summarize the conditions that we might meet in real-world applications. the authors also analyze the corresponding effects for specific downstream tasks and give insightful suggestions to avoid these problems.-----the trade-off between the generative distribution and inference distribution in vaes has been studied for a long time and has been revealed from several perspectives such as information theory, etc. it is good to see in this paper that the two conditions in theorem 1 summarize well why vaes work well or poorly.-----questions: for theorem 1, when only condition 1 holds, we know-----p(x)-----might still be approximated but we may get an unwanted generative distribution since there exist several generative models that fit the data equally well. is there any solution to avoid the duplicated solution?-----in the paper, iwae is applied to avoid the issues mentioned in vaes. however, in figure 9, the learned generative model seems to be different than the ground truth, and the posterior is thus simpler than the true posterior. is there any possible explanation for this multi-modal case?-----in general, this is an interesting paper that well analyzes the training issues in vaes and provides insightful guidance to the vae studies. the paper is well written (the figure labels are too small to read) and easy to follow.-----update after the discussion stage-----i appreciate the authors' responses to address my questions in the experiments. however, i agree with the concerns of the other reviewers, especially the redundancy of theorem 1 raised by reviewer3. after reading the reviews and the discussion, the authors seem not to provide convincing responses to this part and this raises my concerns for this paper. thus i change my rating below the borderline. summary: the paper presents two analysis: (1) characterization of when the training of vaes using the elbo leads to suboptimal generative models (biased towards ones with simple posteriors); and (2) how this suboptimality may affect downstream tasks that use the learned models. specifically, the work focuses on vaes using mean-field gaussians as variational distributions, and explores how their limited modeling capacity affects the final generative model learned. they present some theoretical results and simple and illustrative scenarios. in addition, they present an analysis regarding how the suboptimal models learned affect other tasks, such as learning disentangled or compressed representations.-----pros:-----the paper is very clearly written.-----the theoretical results are novel, although based on well known ideas and maybe not very relevant from a practical perspective (see ""cons"").-----it presents an interesting exploration of how the known failures of vaes affect subsequent tasks that use the suboptimal models. i think this is quite relevant, and often does not receive as much attention as new training methods for vaes.-----cons:-----as mentioned in the paper, the failure modes of vaes are known. the paper's first contribution is a characterization of when they happen. while the theorems give precise conditions and expressions, it is not clear to me whether they are useful in practice for real scenarios. i think most of the analysis of downstream tasks do not really require the precise conditions given in the theorems, but the ideas behind them, which were already known. for instance, the analysis regarding disentangled representations states that given several models with equal likelihood, optimization will choose the one with lower kl divergence (which may be disentangled or not, depending on the scenario). this does not require the conditions from theorem 1, but knowing that for a given log p(x), decreasing the kl divergence increases the elbo (which is known, given that the tightness of the elbo bound is exactly the kl divergence).-----something similar happens with the analysis regarding compressed representations. here, it is mentioned that increasing k (dimensionality of latent space) leads to simpler functions and thus to simpler posteriors, and is therefore preferred when using mean-field gaussians approximating distributions. again, the precise conditions from the theorems do not seem to be used here. (to the authors, please let me know if i am wrong; that is, if the precise results from the theorems are actually used in these analysis and i missed it.)-----the relevance of the result in theorem 2 is not clear to me. the fact that the optimal noise parameters is not recovered is expected (it is known that optimizing the elbo leads to biases in the parameters of the generative model), albeit the expression that precisely quantifies the bias in terms of the approximating distribution was not known. however, this exact expression is not used in the subsequent analysis, only the fact that it is biased/suboptimal. thus the relevance of this result is not clear to me.-----i think that some parts of the second analysis could use further justification. for instance, consider the compressed representations part. it is not clear to me why increasing k leads to generative models with simpler posteriors. i saw the empirical analysis in the appendix, but did not find an analysis justifying the claim. am i missing something simple here?-----all in all, i do not find the first part of the analysis to be very relevant to the community. this is not the case find the second part of the analysis; i consider that studying how these suboptimalities affect other downstream tasks is important. despite the fact that i believe the analysis could be expanded, i think it represents a first step in this direction. therefore, i'm inclined to recommend acceptance. (i updated the score after the discussion.)-----one last comment, i would suggest using larger fonts in all plots.","this paper investigates various pathologies that occur when training vae models. there was quite a bit of discussion (including ""private"" discussion between the reviewers) about the theory presented. particular concerns included: for theorem 1, while the required conditions formalise the setting in which the learned likelihoods are poor, it's unclear whether these particular conditions they are useful in practice or provided deep insight; for theorem 2, its relevance and importance was not necessarily clear. in general the results in these two theorems are closely related to known challenges (e.g. that using the elbo to optimise parameters may lead to bias), without necessarily providing as much new insight as one might hope.-----i would note that all the reviewers included positive feedback as to the quality of the experiments, showing the impact of these pathologies on downstream tasks. however, as written much of the paper focuses on the theory  too many of the (very interesting!) figures and experimental results are relegated to the appendix.",the attempt to cover a number of downstream tasks is also a positive since this is a main goal of unsupervised learning.-----as for the core of the paper the authors have identified two types of vae failures: 1) when the generative quality is traded off for simple posteriors and 2) when the learning of output variance is biased.,"if this is the case, then i would suggest that the statement does not present any new insight into vae failures since a good vae model of p(x) would either match the true simple posterior or it will model p(x) without matching the posterior.-----another concern is that although the authors mention prior work on posterior collapse, they do not consider this when characterizing the conditions that cause pathological behvaviour in vaes.","if this is the case, then i would suggest that the statement does not present any new insight into vae failures since a good vae model of p(x) would either match the true simple posterior or it will model p(x) without matching the posterior.-----another concern is that although the authors mention prior work on posterior collapse, they do not consider this when characterizing the conditions that cause pathological behvaviour in vaes.",summary: the paper presents a characterization of failure modes of gaussian vaes.,summary: the paper presents two analysis: (1) characterization of when the training of vaes using the elbo leads to suboptimal generative models (biased towards ones with simple posteriors); and (2) how this suboptimality may affect downstream tasks that use the learned models.,summary: the paper presents two analysis: (1) characterization of when the training of vaes using the elbo leads to suboptimal generative models (biased towards ones with simple posteriors); and (2) how this suboptimality may affect downstream tasks that use the learned models.,summary: the paper presents a characterization of failure modes of gaussian vaes.,"if this is the case, then i would suggest that the statement does not present any new insight into vae failures since a good vae model of p(x) would either match the true simple posterior or it will model p(x) without matching the posterior.-----another concern is that although the authors mention prior work on posterior collapse, they do not consider this when characterizing the conditions that cause pathological behvaviour in vaes.",0.2678571428571428,0.036036036036036,0.125,0.125,0.2510460251046025,0.0253164556962025,0.1171548117154811,0.1171548117154811,0.2510460251046025,0.0253164556962025,0.1171548117154811,0.1171548117154811,0.0790960451977401,0.0114285714285714,0.0451977401129943,0.0451977401129943,0.2801932367149758,0.0682926829268292,0.1256038647342995,0.1256038647342995,0.2801932367149758,0.0682926829268292,0.1256038647342995,0.1256038647342995,0.0790960451977401,0.0114285714285714,0.0451977401129943,0.0451977401129943,0.2510460251046025,0.0253164556962025,0.1171548117154811,0.1171548117154811,10.195133209228516,10.195133209228516,17.7337589263916,9.20282745361328,11.839385986328123,9.20282745361328,9.20282745361328,17.7337589263916,0.8500342741599299,0.8749365796902419,0.922178922401459,0.828399647575496,0.7650930637557685,0.9251111466257776,0.828399647575496,0.7650930637557685,0.9251111466257776,0.6724588313304766,0.584980416915722,0.9351852641074436,0.664988738659196,0.6766124656949777,0.9238309077955494,0.664988738659196,0.6766124656949777,0.9238308802629192,0.6724588313304766,0.584980416915722,0.9351852641074436,0.828399647575496,0.7650930637557685,0.9251111466257776
6,https://openreview.net/forum?id=B1IzH7cxl,"the authors propose a recurrent variational neural network approach to modelling volatility in financial time series. this model consists of an application of chung et al.s (2015) vrnn model to volatility forecasting, wherein a variational autoencoder (vae) structure is repeated at each time step of the series. ----------------the paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). ----------------the papers main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. this appears to be a novel, if minor contribution. the larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. the demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful.----------------i have a few comments and reservations with the paper:--------1) although not mentioned explicitly, the authors framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. however, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. it would be interesting to discuss how this model could be extended to forecast at longer horizons.-------- --------2) in section 4.4, theres a mention that a garch(1,1) is conditionally deterministic. this is true only when forecasting 1 time-step in the future. at longer horizons, the garch(1,1) volatility forecast is not deterministic. ----------------3) i was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline garch model. however, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. it would be well advised to look into r packages such as `stochvol and fgarch to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature.----------------4) in section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension d_e (section 5.4)----------------5) in section 5.3, more details should be given on the data generating process for the synthetic data experiments. ----------------6) some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (figure 4(b) and (c), respectively around time steps 1300 and 1600). this should be explained and discussed.----------------all in all, i think that the paper provides a nice contribution to the art of volatility modelling. in spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community. the authors propose a recurrent neural network approach for constructing a--------stochastic volatility model for financial time series. they introduce an--------inference network based on a recurrent neural network that computes the--------approximation to the posterior distribution for the latent variables given the--------past data. this variational approximation is used to maximize the marginal--------likelihood in order to learn the parameters of the model. the proposed method--------is validated in experiments with synthetic and real-world time series, showing--------to outperform parametric garch models and a gaussian process volatility model.----------------quality:----------------the method proposed seems technically correct, with the exception that in--------equation (19) the inference model is doing filtering and not smoothing, in the--------sense that the posterior for z_t' only depends on those other z_t and x_t--------values with t<t', but in the true posterior p(z|x) each z_t depends on all the--------x. this means the proposed learning method is inefficient. the reference below--------shows how to perform smoothing too and the results in that paper show indeed that--------smoothing produces better results for learning the model.----------------sequential neural models with stochastic layers fraccaro, marco and s\o nderby,--------s\o ren kaae and paquet, ulrich and winther, ole in nips 2016.----------------it is not clear if the method proposed in the above reference would perform better --------just because of using smoothing when learning the model parameters.----------------clarity:----------------the paper is clearly written and easy to read.----------------for the results on real-world data, in table 1, how is the nll computed? is the--------average nll across the 162 time series?----------------originality:----------------the method proposed is not very original. previous work has already used the--------variational approach with the reparametrization trick to learn recurrent neural--------networks with stochastic units (see the reference above). it seems that the main--------contribution of the authors is to apply this type of techniques to the problem--------of modeling financial time series.----------------significance:----------------the results shown are significant, the method proposed by the authors--------outperforms previous approaches. however, there is a huge amount of techniques--------available for modeling financial time-series. the number of garch variants is--------probably close to hundreds, each one claiming to be better than the others.--------this makes difficult to quantify how important the results are. thank you for an interesting read.----------------i found the application of vrnn type generative model to financial data very promising. but since i don't have enough background knowledge to judge whether the performance gap is significant or not, i wouldn't recommend acceptance at this stage. ----------------to me, the biggest issue for this paper is that i'm not sure if the paper contains significant novelty. the rnn-vae combination has been around for more than a year and this paper does not propose significant changes to it. maybe this paper fits better to an application targeting conference, rather than iclr. but i'm not exactly sure about iclr's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?","this paper presents an interesting application of variational methods for time series, in particular vrnn-like approaches, for stochastic volatility. applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. that said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. also, the number of methods grach and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. this paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference.","the larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms.","the reference below--------shows how to perform smoothing too and the results in that paper show indeed that--------smoothing produces better results for learning the model.----------------sequential neural models with stochastic layers fraccaro, marco and s\o nderby,--------s\o ren kaae and paquet, ulrich and winther, ole in nips 2016.----------------it is not clear if the method proposed in the above reference would perform better --------just because of using smoothing when learning the model parameters.----------------clarity:----------------the paper is clearly written and easy to read.----------------for the results on real-world data, in table 1, how is the nll computed?","it seems that the main--------contribution of the authors is to apply this type of techniques to the problem--------of modeling financial time series.----------------significance:----------------the results shown are significant, the method proposed by the authors--------outperforms previous approaches.",the authors propose a recurrent variational neural network approach to modelling volatility in financial time series.,"the proposed method--------is validated in experiments with synthetic and real-world time series, showing--------to outperform parametric garch models and a gaussian process volatility model.----------------quality:----------------the method proposed seems technically correct, with the exception that in--------equation (19) the inference model is doing filtering and not smoothing, in the--------sense that the posterior for z_t' only depends on those other z_t and x_t--------values with t<t', but in the true posterior p(z|x) each z_t depends on all the--------x. this means the proposed learning method is inefficient.",the authors propose a recurrent neural network approach for constructing a--------stochastic volatility model for financial time series.,"but since i don't have enough background knowledge to judge whether the performance gap is significant or not, i wouldn't recommend acceptance at this stage.","the proposed method--------is validated in experiments with synthetic and real-world time series, showing--------to outperform parametric garch models and a gaussian process volatility model.----------------quality:----------------the method proposed seems technically correct, with the exception that in--------equation (19) the inference model is doing filtering and not smoothing, in the--------sense that the posterior for z_t' only depends on those other z_t and x_t--------values with t<t', but in the true posterior p(z|x) each z_t depends on all the--------x. this means the proposed learning method is inefficient.",0.1438848920863309,0.0145985401459854,0.1007194244604316,0.1007194244604316,0.2816901408450704,0.0379146919431279,0.1408450704225352,0.1408450704225352,0.2516556291390728,0.0268456375838926,0.1324503311258278,0.1324503311258278,0.140625,0.0158730158730158,0.0625,0.0625,0.2980769230769231,0.0194174757281553,0.1442307692307692,0.1442307692307692,0.1384615384615384,0.046875,0.0923076923076923,0.0923076923076923,0.1151079136690647,0.0145985401459854,0.0863309352517985,0.0863309352517985,0.2980769230769231,0.0194174757281553,0.1442307692307692,0.1442307692307692,16.01925277709961,11.077518463134766,16.314001083374023,10.520236015319824,5.840713977813721,13.748124122619627,11.077518463134766,2.6874895095825195,0.9485910012253206,0.954756712849641,0.9240748661375594,0.5150433020682276,0.5885249032056703,0.8931596383213607,0.9363158589331549,0.9499283522489501,0.9573382613542033,0.9707435258221188,0.9709721794086563,0.890089625590196,0.9224067988564542,0.9293120228159056,0.914963122069462,0.9765969138020018,0.9770604038492087,0.817246365812338,0.06258155308531495,0.14263407433108816,0.8965261544296912,0.9224067988564542,0.9293120228159056,0.9149633486050646
7,https://openreview.net/forum?id=B1eWOJHKvB,"this is an interesting, timely study. cyclegan has attracted a lot of attention in unpaired image-to-image translation. although the basic idea of cyclegan seem sensible, its precise behavior is not totally clear--can one really avoid mismatch with cyclegan? do we need additional constraints? this paper provides a nice answer the the first question.----------overall i enjoyed reading this paper. the addressed issue is important, the investigation is reasonable, and the results are intuitive and plausible, with clear practical implications. i think it is a good paper.---------- i acknowledge i read the authors' response and other reviews and would like to keep my original rating. this paper focuses on cyclegan method to show theoretically when the exact solution space is invariant with respect to authomorphisms of the underlying probability spaces for unpaired image-to-image translation. ----------- the paper provides interesting theoretical results on identifying conditions under which cyclegan admits nontrivial symmetries and has a natural structure of a principal homogeneous space. proposition 2.1 provides interesting insights into the invariance of the kernel space. ----------- propositions 2.5 and 2.6 are interesting in that they show that the existence of authomorphisms can worsen the performance of cyclegan, however, it is unclear that in practice, how could one verify the conditions efficiently before applying cyclegan.----------- the experimental results are interesting, however, they are very limited. having a toy experiment is a good sanity check, but it would be more interesting to see the performance on a real-world applications, such as medical images or other use-cases brought in the introduction. also, more discussion on the results provided in fig 3, confusion matrices, would be very helpful. are there any intuitions behind the large and low values in the table? it could be interesting to see what are the effects of other parameters such as alpha in producing the results. ----------- overall this paper presents interesting results regarding the theory of cyclegans, however, the numerical results are very limited, and do not justify the motivations discussed in the introduction and the abstract. moreover, although the paper introduces novel attempts and theoretically analyzing the cyclegan, the scope of the work seems to be limited, and thus, it does not have a sufficient significance to be published in iclr. i strongly suggest the authors to expand and provide more experimental evaluations.----------** update:-----thanks for your comments! i found the additional experiment useful and better aligned with the purpose of the model. the discussion added clarified the confusion about the automorphism. that is why i decided to change my score. i have read the rebuttal of the authors . thank you for you answer and for addressing some concerns. while the question addressed is important, the theory presented here does not seem to hint to a solution, hence i am keeping my score. ----------###-----summary of the paper: ----------this paper shows that the cycle gan loss suffers from the presence of lot of symmetries that make the existence of a unique solution not possible , and moreover adding a regularizer that uses the identity loss is not enough to make the problem less prone to those invariances. ----------review of the paper: ----------the notations and the formalism in the paper are heavy and cumbersome and don't come with any surprising result, since the transforms between unpaired spaces will be found always up to symmetries since we have the composition of one map with another. the use of the identity loss is also shown to not to help either in fixing this invariance issue. ----------experiments are not interesting since without any structure on the map of f and g , the source domain and the target domain, one is expected to get permutations.----------the paper points in the conclusion that the use of skipconnection in f and g has the major influence.---------- the study of cycle gan might need some assessment of what is the mutual information between the domains , as on what information needs be preserved , and information needs to match , skip connection maintain the content in image generation as the information is kept from lower layer and its modified to target the style of the target images. ----------an information theoretic analysis of cycle gan is needed using for example the objective of ""miso: mutual information loss with stochastic style representations for multimodal image-to-image translation"". -----or by using a radically new approach for cycle gan such as the gromov wasserstein distance as done in "" learning generative models across incomparable spaces""","this paper theoretically studied one of the fundamental issue in cyclegan (recently gained much attention for image-to-image translation). the authors analyze the space of exact and approximated solutions under automorphisms.----------reviewers mostly agree with theoretical value of the paper. some concerns on practical values are also raised, e.g., limited or no-surprising experimental results. in overall, i think this is a boarderline paper. but, i am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation.",----------- the paper provides interesting theoretical results on identifying conditions under which cyclegan admits nontrivial symmetries and has a natural structure of a principal homogeneous space.,"----------experiments are not interesting since without any structure on the map of f and g , the source domain and the target domain, one is expected to get permutations.----------the paper points in the conclusion that the use of skipconnection in f and g has the major influence.---------- the study of cycle gan might need some assessment of what is the mutual information between the domains , as on what information needs be preserved , and information needs to match , skip connection maintain the content in image generation as the information is kept from lower layer and its modified to target the style of the target images.","----------- overall this paper presents interesting results regarding the theory of cyclegans, however, the numerical results are very limited, and do not justify the motivations discussed in the introduction and the abstract.","this is an interesting, timely study.",----------- the paper provides interesting theoretical results on identifying conditions under which cyclegan admits nontrivial symmetries and has a natural structure of a principal homogeneous space.,"----------experiments are not interesting since without any structure on the map of f and g , the source domain and the target domain, one is expected to get permutations.----------the paper points in the conclusion that the use of skipconnection in f and g has the major influence.---------- the study of cycle gan might need some assessment of what is the mutual information between the domains , as on what information needs be preserved , and information needs to match , skip connection maintain the content in image generation as the information is kept from lower layer and its modified to target the style of the target images.","this is an interesting, timely study.","----------- overall this paper presents interesting results regarding the theory of cyclegans, however, the numerical results are very limited, and do not justify the motivations discussed in the introduction and the abstract.",0.2051282051282051,0.017391304347826,0.1025641025641025,0.1025641025641025,0.2857142857142857,0.0309278350515463,0.173469387755102,0.173469387755102,0.2601626016260163,0.0165289256198347,0.1626016260162601,0.1626016260162601,0.0612244897959183,0.0208333333333333,0.0408163265306122,0.0408163265306122,0.2051282051282051,0.017391304347826,0.1025641025641025,0.1025641025641025,0.2857142857142857,0.0309278350515463,0.173469387755102,0.173469387755102,0.0612244897959183,0.0208333333333333,0.0408163265306122,0.0408163265306122,0.2601626016260163,0.0165289256198347,0.1626016260162601,0.1626016260162601,9.15134048461914,11.507043838500977,5.602260112762451,9.15134048461914,11.507043838500977,14.648296356201172,14.64829444885254,5.602260112762451,0.980003810309443,0.9745135731744123,0.8455503491435622,0.8709381329853733,0.872494947519395,0.8869171532368895,0.9723131577271228,0.950930981027034,0.877414232992334,0.9637451391195756,0.9630169566347129,0.9445600938098736,0.980003810309443,0.9745135731744123,0.8455501955251989,0.8709381329853733,0.872494947519395,0.8869171532368895,0.9637451391195756,0.9630169566347129,0.9445600938098736,0.9723131577271228,0.950930981027034,0.877414200305617
8,https://openreview.net/forum?id=B1esygHFwS,"i am quite disappointed with the presentation and technical quality of the paper. ----------there are numerous grammatical errors that make the reading unpleasant. the mathematical notations are also inconsistent throughout different places in the paper.----------the extensive literature of modelling time series with seasonality trends, both in the statistics and the machine learning community, is severely under-represented in the motivations and related works. models like sarima have no mention in the paper. ----------the temporal regularization imposed in section 3.2, coupled with an autoencoder, does not seem very different from the state-space models and their more complex and recent variants that use multi-layered networks (a google search will provide plenty references). ----------the experiments, with many of the useful baselines missing, are equally unimpressive. the paper investigates the important problem of detecting changes in seasonal patterns, and proposes atr-cspd to learn a low-dimensional representation of the seasonal pattern and then detects changes with clustering-based approaches. atr-cspd achieves improved results on part of synthetic and real-world datasets.----------the paper may not have enough contribution to be accepted due to the following key concerns:----- - the proposed model is not quite novel, and the design needs more justification.----- - the empirical results are not strong enough to show the effectiveness of atr-cspd. ----------# model design----------the idea of using auto-encoder with temporal smoothing to learn a low-dimensional representation of time-series need more justification.------ what are the main intuitions of using an auto-encoder? e.g., removing anomaly or denoising. why will it be easier for the model to detect the changes on the reconstructed time-series?------ the temporal smoothing makes adjacent periods similar to each other. however, this may have side effects like low recall. for example, in figure 2(a), the pattern in aug 17th (sat) and that in aug 18th(sun) can possibly be different (i.e., a change in seasonal pattern), while the difference is smoothed out by the temporal regularization. is the model sensitive to the regularization, e.g., ? why l1 regularization instead of l2 is used? it will be helpful to provide more justification/intuition of the model design.------ why only the smoothness regularization between adjacent seasons is used? other potential regularization includes penalizing the difference between the same phase in different seasons. ----------# assumption and limitation-----the proposed method requires the seasonal period being provided, and also requires a large number of hyperparameters being specified, e.g., 1) the threshold of silhouette score, 2) the hidden representation dimension , 3) the regularization coefficient, 4) , 5) hyperparameters for constructing the encoder/decoder and 6) training the models. ----------regarding the threshold of the silhouette score in the clustering step, is setting this hyperparameter easier than the number of clusters, i.e., the number of changing points? is atr-cspd sensitive to this parameter? how this hyperparameter is tuned? having too many hyperparameters (that are potentially non-trivial to set/tune) may make the proposed method less robust. ----------# experimental results: -----according to the results in table 1, atr-cspd is mainly better at detecting category c/d/e change points, which are mainly caused by changes of height/position of the spike. however, it performs either similarly or worse than the other baselines on other tasks. besides, the lack of ground-truth data on nyc taxi dataset and the azure monitor dataset makes it hard to evaluate the effectiveness of the proposed algorithm. moreover, only uni-variate time series tasks are investigated. these issues may limit the application domain of the proposed algorithm. ----------# minor notation and presentation issues------ in definition 2, does cspd assume f is the same in ----- and -----? if not, cpd might be a subset of cspd. ----------- in equation 1 and 2, is used to represent the number of observations, while in definition 1, capital is used to represent the same concept.----------- in figure 2, it might be easier to understand if all the weekdays are drawn using the same color (blue or green) and all the weekends are also drawn in the same color (yellow or red). this paper proposed a new model for change point detection, using autoencoders with temporal regularization, in order to impose temporal smoothness in the latent codes. to motivate this new model, the authors also provided a toy example to show how the abnormality in a time series is removed in the reconstructed signal using this additional regularization term. experimental results were provided to support the proposed new model.----------i have a few concerns about some technical details of the paper, as explained below:-----1) the paper motivated the new model with difficulty in detecting change points in seasonal time series. however, the proposed model with the temporal regularization is not directly related to the seasonality or periodicity of the input data. it is more related to the smoothness of the latent code. hence it seems to me that there is a slight disconnection between the motivation and the actually proposed model. it would be nice if the authors can provide more intuitive explanation on why the temporal regularization can handle well change point detection in seasonal temporal series.----------2) the temporal regularization proposed in this paper is very similar to the total variation penalty used extensively in statistics and image processing. it would be nice if the authors can make a connection between the two. for example:-----harchaoui, z., & lvy-leduc, c. (2010). multiple change-point estimation with a total variation penalty. journal of the american statistical association, 105(492), 1480-1493.-----beck, a., & teboulle, m. (2009). fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. ieee transactions on image processing, 18(11), 2419-2434.----------3) in the experimental section 4.3, the authors mentioned that ""in table 2, ... we can see that our algorithm outperforms the baseline model"". however, in table 2 the precision of the proposed model (67%) is lower than the baseline model (68%). hence it is not obvious to the reader that the proposed model outperforms the baseline. ----------4) in the appendix b, the authors explained the architecture of the autoencoder used in the paper. i wonder why the authors chose an asymmetric structure between the encoder and decoder, as most autoencoders have a symmetric structure.","the paper proposes atr-cspd, which learns a low-dimensional representation of seasonal pattern, for detecting changes with clustering-based approaches. ----------while atr-cspd is simple and intuitive, it lacks novel contribution in methodology. it is unclear how it is different from existing approaches. the evaluation and the writing could be improved significantly. ----------in short, the paper is not ready for publication. we hope the reviews can help improve the paper for a strong submission in the future.","the mathematical notations are also inconsistent throughout different places in the paper.----------the extensive literature of modelling time series with seasonality trends, both in the statistics and the machine learning community, is severely under-represented in the motivations and related works.",it would be nice if the authors can provide more intuitive explanation on why the temporal regularization can handle well change point detection in seasonal temporal series.----------2) the temporal regularization proposed in this paper is very similar to the total variation penalty used extensively in statistics and image processing.,it would be nice if the authors can provide more intuitive explanation on why the temporal regularization can handle well change point detection in seasonal temporal series.----------2) the temporal regularization proposed in this paper is very similar to the total variation penalty used extensively in statistics and image processing.,i am quite disappointed with the presentation and technical quality of the paper.,it would be nice if the authors can provide more intuitive explanation on why the temporal regularization can handle well change point detection in seasonal temporal series.----------2) the temporal regularization proposed in this paper is very similar to the total variation penalty used extensively in statistics and image processing.,it would be nice if the authors can provide more intuitive explanation on why the temporal regularization can handle well change point detection in seasonal temporal series.----------2) the temporal regularization proposed in this paper is very similar to the total variation penalty used extensively in statistics and image processing.,"moreover, only uni-variate time series tasks are investigated.",it would be nice if the authors can provide more intuitive explanation on why the temporal regularization can handle well change point detection in seasonal temporal series.----------2) the temporal regularization proposed in this paper is very similar to the total variation penalty used extensively in statistics and image processing.,0.3025210084033614,0.0512820512820512,0.1848739495798319,0.1848739495798319,0.28125,0.0158730158730158,0.140625,0.140625,0.28125,0.0158730158730158,0.140625,0.140625,0.1318681318681318,0.0224719101123595,0.1098901098901098,0.1098901098901098,0.28125,0.0158730158730158,0.140625,0.140625,0.28125,0.0158730158730158,0.140625,0.140625,0.0,0.0,0.0,0.0,0.28125,0.0158730158730158,0.140625,0.140625,9.36672019958496,9.36672019958496,14.450206756591797,9.36672019958496,12.894264221191406,9.36672019958496,9.36672019958496,6.774904251098633,0.9453994069095124,0.9544768871617224,0.9146135888048834,0.7189238549149665,0.7646047308824222,0.702101824401749,0.7189238549149665,0.7646047308824222,0.702101824401749,0.9576559228218956,0.9619549429189008,0.9478326827671969,0.7189238549149665,0.7646047308824222,0.702101824401749,0.7189238549149665,0.7646047308824222,0.702102065997831,0.9543180307043017,0.9592212408121782,0.8911412737379772,0.7189238549149665,0.7646047308824222,0.702101824401749
9,https://openreview.net/forum?id=B1i7ezW0-,"in summary, the paper is based on a recent work balestriero & baraniuk 2017 to do semi-supervised learning. in balestriero & baraniuk, it is shown that any dnn can be approximated via a linear spline and hence can be inverted to produce the ""reconstruction"" of the input, which can be naturally used to do unsupervised or semi-supervised learning. this paper proposes to use automatic differentiation to compute the inverse function efficiently. the idea seems interesting. however, i think there are several main drawbacks, detailed as follows:----------------1. the paper lacks a coherent and complete review of the semi-supervised deep learning. herewith some important missing papers, which are the previous or current state-of-the-art.----------------[1] laine s, aila t. temporal ensembling for semi-supervised learning[j]. arxiv preprint arxiv:1610.02242, iclr 2016.--------[2] li c, xu k, zhu j, et al. triple generative adversarial nets[j]. arxiv preprint arxiv:1703.02291, nips 2017.--------[3] dai z, yang z, yang f, et al. good semi-supervised learning that requires a bad gan[j]. arxiv preprint arxiv:1705.09783, nips 2017.----------------besides, some papers should be mentioned in the related work such as kingma et. al. 2014. i'm not an expert of the network inversion and not sure whether the related work of this part is sufficient or not.----------------2. the motivation is not sufficient and not well supported. ----------------as stated in the introduction, the authors think there are several drawbacks of existing methods including ""training instability, lack of topology generalization and computational complexity."" based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not. the generative approaches based on vaes and gans are time consuming, but according to my experience, the training of vae-based methods are stable and the topology generalization ability of such methods are good. besides, the feed-forward approaches including [1] mentioned above are efficient and not too sensitive with respect to the network architectures. overall, i think the drawbacks mentioned in the paper are not common in existing methods and i do not see clear benefits of the proposed method. again, i strongly suggest the authors to provide a complete review of the literature.----------------further, please explicitly support your claim via experiments. for instance, the proposed method should be compared with the discriminative approaches including vat and [1] in terms of the training efficiency. it's not fair to say gan-based methods require more training time because these methods can do generation and style-class disentanglement while the proposed method cannot.----------------3. the experimental results are not so convincing. ----------------first, please systematically compare your methods with existing methods on the widely adopted benchmarks including mnist with 20, 100 labels and svhn with 500, 1000 labels and cifar10 with 4000 labels. it is not safe to say the proposed method is the state-of-the-art by only showing the results in one setting.----------------second, please report the results of the proposed method with comparable architectures used in previous methods and state clearly the number of parameters in each model. resnet is powerful but previous methods did not use that.----------------last, show the sensitive results of the proposed method by tuning alpha and beta. for instance, please show what is the actual contribution of the proposed reconstruction loss to the classification accuracy with the other losses existing or not?----------------i think the quality of the paper should be further improved by addressing these problems and currently it should be rejected. after reading the revision:----------------the authors addressed my detailed questions on experiments. it appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution.----------------on the other hand, the theoretical part of the paper is not really improved in my opinion, i still can not see how previous work by balestriero and baraniuk 2017 motivates and backups the proposed method.----------------my rating of this paper would remain the same. ----------------============================================================================================----------------this paper propose to use the reconstruction loss, defined in a somewhat unusual way, as a regularizar for semi-supervised learning.----------------pros:----------------the intuition is that the relu network output is locally linear for each input, and one can use the conjugate mapping (which is also linear) for reconstructing the inputs, as in pca. realizing that the linear mapping is the derivative of network output w.r.t. the input (the jacobian), the authors proposed to use the reconstruction loss defined in (8). different from typical auto-encoders, this work does not require another reconstruction network, but instead uses the ""derivative"". this observation is neat in my opinion, and does suggest a different use of the jacobian in deep learning. the related work include auto-encoders where the weights of symmetric layers are tied. ----------------cons:----------------the motivation (section 2) needs to be improved. in particular, the introduction/review of the work of balestriero and baraniuk 2017 not very useful to the readers. notations in eqns (2) and (3) are not fully explained (e.g., boldface c). intuition and implications of theorem 1 is not sufficiently discussed: what do you mean by optimal dnn, what is the criteria for optimality? is there a generative assumption of the data underlying the theorem? and the assumption of all samples being norm 1 seems too strong and perhaps limits its application? as far as i see, section 2 is somewhat detached from the rest of the paper.----------------the main contribution of this paper is supposed to be the reconstruction mapping (6) and its effect in semi-supervised learning. the introduction of entropy regularization in sec 2.3 seems somewhat odd and obscures the contribution. it also bears the questions that how important is the entropy regularization vs. the reconstruction loss. in experiments, results with beta=1.0 need to be presented to assess the importance of network inversion and the reconstruction loss. also, a comparison against typical auto-encoders (which uses another decoder networks, with weights possibly tied with the encoder networks) is missing.","the paper proposes a novel approach for dnn inversion mainly targeted towards semi-supervised learning. however the semi-supervised learning results are not competitive enough. although the authors mention in the author-response that semi-supervised learning is not the main goal of the paper, the experiments and claims of the paper are mainly targeted towards semi-supervised learning. as the approach for inversion is novel, the paper could be motivated from a different angle with appropriate supporting experiments. in its current form it's not suitable for publication.","in balestriero & baraniuk, it is shown that any dnn can be approximated via a linear spline and hence can be inverted to produce the ""reconstruction"" of the input, which can be naturally used to do unsupervised or semi-supervised learning.","it appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution.----------------on the other hand, the theoretical part of the paper is not really improved in my opinion, i still can not see how previous work by balestriero and baraniuk 2017 motivates and backups the proposed method.----------------my rating of this paper would remain the same.","in summary, the paper is based on a recent work balestriero & baraniuk 2017 to do semi-supervised learning.","in summary, the paper is based on a recent work balestriero & baraniuk 2017 to do semi-supervised learning.","arxiv preprint arxiv:1703.02291, nips 2017.--------[3] dai z, yang z, yang f, et al. good semi-supervised learning that requires a bad gan[j].","it appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution.----------------on the other hand, the theoretical part of the paper is not really improved in my opinion, i still can not see how previous work by balestriero and baraniuk 2017 motivates and backups the proposed method.----------------my rating of this paper would remain the same.","based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not.","it is not safe to say the proposed method is the state-of-the-art by only showing the results in one setting.----------------second, please report the results of the proposed method with comparable architectures used in previous methods and state clearly the number of parameters in each model.",0.2170542635658914,0.0472440944881889,0.124031007751938,0.124031007751938,0.3144654088050314,0.089171974522293,0.1886792452830188,0.1886792452830188,0.1495327102803738,0.0571428571428571,0.1121495327102803,0.1121495327102803,0.1495327102803738,0.0571428571428571,0.1121495327102803,0.1121495327102803,0.0869565217391304,0.0353982300884955,0.0869565217391304,0.0869565217391304,0.3144654088050314,0.089171974522293,0.1886792452830188,0.1886792452830188,0.1238938053097345,0.018018018018018,0.1238938053097345,0.1238938053097345,0.2753623188405797,0.0441176470588235,0.1594202898550724,0.1594202898550724,6.411610126495361,3.99296236038208,8.612386703491211,6.411610126495361,10.941631317138672,8.612386703491211,6.36205005645752,5.920459747314453,0.9678620530324822,0.968320197888494,0.8926004850006636,0.2304960463734501,0.2787497955856271,0.9223503740836504,0.9651553751863823,0.9565703398193698,0.3924383795567935,0.9651553751863823,0.9565703398193698,0.3924383795567935,0.9356753010734277,0.9384790381149805,0.22687468009122813,0.2304960463734501,0.2787497955856271,0.9223504658056814,0.9528382624979245,0.958287804007217,0.9074434283489133,0.9616374814782876,0.9666935357853589,0.8731891320301846
10,https://openreview.net/forum?id=B1lnjo05Km,"authors present a novel regularizer to impose graph structure upon hidden layers of a neural network. the intuition is that neural networks has typically symmetric computation among different channels in one layer. due to the lack of order, visually inspecting the hidden representation is not feasible. by adding edges one can impose a structure upon nodes in one layer and add for example a laplacian regularizer rather than simple l2 norm regularizer to force the activations to follow the imposed structure. ----------------pros: ----------------interesting idea for bringing some benefits of graphical models into neural networks using a regularizer.----------------experiments verify that one can successfully improve the intrepretability of hidden representations. also, they provide examples of use cases for such technique like aligning the capsule dimmensions. ----------------cons:----------------the major flaw is the lack of comparison with ``any'' of the related work on interpretability or the prior work on imposing structure upon hidden representations. also, the manuscripts lacks a clear discussion of where does this work stands in the literature like structured vaes, graphical models, sum product nets + factor graphs. ----------------also, in none of the experiments authors mention how the added regularizer affects the model performance. whether imposing the grid structure on cnn (last experiment) drops the cnn accuracy or has no effect? same for the capsnet.----------------furthermore, the feasibility of calculating the laplacian for larger scale hidden layers or approximating it is not addressed. the paper introduces a spectral regularization with the aim of obtaining representations--------that are easier to interpret.----------------some sentences are often confusing and, in general, clarity needs to be improved.----------------the motivation of the work is not very strong in my opinion, in particular by adding such--------a prior the space of possible solutions greatly shrinks and i am afraid--------that interesting solutions will be lost. i think one should focus on properties--------rather than visual inspection.--------also, isn't it that if we can clearly see the pattern, perhaps that pattern is--------linear and of easy discovery also by simpler models?----------------more importantly, it seems that all experiments are performed on tasks where the--------underlying structure is known, however this is almost never the case in practice.--------assuming one uses the proposed spectral regularization, how would one interpret--------it in such cases?----------------in section 2 please clarify the paragraph on bounded lp norm.----------------i am sorry but why isn't there a relation, for convolutional nets,--------between neurons in different channels? each element in the feature map represents--------the input surrounding that location in a k dimensional space.----------------the authors state that the usual bottleneck for autoencoders is composed of 2/3--------neurons, this is simply not true. there has been extensive work on--------overcomplete representations that shows that is better to have many more dimensions--------but only few degrees of freedom.----------------the spectral bottleneck should cite vqvae as the approach is very similar and the --------authors should compare to it.----------------for the topological inference experiment it is assumed that one knows the structure,--------but how to address the more general problem?--------more practically, the regularization enforces smoothing (if few eigenfunctions--------are used, which is never explained in the paper) between connected nodes, did--------the authors try to have a simple l2 penalty instead? e.g. minimize the difference--------between activations in the group.----------------regarding the capsule network example, when you write that without regularization--------each digit responds differently to perturbation of the same dimension, isn't it--------possibly true only up to a, unknown, permutation of the neurons?----------------to summarize, while the idea sounds interesting, i miss to find the easy interpretability--------of results and also the overall motivation sounds a bit weak. --------more importantly the selection of w, crucial for defining structure, is not discussed at all in the paper.--------experiments are performed on toy examples only whereas here, given that we can--------possibly interpret the results i would have liked something more involved to--------better show that this kind of interpretability is needed.----------------missing cites:--------[1] van den oord et al, neural discrete representation learning.--------[2] koutnik et al, evolving neural networks in compressed weight space. authors highlight the contribution of graph spectral regularizer to the interpretability of neural networks. specifically, authors consider the laplacian smoothing regularizer to enhance the local consistency/smoothness between a neuron and its neighbors. furthermore, by extending the graph fourier transformation to overcomplete dictionary representation, authors further propose a spectral bottleneck regularizer. experimental results show that when suitable structural information and corresponding regularizers are imposed, the interpretability of the intermediate layers is improved.----------------my main concern is that the power of graph-based regularizer has been well-known in the ml community for a long time. it is not surprising that adding such regularizers to the training process of neural networks can help to get more structural activations. the key points are ----------------1) how to define the laplacian graph for the neurons? for the simple case shown in figures 1 and 2, the topology of the neurons has been predefined and the functionality of them is predefined implicitly. for more challenging cases, how to build the laplacian graph reasonably? ----------------2) how to add the regularizers with good scalability? the complexity of the proposed regularizers is o(n^2) where n is the number of neurons. when the layers contains thousands of neurons or more, how to add the regularizers efficiently?----------------3) which regularizer should be selected? authors propose a class of graph spectral regularizers and their performance is different in different tasks. is there any strategy helping us to select suitable regularizers for specific tasks?----------------unfortunately, authors provide little analysis on these key points.","the work presents a method of imposing harmonic structural regularizations to layers of a neural network. while the idea is interesting, the reviewers point out multiple issues. pros: + interesting method + hidden layer coherence tends to improve cons: - deficient comparisons to baselines or context with other works. - insufficient assessment of impact to model performance. - lack of strategy to select regularizers - lack of evaluation on more realistic datasets","each element in the feature map represents--------the input surrounding that location in a k dimensional space.----------------the authors state that the usual bottleneck for autoencoders is composed of 2/3--------neurons, this is simply not true.",by adding edges one can impose a structure upon nodes in one layer and add for example a laplacian regularizer rather than simple l2 norm regularizer to force the activations to follow the imposed structure.,authors present a novel regularizer to impose graph structure upon hidden layers of a neural network.,authors present a novel regularizer to impose graph structure upon hidden layers of a neural network.,by adding edges one can impose a structure upon nodes in one layer and add for example a laplacian regularizer rather than simple l2 norm regularizer to force the activations to follow the imposed structure.,"i think one should focus on properties--------rather than visual inspection.--------also, isn't it that if we can clearly see the pattern, perhaps that pattern is--------linear and of easy discovery also by simpler models?----------------more importantly, it seems that all experiments are performed on tasks where the--------underlying structure is known, however this is almost never the case in practice.--------assuming one uses the proposed spectral regularization, how would one interpret--------it in such cases?----------------in section 2 please clarify the paragraph on bounded lp norm.----------------i am sorry but why isn't there a relation, for convolutional nets,--------between neurons in different channels?","when the layers contains thousands of neurons or more, how to add the regularizers efficiently?----------------3) which regularizer should be selected?",by adding edges one can impose a structure upon nodes in one layer and add for example a laplacian regularizer rather than simple l2 norm regularizer to force the activations to follow the imposed structure.,0.116504854368932,0.0,0.0970873786407767,0.0970873786407767,0.2178217821782178,0.0202020202020202,0.1188118811881188,0.1188118811881188,0.2926829268292683,0.175,0.2195121951219512,0.2195121951219512,0.2926829268292683,0.175,0.2195121951219512,0.2195121951219512,0.2178217821782178,0.0202020202020202,0.1188118811881188,0.1188118811881188,0.1395348837209302,0.0,0.0813953488372093,0.0813953488372093,0.2298850574712643,0.0,0.1379310344827586,0.1379310344827586,0.2178217821782178,0.0202020202020202,0.1188118811881188,0.1188118811881188,8.207996368408203,9.908631324768066,15.690637588500977,9.90863037109375,9.625597953796388,15.690637588500977,9.908631324768066,13.665075302124023,0.9735509718764177,0.9763154598777527,0.9172406535640842,0.9642603530174757,0.9684594386473873,0.8874783256765538,0.9746431652059698,0.9752180474535054,0.8253448233416878,0.9746431652059698,0.9752180474535054,0.8253448233416878,0.9642603530174757,0.9684594386473873,0.8874783256765538,0.9826480932626199,0.9868195246660372,0.9318589492876089,0.6265030473941973,0.7762442809280503,0.6955285697106484,0.9642603530174757,0.9684594386473873,0.8874783256765538
11,https://openreview.net/forum?id=B1lqDertwr,"the paper provides an empirical study of regularization in policy optimization methods in multiple continuous control tasks. the paper focuses on the effect of conventional regularization on performance in training environments, not generalization ability to different (but similar) testing environments. their findings suggest that l2 and entropy regularization can improve the performance, be robust to hyperparameters on the tasks studied in the paper. ----------overall, the paper is well written. however, i am leaning to reject this paper because (1) the experimental finding is not well justified (2) the experiments are missing some details and do not provide convincing evidence. ----------first, the paper does not well justify why regularization methods improve performance in training environments. one potential reason is discussed in section 7: regularization can improve generalization to unseen samples. however, the improvement can simply due to better hyperparemer optimization. when we introduce more hyperparemers and computation compared to baselines, its not surprising to see a better performance, especially in deep rl where using a different seed or using a different implementation can have significant difference in performance [1]. moreover, it is unclear that inability to generalize to unseen samples is a problem in the continuous control tasks evaluated in the paper. i think the paper should demonstrate that this is indeed a problem. if it is not a problem, why would you expect regularization to help?----------there are some missing details which makes it difficult to draw conclusion:-----1. how was \sigma_{env,r} computed? is it the standard error of the mean return, or the standard deviation of the return? -----2. what does the average rank mean (in table 2 and 3)? the average ranking over 5 seeds and all environments? if so, does it make sense to compare these numbers? e.g. algorithm a with rank 1, 1, 7, 7 and algorithm b with rank 4, 4, 4, 4 have the same average rank, but totally different performance. -----3. the experiment in figure 3 seems very interesting, however, whats the conclusion here? -----4. why do you use difference hyperparamer ranges (lambda for l2, l1 and entropy regularization) for different algorithms in appendix a? ----------minor comment which does not impact the score:-----1. it would have been better if theres a brief description of each algorithm (before section 4 or in appendix). ----------[1] reproducibility of benchmarked deep reinforcement learning tasks for continuous control an interesting paper on the role of regularization in policy optimization---------------in this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning. the authors provide a detailed investigation of the effect of regulations on the performance and behavior of agents following these methods.----------the authors present that regularization methods mostly help to improve the agents' performance in terms of final scores. specifically, they show that direct regularizations on model parameters, such as the standard case of l2 or l1 regularization, generally improve the agent performance. they also show that these regularizations, in their study, is more proper than entropy regularization. the authors also show that, in the presence of such regularizations, the learning algorithms become less sensitive to the hyperparameters. ----------few comments:-----1) the paper is well written and easy to follow. i appreciate it. i found the writing of the paper has a bit of repetition. the authors might find it slightly more proper to remove some of the repetitions (e.g. section 4.2)----------2) while i appreciate the clear writing and reasoning in this paper, i might suggest a slight change in the second paraphrase of the intro. i agree with the authors' reason on the first three lines, but i think it would be useful to also emphasize the role of the questions the researchers investigate to answer. i might also add one the main reason that the researchers in the field of drl have spent less time on regulation or architecture search was their focus on more high-level algorithm design which is in the more immediate step of relevance and specialty to the field of reinforcement learning. ----------3) i would suggest rephrasing the last two sentences of the second paragraph in related work: ""also, these techniques consider ..."". regularizing the output also regularizes the parameters, i think the authors' point was ""directly regularize"" the parameters. ----------4) in the ""entropy regularization"" part of section 3, i guess the hs has not been defined. ----------5) repeated ""the"" in the last paragraph of section 4.1 (despite it already incorporates the the maximization of)----------6) the authors used the term ""not converge"" multiple times. while it is hard from the plots to see whether the series converges or not, i have a strong feeling that by this term the authors mean the algorithm does not converge to a resealable solution rather than being divergent up to a bandwidth. maybe clarifying would be helpful.----------7) in section 5, the authors study the sensitivity to the hyperparameters. in this section, i had a hard time to understand the role of term 3-----""bn and dropout hurts on-policy algorithms but can bring improvement only for the off-policy sac algorithm."" does it mean that deploying bn, results in a more sensitive algorithm? or it means that the performance degrades (which is a different topic than section 5 is supposed to serve)?----------8) in section 7, the authors put out a hypothesis ""-----however, there is still generalization between samples: the agents are only trained on the limited"" but the provided empirical study might not fully be considered to be designed to test this hypothesis. in order to test this hypothesis, the author might be interested in training the models with bigger sample sizes, more training iteration, different function classes, and more fitting in order to test this hypothesis.---------------9) section 7 on ""why do bn and dropout work only with off-policy algorithms?"" while i agree with the authors on their first reason which is quite commonly known, i might hesitate to make the second statement (2)--------------------generally, i found this paper an interesting paper and appreciate the authors for their careful empirical study. but i found the contribution of this work to be not significant enough. most of the statements and claims in this paper are well know in the community, especially among deep learning practitioners. while i acknowledge the scientific value of this study, its concreteness, and appreciate the contribution of this paper, due to the low acceptance rate of this conference, i might be reluctant in accepting this paper.","this paper proposes an analysis of regularization for policy optimization. while the multiple effects of regularization are well known in the statistics and optimization community, it is less the case in the rl community. this makes the novelty of the paper difficult to judge as it depends on the familiarity of rl researchers with the two aforementioned communities.----------besides the novelty aspect, which is debatable, reviewers had doubts on the significance of the results, and in particular on the metrics chosen (based on the rank). while defining a ""best"" algorithm is notoriously difficult, and could be considered outside of the scope of this paper, the fact is that the conclusions reached are still sensitive to that difficulty.----------i thus regret to reject this paper as i feel not much more work is necessary to provide a compelling story. i encourage the authors to extend their choice of metrics to be more convincing in their conclusions.",the paper provides an empirical study of regularization in policy optimization methods in multiple continuous control tasks.,"----------[1] reproducibility of benchmarked deep reinforcement learning tasks for continuous control an interesting paper on the role of regularization in policy optimization---------------in this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning.","their findings suggest that l2 and entropy regularization can improve the performance, be robust to hyperparameters on the tasks studied in the paper.",the paper provides an empirical study of regularization in policy optimization methods in multiple continuous control tasks.,"----------[1] reproducibility of benchmarked deep reinforcement learning tasks for continuous control an interesting paper on the role of regularization in policy optimization---------------in this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning.","----------[1] reproducibility of benchmarked deep reinforcement learning tasks for continuous control an interesting paper on the role of regularization in policy optimization---------------in this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning.","while it is hard from the plots to see whether the series converges or not, i have a strong feeling that by this term the authors mean the algorithm does not converge to a resealable solution rather than being divergent up to a bandwidth.","----------[1] reproducibility of benchmarked deep reinforcement learning tasks for continuous control an interesting paper on the role of regularization in policy optimization---------------in this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning.",0.127906976744186,0.0352941176470588,0.0813953488372093,0.0813953488372093,0.2233502538071065,0.0717948717948718,0.1522842639593908,0.1522842639593908,0.1348314606741573,0.034090909090909,0.0898876404494382,0.0898876404494382,0.127906976744186,0.0352941176470588,0.0813953488372093,0.0813953488372093,0.2233502538071065,0.0717948717948718,0.1522842639593908,0.1522842639593908,0.2233502538071065,0.0717948717948718,0.1522842639593908,0.1522842639593908,0.2010050251256281,0.0203045685279187,0.1407035175879397,0.1407035175879397,0.2233502538071065,0.0717948717948718,0.1522842639593908,0.1522842639593908,10.658467292785645,10.658467292785645,12.91732120513916,10.658466339111328,12.91732120513916,12.857181549072266,10.658467292785645,4.357550621032715,0.9410454761206363,0.9477101990868578,0.9139446516755529,0.9872563468128508,0.9850747109350936,0.006698984899684169,0.976255457629711,0.9735195647117952,0.43023043202446765,0.9410454761206363,0.9477101990868578,0.9139448781597672,0.9872563468128508,0.9850747109350936,0.006698984899684169,0.9872563468128508,0.9850747109350936,0.006698965935425919,0.0859273923095406,0.19541093354488806,0.887097974111188,0.9872563468128508,0.9850747109350936,0.006698984899684169
12,https://openreview.net/forum?id=B1mSWUxR-,"this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rule.----------------i have a few questions on the motivation and the results.--------- in the section ""open problems in raml"", both (i) and (ii) are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q. this is not true. the globally optimal solution is related to both the underlying data distribution p and q, and not the same as q. it is given by q'(y | x, \tau) = \sum_{y'} p(y' | x) q(y | y', \tau).--------- both theorem 1 and theorem 2 do not directly justify that raml has similar reward as the bayes decision rule. can anything be said about this? are the kl divergence small enough to guarantee similar predictive rewards?--------- in theorem 2, when does the exponential tail bound assumption hold?--------- in table 1, the differences between raml and sqdml do not seem to support the claim that sqdml is better than raml. are the differences actually significant? are the differences between sqdml/raml and ml significant? in addition, how should \tau be chosen in these experiments? this paper dives deeper into understand reward augmented maximum likelihood training. overall, i feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the bayes decision rule. please elaborate on this.----------------did you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates?----------------section 4.2.2 says that ranzato et al. and bahdanau et al. require sampling from the model distribution. however, the methods analyzed in this paper also require sampling (cf. appendix d.2.4 where you mention a sample size of 10). please explain the difference.","there are some interesting ideas discussed in the paper, but the reviewers expressed difficulty understanding the motivation and the theoretical results. the experiments do not seem convincing in showing that sqdml achieves significant gains. overall, the the paper needs either stronger and clearer theoretical results, or more convincing experiments for publication at iclr.","overall, i feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the bayes decision rule.","this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rule.----------------i have a few questions on the motivation and the results.--------- in the section ""open problems in raml"", both (i) and (ii) are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q. this is not true.","this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rule.----------------i have a few questions on the motivation and the results.--------- in the section ""open problems in raml"", both (i) and (ii) are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q. this is not true.","this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rule.----------------i have a few questions on the motivation and the results.--------- in the section ""open problems in raml"", both (i) and (ii) are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q. this is not true.","this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rule.----------------i have a few questions on the motivation and the results.--------- in the section ""open problems in raml"", both (i) and (ii) are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q. this is not true.","the globally optimal solution is related to both the underlying data distribution p and q, and not the same as q. it is given by q'(y | x, \tau) = \sum_{y'} p(y' | x) q(y | y', \tau).--------- both theorem 1 and theorem 2 do not directly justify that raml has similar reward as the bayes decision rule.",this paper dives deeper into understand reward augmented maximum likelihood training.,"this paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the bayes decision rule.----------------i have a few questions on the motivation and the results.--------- in the section ""open problems in raml"", both (i) and (ii) are based on the statement that the globally optimal solution of raml is the exponential payoff distribution q. this is not true.",0.1978021978021977,0.0224719101123595,0.1538461538461538,0.1538461538461538,0.2833333333333333,0.0677966101694915,0.1999999999999999,0.1999999999999999,0.2833333333333333,0.0677966101694915,0.1999999999999999,0.1999999999999999,0.2833333333333333,0.0677966101694915,0.1999999999999999,0.1999999999999999,0.2833333333333333,0.0677966101694915,0.1999999999999999,0.1999999999999999,0.1621621621621621,0.018348623853211,0.1441441441441441,0.1441441441441441,0.0625,0.0,0.0625,0.0625,0.2833333333333333,0.0677966101694915,0.1999999999999999,0.1999999999999999,14.387985229492188,15.990057945251465,15.990057945251465,15.990057945251465,8.35200309753418,15.990057945251465,15.990057945251465,9.490095138549805,0.9421774562454878,0.935437763739291,0.9379562616325688,0.9840299352736025,0.9825769715392678,0.8768303053937553,0.9840299352736025,0.9825769715392678,0.8768303053937553,0.9840299352736025,0.9825769715392678,0.8768303053937553,0.9840299352736025,0.9825769715392678,0.8768303053937553,0.9760277744983716,0.9757910140081864,0.8222629874815586,0.9614729060783523,0.9657273660475157,0.009591587874212142,0.9840299352736025,0.9825769715392678,0.8768303053937553
13,https://openreview.net/forum?id=B1x996EKPS,"the paper considers distributed stochastic gradient descent, where some (unknown) compute nodes may be unreliable. new heuristics for filtering out replies from unreliable servers are introduced alongside a new protocol that helps keeping nodes in sync.----------in general, i miss a more clear indication of how the individual contributions are different from other methods. i am also missing more detailed ablation studies showing which of the new ideas contribute the most to efficient learning. as far as i can tell, the experiments do not really show an improvement over existing methods in this domain.----------this is not my area of expertise, but i cannot recommend the paper for publication in its current form as-----(a) it's not clear to me that the paper improves on existing methods, and-----(b) it's not clear to me what the real novelty of the work is.----------post-rebuttal:-----i acknowledge the response of the authors. they clarified some aspects for me, and the paper appears to have improved over the rebuttal period.-----i did not change my rating, but i want to emphasize that this is only because my knowledge of this field is so limited. my rating is largely based on ""gut feeling"" rather than actual knowledge, and i won't argue against acceptance. this paper introduces an algorithm to build distributed sdg-based training algorithm that are robust to byzantine workers and servers.----------i am not very familiar with this area of research, but i feel the authors did a good job providing clear explanations and introducing all the relevant concepts needed to understand the proposed algorithm. overall, i found the paper an interesting read.----------the experimental section of the paper is lacking in some aspects:------ one of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers. while these ideas are sensible from a technical point of view, i feel the experimental section is not properly demonstrating all the robustness claims made in the paper. for example, in the beginning of training with high learning rates the models will change a lot, are these filters effective in this situation as well? how are these filters working in terms of false positive/negatives in the experiments?------ how are models corrupted during training? what's the performances of the filters with different corruption techniques (e.g. adversarial attacks)?------ what's the impact of the choice of t in the experiments?","this paper is concerned with learning in the context of so-called byzantine failures. this is relevant for for example distributed computation of gradients of mini-batches and parameter updates. the paper introduces the concept and byzantine servers and gives theoretical and practical results for algorithm for this setting.----------the reviewers had a hard time evaluating this paper and the ac was unable to find an expert reviewer. still, the feedback from the reviewers painted a clear picture that the paper did not do enough to communicate the novel concepts used in the paper.----------rejection is recommended with a strong encouragement to use the feedback to improve the paper for the next conference.","new heuristics for filtering out replies from unreliable servers are introduced alongside a new protocol that helps keeping nodes in sync.----------in general, i miss a more clear indication of how the individual contributions are different from other methods.","overall, i found the paper an interesting read.----------the experimental section of the paper is lacking in some aspects:------ one of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers.","new heuristics for filtering out replies from unreliable servers are introduced alongside a new protocol that helps keeping nodes in sync.----------in general, i miss a more clear indication of how the individual contributions are different from other methods.","the paper considers distributed stochastic gradient descent, where some (unknown) compute nodes may be unreliable.","this paper introduces an algorithm to build distributed sdg-based training algorithm that are robust to byzantine workers and servers.----------i am not very familiar with this area of research, but i feel the authors did a good job providing clear explanations and introducing all the relevant concepts needed to understand the proposed algorithm.","overall, i found the paper an interesting read.----------the experimental section of the paper is lacking in some aspects:------ one of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers.","as far as i can tell, the experiments do not really show an improvement over existing methods in this domain.----------this is not my area of expertise, but i cannot recommend the paper for publication in its current form as-----(a) it's not clear to me that the paper improves on existing methods, and-----(b) it's not clear to me what the real novelty of the work is.----------post-rebuttal:-----i acknowledge the response of the authors.","overall, i found the paper an interesting read.----------the experimental section of the paper is lacking in some aspects:------ one of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers.",0.1578947368421052,0.0,0.0921052631578947,0.0921052631578947,0.2857142857142857,0.0657894736842105,0.1688311688311688,0.1688311688311688,0.1578947368421052,0.0,0.0921052631578947,0.0921052631578947,0.078125,0.0158730158730158,0.046875,0.046875,0.311377245508982,0.0242424242424242,0.155688622754491,0.155688622754491,0.2857142857142857,0.0657894736842105,0.1688311688311688,0.1688311688311688,0.3125,0.0526315789473684,0.1979166666666666,0.1979166666666666,0.2857142857142857,0.0657894736842105,0.1688311688311688,0.1688311688311688,7.369715690612793,7.824554443359375,14.640565872192385,7.369715690612793,13.255327224731444,13.255327224731444,7.369714736938477,10.04059886932373,0.9745423640233122,0.9732061013293313,0.8716926328800536,0.984799906579818,0.9830605688534215,0.9442558680779559,0.9745423640233122,0.9732061013293313,0.8716928995896064,0.9652903114546828,0.9708166841745554,0.9227553400970095,0.9861835214664723,0.9842112446030223,0.9322355883720447,0.984799906579818,0.9830605688534215,0.9442559489841431,0.986948153648235,0.9866840018176919,0.9553864642792746,0.984799906579818,0.9830605688534215,0.9442558680779559
14,https://openreview.net/forum?id=B1xeZJHKPB,"this paper, inspired by the established technique of model ensembling, proposes two methods (agg-mean and agg-var) for aggregating different model explanations into a single unified explanation. the authors mathematically prove that the derived explanation is guaranteed to be more truthful than the average performance of the constituent explanations. in practice, the aggregation consistently outperforms *all* individual explanations, not just their aggregated performance. additionally, the paper introduces a new quantitative evaluation metric for explanations, free of human intervention: irof (incremental removal of features) incrementally grays out the segments deemed as relevant by an explanation method and observes how quickly the end-task performance is degraded (good explanations will cause fast degradation). solid validation confirms that the irof metric is sound.----------i support paper acceptance. the experimental section is particularly strong, and makes a convincing argument for both the aggregation methods and the irof metric. even though i am not very familiar with the explainability literature and i would not be able to point out an omitted baseline for instance, the wide range of model architectures and aggregated explanation techniques makes a solid case. i appreciate the experiments on low-dimensional input, where the authors are deliberately showing a scenario in which their method does not score huge gains; this brings even more credibility to the paper. the presentation itself is clear, and there are no language or formatting issues.----------the only obvious downside of agg-mean and agg-var is that one would have to implement and run all constituent evaluation methods, which is expensive. just as an idea for future work: given n explanation methods, one could ablate away one method at a time, thus getting an idea of whether any of the n explanations are redundant in the presence of others. recommending a minimal set of useful explanation methods to the nlp community would then decrease the overall complexity of replicating the end-to-end explanation system. the paper has two main messages: 1- averaging over the explanation (saliency map in the case of image data) of different methods results in a smaller error than an expected error of a single explanation method. 2- introducing a new saliency map evaluation method by seeking to mitigate the effect of high spatial correlation in image data through grouping pixels into coherent segments. the paper then reports experimental results of the methods introduced in the first message being superior to existing saliency map methods using the second message (and an additional saliency map evaluation method in the literature). they also seek to magnify the capability of the 2nd message's evaluation method by showing its better capability at distinguishing between a random explanation and an explanation method with a signal in it.---------------i vote for rejecting this paper for two main reasons: the contributions are not enough for this veneue, and the paper's introduced methods are not backed by convincing motivations. the first message of the paper is trivial and cannot be considered as a novel contribution: the ''proof'' is basically the error of the mean is smaller than the mean of the errors. additionally, this could have been useful if the case was that there was a need for playing the safe-card: that is, all of the existing methods have equal saliency map error and averaging will decrease the risk. not only authors do not provide any evidence but also both the experimental results of the paper itself (results in table 2 and fig 4 are disproving this assumption) and the existing literature disprove it. even considering this assumption to be correct, the contribution is minimal to the field and benefits of averaging saliency maps have been known since the smoothgrad paper. the second contribution is an extension of existing evaluation methods (e.g. sdc) where instead of removing (replacing by mean) individual pixels, the first segment the image and remove the segments. the method, apart from being very similar to what is already there in the literature, is not introduced in a well-motivated manner. the authors claim that their evaluation method is able to circumvent the problem with removing individual pixels (which is the removed information of one pixel is mitigated by the spatial correlations in the image and therefore will not result in a proportional loss of prediction power) by removing ''features'' instead. their definition of a feature, though, are segments generated by simple segmentation methods. there is a long line of literature showing the incorrectness of this assumption; i.e. a group of coherent nearby pixels does not necessarily constitute a feature seen by the network and does not necessarily remove the mentioned problem of the high correlation of pixels. this method does not remove ""the interdependency of inputs"" for the saliency evalatuion metric. even assuming the correctness of this assumption, the contribution over what already exists in the literature is not enough for this venue.----------a few suggestions:----------* the authors talk about a ''true explanation''. this concept needs to be discussed more clearly and extensively. what does it mean to be a true evaluation? it is also important to prove that the introduced evaluation metric of irof would assign perfect score for a given true explanation.----------* the mentioned problem of pixel correlations that irof seeks to mitigate is also existing in other modalities of data and the authors do not talk about how irof could potentially be extended.----------* the qualitative results in the text and the appendix do not show an advantage. it would be more crips if the authors could run simple tests on human subjects following the methods in the previous literature.----------* there are many many grammatical and spelling errors in the paper. the font size for figures is very small and unreadable unless by zooming in.----------* many of the introduced heuristics are not backed by evidence or arguments. one example is normalizing individual saliency maps between 0-1 which can naturally be harmful; e.g. aggregating a noisy low-variance method with almost equal importance everywhere (plus additive noise) and a high-variance one which does a good job at distinguishing important pixels - agg-var will not mitigate this issue. ----------one question:----------the authors introduced aggregation as a method for a ''better explanation''. it has been known that another problem with saliency maps is robustness: one can generate adversarial examples against saliency maps. it would be an interesting question to see whether aggregation would improve robustness rather than how good the map itself is.","this paper describes a new method for explaining the predictions of a cnn on a particular image. the method is based on aggregating the explanations of several methods. they also describe a new method of evaluating explanation methods which avoids manual evaluation of the explanations.----------however, the most critical reviewer questions the contribution of the proposed method, which is simple. simple isn't always a bad thing, but i think here the reviewer has a point. the new method for evaluating explanation methods is interesting, but the sample images given are also very simple -- how does the method work when the image is cluttered? how about when the prediction is uncertain or wrong?","even though i am not very familiar with the explainability literature and i would not be able to point out an omitted baseline for instance, the wide range of model architectures and aggregated explanation techniques makes a solid case.",the paper then reports experimental results of the methods introduced in the first message being superior to existing saliency map methods using the second message (and an additional saliency map evaluation method in the literature).,the paper then reports experimental results of the methods introduced in the first message being superior to existing saliency map methods using the second message (and an additional saliency map evaluation method in the literature).,"this paper, inspired by the established technique of model ensembling, proposes two methods (agg-mean and agg-var) for aggregating different model explanations into a single unified explanation.",2- introducing a new saliency map evaluation method by seeking to mitigate the effect of high spatial correlation in image data through grouping pixels into coherent segments.,the paper then reports experimental results of the methods introduced in the first message being superior to existing saliency map methods using the second message (and an additional saliency map evaluation method in the literature).,"the second contribution is an extension of existing evaluation methods (e.g. sdc) where instead of removing (replacing by mean) individual pixels, the first segment the image and remove the segments.",the paper then reports experimental results of the methods introduced in the first message being superior to existing saliency map methods using the second message (and an additional saliency map evaluation method in the literature).,0.1447368421052631,0.0,0.0789473684210526,0.0789473684210526,0.1486486486486486,0.0273972602739726,0.1351351351351351,0.1351351351351351,0.1486486486486486,0.0273972602739726,0.1351351351351351,0.1351351351351351,0.1560283687943262,0.014388489208633,0.1276595744680851,0.1276595744680851,0.0999999999999999,0.0144927536231884,0.0999999999999999,0.0999999999999999,0.1486486486486486,0.0273972602739726,0.1351351351351351,0.1351351351351351,0.1527777777777778,0.0140845070422535,0.1388888888888889,0.1388888888888889,0.1486486486486486,0.0273972602739726,0.1351351351351351,0.1351351351351351,5.759679317474365,4.488672256469727,12.165105819702148,5.759679317474365,9.04120922088623,5.759679317474365,5.759680271148682,5.607008457183838,0.9095705679399044,0.9215687640460907,0.9316133997178201,0.9561315190416837,0.9553414633048739,0.9278992406946142,0.9561315190416837,0.9553414633048739,0.9278992406946142,0.9773953584216454,0.9746029603331857,0.9627078570249934,0.9545063474398965,0.9561537541129317,0.8961499715991216,0.9561315190416837,0.9553414633048739,0.9278992199541973,0.9038925472312457,0.9308737609484553,0.235131060651089,0.9561315190416837,0.9553414633048739,0.9278992406946142
15,https://openreview.net/forum?id=B1xewR4KvH,"the goal of the paper is clear. however, the proposed method has only a very incremental novelty compared to sporf and the previous approaches in computer vision (e.g., shotton et al., 2011). although the authors claim the method can take advantage of structure in all kinds of data, the only conducted experiment is on the image data which is fairly limited.----------in fig. 1, they claim morf outperforms other methods given fewer training data. however, for classifying orthogonal bars, cnn still outperforms morf when few training data are given. the results on mnist is also not impressive. cnn is still much better compared to other tree-based methods. moreover, no other real-world dataset has been conducted experiments on.----------in general, the paper's goal is clear and interesting. but the authors failed to propose a novel method and the results are not convincing. hence, i recommend rejection. this paper proposed a new method called manifold forest to improve decision forest (df) classification results. it is motivated by that, natural data is often in some manifold but not randomly distributed. it showed how to use the 2d spatial structures of natural images by constructing structured atoms. results on 3 toy examples and mnist showed the better performance than standard rf and sporf.----------overall, the paper is easy to follow and well written. the idea is intuitive: using structured 2d information to improve the classification results. but there are some issues with the implemetation.----------1. in image classification, we definitely need 2d structure information. this is normally extracted by the descriptors such as sift, gist, where the 2d information has been included. it is rare to use the pixel values as the features directly for classification. in this case, the benefit of the proposed method is very weak. this is the main issue of the paper. no results on real features.----------2. the results are weak too. the real data is mnist, which is also a very toy dataset. it would be good to include some real world image dataset, such as cifar, imagenet etc.----------3. the algorithm is somehow incremental compared with sporf.","this work explores how to leverage structure of this input in decision trees, the way this is done for example in convolutional networks.-----all reviewers agree that the experimental validation of the method as presented is extremely weak. authors have not provided a response to answer the many concerns raised by reviewers.-----therefore, we recommend rejection.","results on 3 toy examples and mnist showed the better performance than standard rf and sporf.----------overall, the paper is easy to follow and well written.",this paper proposed a new method called manifold forest to improve decision forest (df) classification results.,"in image classification, we definitely need 2d structure information.",the goal of the paper is clear.,this paper proposed a new method called manifold forest to improve decision forest (df) classification results.,"although the authors claim the method can take advantage of structure in all kinds of data, the only conducted experiment is on the image data which is fairly limited.----------in fig.","it is motivated by that, natural data is often in some manifold but not randomly distributed.",this paper proposed a new method called manifold forest to improve decision forest (df) classification results.,0.1219512195121951,0.0,0.1219512195121951,0.1219512195121951,0.1388888888888889,0.0,0.0833333333333333,0.0833333333333333,0.0923076923076923,0.0,0.0615384615384615,0.0615384615384615,0.1269841269841269,0.0327868852459016,0.1269841269841269,0.1269841269841269,0.1388888888888889,0.0,0.0833333333333333,0.0833333333333333,0.3218390804597701,0.0235294117647058,0.160919540229885,0.160919540229885,0.1666666666666666,0.0,0.1111111111111111,0.1111111111111111,0.1388888888888889,0.0,0.0833333333333333,0.0833333333333333,9.851360321044922,6.893932819366455,6.435276031494141,6.893930912017822,7.423111915588379,5.534617900848389,6.893932342529297,7.12482500076294,0.9801339245164156,0.9807728006113906,0.13601619250782684,0.9820614907214466,0.9832129043289515,0.9461825727952733,0.9439391295771167,0.9577473074659006,0.5745933757391224,0.9674092347152548,0.9654540859691693,0.8830994051540905,0.9820614907214466,0.9832129043289515,0.9461826708567476,0.9854273946357306,0.9849016252269773,0.09312604451305473,0.9338466348366329,0.9670118043688682,0.7043588378542431,0.9820614907214466,0.9832129043289515,0.9461825727952733
16,https://openreview.net/forum?id=B1xwcyHFDr,"this paper extends the information bottleneck method of tishby et al. (2000) to the unsupervised setting. by taking advantage of multi-view data, they provide two views of the same underlying entity. experimetal results on two standard multi-view datasets validate the efficacy of the proposed method.-----i have three questions about this work.-----1. the proposed method only provides two views of the same underlying entity, what about 3 or more views?-----2. can this method be used for multi-modality case?-----3. what about the time efficiency of the proposed method? in this paper, the authors extend the information bottleneck method (to build robust representations by removing information unrelated to the target labels) to the unsupervised setting. since label information is not available in this setting, the authors leverage multi-view information (e.g., using two images of the same object) , which requires assuming that both views contain all necessary information for the subsequent label prediction task. the representation should then focus on capturing the information shared by both views and discarding the rest. a loss function for learning such representations is proposed. the effectiveness of the proposed technique is confirmed on two datasets. it is also shown to work when doing data augmentation with a single view.----------overall the paper is well motivated, well placed in the literature and well written. mathematical derivations are provided. experimental methodology follows the existing literature, seem reasonable and results are convincing. i do not have major negative comments for the authors. this is however not my research area and have only a limited knowledge of the existing body of work.----------comments/questions:------ how limiting is the multi-view assumption? are there well-known cases where it doesn't hold? i feel it would be hard to use, say, with text. has this been discussed in the literature? some pointers or discussion would be interesting.------ sketchy dataset: could the dsh algorithm (one of the best prior results) be penalized by not using the same feature extractor you used?------ sketchy dataset: can a reference for the {siamese,triplet}-alexnet results be provided?------ sketchy dataset: for reproducibility, what is the selected \beta?------ i find it very hard to believe that the accuracy stays constant no matter the number of examples per label used. how can an encoder be trained on 10 images? did i misunderstand the meaning of this number? can this be clarified?------ again for reproducibility, listing the raw numbers for the mnist experiments would be nice.------ if i understood the experiments correctly, ""scarce label regime"" is used for both the mir-flickr and mnist datasets, meaning two different things (number of labels per example vs number of examples per label), which is slightly confusing.----------typos:-----page 1: it's -> its-----page 6: the the -> the-----page 7: classifer -> classifier-----page 8: independently -> independent this is a good multiview representation learning paper with new insights. the authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible.-----the paper relies on mutual information estimation and is reconstruction-free. it is mentioned in some previous works (e.g. aaron van den oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.-----comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. the authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches.-----the experimental results on the right side of figure 3, deliver a very interesting conclusion. in low-resource case, robust feature (obtained by using the larger beta, discarding more superfluous information) is crucial for achieving good performance. while when the amount of labeled data samples is enough, vice-versa.----------here are my major concerns:-----1. in the paper, the authors said the original formulation of ib is only applicable to supervised learning. that is true, but the variational information bottleneck paper [alexander a. alem et al. 2017] already showed the connection of unsupervised vib to vae in the appendix.-----2. i would not consider the data augmentation used to extend single-view data to pseudo-multiview as a contribution. this has been done before (e.g. in the multiview mnist experiment part of the paper ""on deep multi-view representation learning"").-----3. which mv-infomax do you really compare to? you listed a few of them: (ji et al., 2019; henaff et al.,  2019; tian et al., 2019; bachman et al., 2019) in the related work section.-----4. i think the authors should also make a more careful claim on their results in mir-flickr. -----id rather not saying mib generally outperforms mv-infomax on mir-flickr, as mib does not (clearly) outperform mv-infomax when enough labeled data is available for training downstream recognizers. but mib does clearly outperform mv-infomax when scaling down the percentage of labeled samples used.-----5. regarding baselines/experiments-----a. in figure 4, it seems that vae (with beta=4) outperforms mv-infomax. why the ``""pseudo-second view"" does not help mv-infomax in this scenario? why vae is clearly better than infomax?-----b. in figure 3, you might also tune beta for vcca and its variants, like what you did for vae/vib in a single view. -----6. do you think your approach can be extended to more than two views easily? -----for me, it seems the extension is not trivial, as it requires o(n^2) terms in your loss for n views.-----but this is minor.","this paper extends the information bottleneck method to the unsupervised representation learning under the multi-view assumption. the work couples the multi-view infomax principle with the information bottleneck principle to derive an objective which encourages the representations to contain only the information shared by both views and thus eliminate the effect of independent factors of variations. recent advances in estimating lower-bounds on mutual information are applied to perform approximate optimisation in practice. the authors empirically validate the proposed approach in two standard multi-view settings.-----overall, the reviewers found the presentation clear, and the paper well written and well motivated. the issues raised by the reviewers were addressed in the rebuttal and we feel that the work is well suited for iclr. we ask the authors to carefully integrate the detailed comments from the reviewers into the manuscript. finally, the work should investigate and briefly establish a connection to [1].----------[1] wang et al. ""deep multi-view information bottleneck"". international conference on data mining 2019 (https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.5)","the authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches.-----the experimental results on the right side of figure 3, deliver a very interesting conclusion.","it is mentioned in some previous works (e.g. aaron van den oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.-----comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output.","regarding baselines/experiments-----a. in figure 4, it seems that vae (with beta=4) outperforms mv-infomax.",this paper extends the information bottleneck method of tishby et al. (2000) to the unsupervised setting.,"it is mentioned in some previous works (e.g. aaron van den oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.-----comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output.","it is mentioned in some previous works (e.g. aaron van den oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.-----comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output.","the authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible.-----the paper relies on mutual information estimation and is reconstruction-free.","it is mentioned in some previous works (e.g. aaron van den oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.-----comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output.",0.1421800947867298,0.0382775119617224,0.0853080568720379,0.0853080568720379,0.2440944881889763,0.0396825396825396,0.1259842519685039,0.1259842519685039,0.0512820512820512,0.0,0.0205128205128205,0.0205128205128205,0.1443298969072165,0.09375,0.1134020618556701,0.1134020618556701,0.2440944881889763,0.0396825396825396,0.1259842519685039,0.1259842519685039,0.2440944881889763,0.0396825396825396,0.1259842519685039,0.1259842519685039,0.2037037037037037,0.0373831775700934,0.1203703703703703,0.1203703703703703,0.2440944881889763,0.0396825396825396,0.1259842519685039,0.1259842519685039,7.102652072906494,7.102652072906494,11.745574951171877,7.102652072906494,8.909626007080078,5.498027801513672,7.102652072906494,7.886603355407715,0.9608994974916867,0.9646987373931626,0.9496436281268475,0.9623090207982586,0.3369433825935586,0.3143112904229337,0.4456731780534125,0.6180300923451305,0.8007031510858835,0.9677562464440049,0.6756037565879649,0.436303784664281,0.9623090207982586,0.3369433825935586,0.3143112904229337,0.9623090207982586,0.3369433825935586,0.3143112904229337,0.9802055437185347,0.9790203936535942,0.8958596626680563,0.9623090207982586,0.3369433825935586,0.3143112904229337
17,https://openreview.net/forum?id=BJ8vJebC-,"this paper investigates the impact of noisy input on machine translation, and tests simple ways to make nmt models more robust.----------------overall the paper is a clearly written, well described report of several experiments. it shows convincingly that standard nmt models completely break down on both natural ""noise"" and various types of input perturbations. it then tests how the addition of noise in the input helps robustify the charcnn model somewhat. the extent of the experiments is quite impressive: three different nmt models are tried, and one is used in extensive experiments with various noise combinations.----------------this study clearly addresses an important issue in nmt and will be of interest to many in the nlp community. the outcome is not entirely surprising (noise hurts and training and the right kind of noise helps) but the impact may be. i wonder if you could put this in the context of ""training with input noise"", which has been studied in neural network for a while (at least since the 1990s). i.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise. also, the bit of analysis in sections 6.1 and 7.1 is promising, if maybe not so conclusive yet.----------------a few constructive criticisms:----------------the way noise is included in training (sec. 6.2) could be clarified (unless i missed it) e.g. are you generating a fixed ""noisy"" training set and adding that to clean data? or introducing noise ""on-line"" as part of the training? if fixed, what sizes were tried? more information on the experimental design would help.----------------table 6 is highly suspect: some numbers seem to have been copy-pasted in the wrong cells, eg. the ""rand"" line for german, or the swap/mid/rand lines for czech. it's highly unlikely that training on noisy swap data would yield a boost of +18 bleu points on czech -- or you have clearly found a magical way to improve performance.----------------although the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way.----------------[response read -- thanks]--------i agree with authors that this paper is suitable for iclr, although it will clearly be of interest to acl/mt-minded folks. this paper investigates the impact of character-level noise on various flavours of neural machine translation. it tests 4 different nmt systems with varying degrees and types of character awareness, including a novel meanchar system that uses averaged unigram character embeddings as word representations on the source side. the authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables. they show that all nmt systems, whether bpe or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data. unfortunately, they are not able to show any types of synthetic noise helping address natural noise. however, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise.----------------this is a thorough exploration of a mostly under-studied problem. the paper is well-written and easy to follow. the authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty. the inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work. their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others. in terms of negatives, it feels like this work is just starting to scratch the surface of noise in nmt. the proposed meanchar architecture doesnt look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isnt extremely satisfying. furthermore, the use of these replacement tables means that even when the noise is natural, its still kind of artificial. finally, this paper doesnt seem to be a perfect fit for iclr, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *acl conference.----------------regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial. first of all, errors learned from the noisy data sources are constrained to exist within a word. this tilts the comparison in favour of architectures that retain word boundaries (such as the charcnn system here), while those systems may struggle with other sources of errors such as missing spaces between words. second, if i understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data. this seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur). i feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.----------------also, it is somewhat jarring that only the charcnn approach is included in the experiments with noisy training data (table 6). i realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner. on a related note, the line in the abstract stating that ... a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise implies that the other (non-charcnn) architectures could not learn these representations, when in reality, they simply werent given the chance.----------------section 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an iclr audience. from my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies.----------------i have only one small, specific suggestion: at the end of section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charcnn currently has two paragraphs).----------------[edited for typos] this paper empirically investigates the performance of character-level nmt systems in the face of character-level noise, both synthesized and natural. the results are not surprising:----------------* nmt is terrible with noise.----------------* but it improves on each noise type when it is trained on that noise type.----------------what i like about this paper is that:----------------1) the experiments are very carefully designed and thorough.----------------2) this problem might actually matter. out of curiosity, i ran the example (table 4) through google translate, and the result was gibberish. but as the paper shows, its easy to make nmt robust to this kind of noise, and google (and other nmt providers) could do this tomorrow. so this paper could have real-world impact.----------------3) most importantly, it shows that nmts handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different. so solving the problem of natural noise is not so simple its a *real* problem. speculating, again: commercial mt providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale. so these methods could be applied in the real world. (it would be excellent if an outcome of this paper was that commercial mt providers answered its call to provide more realistic noise by actually providing examples.)----------------there are no fancy new methods or state-of-the-art numbers in this paper. but its careful, curiosity-driven empirical research of the type that matters, and it should be in iclr.","the pros and cons of this paper cited by the reviewers can be summarized below: pros: * the paper is a first attempt to investigate an under-studied area in neural mt (and potentially other applications of sequence-to-sequence models as well) * this area might have a large impact; existing models such as google translate fail badly on the inputs described here * experiments are very carefully designed and thorough * experiments on not only synthetic but also natural noise add significant reliability to the results * paper is well-written and easy to follow cons: * there may be better architectures for this problem than the ones proposed here * even the natural noise is not entirely natural, e.g. artificially constrained to exist within words * paper is not a perfect fit to iclr (although iclr is attempting to cast a wide net, so this alone is not a critical criticism of the paper) this paper had uniformly positive reviews and has potential for large real-world impact.","the proposed meanchar architecture doesnt look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isnt extremely satisfying.","they show that all nmt systems, whether bpe or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data.","but as the paper shows, its easy to make nmt robust to this kind of noise, and google (and other nmt providers) could do this tomorrow.","this paper investigates the impact of noisy input on machine translation, and tests simple ways to make nmt models more robust.----------------overall the paper is a clearly written, well described report of several experiments.","this paper investigates the impact of noisy input on machine translation, and tests simple ways to make nmt models more robust.----------------overall the paper is a clearly written, well described report of several experiments.","they show that all nmt systems, whether bpe or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data.","it shows convincingly that standard nmt models completely break down on both natural ""noise"" and various types of input perturbations.","so this paper could have real-world impact.----------------3) most importantly, it shows that nmts handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different.",0.13,0.0,0.08,0.08,0.169811320754717,0.0095238095238095,0.1226415094339622,0.1226415094339622,0.1578947368421052,0.0212765957446808,0.0947368421052631,0.0947368421052631,0.2121212121212121,0.0408163265306122,0.1313131313131313,0.1313131313131313,0.2121212121212121,0.0408163265306122,0.1313131313131313,0.1313131313131313,0.169811320754717,0.0095238095238095,0.1226415094339622,0.1226415094339622,0.0760869565217391,0.0109890109890109,0.0652173913043478,0.0652173913043478,0.1890547263681592,0.0703517587939698,0.1194029850746268,0.1194029850746268,12.482019424438477,16.009658813476562,16.00965690612793,12.482019424438477,7.701154708862305,13.356395721435549,14.335917472839355,15.572713851928713,0.45711328910810395,0.5378139213907711,0.9256517851244859,0.9127605898658755,0.9457247381306867,0.9128057433201918,0.21999735376405902,0.3269442120122117,0.9015298943877079,0.9881077763834236,0.9838972325605796,0.9440220754637414,0.9881077763834236,0.9838972325605796,0.9440221173049342,0.9127605898658755,0.9457247381306867,0.9128057433201918,0.9333584917047224,0.9499397621200724,0.8907195951046439,0.1892790236094624,0.41150381684894916,0.06021169847282246
18,https://openreview.net/forum?id=BJe-DsC5Fm,"the paper presents algorithms for optimization using sign-sgd when the access is restricted to a zero order oracle only, and provide detailed analysis and convergence rates. they also run optimization experiments on synthetic data. additionally, they demonstrate superiority of the algorithm in the number of oracle calls for black box adversarial attacks for mnist and cifar-10. the provided algorithm has optimal iteration complexity from a theoretical viewpoint. ----------------the paper was, overall very well written and sufficient experiment were presented. the math also seems correct. however, i think they should have explained the motivation for the need of developing such an algorithm better. section 3 can be improved. ----------------i think this is an important paper because it provides a guaranteed algorithm for zero order sign-gradient descent. however, the ideas and the estimators are not novel. they show applicability of standard gradient estimators for zero order oracles for sign-sgd algorithm. in this paper, the authors studied zeroth order sign sgd. sign sgd is commonly used in adversarial example generation. compared to sign sgd, zeroth-order sign sgd does not require the knowledge of the magnitude of the gradient, which makes it suitable to optimize black-box systems. the authors studied the convergence rate of zeroth-order sign sgd, and showed that under common assumptions, zero-order sign sgd achieves o(sqrt(d/t)) convergence rate, which is slower than sign sgd by a factor of sqrt(d). however, sign sgd requires an unrealisitcally large mini-batch size, which zeroth-order sign sgd does not. the authors demonstrated the performance of zeroth-order sign sgd in numerical experiments.----------------overall, this is a well written paper. the convergence property of the zeroth-order sign sgd is sufficiently studied. the proposal seems to be useful in real world tasks.----------------weaknesses: --------1) out of curiosity, can we improve the convergence rate of the zeroth-order sign sgd if we assume the mini-batch size is of order o(t)? this could help us better compare zeroth-order sign sgd and sign sgd.--------2) figure 2 is too small to be legible. also, it seems that the adversarial examples generated by zeroth-order sign sgd have higher distortion than those found by zeroth-order sgd on cifar-10 dataset. is it true? if so, it would be beneficial to have a qualitative explanation of such behavior.","this is a solid paper that proposes and analyzes a sound approach to zero order optimization, covering a variants of a simple base algorithm. after resolving some issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance. some concerns regarding the necessity for such algorithms persisted, but the connection to adversarial examples provides an interesting motivation.","the proposal seems to be useful in real world tasks.----------------weaknesses: --------1) out of curiosity, can we improve the convergence rate of the zeroth-order sign sgd if we assume the mini-batch size is of order o(t)?","the authors studied the convergence rate of zeroth-order sign sgd, and showed that under common assumptions, zero-order sign sgd achieves o(sqrt(d/t)) convergence rate, which is slower than sign sgd by a factor of sqrt(d).","the paper presents algorithms for optimization using sign-sgd when the access is restricted to a zero order oracle only, and provide detailed analysis and convergence rates.","the paper presents algorithms for optimization using sign-sgd when the access is restricted to a zero order oracle only, and provide detailed analysis and convergence rates.","the authors studied the convergence rate of zeroth-order sign sgd, and showed that under common assumptions, zero-order sign sgd achieves o(sqrt(d/t)) convergence rate, which is slower than sign sgd by a factor of sqrt(d).","the authors studied the convergence rate of zeroth-order sign sgd, and showed that under common assumptions, zero-order sign sgd achieves o(sqrt(d/t)) convergence rate, which is slower than sign sgd by a factor of sqrt(d).","----------------the paper was, overall very well written and sufficient experiment were presented.","the authors studied the convergence rate of zeroth-order sign sgd, and showed that under common assumptions, zero-order sign sgd achieves o(sqrt(d/t)) convergence rate, which is slower than sign sgd by a factor of sqrt(d).",0.202020202020202,0.0,0.1414141414141414,0.1414141414141414,0.2,0.0204081632653061,0.1,0.1,0.2988505747126436,0.0235294117647058,0.1379310344827586,0.1379310344827586,0.2988505747126436,0.0235294117647058,0.1379310344827586,0.1379310344827586,0.2,0.0204081632653061,0.1,0.1,0.2,0.0204081632653061,0.1,0.1,0.0833333333333333,0.0,0.0555555555555555,0.0555555555555555,0.2,0.0204081632653061,0.1,0.1,7.047281742095947,7.0472822189331055,13.193437576293944,7.047281742095947,9.99487590789795,13.193437576293944,7.0472822189331055,11.100421905517578,0.9772149738622884,0.9791211471256448,0.9022301186041802,0.9727380249113958,0.9725101801688458,0.833830869880331,0.9764947390305665,0.9764039025654521,0.8535732632292689,0.9764947390305665,0.9764039025654521,0.8535731567357971,0.9727380249113958,0.9725101801688458,0.833830869880331,0.9727380249113958,0.9725101801688458,0.833830869880331,0.9875037878937561,0.9845437855444601,0.6799319346534048,0.9727380249113958,0.9725101801688458,0.833830869880331
19,https://openreview.net/forum?id=BJe_z1HFPr,"this paper proposes a new method that involves multi-scale inputs for each layer that could be used as network architecture search or data agumentation or ----------pros)-----(+) the idea looks interesting.-----(+) the experimental results look promising.----------cons)-----(-) many typos when denoting figures and tables. see the minor comments below.-----(-) i believe the authors could organize the paper better. tables and figures that are referred in a page are hard to find quickly. i recommend the authors refine the paper again for better readability.-----(-) some notations (such as rs-, rs-nas, and so on) are so vague that hard to follow. -----(-) i recommend the authors redraw all the figures for clarity. for example, each legend in figure 2 is hard to take a look at.-----(-) + the comments below.----------comments)------ when doing feature map resize in terms of the resolution, why bilinear sampling was chosen? could the authors provide a comparison with other sampling methods?------ in the related work section for dynamic neural networks, the authors claimed that ""most dynamic networks methods sacrifice accuracy in exchange of adaption in inference"", but it seems to be quite overclaimed. as shown in the paper [1], one can find that the author presented they could improve both accuracy and efficiency.------ how did you find the architecture shown in figure 3 in the appendix? what is xception? please specify the details.------ designing the pre-defined spatial list of l looks critical, so the authors should describe l in the implementation details. ------ one of the main problem i think is the training budget issue. according to algorithm 1, the inner loop of ""for j=0,..,len(l)"", the overall training time will clearly take l times longer than that of the training setting w/o resizable training. thus, it does not seem to be fair comparison in terms of the training budget. namely, it seems that the authors compared with the other data augmentation methods which spend much less training budgets.------ hard to grasp section 3.5 of adaptive resizable neural network. resizelearner looks being attached at the last stage of the original network after the original network is trained, but there is no further information about what resizelearner learns and how resizelearner selects the optimal sub-network. ----------[1] universally slimmable networks and improved training techniques, https://arxiv.org/pdf/1903.05134.pdf.----------minor comments)------ wrong section and figure references:----- - 'it also mitigates the co-adaptation issue which we will discuss in section 3.3'(indeed it is section 3.4), 'the network architecture along with feature map resolution and channels number are shown in figure 4' (it should be figure 3).----- - - figure 3(d) referred to in section 4.4 would be figure 2(d) indeed.----------about rating)-----the authors provided a novel technique about the resizable approach and the experimental results look promising. however, the paper needs to be revised and looks like it does not ready to be published now. if the authors could revise the paper and concern my comments well, i would increase my rating. this paper proposes resizable neural networks, which trains networks with different resolution scalings at the same time with shared weights. it serves as data-augmentation and improves accuracy over base networks. additionally, the same technique can perform an architecture search. experimental results show significant accuracy gains.----------the reported accuracy gains are substantial. the proposed method is potentially useful in many applications. however, several details are missing or hard to understand. without additional descriptions, it is not straightforward to implement the method. thus, i suggest for rejection. the score might be raised depending on updates and code release.----------major comments:-----1) algorithm 1 has ""predefined spatial list l."" how to choose it in practice?-----2) algorithm 1 indicates that the training time is len(l) times longer. additionally, according to the implementation details, resizable networks are trained two times many epochs. it seems hard to justify such a longer training time.-----3) this is similar to (2), but why resizable-nas is better than the all len(l) models separately to find better architectures?-----4) in sec. 3.4, how the ""target model"" is selected after the training of resizable networks?-----5) why is resizable-adapt omitted from table 2?-----6) references are out-dated. for example, there are no ""figure 5.""----------minor comments:-----1) are any ablation studies available for fair sampling?----------===== update----------thank you for the response and update. the revision made the paper easier to understand. however, i still have concerns about the presentation of the paper, and i keep my score. i think the novelty and experimental results are significant and match the bar of iclr. if the other two reviewers think that the revised paper is fairly well-written and recommend acceptance, i will not challenge the decision.----------major comments:-----1) i do not understand how the training times of random-sampling in appendix c were estimated.-----2) concerning table 11, how was the result when we randomly select each scale and train network with the same number of epochs with fair-sampling? if there were no much difference with the result in table 11, it seems fair sampling does not have clear advantages over random sampling. if so, i suggest to alter fiar-sampling by random-sampling and reduce the complexity of the proposed method.----------minor comments:-----1) table 10 has two captions.-----2) i did not understand that l is a list of possible scaling factors (scalar) in the initial review. i guess it is partially because i did not understand why the number of sampling should be len(l) for each mini-batch (it is actually due to fair-sampling). i think adding some remarks on it will help to make the pseudo-code easier to understand. (nit: for j= 0, ..., len(l) should be for j=0, ..., len(l) -1 or for j=1, ..., len(l))","this paper offers likely novel schemes for image resizing. the performance improvement is clear. unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the ac concurs that this is still an issue. the paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately.",this paper proposes a new method that involves multi-scale inputs for each layer that could be used as network architecture search or data agumentation or ----------pros)-----(+) the idea looks interesting.-----(+) the experimental results look promising.----------cons)-----(-) many typos when denoting figures and tables.,"----------[1] universally slimmable networks and improved training techniques, https://arxiv.org/pdf/1903.05134.pdf.----------minor comments)------ wrong section and figure references:----- - 'it also mitigates the co-adaptation issue which we will discuss in section 3.3'(indeed it is section 3.4), 'the network architecture along with feature map resolution and channels number are shown in figure 4' (it should be figure 3).----- - - figure 3(d) referred to in section 4.4 would be figure 2(d) indeed.----------about rating)-----the authors provided a novel technique about the resizable approach and the experimental results look promising.","if the other two reviewers think that the revised paper is fairly well-written and recommend acceptance, i will not challenge the decision.----------major comments:-----1) i do not understand how the training times of random-sampling in appendix c were estimated.-----2) concerning table 11, how was the result when we randomly select each scale and train network with the same number of epochs with fair-sampling?",this paper proposes a new method that involves multi-scale inputs for each layer that could be used as network architecture search or data agumentation or ----------pros)-----(+) the idea looks interesting.-----(+) the experimental results look promising.----------cons)-----(-) many typos when denoting figures and tables.,"----------[1] universally slimmable networks and improved training techniques, https://arxiv.org/pdf/1903.05134.pdf.----------minor comments)------ wrong section and figure references:----- - 'it also mitigates the co-adaptation issue which we will discuss in section 3.3'(indeed it is section 3.4), 'the network architecture along with feature map resolution and channels number are shown in figure 4' (it should be figure 3).----- - - figure 3(d) referred to in section 4.4 would be figure 2(d) indeed.----------about rating)-----the authors provided a novel technique about the resizable approach and the experimental results look promising.","----------[1] universally slimmable networks and improved training techniques, https://arxiv.org/pdf/1903.05134.pdf.----------minor comments)------ wrong section and figure references:----- - 'it also mitigates the co-adaptation issue which we will discuss in section 3.3'(indeed it is section 3.4), 'the network architecture along with feature map resolution and channels number are shown in figure 4' (it should be figure 3).----- - - figure 3(d) referred to in section 4.4 would be figure 2(d) indeed.----------about rating)-----the authors provided a novel technique about the resizable approach and the experimental results look promising.",tables and figures that are referred in a page are hard to find quickly.,"----------[1] universally slimmable networks and improved training techniques, https://arxiv.org/pdf/1903.05134.pdf.----------minor comments)------ wrong section and figure references:----- - 'it also mitigates the co-adaptation issue which we will discuss in section 3.3'(indeed it is section 3.4), 'the network architecture along with feature map resolution and channels number are shown in figure 4' (it should be figure 3).----- - - figure 3(d) referred to in section 4.4 would be figure 2(d) indeed.----------about rating)-----the authors provided a novel technique about the resizable approach and the experimental results look promising.",0.1584158415841584,0.0202020202020202,0.1188118811881188,0.1188118811881188,0.2077922077922077,0.0131578947368421,0.1298701298701298,0.1298701298701298,0.2719999999999999,0.048780487804878,0.176,0.176,0.1584158415841584,0.0202020202020202,0.1188118811881188,0.1188118811881188,0.2077922077922077,0.0131578947368421,0.1298701298701298,0.1298701298701298,0.2077922077922077,0.0131578947368421,0.1298701298701298,0.1298701298701298,0.1408450704225352,0.0,0.1126760563380281,0.1126760563380281,0.2077922077922077,0.0131578947368421,0.1298701298701298,0.1298701298701298,9.704733848571776,9.704733848571776,15.11566162109375,9.704733848571776,15.11566162109375,8.219084739685059,9.704733848571776,8.554291725158691,0.9858039237317106,0.9861098983977333,0.19155603724409193,0.9620727666790229,0.9656496489205643,0.13882344659412277,0.3260122375172478,0.3643625951874521,0.9307032442188938,0.9858039237317106,0.9861098983977333,0.19155572716757163,0.9620727666790229,0.9656496489205643,0.13882344659412277,0.9620727666790229,0.9656496489205643,0.13882344659412277,0.7233259278757066,0.8427628617400118,0.9381055614150374,0.9620727666790229,0.9656496489205643,0.13882344659412277
20,https://openreview.net/forum?id=BJg15lrKvS,"the paper aims to provide theoretical justification for a ""spectral bias"" that is observed in training of neural networks: a phenomenon recorded in literature (rahaman et al.), where lower frequency components of a signal are fit faster than higher frequency ones. the contributions of the paper are as follows:-----1. proves an upper bound on the rate of convergence on the residual error projected on top few eigenfunctions (of a certain integral operator). the upper bound is in terms of the eigenvalues of the corresponding eigenfunctions and is distribution independent.-----2. provides an upper bound on the decay of eigenvalues in the case of depth-2 relu networks and also a exact characterization of the eigenfunctions. while such upper bounds and the characterization of eigenfunctions existed in literature earlier, it is argued that the new bounds are better.-----3. combining the above two results, a justification is obtained for the ""spectral bias"" phenomenon that is recorded in literature.-----4. some toy experiments are provided to exhibit the spectral bias phenomenon.----------recommendation:-----i recommend ""weak acceptance"". the paper takes a step towards explaining the phenomenon of spectral bias in deep learning. while concrete progress is made in the context of depth-2 relu networks (even though in ntk regime), perhaps the ideas could be extended to deeper networks.----------technical comments:------ it is argued that the new bound of is better than the bound of from the previous work of bietti and mairal, in the regime where . i think there is a typo here. in the regime of , the bound is the smaller one so both bounds are comparable. it is argued that is the more relevant regime, but then there isn't any improvement here.------ the proof of spectral analysis is said to follow a similar outline as compared to the prior work of bietti-mairal, but it is not clear to me where this new proof deviates and improves on prior techniques? or is it just a more careful analysis of the prior techniques?------ the proof operates in the ""neural tangent kernel"" regime, by considering hugely overparameterized networks. this can be viewed as a negative thing, but then, most results in literature also operate in this regime and it is a major challenge for the field to prove results in the mildly overparameterized / non-ntk regime!----------potential suggestions for improvement:------ in section 4: the y-axis of the graph is labeled ""error's coefficient"" which is non-informative. is it ----- ? i also had a question here about the proposed nystrom method: why is it okay to use the training points in the nystrom method. ideally, we should use freshly sampled points. is there a justification for using the training points? if not, perhaps it is best to go with freshly sampled points.------ i felt the proofs in the appendix are very opaque and it is hard to pinpoint what the new insight is (at least for a reader, like me, who does not have an in-depth familiarity with these convergence proofs). i must qualify my review by stating that i am not an expert in kernel methods, and the mathematics in the proof is more advanced than i typically use. so it is possible that there are technical flaws to this work that i did not notice.----------that being said, i found this to be quite an interesting paper. it provides a concise explanation for the types of features learned by anns: those that correspond to the largest eigenvalues of the kernel function. because these typically correspond to the lowest-frequency components, this means that the anns tend to first learn the low frequency components of their target functions. this provides a nice explanation for how anns can both: a) have enough capacity to memorize random data; yet b) generalize fairly well in many tasks with structured input data. in the case of structured data, there are low frequency components that correspond to successfully generalized solutions.----------i have a few questions about the generality of this result, and its application to make better machine learning systems:----------1) as far as i can tell, the proof applies strictly vanilla sgd (algorithm 1). would it be possible to extend this proof to other optimizers (say, adam)? that extension would help to connect this theory to the practical side of the field.----------2) given that the kernel depends on the loss function, and it's the eigenspectrum of the kernel's integrator operator that determines the convergence properties, can this work be applied to engineering better loss functions for practical applications?","the authors propose to understand spectral bias during training of neural networks from the perspective of the ntk. while reviewers appreciated aspects of the work, the general consensus was that the current version is not ready for publication; some concerns stem from whether the the ntk model and finite neural networks are sufficiently similar that we should be able to gain real practical insights into the behaviour of finite models. this is partly an empirical question, and stronger experiments are required to have a better sense of the answer. nonetheless, the authors are encouraged to persist with this work, taking into account reviewer comments in future revisions.",the contributions of the paper are as follows:-----1. proves an upper bound on the rate of convergence on the residual error projected on top few eigenfunctions (of a certain integral operator).,"the paper aims to provide theoretical justification for a ""spectral bias"" that is observed in training of neural networks: a phenomenon recorded in literature (rahaman et al.), where lower frequency components of a signal are fit faster than higher frequency ones.","while concrete progress is made in the context of depth-2 relu networks (even though in ntk regime), perhaps the ideas could be extended to deeper networks.----------technical comments:------ it is argued that the new bound of is better than the bound of from the previous work of bietti and mairal, in the regime where .","the paper aims to provide theoretical justification for a ""spectral bias"" that is observed in training of neural networks: a phenomenon recorded in literature (rahaman et al.), where lower frequency components of a signal are fit faster than higher frequency ones.","or is it just a more careful analysis of the prior techniques?------ the proof operates in the ""neural tangent kernel"" regime, by considering hugely overparameterized networks.","the paper aims to provide theoretical justification for a ""spectral bias"" that is observed in training of neural networks: a phenomenon recorded in literature (rahaman et al.), where lower frequency components of a signal are fit faster than higher frequency ones.",the paper takes a step towards explaining the phenomenon of spectral bias in deep learning.,"while concrete progress is made in the context of depth-2 relu networks (even though in ntk regime), perhaps the ideas could be extended to deeper networks.----------technical comments:------ it is argued that the new bound of is better than the bound of from the previous work of bietti and mairal, in the regime where .",0.1438848920863309,0.0145985401459854,0.1151079136690647,0.1151079136690647,0.2027027027027027,0.0547945205479452,0.1486486486486486,0.1486486486486486,0.3086419753086419,0.025,0.1851851851851851,0.1851851851851851,0.2027027027027027,0.0547945205479452,0.1486486486486486,0.1486486486486486,0.1353383458646616,0.015267175572519,0.0902255639097744,0.0902255639097744,0.2027027027027027,0.0547945205479452,0.1486486486486486,0.1486486486486486,0.1311475409836065,0.0166666666666666,0.0655737704918032,0.0655737704918032,0.3086419753086419,0.025,0.1851851851851851,0.1851851851851851,11.03071117401123,10.098518371582031,11.03071117401123,11.030710220336914,11.371519088745115,6.561925411224365,6.561925411224365,9.26362133026123,0.9634015860703609,0.9636970716662195,0.9255082869700911,0.9741786724717564,0.9598301712720171,0.3463809939002389,0.9674665030789814,0.9697111682892352,0.4474317300003955,0.9741786724717564,0.9598301712720171,0.3463810290431536,0.9508232666250895,0.9267132138493795,0.6777290936072549,0.9741786724717564,0.9598301712720171,0.3463809939002389,0.9658349170107533,0.9611933789788698,0.9305743804423502,0.9674665030789814,0.9697111682892352,0.44743213077622307
21,https://openreview.net/forum?id=BJg7x1HFvB,"this paper proposes to pre-train a student before training with a teacher, which is easy to understand. although the authors provide extensive empirical studies, i do not think they can justify the claims in this paper. ---------------** argument----------one concern is that compared to other baselines such as ""patient knowledge distillation"" [1], the proposed method is not consistently better. the authors argue that [1] is more sophisticated in that they distill task knowledge from intermediate teacher activations. however, the proposed method introduces other extra complexities, such as pre-training the student. i do not agree that the proposed method is less elaborate than previous methods. ---------------although the investigation on influence of model size and the amount/quality of unlabeled data is interesting in itself, this does not help justify the usefulness of pre-training a student. i hypothesize that when considering the intermediate feature maps as additional training signals, randomly initialized students can catch up with pre-trained students. ----------furthermore, the mixed results shown in table 3 do not justify the proposed method well enough. ----------[1] patient knowledge distillation for bert model compression, https://arxiv.org/abs/1908.09355 this submission revisits the student-teacher paradigm and shows through extensive experiments that pre-training a student directly on masked language modeling is better than distillation (from scratch). it also shows that the best is to combine both and distill from that pre-trained student model.----------my rating is weak accept. i think the submission highlights a very useful observation about knowledge distillation that i imagine is overlooked by many researchers and practitioners. the decision of weak as opposed to a strong accept is because the submission does not introduce anything truly novel, but simply points out observations and offers a recommended training strategy. however, i do argue for its acceptance, because it does a thorough job and presents many interesting findings that can benefit the community.----------comparison with prior work:----------the submission focuses on comparison with sun et al. and sanh. these comparisons are important, but not the most compelling part of the paper. comparison with more prior work that show large benefits would make the paper even stronger.----------interesting experiments:----------the paper presents many interesting experiments useful for anyone trying to develop a compressed model. first, it shows that distillation (from scratch) by itself may be overrated, since simply repeating the pre-training+fine-tuning procedure on the small model directly is effective. however, distillation remains relevant since it also shows that pre-training the student, then distilling against a teacher, is a potent combination. in the case when the transfer set is the same size as the pre-training set, it surprisingly still has some benefits. this is not experimentally explained, but i suspect there are optimization benefits that are hard to pin down exactly. the paper hypothesizes that the two methods learn different linguistic aspects, but i think it is a bit too speculative to put it in such terms.----------the experiments are thorough, with many student sizes, transfer set sizes, transfer set/task set correlation, etc. it also compares against the truncation technique, where the student is initialized with a truncated version of the teacher. there are no error bars in the plots, but there are so many plots with clear trends, that this is not a big concern. i cant think of any experiments that are obviously missing.----------misc:----------- the introduction says that the pre-training+fine-tuning baseline has been overlooked. it would be great to point out papers that has actually overlooked this baseline. including this in the results would be even better.------ during my first read-through, i got confused because i didnt realize pre-training in most of the paper refers to student pre-training (as opposed to simply training the teacher). making this a bit more explicit here and there can avoid this confusion. the authors investigate the problem of training compact pre-trained language model via distillation. their method consists of three steps: -----1. pre-train the compact model lm-----2. distill the compact model lm with a larger model (teacher)-----3. fine-tune the compact model on target task ----------this idea is not significantly new since it is quite common to apply distillation to compress models, and the results are largely empirical. from table 3 the results on test sets are better than previous works, but not by much. the authors spend quite a of space on ablation studies to investigate the contribution of different factors, and on cross-domain transfers. they do manage to show that using a teacher for distilling a compact student model does better than directly pre-training a compact model on the nli* task in section 6.3. it would be better if they could show it for other tasks on the benchmark as well. ----------overall i think this work is somewhat incremental, and falls below the acceptance threshold.","though the reviewers thought the ideas in this paper were interesting, they questioned the importance and magnitude of the contribution. though it is important to share empirical results, the reviewers were not sure that there was enough for this paper to be accepted.","the decision of weak as opposed to a strong accept is because the submission does not introduce anything truly novel, but simply points out observations and offers a recommended training strategy.","their method consists of three steps: -----1. pre-train the compact model lm-----2. distill the compact model lm with a larger model (teacher)-----3. fine-tune the compact model on target task ----------this idea is not significantly new since it is quite common to apply distillation to compress models, and the results are largely empirical.","---------------** argument----------one concern is that compared to other baselines such as ""patient knowledge distillation"" [1], the proposed method is not consistently better.","this paper proposes to pre-train a student before training with a teacher, which is easy to understand.",comparison with more prior work that show large benefits would make the paper even stronger.----------interesting experiments:----------the paper presents many interesting experiments useful for anyone trying to develop a compressed model.,"their method consists of three steps: -----1. pre-train the compact model lm-----2. distill the compact model lm with a larger model (teacher)-----3. fine-tune the compact model on target task ----------this idea is not significantly new since it is quite common to apply distillation to compress models, and the results are largely empirical.","first, it shows that distillation (from scratch) by itself may be overrated, since simply repeating the pre-training+fine-tuning procedure on the small model directly is effective.","their method consists of three steps: -----1. pre-train the compact model lm-----2. distill the compact model lm with a larger model (teacher)-----3. fine-tune the compact model on target task ----------this idea is not significantly new since it is quite common to apply distillation to compress models, and the results are largely empirical.",0.2162162162162162,0.0,0.1351351351351351,0.1351351351351351,0.303030303030303,0.0206185567010309,0.1414141414141414,0.1414141414141414,0.1538461538461538,0.0,0.123076923076923,0.123076923076923,0.1639344262295082,0.0338983050847457,0.1311475409836065,0.1311475409836065,0.2133333333333333,0.0,0.1599999999999999,0.1599999999999999,0.303030303030303,0.0206185567010309,0.1414141414141414,0.1414141414141414,0.1690140845070422,0.0,0.0845070422535211,0.0845070422535211,0.303030303030303,0.0206185567010309,0.1414141414141414,0.1414141414141414,8.920645713806152,8.628732681274414,15.632390975952148,8.920645713806152,7.816235065460205,11.929461479187012,8.920645713806152,8.75487995147705,0.9563971133655815,0.9556990877311878,0.8352580160109737,0.6872577590928437,0.8670512031161239,0.8348928694281949,0.9750655085319533,0.9708423141256802,0.8825046214392521,0.9711094259576695,0.9581275002119978,0.8213362401806432,0.9387951563135324,0.9603104841817303,0.0435731683707481,0.6872577590928437,0.8670512031161239,0.8348928694281949,0.8987092838799849,0.923775035489578,0.9218824245796816,0.6872577590928437,0.8670512031161239,0.8348928694281949
22,https://openreview.net/forum?id=BJgMFxrYPB,"the paper proposes an interesting, and to the best of my knowledge novel, pipeline for learning a semantic map of the environment with respect to navigability, and simultaneously uses it for further exploring the environment.----------the pipeline can be summarized as follows: navigate somewhere using some heuristic. when navigation ""works"", as well as when encountering something ""negative"", back-project that into past frames, and label the corresponding pixels as such: either positive or negative. this generates a collection of partially densely labelled images, on which a segmentation network can be learned that learns which part of the rgbd input are navigable and which should be avoided. for navigation, navigability of the current frame is predicted, and that prediction is down-projected into an ""affordance map"" that is used for navigation. one experiment confirms the usefulness of such an affordance map.---------------i am marking weak reject currently because of the following concerns, which might be me just missing something. on the one hand, i am glad to see something that is not just blind ""end to end rl with exploration bonus"", sounds reasonable, and works well. on the other hand, i do have several major concerns about the method, outlined as follows:----------1. how can this approach work for moving obstacles? let's say a monster walks from point a to point b, and collides with the agent at point b. then, point b is marked as a hazard, but in the previous frames, the monster is not located at point b, and thus an image region that does not contain the monster is marked as hazard. am i missing something here?-----2. the method does not seem practical for actual mobile robots, only for in-game or in-simulation agents. the reason being that in order to learn ""robot should not bump into baby"", the robot actually needs to bump into multiple babies in order to collect data about that hazard. to be fair, blind ""ppo+exploration bonus"" suffers from the same problem, but in this paper, the whole motivation is about mobile robots (at least that was my impression after reading it).----------furthermore, i do not think i would be able to reproduce any of the experiments, as many details are missing. will code be released?---------------###### post-rebuttal update----------i am happy with the author's response to my concerns, and they have included corresponding discussions in their paper. thus, i am improving my rating to recommend acceptance of this paper to icrl2020. the paper proposes to learn affordance maps: a method to judge whether a certain location is accessible. this is done by distilling a series of ""trial and error"" runs and the relation of a pixel in the image/depth plane to a corrdinate into a model.----------i like the idea and think the paper should be accepted. the idea to use trial and error (something i prefer to self-supervision, which is used differently in many contexts, i believe) to obtain a data set for learning a model is nice and very practical.----------some concerns that i think should be adressed.----------- the term information gain is used wrongly. the entropy of class labels is not infogain. infogain is the expected kl of the model posterior from the model prior. please correct this. see [1, 2].------ *learning* a model of the environment and using it for navigation/exploratin has also been tackled recently by [1]. i think the authors should draw connetions to that work.------ self-supervision has recently been proposed by lecun as a subsitute (of sorts) for unsupervised learning. what he means is that a part of the data is used to predict another part of the data. i have no hard feelings about the term, personally preferring unsupervised, but the authors should be aware of the name clash.----------i wonder how the authors envision to extend this method to real scenarios. the ""trial and error"" method is clearly not viable for robotics setups, as hazards are costly. it would be nice if the authors could give there perspective on things.----------[1] depeweg et al, ""decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning"", proceedings of the 35th international conference on machine learning-----[2] mirchev et al, ""approximate bayesian inference in spatial environments"" in proceedings of robotics: science and systems xv.","this paper presents a framework for navigation that leverages learning spatial affordance maps (ie what parts of a scene are navigable) via a self-supervision approach in order to deal with environments with dynamics and hazards. they evaluate on procedurally generated vizdoom levels and find improvements over frontier and rl baseline agents.----------reviewers all agreed on the quality of the paper and strength of the results. authors were highly responsive to constructive criticism and the engagement/discussion appears to have improved the paper overall. after seeing the rebuttal and revisions, i believe this paper will be a useful contribution to the field and im happy to recommend accept.","to be fair, blind ""ppo+exploration bonus"" suffers from the same problem, but in this paper, the whole motivation is about mobile robots (at least that was my impression after reading it).----------furthermore, i do not think i would be able to reproduce any of the experiments, as many details are missing.","it would be nice if the authors could give there perspective on things.----------[1] depeweg et al, ""decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning"", proceedings of the 35th international conference on machine learning-----[2] mirchev et al, ""approximate bayesian inference in spatial environments"" in proceedings of robotics: science and systems xv.","for navigation, navigability of the current frame is predicted, and that prediction is down-projected into an ""affordance map"" that is used for navigation.","the paper proposes an interesting, and to the best of my knowledge novel, pipeline for learning a semantic map of the environment with respect to navigability, and simultaneously uses it for further exploring the environment.----------the pipeline can be summarized as follows: navigate somewhere using some heuristic.","the reason being that in order to learn ""robot should not bump into baby"", the robot actually needs to bump into multiple babies in order to collect data about that hazard.","it would be nice if the authors could give there perspective on things.----------[1] depeweg et al, ""decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning"", proceedings of the 35th international conference on machine learning-----[2] mirchev et al, ""approximate bayesian inference in spatial environments"" in proceedings of robotics: science and systems xv.","the paper proposes an interesting, and to the best of my knowledge novel, pipeline for learning a semantic map of the environment with respect to navigability, and simultaneously uses it for further exploring the environment.----------the pipeline can be summarized as follows: navigate somewhere using some heuristic.","it would be nice if the authors could give there perspective on things.----------[1] depeweg et al, ""decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning"", proceedings of the 35th international conference on machine learning-----[2] mirchev et al, ""approximate bayesian inference in spatial environments"" in proceedings of robotics: science and systems xv.",0.175,0.0253164556962025,0.1125,0.1125,0.1939393939393939,0.0122699386503067,0.0848484848484848,0.0848484848484848,0.1515151515151515,0.0461538461538461,0.106060606060606,0.106060606060606,0.2838709677419355,0.0522875816993463,0.1677419354838709,0.1677419354838709,0.158273381294964,0.0291970802919708,0.1007194244604316,0.1007194244604316,0.1939393939393939,0.0122699386503067,0.0848484848484848,0.0848484848484848,0.2838709677419355,0.0522875816993463,0.1677419354838709,0.1677419354838709,0.1939393939393939,0.0122699386503067,0.0848484848484848,0.0848484848484848,5.362485885620117,2.0839810371398926,14.14948558807373,5.362485885620117,4.185730457305908,7.558347225189209,5.362485885620117,14.14948558807373,0.91118972243998,0.9331592775317789,0.9106379051751806,0.22567061644627545,0.15854582791689434,0.702411929000787,0.952583175387322,0.9596960517571754,0.06408240261409168,0.9847119461727449,0.9829628610484855,0.9326994781612277,0.9439986759329998,0.9516010879314388,0.9330852277091811,0.22567061644627545,0.15854582791689434,0.702411929000787,0.9847119461727449,0.9829628610484855,0.9326994781612277,0.22567061644627545,0.15854582791689434,0.7024132612646292
23,https://openreview.net/forum?id=BJgcwh4FwS,"this paper proposed a graph net based approach for subgraph matching. the general idea is based on the graph matching network (li et.al, icml 2019) that computes the node embeddings of two graphs with co-attentions. the training requires the supervision of ground truth matching. during inference an iterative method with heuristic stopping criteria is used. experiments on tiny graphs show better results than learning based baselines, but worse results than mcs solver.----------overall the paper is well motivated. however there are several major concerns with the paper:----------1. since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision. also it seems that getting slightly faster but much worse results than the solver on small graphs is not that exciting. ----------2. it seems there's a mismatch between training and inference. the inference method is done iteratively, where the eq (6) is somewhat not clear to me: as this ||w1-w2|| criteria is not trained during training, it seems quite heuristic by doing so. ----------3. im not sure why the two stop conditions are needed. one can easily check (incrementally) whether the added nodes are isomorphic. ----------4. the graphs used in experiments are too small. ----------some other minor issues:----------it would be better to define y with eq (1) and eq (2) in the paper. there seems to be no explicit definition of y. the authors proposed a novel method to find the maximum common subgraph (mcs) of two graphs. i am familiar with the quadratic assignment problem (qap) based graph matching and i am not very familiar with the mcs problem. ----------the authors adopt graph matching networks (gmn) for feature embedding, and then similarity matrix x can be generated by computing the similarities between the embeddings. the similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. the assignment matrix then can be given from x. then a novel procedure, named guided subgraph extraction (gse, which is considered as the main contribution of this paper), is used to get an mcs from assignment matrix. here the authors may consider a simple baseline, which is to use qap to give the assignment matrix, and then run gse to obtain the mcs. ----------overall the paper is well written, and the experiment is good and solid. ----------some suggestions:-----the gcn based gmn might not be the best choice for graph embedding. the authors may consider stronger graph neural networks such as dgcnn (used in [3]) or message passing neural network (used in [4] and [5]) as the graph embedding module in the future work. ----------[1] deep learning of graph matching, cvpr18-----[2] learning combinatorial embedding networks for deep graph matching. iccv19-----[3] deep closest point: learning representations for point cloud registration. iccv19-----[4] deep graphical feature learning for the feature matching problem, iccv19-----[5] neural message passing for quantum chemistry, icml17 this paper proposes a novel algorithm neuralmcs for maximum common subgraph (mcs) identification. the proposed algorithm consists of two components. one is a neural-network model based on graph matching networks (gmn, li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth mcs results. another is the algorithm called gse (guided subgraph extraction) to obtain an mcs by making an explicit assignment from the estimated matching matrix by the nn model. experimental comparisons are made to other nn-based approaches combined with threshold-based assignments by the hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm mcsplit, and show the effectiveness of neuralmcs.----------this paper proposes an interesting method for mcs detection which would have large application interests. though the basic idea is nice, the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. ----------the algorithm has two parts, and the first nn part to learn a matching matrix is mostly based on the already existing algorithm of gmn (li et al, 2019). the novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other nns (for example, gat?). the second part gse to get an explicit subgraph also seem to be applied independently to the first part. we can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part.----------i understand that mcs detection from a matching matrix (and node state vectors) is not exact if we just use hungarian-like linear assignment problem (lap) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and gse parts (which brings the novelty) can be more carefully evaluated through some 'ablation studies' using some simple alternative substitutes.","this paper proposed graph neural networks based approach for subgraph detection. the reviewers find that the overall the paper is interesting, however further improvements are needed to meet iclr standard: -----1. experiments on larger graph. slight speedup in small graphs are less exciting. -----2. it seems there's a mismatch between training and inference. -----3. the stopping criterion is quite heuristic.","we can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part.----------i understand that mcs detection from a matching matrix (and node state vectors) is not exact if we just use hungarian-like linear assignment problem (lap) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and gse parts (which brings the novelty) can be more carefully evaluated through some 'ablation studies' using some simple alternative substitutes.","----------[1] deep learning of graph matching, cvpr18-----[2] learning combinatorial embedding networks for deep graph matching.","----------the algorithm has two parts, and the first nn part to learn a matching matrix is mostly based on the already existing algorithm of gmn (li et al, 2019).",this paper proposed a graph net based approach for subgraph matching.,"----------[1] deep learning of graph matching, cvpr18-----[2] learning combinatorial embedding networks for deep graph matching.","one is a neural-network model based on graph matching networks (gmn, li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth mcs results.",----------4. the graphs used in experiments are too small.,"one is a neural-network model based on graph matching networks (gmn, li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth mcs results.",0.1456953642384106,0.0,0.0794701986754966,0.0794701986754966,0.1558441558441558,0.0,0.1038961038961038,0.1038961038961038,0.2,0.0,0.1111111111111111,0.1111111111111111,0.2499999999999999,0.1428571428571428,0.2222222222222222,0.2222222222222222,0.1558441558441558,0.0,0.1038961038961038,0.1038961038961038,0.1935483870967742,0.0219780219780219,0.1505376344086021,0.1505376344086021,0.1714285714285714,0.0,0.1142857142857143,0.1142857142857143,0.1935483870967742,0.0219780219780219,0.1505376344086021,0.1505376344086021,8.411170959472656,11.86920166015625,15.186753273010254,11.869199752807615,6.069941997528076,6.91154670715332,8.411170959472656,10.74731731414795,0.5462835930915049,0.6582962521610318,0.9485784059432354,0.9681131650245689,0.9681967914119123,0.11087959008579258,0.7877268500792598,0.8024341544339241,0.8447177276325023,0.9565425895555159,0.9583726136065223,0.9248088801107752,0.9681131650245689,0.9681967914119123,0.11087959008579258,0.7385385987947867,0.8619071815296658,0.357496642238045,0.9615890063672471,0.9557027435066708,0.2989013649653819,0.7385385987947867,0.8619071815296658,0.35749541179106287
24,https://openreview.net/forum?id=BJglA3NKwS,"in this paper, the authors propose a new mechanism to perform the attention operators. the similarity between a key and a query is performed as the dot product between a trainable weight and the addition of the key and query. the proposed siamese attention operator is much more efficient than prior attention methods in terms of speed. the evaluation on a few computer vision tasks shows the presented method performs as well as the typical attention methods, but it runs much faster. ----------first of all, i think this attention method should be quite useful for various neural networks. it is faster and performs equally well as other attention operators. however, i do have some concerns about this method.----------- it is not clear to me why this method works. (a+b)^t*w is a strange expression to compute the similarity between a and b. no much explanation or intuition is given in the paper, and i have no clue why this works.----------- it seems to me that the proposed method performs slightly worse than the regular attention, according to figure 6. ----------my overall rating is borderline. it would be great if the authors can resolve my concerns.----------other questions:------ why sanet (mobilenetv2+sao) has fewer parameters than mobilenetv2 in table 3?------ is attention an operator that significantly slows the speed of the whole network? if not, the speedup of attention is not that important. the authors introduce a novel self-attention operator for neural networks. their self-attention operator computes similarity between elements a and b as (a+b)^tw where w is a learned parameter and does not use the softmax operator. this leads to improvements in space and time complexity compared to regular self-attention which uses the dot product (a^tb).-----they show that concatenating their operation with convolution brings improvements over the mobilenetv2 baseline on imagenet classification and over u-net on restoration tasks.----------attention has been empirically shown to bring improvements in many visual tasks but certain methods (such as self-attention) can be quite expensive in computations and memory. identifying cheaper attention mechanisms that obtain similar accuracy performance as expensive attention mechanisms is therefore an important direction for work.----------however, i take several crucial issues with this work and especially the evaluation/presentation of the methods:------ although this is the focus of the work, the authors do not report actual memory consumption and latency times for mobilenetv2, sanet and the other attention mechanisms. (there is no need for the simulated scenarios of table 2 since we already know the theoretical complexities of the different methods).------ the authors only compare their methods against regular self-attention (without or with pooling/softmax), and ignore a longstanding literature of other (potentially cheaper in terms of memory and computations) attention mechanisms in vision (see below). without comparison to at least squeeze-and-excite, it is hard to evaluate the significance of the method presented in the draft.------ the motivation for naming the method ""siamese"" is quite poor. siamese networks typically are more complex than a single layer feed forward (which is just a dot-product). furthermore, the siamese similarity (as introduced by the authors) does not respect the usual properties of similarity functions. for example siasim(a, 0) = a^tw = 1/2 siasim(a,a) can take arbitrary values including negative values (""a can be dissimilar with itself"")------ x vs q, k, v? self-attention is incorrectly described as ""a special case of the attention operator with q = k = v"", instead of q = xw^q, k=xw^k, v = xw^v.------ in table 7, shouldn't sanet w/o params have less params than sanet?----------in summary, the paper addresses an important challenge and proposes a technically sound method. however, the current draft has fundamental experimental flaws in its evaluation/presentation and lacks comparison against relevant cheap channelwise attention mechanisms (such as squeeze-and-excitation). i argue for rejection.----------relevant literature:------ channelwise attention: squeeze-and-excitation, gather-excite------ channelwise and spatial attention: bottleneck attention module, convolutional block attention module------ relative self-attention for vision: attention augmented convolutional networks, an empirical study of spatial attention mechanisms in deep networks.","the submission presents a siamese attention operator that lowers the computational costs of attention operators for applications such as image recognition. the reviews are split. r1 posted significant concerns with the content of the submission. the concerns remain after the authors' responses and revision. one of the concerns is the apparent dual submission with ""kronecker attention networks"". the ac agrees with these concerns and recommends rejecting the submission.","(a+b)^t*w is a strange expression to compute the similarity between a and b. no much explanation or intuition is given in the paper, and i have no clue why this works.----------- it seems to me that the proposed method performs slightly worse than the regular attention, according to figure 6.","i argue for rejection.----------relevant literature:------ channelwise attention: squeeze-and-excitation, gather-excite------ channelwise and spatial attention: bottleneck attention module, convolutional block attention module------ relative self-attention for vision: attention augmented convolutional networks, an empirical study of spatial attention mechanisms in deep networks.","in this paper, the authors propose a new mechanism to perform the attention operators.","in this paper, the authors propose a new mechanism to perform the attention operators.","i argue for rejection.----------relevant literature:------ channelwise attention: squeeze-and-excitation, gather-excite------ channelwise and spatial attention: bottleneck attention module, convolutional block attention module------ relative self-attention for vision: attention augmented convolutional networks, an empirical study of spatial attention mechanisms in deep networks.","i argue for rejection.----------relevant literature:------ channelwise attention: squeeze-and-excitation, gather-excite------ channelwise and spatial attention: bottleneck attention module, convolutional block attention module------ relative self-attention for vision: attention augmented convolutional networks, an empirical study of spatial attention mechanisms in deep networks.","if not, the speedup of attention is not that important.","i argue for rejection.----------relevant literature:------ channelwise attention: squeeze-and-excitation, gather-excite------ channelwise and spatial attention: bottleneck attention module, convolutional block attention module------ relative self-attention for vision: attention augmented convolutional networks, an empirical study of spatial attention mechanisms in deep networks.",0.1818181818181818,0.0,0.1322314049586777,0.1322314049586777,0.1621621621621621,0.0,0.1081081081081081,0.1081081081081081,0.1463414634146341,0.0499999999999999,0.1219512195121951,0.1219512195121951,0.1463414634146341,0.0499999999999999,0.1219512195121951,0.1219512195121951,0.1621621621621621,0.0,0.1081081081081081,0.1081081081081081,0.1621621621621621,0.0,0.1081081081081081,0.1081081081081081,0.1282051282051282,0.0263157894736842,0.1025641025641025,0.1025641025641025,0.1621621621621621,0.0,0.1081081081081081,0.1081081081081081,8.253093719482422,8.253093719482422,14.077024459838867,8.253093719482422,7.444435596466064,14.077024459838867,8.253093719482422,9.44434928894043,0.9782810763669242,0.9750898081394046,0.8954586397937563,0.03310871239538443,0.10443430509402904,0.4417739219492153,0.9700551591593283,0.9714101677991777,0.10347209396147261,0.9700551591593283,0.9714101677991777,0.10347218645367212,0.03310871239538443,0.10443430509402904,0.4417739219492153,0.03310871239538443,0.10443430509402904,0.4417739219492153,0.8442279209199212,0.9006274537362505,0.6241031971925906,0.03310871239538443,0.10443430509402904,0.4417728656682967
25,https://openreview.net/forum?id=BJl6AjC5F7,"this paper looks at learning to represent edits for text revisions and code changes. the main contributions are as follows:--------* they define a new task of representing and predicting textual and code changes --------* they make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change--------* they try simple neural network models that show good performance in representing and predicting the changes----------------the nlp community has recently defined the problem of predicting atomic edits for text data (faraqui, et al. emnlp 2018, cited in the paper), and that is the source of their wikipedia revision dataset. although it is an interesting problem, it is not immediately clear from the introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and i hope the next version would elaborate on the motivation and significance for this new task. ----------------the ""fixer"" dataset that they created is interesting. those edits supposedly make the code better, so modeling those edits could lead to ""better"" code. having that as labeled data enables a clean and convincing evaluation task of predicting similar edits.----------------the paper focuses on the novelty of the task and the dataset, so the models are simple variations of the existing bidirectional lstm and the gated graph neural network. because much of the input text (or code) does not change, the decoder gets to directly copy parts of the input. for code data, the ast is used instead of flat text of the code. these small changes seem reasonable and work well for this problem.----------------evaluation is not easy for this task. for the task of representing the edits, they show visualizations of the clusters of similar edits and conduct a human evaluation to see how similar these edits actually are. this human evaluation is not described in detail, as they do not say how many people rated the similarity, who they were (how they were recruited), how they were instructed, and what the inter-rater agreement was. the edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better. that may be true, but then without another metric for better generalization, one cannot say that better performance means worse generalization. ----------------despite these minor issues, the paper contributes significantly novel task, dataset, and results. i believe it will lead to interesting future research in representing text and code changes. the main contributions of the paper are an edit encoder model similar to (guu et al. 2017 http://aclweb.org/anthology/q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. the paper is clearly written, and provides clear support for each of their main claims.----------------i think this would be of interest to nlp researchers and others working on sequence- and graph-transduction models, but i think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. this would also benefit greatly from a more direct comparison to guu et al. 2017, which presents a very similar ""neural editor"" model.----------------some more specific points:----------------- i really like the idea of transferring edits from one context to another. the one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are.----------------- if i'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. i wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. have you explored more constrained versions of the edit encoder (such as the bag-of-edits from guu et al. 2017) or alternate learning objectives to control for this?----------------- the wikiatomicedits corpus has 13.7 million english insertions - why did you subsample this to only 1m? there is also a human-annotated subset of that you might use as evaluation data, similar to the c#fixers set.----------------- on the human evaluation: who were the annotators? the categories ""similar edit"", and ""semantically or syntactically same edit"" seem to leave a lot to interpretation; were more specific instructions given? it also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits.----------------- on the automatic evaluation: accuracy seems brittle for evaluating sequence output. did you consider reporting bleu, rouge, or another ""soft"" sequence metric?----------------- it would be worth citing existing literature on classification of wikipedia edits, for example yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). an interesting experiment would be to correlate your edit encodings with their taxonomy.","this paper investigates learning to represent edit operations for two domains: text and source code. the primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). the technical novelty is relatively weak. pros: the paper introduces a new dataset for source code edits. cons: reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. as a result, r3 updated their score from 4 to 6. verdict: possible weak accept. none of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real-world application of the automatic edits). however, the overall impact and novelty of the paper is relatively weak.","although it is an interesting problem, it is not immediately clear from the introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and i hope the next version would elaborate on the motivation and significance for this new task.","the main contributions are as follows:--------* they define a new task of representing and predicting textual and code changes --------* they make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change--------* they try simple neural network models that show good performance in representing and predicting the changes----------------the nlp community has recently defined the problem of predicting atomic edits for text data (faraqui, et al. emnlp 2018, cited in the paper), and that is the source of their wikipedia revision dataset.","the main contributions are as follows:--------* they define a new task of representing and predicting textual and code changes --------* they make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change--------* they try simple neural network models that show good performance in representing and predicting the changes----------------the nlp community has recently defined the problem of predicting atomic edits for text data (faraqui, et al. emnlp 2018, cited in the paper), and that is the source of their wikipedia revision dataset.",this paper looks at learning to represent edits for text revisions and code changes.,"the main contributions are as follows:--------* they define a new task of representing and predicting textual and code changes --------* they make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change--------* they try simple neural network models that show good performance in representing and predicting the changes----------------the nlp community has recently defined the problem of predicting atomic edits for text data (faraqui, et al. emnlp 2018, cited in the paper), and that is the source of their wikipedia revision dataset.","the main contributions of the paper are an edit encoder model similar to (guu et al. 2017 http://aclweb.org/anthology/q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings.",i believe it will lead to interesting future research in representing text and code changes.,"the main contributions are as follows:--------* they define a new task of representing and predicting textual and code changes --------* they make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change--------* they try simple neural network models that show good performance in representing and predicting the changes----------------the nlp community has recently defined the problem of predicting atomic edits for text data (faraqui, et al. emnlp 2018, cited in the paper), and that is the source of their wikipedia revision dataset.",0.2116402116402116,0.0106951871657754,0.1058201058201058,0.1058201058201058,0.3391304347826087,0.043859649122807,0.1913043478260869,0.1913043478260869,0.3391304347826087,0.043859649122807,0.1913043478260869,0.1913043478260869,0.130718954248366,0.0529801324503311,0.130718954248366,0.130718954248366,0.3391304347826087,0.043859649122807,0.1913043478260869,0.1913043478260869,0.2197802197802198,0.1111111111111111,0.1978021978021978,0.1978021978021978,0.0779220779220779,0.0131578947368421,0.0649350649350649,0.0649350649350649,0.3391304347826087,0.043859649122807,0.1913043478260869,0.1913043478260869,11.850996017456056,16.679298400878906,21.23236846923828,16.679298400878906,8.742019653320312,16.679298400878906,16.679298400878906,13.555705070495604,0.897062612311125,0.9228355002426305,0.31050754121953805,0.9888644479387974,0.8315674697005286,0.44213757777981494,0.9888644479387974,0.8315674697005286,0.4421377372729991,0.9789839173198723,0.9754097532997973,0.8432508073748772,0.9888644479387974,0.8315674697005286,0.44213757777981494,0.9659476494421126,0.5436911557435827,0.22887695059642724,0.9639314044031383,0.9663781741629429,0.7262511400634185,0.9888644479387974,0.8315674697005286,0.4421377372729991
26,https://openreview.net/forum?id=BJl6bANtwH,"this paper presents local ensembles, a method for detecting underdetermination when extrapolating to test points. the authors define an extrapolation score which is used to estimate the standard deviation of predictions at test points. the extrapolation score is chosen to represent the variability in predictions that would be generated by models with similar training loss. by considering the eigenvectors of the hessian that are associated with minimum eigenvalues the directions of the loss surface with minimal curvature are found, and perturbations of the parameters in the subspace of minimal curvature correspond to models with similar training loss. ----------the authors show that their extrapolation score is proportional to the first order approximation to the change in prediction under a perturbation of the parameters with minimal change in loss. in practice the minimum eigenvalue/eigenvector pairs are computationally challenging to compute for large matrices. for this reason the subspaces with minimal change in loss are computed by finding sets of vectors that are mutually orthogonal to the eigenvectors associated with dominant eigenvalues of the hessian. ----------the method is validated experimentally first through out-of-distribution detection on synthetic data. the authors then test performance by constructing a ""blind spot"" by generating features that are a linear combination of existing features. data can be generated as either within or out of distribution and the auc metric can be applied to test model performance. in the final experiment the authors demonstrate the use of local ensembles in active learning. by determining which of the training samples are in the model's blind spots at each iteration and training based on these examples rather than randomly selecting training examples rates of convergence can be increased. ----------i vote to accept this paper as the proposed local ensemble method builds on a growing body of literature regarding loss-surface inference, providing a new way to connect the shape of the loss surface to extrapolation detection. the theoretical result showing the first order relationship between the standard deviation of extrapolation predictions and perturbations in solutions is a useful insight. ----------there are some points that should be addressed for clarity however. firstly the proof of proposition 1 should be made clearer. this is central to the work of the paper and a more full treatment of the proof here could help illuminate some intuition about the connection to perturbations and variance of predictions. ----------the other main point that is not addressed is that in principal we aim to find the subspace associated with minimal eigenvalues, but in practice this is computationally prohibitive. therefore a space that has a basis that is mutually orthogonal to the dominant eigenvectors is sought (the found subspace), and this could have minimal relation to the subspace that is actually sought (the optimal subspace). some experimentation showing how the found subspace relates to the optimal subspace would be informative, as well as how sensitive the results are to how much the found and optimal subspaces differ.----------some minor points:------ many of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved------ some explanation about the choice of auc as a metric would be informative and could help connect to the initial motivation of the method------ experiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a ""neural network"" is trained, the architecture should be specifically given alongside the discussion of the experiment the paper focusses on underdetermination as being key to extrapolation. in the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.-----the underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.----------the authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.----------the score approximates the variance of predictions by estimating the norm of the component of the test points gradient that aligns well with the low curvature directions of the hessian, thus providing a tractable quantity in quantifying uncertainty in predictions. the motivation is that if the models have been trained to a local minimum or saddle point, then parameter perturbations in flat directions (small eigenvalues of the hessian) do not change the training loss substantially. these models with small perturbations on the flat regions then form the local ensemble for measuring the extrapolation and predicting on out of distribution samples, spurious correlated samples and for active learning on uncertain data.----------the authors prove that the extrapolation score is proportional to the standard deviations of predictions across a model ensemble with similar training loss. the math in the derivation checks out.---------- ----------one of the novel contributions of the paper is in using computationally cheap post-hoc local ensembles over fully trained ensembles in the baselines that require complicated training procedure. the other key differentiation over baselines is their method leverages the ill conditioned hessian where the baselines struggle in requiring an inverse of that ill conditioned hessian.----------the limitations of their method is in the determination of sufficiently small eigenvalues from the ensemble subspace. further, the sensitivity of the small set of eigenvalues towards overestimating the predictions sensitivity to loss preserving perturbations and being less sensitive to some other under-constrained directions.-----below are the potential places where more clarity will help:-----it would have been good to see a way of measuring the sensitivity in the set of small eigenvalues determination. i urge the authors to think of a way of quantifying this sensitivity if possible especially since the model class is low dimensional.----------in the experimental section with label, class prediction task, how correlated are the confounders eyeglasses and hat? what happens if the models are allowed to train for a longer; does the inconsistency in the behavior of auc over more eigenvalues change?----------in section 5.4, i think a comparison with the resampling under uncertainty baselines is imperative. -----there is a typo in e.4 label attribute->attractive.----------there is lack of clarity in how similar the models are during training. although, the ensemble is used post-hoc, its unclear if the models during training differ in initialization only? ----------what are the implications of the method in the case of finetuning models especially is the training data available for fine-tuning is low. ----------further, is there any notion of how the method scales with increasing depth in the neural network models? a comparison with larger test set and models trained on deeper architecture such as resnet and the like will be interesting to see.----------with noisier data and the inconsistencies in the expected behavior of the method, is there a way of quantifying the amount of noise and the extrapolation? i do see the empirical experiments demonstrating this but some more insight into this is perhaps important.----------overall, its an extremely well written paper with great clarity. the method described by the authors is well differentiated from the baselines in making the clever use of the projection of the ill conditioned hessian on the low curvature directions of the test points gradient.","this paper presents an ensembling approach to detect underdetermination for extrapolating to test points. the problem domain is interesting and the approach is simple and useful. while reviewers were positive about the work, they raised several points for improvement. the authors are strongly encouraged to include the discussion here in the final version.","----------i vote to accept this paper as the proposed local ensemble method builds on a growing body of literature regarding loss-surface inference, providing a new way to connect the shape of the loss surface to extrapolation detection.","in the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.-----the underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.----------the authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.----------the score approximates the variance of predictions by estimating the norm of the component of the test points gradient that aligns well with the low curvature directions of the hessian, thus providing a tractable quantity in quantifying uncertainty in predictions.","in the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.-----the underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.----------the authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.----------the score approximates the variance of predictions by estimating the norm of the component of the test points gradient that aligns well with the low curvature directions of the hessian, thus providing a tractable quantity in quantifying uncertainty in predictions.","this paper presents local ensembles, a method for detecting underdetermination when extrapolating to test points.","in the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.-----the underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.----------the authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.----------the score approximates the variance of predictions by estimating the norm of the component of the test points gradient that aligns well with the low curvature directions of the hessian, thus providing a tractable quantity in quantifying uncertainty in predictions.","in the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.-----the underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.----------the authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.----------the score approximates the variance of predictions by estimating the norm of the component of the test points gradient that aligns well with the low curvature directions of the hessian, thus providing a tractable quantity in quantifying uncertainty in predictions.",the method described by the authors is well differentiated from the baselines in making the clever use of the projection of the ill conditioned hessian on the low curvature directions of the test points gradient.,"in the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.-----the underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.----------the authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.----------the score approximates the variance of predictions by estimating the norm of the component of the test points gradient that aligns well with the low curvature directions of the hessian, thus providing a tractable quantity in quantifying uncertainty in predictions.",0.2417582417582418,0.0224719101123595,0.1538461538461538,0.1538461538461538,0.2487046632124352,0.0418848167539267,0.1347150259067357,0.1347150259067357,0.2487046632124352,0.0418848167539267,0.1347150259067357,0.1347150259067357,0.3235294117647058,0.1818181818181818,0.2941176470588235,0.2941176470588235,0.2487046632124352,0.0418848167539267,0.1347150259067357,0.1347150259067357,0.2487046632124352,0.0418848167539267,0.1347150259067357,0.1347150259067357,0.2727272727272727,0.0465116279069767,0.1818181818181818,0.1818181818181818,0.2487046632124352,0.0418848167539267,0.1347150259067357,0.1347150259067357,11.94230842590332,11.94230842590332,13.56361484527588,11.94230842590332,9.825653076171877,11.94230842590332,11.94230842590332,8.409223556518555,0.9759993741793385,0.9716125632549203,0.8916036125460759,0.8363221010979269,0.9013606902905414,0.9430705319523711,0.8363221010979269,0.9013606902905414,0.9430704058355979,0.9732044495288317,0.9713946448995691,0.9063578964566255,0.8363221010979269,0.9013606902905414,0.9430705319523711,0.8363221010979269,0.9013606902905414,0.9430705319523711,0.4071033567429142,0.462800011003467,0.5123505818884598,0.8363221010979269,0.9013606902905414,0.9430704058355979
27,https://openreview.net/forum?id=BJlNs0VYPB,"this paper carefully observes the behavior of weight magnitudes during training, finding the is a stage of saturation that is closely related to the winning lottery tickets drawing. based on this observation the authors hypothesize that we can draw lottery tickets early but too early pruning can irreversibly hurt the learning capability for complex pattern. to remedy this and draw the tickets as soon as possible, the authors propose to adopt gradual pruning, which 1) can start early without hurting the learning capability too much; 2) avoid computation-heavy iterative pruning in previous works.----------questions:----------1. overall i am very happy with the interesting observations and analysis of the dynamics of weight magnitudes and how it can be related to the early winning lottery tickets drawing. but how valuable is it for practical use? in practice, we cannot know in advance when to start (gradual) pruning.----------2. in fig.1, what does it mean if we perform weight-magnitude based pruning at 10th epoch but rewind the weight to the 20th epoch? is there a baseline network that is normally trained straight to the end and to which we rewind all pruned models?----------3. i am not quite convinced by the experiment of fig. 4 and argument at the bottom half of page 5. i buy the intuition that pruning too early might irreversibly hurt the capability of learning complex pattern. but i have trouble understanding how the experiment of fig. 4 supports this intuition. the curve of retraining with smaller lr (0.01) has the save trend as the baseline and retraining with larger lr (0.1). retraining only one epoch can hardly convince me of its relationship with learning capability. also, for the experiment in fig. 5, to validate the proposed hypothesis, it's more valuable to provide results around the claimed turning point, i.e. around 100 epoch instead of suddenly jumping from 20 epoch to 120 epoch.----------4. in tab. 2, the results of resnet56 with gradual pruning is not presented. in tab. 3, the results of resnet50 with one-shot pruning is not presented. it would be better to have these results for clear comparison.----------overall, i love the empirical observation of weight magnitudes and think it would help the community to understand lottery tickets and training process of deep models.----------update:-----the response from the authors addressed some of my questions and more experiments were added per my suggestions. however, also considering the authors' response to r#1 and r#3, i don't think it's strong enough for me to raise my score. therefore i will keep my current score. overview:----------the paper is dedicated to conducting an in-depth investigation of the structure of winning lottery tickets. the author provides supporting evidence for the structure of the early winning tickets: 1) lottery tickets emerge when the weight magnitude of a model saturates with sgd optimization. 2) pruning before model saturation may result in accuracy degradation. in the experiment part, they employ the memorization capacity analysis and discover the early wining tickets without expensive iterative pruning. the author also conducts extensive experiments with various resnet architectures on both cifar 10 and imagenet, achieving state-of-the-art results with only 1/5 of the total epochs for iterative pruning.----------strength bullets:----------1. the experiment organization is complete and convincing. especially for the figure, it not only clearly shows that lottery tickets emerge much earlier before full training ends, but also shows the effect of rewinding. -----2. the author not reveal interesting observations, but also provides useful guidance. it is a complete logic chain that is also aligned with my intuition. for example, first, the author discusses the memorization capacity of different pruned models at different epochs. then, they introduce a reasonable gradual pruning technique. finally, they conduct experiments to confirm it.-----3. the early winning tickets in this paper achieve state-of-the-art results with only 1/5 of the total epochs for iterative pruning.----------weakness bullets:----------1. for lottery tickets, especially for early winning tickets, i think there is a lot of randomnesses. thus, for the plot like figure 2, figure 5, they need to contain an error bar and the curve should be the average of tens of experiments. it will be more convincing if it decouples the randomness from the real patterns.-----2. the description and organization of section 4 need to be more clear. for example, an algorithm pseudo code will definitely give readers a much more clear understanding of the early winning tickets finding strategy.--------------------question the authors don't answer which confuses me more:-----need more convincing analysis about the indicator - hamming distance ----------just as the comment i posted after review 1, we would like to see more analysis about hamming distance between different winning tickets. the author mentioned in the following way:----------''however, as we demonstrate in fig. 3 of our paper, the mask-distance does not well characterize the winning tickets. e.g., the lottery tickets drawn at epoch 120 and 200 have a mask distance of 0.082 in fig. 3, which is much larger than the mask distance between epoch 180 and 200. whereas, all three tickets achieve comparably high accuracy as shown in fig. 1, implying a shallow correlation between the accuracy and the mask distance''----------as far as i know, this observation only tells us that the structure of the lottery tickets changes, which are drawn from 120 epochs to 180 epochs (although the maintain a similar accuracy). however, we can not conclude that the mask distance is not a reliable measure. as mention by the [eb] paper provided by the authors, you can use the rate of the distance change to indicate the early winning tickets. in this way, we can find winning tickets much earlier than authors' work.----------to better address this question, i suppose the authors need to provide more analysis of the mask distance indicator. i think all three reviewers would like to see the results. a good indicator for early winning tickets is very important, otherwise authors' notion of early still involves quite a lot of training.----------recommendation:----------due to the unsolved important question, here is a weak reject. this paper attempts an in depth study of the lottery ticket hypothesis. the lottery ticket hypothesis holds that sparse sub-networks exist inside dense large models and that the sparse sub-networks achieve at least as good an accuracy as the underlying large model. these sub-networks are discovered by training and iteratively pruning the dense model. this paper investigates the epoch at which pruning should occur as well as the epoch at which weights should be rewound when retraining. then, the authors conduct experiments with different pruning strategies (one-shot vs. gradual) in an attempt to find such sparse models (or ""winning tickets"") earlier than they otherwise would have been found.----------the experiments conducted by the authors seem to be very extensive, and i think the paper contains useful data to have for researchers interested in better understanding the lottery ticket hypothesis. however, my main issue is with both the originality and significance of this work. this paper gives evidence that winning tickets may be found ""early,"" although their notion of early still involves quite a lot of training. ----------although the paper is interested in addressing the structure of the winning tickets, i really didn't find any of the discussion of structure to give much insight into the lottery ticket hypothesis. most of the section focuses on analyzing weight magnitude, though i was hoping for something more about the actual structure of the sparse subnetwork -- especially given the title of the paper. figure 3 is notable, showing that different winning tickets (parameterized by different prune and rewind epochs) can have a large hamming distance between them. this is very interesting, and i wish the authors had more to say. how is this affected by different initializations? are these solutions connected on a loss landscape? is there something invariant about the sparse architecture after symmetries are taken into account? it's not clear to me that hamming distance alone is enough.----------in conclusion, the paper presents a set of nice experiments, but doesn't really shed too much additional light on the scientific nature of the lottery ticket hypothesis.",this paper does extensive experiments to understand the lottery ticket hypothesis. the lottery ticket hypothesis is that there exist sparse sub-networks inside dense large models that achieve as good accuracy as the original model. the reviewers have issues with the novelty and significance of these experiments. they felt that it didn't shed new scientific light. they felt that epochs needed to do early detection was still expensive. i recommend doing further studies and submitting it to another venue.,"it would be better to have these results for clear comparison.----------overall, i love the empirical observation of weight magnitudes and think it would help the community to understand lottery tickets and training process of deep models.----------update:-----the response from the authors addressed some of my questions and more experiments were added per my suggestions.","then, the authors conduct experiments with different pruning strategies (one-shot vs. gradual) in an attempt to find such sparse models (or ""winning tickets"") earlier than they otherwise would have been found.----------the experiments conducted by the authors seem to be very extensive, and i think the paper contains useful data to have for researchers interested in better understanding the lottery ticket hypothesis.","the early winning tickets in this paper achieve state-of-the-art results with only 1/5 of the total epochs for iterative pruning.----------weakness bullets:----------1. for lottery tickets, especially for early winning tickets, i think there is a lot of randomnesses.","this paper carefully observes the behavior of weight magnitudes during training, finding the is a stage of saturation that is closely related to the winning lottery tickets drawing.",the author provides supporting evidence for the structure of the early winning tickets: 1) lottery tickets emerge when the weight magnitude of a model saturates with sgd optimization.,"the early winning tickets in this paper achieve state-of-the-art results with only 1/5 of the total epochs for iterative pruning.----------weakness bullets:----------1. for lottery tickets, especially for early winning tickets, i think there is a lot of randomnesses.",the author provides supporting evidence for the structure of the early winning tickets: 1) lottery tickets emerge when the weight magnitude of a model saturates with sgd optimization.,"then, the authors conduct experiments with different pruning strategies (one-shot vs. gradual) in an attempt to find such sparse models (or ""winning tickets"") earlier than they otherwise would have been found.----------the experiments conducted by the authors seem to be very extensive, and i think the paper contains useful data to have for researchers interested in better understanding the lottery ticket hypothesis.",0.2794117647058824,0.0298507462686567,0.1470588235294117,0.1470588235294117,0.3356643356643356,0.0567375886524822,0.1398601398601398,0.1398601398601398,0.2601626016260163,0.0330578512396694,0.1463414634146341,0.1463414634146341,0.2037037037037037,0.0377358490566037,0.1481481481481481,0.1481481481481481,0.2037037037037037,0.0188679245283018,0.1296296296296296,0.1296296296296296,0.2601626016260163,0.0330578512396694,0.1463414634146341,0.1463414634146341,0.2037037037037037,0.0188679245283018,0.1296296296296296,0.1296296296296296,0.3356643356643356,0.0567375886524822,0.1398601398601398,0.1398601398601398,12.930569648742676,12.437435150146484,11.993316650390623,10.77610969543457,9.213160514831545,12.930569648742676,10.77610969543457,12.437435150146484,0.9674149004554206,0.9724230152376119,0.9025932722153654,0.45073568991301927,0.4882656980357662,0.9511706974802101,0.7740363929602364,0.7733754690376723,0.8447690835322003,0.9753080291623778,0.9764895426670668,0.03444859098929321,0.9399818240350344,0.9492384472165438,0.8986252965158638,0.7740363929602364,0.7733754690376723,0.8447688636120605,0.9399818240350344,0.9492384472165438,0.8986254358133965,0.45073568991301927,0.4882656980357662,0.9511706726763353
28,https://openreview.net/forum?id=BJlPLlrFvH,"edit after author rebuttal and author additions:----------i have updated my score from a weak reject (3) to a weak accept (6).----------justification:----------1. the authors have pointed out that i misunderstood one of their contributions: pointing out that they are demonstrating that the univariate case is an insufficient setting to test causal discovery methods because it can be done without even looking at the conditional distributions (just the marginals). this contribution seems important to orient future work. they have made this more clear in their recent upload and would likely make it even more clear in a camera-ready version.----------2. the authors have made their contribution to the multivariate setting more substantial by adding evaluation on the mous-meg real-world dataset and have better positioned their work relative to others by adding comparisions to multivariate extensions of pnl and cgnn.----------====================================================================================================----------original review:----------summary: the authors focus on the problem of inferring whether the causal structure x > y or y > x. they first consider the case where both x and y are scalar (univariate) random variables and then consider the case where x and y are vector-valued (multivariate) random variables. in the scalar case, motivated by the idea that the effect could be less entropic than the cause (due to data processing inequality), they introduce a method based on comparing reconstruction losses of x and y and show competitive results in tables 1 and 2. they establish that this method is not sufficient for the multivariate case in lemma 2 and move to a new method for the multivariate case. they prove identifiability for this new method in for the multivariate case in section 4.2 and claim state-of-the-art (sota) results in table 3. ----------main contributions:------ presents a causal discovery technique for the univariate cases that only examines the marginal distributions of x and y and seems fairly competitive (tables 1 and 2)------ extends the post-nonlinear identifiability analysis of zhang & hyvarinen (2009) from scalars to vectors and proved that their method will actually identify the correct causal direction------ demonstrates competitive experimental results for both their univariate method------ claims sota results for their multivariate method----------decision: i lean toward rejecting this paper because 1) i have several questions about the univariate case (see below) that would need to be resolved before i lean toward accept, 2) although i am not too familiar with the literature, i believe that this paper may be missing key related work that also uses independence testing for causal discovery (see, e.g., heinze-deml et al. (2017)s invariant causal prediction for nonlinear models), and 3) i am not yet convinced that the comparison done in table 3 is fair and exhaustive. ----------sufficient reason to accept: if the theorems in section 4.2 are found to checkout, and the sota results in table 3 are found to be fair, exhaustive comparisons to the previous sota, their contribution to the multivariate case would seem to be sufficient for acceptance. i believe more discussion between the authors and reviewers is necessary here.---------- questions about univariate case:---------- 1. the motivation for the first method (entropy decreasing along a markov chain due the data processing inequality) seems to only be valid when y := f(x), but not necessarily when y := f(x) + e. for example, let f be the identity function and e be independent to x. how did you resolve this argument against the intuition? ----------2. also, i thought the data processing inequality relates mutual information between variables, not necessarily their entropies. can you make this connection more clear? ----------context for questions 3 and 4: in section 3.1, you write, estimating the entropy of each random variable from its samples does not present a consistent difference between the entropies h(x) and h(y). our method, therefore, computes an alternative complexity score for x and, independently, for y. you then go on to link the entropy to the reconstruction error (your method) in lemma 1 and show competitive results in tables 1 and 2.----------3. why do you want to link the reconstruction error to entropy if you found a purely entropy-based method did not work?----------4. why did the purely entropy-based method not work while your method worked if the two are linked?----------questions about multivariate case:----------5. are you certain that bivariatefit and anm are the only models that you should be comparing against for this multivariate setting?----------6. what is cgnns runtime? would you be able to compare against cgnn in time for a potential camera-ready version of this paper? update:----------the authors have successfully justified my concerns. therefore, i have increased my score to 6.----------original comments:----------in this paper, the authors consider learning causal directions from observational data from both univariate case and multi-dimensional case. in the univariate case, the authors propose a new method to learn causal directions by exploiting the complexities of cause and effect variables. in the multi-dimensional case, where the complexity can be balanced, the authors proposed a method that learns causal direction based on independence loss.----------1. the independence loss part looks confusing to me. standard results in scm yields that the error term e is a function of both the outcome y and x. how can you learn the term e just from y itself? in other words, i am not sure if the conditions required in theorem 2 is feasible. the authors need to provide some examples to justify that the conditions in theorem 2 are feasible conditions.----------2. in fact, the novel idea of learning causal directions based on independence test has been extensively studied in the previous literature. i regret that this has not been mentioned in the current manuscript. examples include:----------http://www.jmlr.org/papers/v12/shimizu11a.html----------in conclusion, since the idea of using independence relations for learning the causal directions is not a very new idea and a lot of discussion of the theoretical analysis is still missing. i regret that this work seems not strong enough to be accepted by iclr.","the author response and revisions to the manuscript motivated two reviewers to increase their scores to weak accept. while these revisions increased the quality of the work, the overall assessment is just shy of the threshold for inclusion.","i believe more discussion between the authors and reviewers is necessary here.---------- questions about univariate case:---------- 1. the motivation for the first method (entropy decreasing along a markov chain due the data processing inequality) seems to only be valid when y := f(x), but not necessarily when y := f(x) + e. for example, let f be the identity function and e be independent to x. how did you resolve this argument against the intuition?","----------main contributions:------ presents a causal discovery technique for the univariate cases that only examines the marginal distributions of x and y and seems fairly competitive (tables 1 and 2)------ extends the post-nonlinear identifiability analysis of zhang & hyvarinen (2009) from scalars to vectors and proved that their method will actually identify the correct causal direction------ demonstrates competitive experimental results for both their univariate method------ claims sota results for their multivariate method----------decision: i lean toward rejecting this paper because 1) i have several questions about the univariate case (see below) that would need to be resolved before i lean toward accept, 2) although i am not too familiar with the literature, i believe that this paper may be missing key related work that also uses independence testing for causal discovery (see, e.g., heinze-deml et al. (2017)s invariant causal prediction for nonlinear models), and 3) i am not yet convinced that the comparison done in table 3 is fair and exhaustive.","----------main contributions:------ presents a causal discovery technique for the univariate cases that only examines the marginal distributions of x and y and seems fairly competitive (tables 1 and 2)------ extends the post-nonlinear identifiability analysis of zhang & hyvarinen (2009) from scalars to vectors and proved that their method will actually identify the correct causal direction------ demonstrates competitive experimental results for both their univariate method------ claims sota results for their multivariate method----------decision: i lean toward rejecting this paper because 1) i have several questions about the univariate case (see below) that would need to be resolved before i lean toward accept, 2) although i am not too familiar with the literature, i believe that this paper may be missing key related work that also uses independence testing for causal discovery (see, e.g., heinze-deml et al. (2017)s invariant causal prediction for nonlinear models), and 3) i am not yet convinced that the comparison done in table 3 is fair and exhaustive.",edit after author rebuttal and author additions:----------i have updated my score from a weak reject (3) to a weak accept (6).----------justification:----------1.,"----------main contributions:------ presents a causal discovery technique for the univariate cases that only examines the marginal distributions of x and y and seems fairly competitive (tables 1 and 2)------ extends the post-nonlinear identifiability analysis of zhang & hyvarinen (2009) from scalars to vectors and proved that their method will actually identify the correct causal direction------ demonstrates competitive experimental results for both their univariate method------ claims sota results for their multivariate method----------decision: i lean toward rejecting this paper because 1) i have several questions about the univariate case (see below) that would need to be resolved before i lean toward accept, 2) although i am not too familiar with the literature, i believe that this paper may be missing key related work that also uses independence testing for causal discovery (see, e.g., heinze-deml et al. (2017)s invariant causal prediction for nonlinear models), and 3) i am not yet convinced that the comparison done in table 3 is fair and exhaustive.","in the scalar case, motivated by the idea that the effect could be less entropic than the cause (due to data processing inequality), they introduce a method based on comparing reconstruction losses of x and y and show competitive results in tables 1 and 2. they establish that this method is not sufficient for the multivariate case in lemma 2 and move to a new method for the multivariate case.","in the scalar case, motivated by the idea that the effect could be less entropic than the cause (due to data processing inequality), they introduce a method based on comparing reconstruction losses of x and y and show competitive results in tables 1 and 2. they establish that this method is not sufficient for the multivariate case in lemma 2 and move to a new method for the multivariate case.","----------main contributions:------ presents a causal discovery technique for the univariate cases that only examines the marginal distributions of x and y and seems fairly competitive (tables 1 and 2)------ extends the post-nonlinear identifiability analysis of zhang & hyvarinen (2009) from scalars to vectors and proved that their method will actually identify the correct causal direction------ demonstrates competitive experimental results for both their univariate method------ claims sota results for their multivariate method----------decision: i lean toward rejecting this paper because 1) i have several questions about the univariate case (see below) that would need to be resolved before i lean toward accept, 2) although i am not too familiar with the literature, i believe that this paper may be missing key related work that also uses independence testing for causal discovery (see, e.g., heinze-deml et al. (2017)s invariant causal prediction for nonlinear models), and 3) i am not yet convinced that the comparison done in table 3 is fair and exhaustive.",0.25,0.0181818181818181,0.1607142857142857,0.1607142857142857,0.1592039800995024,0.0,0.1094527363184079,0.1094527363184079,0.1592039800995024,0.0,0.1094527363184079,0.1094527363184079,0.1935483870967742,0.0333333333333333,0.1935483870967742,0.1935483870967742,0.1592039800995024,0.0,0.1094527363184079,0.1094527363184079,0.2407407407407407,0.0,0.1481481481481481,0.1481481481481481,0.2407407407407407,0.0,0.1481481481481481,0.1481481481481481,0.1592039800995024,0.0,0.1094527363184079,0.1094527363184079,5.930183410644531,9.613049507141112,15.373575210571287,9.613049507141112,9.55169677734375,9.613049507141112,9.613049507141112,5.930183410644531,0.7131860017304134,0.6919778961739359,0.9336244501270861,0.958325289621766,0.7155087941342245,0.46747322118994816,0.958325289621766,0.7155087941342245,0.46747322118994816,0.97858219770868,0.974019001840314,0.8120782827952814,0.958325289621766,0.7155087941342245,0.46747322118994816,0.960456168775652,0.9536555338970344,0.8137335722883688,0.960456168775652,0.9536555338970344,0.8137332378717308,0.958325289621766,0.7155087941342245,0.46747322118994816
29,https://openreview.net/forum?id=BJxYEsAqY7,"i do not necessarily see something wrong with the paper, but i'm not convinced of the significance (or sufficient novelty) of the approach. ----------------the way i understand it, a translator is added on top of the top layer of the student, which is nothing but a few conv layers that project the output to potentially the size of the teacher (by the way, why do you need both a paraphraser and translator, rather than making the translator always project to the size of the teacher which basically will do the same thing !? )--------and then a distance is minimized between the translated value of the students and the teacher output layer. the distance is somewhat similar to l2 (though the norm is removed from the features -- which probably helps with learning in terms of gradient norm). ----------------comparing with normal distillation i'm not sure how significant the improvement is. and technically this is just a distance metric between the output of the student and teacher. sure it is a more involved distance metric, however it is in the spirit of what the distillation work is all about and i do not see this as being fundamentally different, or at least not different enough for an iclr paper.----------------some of the choices seem arbitrary to me (e.g. using both translator and paraphraser). does the translator need to be non-linear? could it be linear? what is this mapping doing (e.g. when teacher and student have the same size) ? is it just finding a rotation of the features? is it doing something fundamentally more interesting? ----------------why this particular distance metric between the translated features? why not just l2? ----------------in the end i'm not sure the work as is, is ready for iclr. in this paper, the authors present two methods, sequential and parallel-feed for learning student networks that share architectures with their teacher.----------------firstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.----------------the paper could do with some further grammar/spell checks.----------------it isn't clear to me where the novelty lies in this work. sequential-feed appears to be identical to bans (https://arxiv.org/abs/1805.04770) with an additional non-linear transformation on the network outputs as in https://arxiv.org/abs/1802.04977. parallel-feed is just an ensemble of teachers; please correct me if i'm wrong.----------------the experimental results aren't convincing. there aren't any fair comparisons. for instance, in table 6 a wrn-28-10(sfeed) after 5 whole training iterations is compared to a wrn-28-1(ban) after 1. it would be good to run ban for as many iterations. a comparison to attention transfer (https://arxiv.org/abs/1612.03928) would be ideal for the imagenet experiments. furthermore, if one isn't interested in compression, then table 4 indicates that an ensemble is largely preferable.----------------this work would benefit from a cifar-10 experiment as it's so widely used (interestingly, bans perform poorly on cifar-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.----------------in summary i believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.----------------pros:--------- simple method--------- largely written with clarity----------------cons:--------- method is not very novel--------- no compared thoroughly enough to other work","the paper describes knowledge distillation methods. as noted by all reviewers, the methods are very similar to the prior art, so there is not enough novelty for the paper to be accepted. the reviewers' opinion didn't change after the rebuttal.","in this paper, the authors present two methods, sequential and parallel-feed for learning student networks that share architectures with their teacher.----------------firstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.----------------the paper could do with some further grammar/spell checks.----------------it isn't clear to me where the novelty lies in this work.","furthermore, if one isn't interested in compression, then table 4 indicates that an ensemble is largely preferable.----------------this work would benefit from a cifar-10 experiment as it's so widely used (interestingly, bans perform poorly on cifar-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.----------------in summary i believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.----------------pros:--------- simple method--------- largely written with clarity----------------cons:--------- method is not very novel--------- no compared thoroughly enough to other work","sure it is a more involved distance metric, however it is in the spirit of what the distillation work is all about and i do not see this as being fundamentally different, or at least not different enough for an iclr paper.----------------some of the choices seem arbitrary to me (e.g. using both translator and paraphraser).","i do not necessarily see something wrong with the paper, but i'm not convinced of the significance (or sufficient novelty) of the approach.","in this paper, the authors present two methods, sequential and parallel-feed for learning student networks that share architectures with their teacher.----------------firstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.----------------the paper could do with some further grammar/spell checks.----------------it isn't clear to me where the novelty lies in this work.","furthermore, if one isn't interested in compression, then table 4 indicates that an ensemble is largely preferable.----------------this work would benefit from a cifar-10 experiment as it's so widely used (interestingly, bans perform poorly on cifar-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.----------------in summary i believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.----------------pros:--------- simple method--------- largely written with clarity----------------cons:--------- method is not very novel--------- no compared thoroughly enough to other work",--------and then a distance is minimized between the translated value of the students and the teacher output layer.,"----------------the way i understand it, a translator is added on top of the top layer of the student, which is nothing but a few conv layers that project the output to potentially the size of the teacher (by the way, why do you need both a paraphraser and translator, rather than making the translator always project to the size of the teacher which basically will do the same thing !",0.2689075630252101,0.0341880341880341,0.1512605042016807,0.1512605042016807,0.2377622377622377,0.0283687943262411,0.1538461538461538,0.1538461538461538,0.2653061224489796,0.0,0.163265306122449,0.163265306122449,0.1846153846153846,0.0317460317460317,0.1538461538461538,0.1538461538461538,0.2689075630252101,0.0341880341880341,0.1512605042016807,0.1512605042016807,0.2377622377622377,0.0283687943262411,0.1538461538461538,0.1538461538461538,0.135593220338983,0.0,0.135593220338983,0.135593220338983,0.1818181818181818,0.0185185185185185,0.1454545454545454,0.1454545454545454,4.972447395324707,7.758553504943848,9.05023956298828,4.972447395324707,7.758553504943848,5.914534568786621,14.92411994934082,14.095120429992676,0.989336462635094,0.98705510522065,0.9383703284823253,0.92509813236328,0.936596315433385,0.7442657343553518,0.9594673823826417,0.9626745609774796,0.45170836308326057,0.9732005586461064,0.969250652561275,0.9605791511868503,0.989336462635094,0.98705510522065,0.9383703809182684,0.92509813236328,0.936596315433385,0.7442663525980134,0.9862573883903457,0.9832580596042645,0.8989029462025002,0.95648099329321,0.9648646848521603,0.8788642961279114
30,https://openreview.net/forum?id=BkMq0oRqFQ,"the authors propose a new interpretation of the batch normalization step inside a neural network.--------the main result shows that the backpropagation of the gradient of some loss function through a batch normalization can be seen as a scaled residual of a least square linear fit. this new interpretation is extended to other normalization technics used in the literature and thus give a ""unified"" view of such methods. ----------------the idea is simple yet very interesting and well introduced. the theoretical results are good and the proofs are well written and easy to follow.----------------however the arguments brought forward by this new vision of batch normalization in applications look light (see sections 3.3, 4.1, 4.2). a more detailed interpretation of this new vision on a single application and its impact would have been preferred than numerous applications as it is done in this paper. --------not all the existing normalization methods have been extended with success yet, this makes this unified vision a bit less convincing. the primary technical contribution comes from section 2, where it is demonstrated that the normalized back-propagated gradients obtained from a bn layer can be viewed as the residuals of the gradients obtained without bn regressed via a simple two-parameter model of the activations. in some sense though this result is to be expected, since centering data (i.e., removing the mean as in bn) can be generically viewed as computing the residuals after a least squares fit of a single constant, and similarly for de-trending with respect to a single independent variable, in this case the activations. so i'm not sure that theorem 1 is really that much of an insightful breakthrough, even if it may be nice to work through the precise details in the specific case of a bn layer and the relationship to gradients.----------------but beyond this a larger issue is as follows: this paper is framed as taking a step in explaining why batch normalization (bn) works so well. for example, even the abstract mentions this as an unsettled issue in motivating the proposed analysis. however, to me the interpretation of bn as introducing a form of least squares fit does not really extend our understanding of why it actually works better in practice, and this is the biggest disconnect of the paper. the new perspective presented might be another way to interpret bn layers, but it unfortunately remains mostly unanswered exactly why this new perspective is relevant in actually explaining bn behavior.----------------the presented normalization theory is also used to motivate heuristic modifications to standard bn schemes. for example, the paper proposed concatenating bn with a layer normalization layer, demonstrating some modest improvement on cifar-10 data. but again, i don't see how viewing these normalization schemes as least-squares residuals motivates such concatenation any more than the merits of the original versions themselves. moreover, it is not even clear that bn+ln is in fact generally better since only a single data set is considered. there are also no comparisons against competing bn modifications such as switch normalization (luo et al. 2018) which also involves a hybrid method combining aspects of ln and bn. why not compare against approaches like this?----------------to conclude, in section 6 the paper asks ""why do empirical improvements in neural networks with bn keep the gradient-least-squares residuals and drop the explained portion?"" but this question is not at all answered but rather deferred to future work. for me this was a disappointment as this would seem to be an essential ingredient for actually developing a meaningful theory for why bn is helpful in practice.------------------------other comments:----------------* the analysis from section 2, including theorem 1, assume that the bn parameters c and b can be ignored (presumably this means fixing c = 1 and b = 0). i did not carefully check the details, but do all the same derivations and conclusions still seamlessly go through when these parameters have general values that deviate from this standard initialization? if not, then i don't really see what is the practical relevance, since once learning begins, both b and c will typically shift to arbitrary values. below eq. (1) it states that c and b are only ignored for clarity, but then later i did not see any subsequent discussion to handle the general case, which is what would be actually needed for explaining bn behavior in practice.----------------* please run a speck-checker. example, ""on some leve, the matrix gradient ...""----------------* the paper cites (lipton and steinhardt, 2018) in arguing that reasons for the effectiveness of bn are lacking. indeed (lipton and steinhardt, 2018) criticize the original bn paper for conflating speculation with explanation, or more precisely, framing speculation about why bn should be helpful as an actual true explanation without clear evidence. but to me this submission is hovering somewhere in the same category, speculating that regressing away certain portions of the gradient could be useful but never really providing concrete evidence for why this should offer an improvement. the paper aims at a better understanding of the positive impacts of batch normalisation (bn) on network generalisation (mainly) and convergence of learning. first, the authors propose a novel interpretation of the bn re-parametrisation. they show that an affine transform of the variables with their local variance (scale) and mean (shift) can be interpreted as a decomposition of the gradient of the objective function into a regressor assuming that the gradient is parallel to the variables (up to a shift) and the residual part which is the gradient w.r.t. to the new variables. in the second part of the paper, authors review various normalisation proposals (differing mainly in the subset of variables over which the normalisation statistics is computed) as well as the known empirical findings about the dependence of bn on the batch size. the paper presents an experiment that combines two normalisation variants. a further experiment strives at regularising bn for small batch sizes.----------------unfortunately, it remains unclear what questions precisely the authors answer in the second part of the paper and, what is more important, how they are related to the novel interpretation of bn presented in the first part. this interpretation holds for any function and can be possibly seen as a gradient pre-conditioning. however, the authors do not ""extend"" it towards the gradients w.r.t. the network parameters and do not consider the specifics of the learning objectives (a sum of functions, each one depending on one training example only). the main presented experiment combines layer normalisation with standard batch normalisation for a convolutional network. the first one normalises using the statistics over channel and spatial dimensions, whereas the second one uses the statics over the batch and spatial dimensions. the improvements are rather marginal, but, what is more important, the authors do not explain how and why this proposal follows from their new interpretation of bn.----------------overall, in my view, this paper is premature and not appropriate for publishing at iclr in its present form.","this paper interprets batch norm in terms of normalizing the backpropagated gradients. all of the reviewers believe this interpretation is novel and potentially interesting, but that the paper doesn't make the case that this helps explain batch norm, or provide useful insights into how to improve it. the authors have responded to the original set of reviews by toning down some of the claims in the original paper, but haven't addressed the reviewers' more substantive concerns. there may potentially be interesting ideas here, but i don't think it's ready for publication at iclr.","why not compare against approaches like this?----------------to conclude, in section 6 the paper asks ""why do empirical improvements in neural networks with bn keep the gradient-least-squares residuals and drop the explained portion?""","the new perspective presented might be another way to interpret bn layers, but it unfortunately remains mostly unanswered exactly why this new perspective is relevant in actually explaining bn behavior.----------------the presented normalization theory is also used to motivate heuristic modifications to standard bn schemes.","however, to me the interpretation of bn as introducing a form of least squares fit does not really extend our understanding of why it actually works better in practice, and this is the biggest disconnect of the paper.",the authors propose a new interpretation of the batch normalization step inside a neural network.--------the main result shows that the backpropagation of the gradient of some loss function through a batch normalization can be seen as a scaled residual of a least square linear fit.,the main presented experiment combines layer normalisation with standard batch normalisation for a convolutional network.,the authors propose a new interpretation of the batch normalization step inside a neural network.--------the main result shows that the backpropagation of the gradient of some loss function through a batch normalization can be seen as a scaled residual of a least square linear fit.,the paper presents an experiment that combines two normalisation variants.,"the new perspective presented might be another way to interpret bn layers, but it unfortunately remains mostly unanswered exactly why this new perspective is relevant in actually explaining bn behavior.----------------the presented normalization theory is also used to motivate heuristic modifications to standard bn schemes.",0.1818181818181818,0.0153846153846153,0.1212121212121212,0.1212121212121212,0.1971830985915493,0.0,0.1126760563380281,0.1126760563380281,0.2222222222222222,0.0300751879699248,0.1333333333333333,0.1333333333333333,0.2657342657342657,0.0709219858156028,0.1398601398601398,0.1398601398601398,0.0535714285714285,0.0,0.0535714285714285,0.0535714285714285,0.2657342657342657,0.0709219858156028,0.1398601398601398,0.1398601398601398,0.0560747663551401,0.019047619047619,0.0560747663551401,0.0560747663551401,0.1971830985915493,0.0,0.1126760563380281,0.1126760563380281,12.572683334350586,9.592599868774414,12.572683334350586,7.688336372375488,10.00774383544922,6.668306350708008,7.688336372375488,10.67402458190918,0.9773044008668965,0.9737524818901954,0.8624521358605287,0.9803736887241783,0.9798430428906063,0.9272408148324912,0.9609188979698052,0.9367339691603043,0.9199392687710044,0.9823791500319989,0.9807971095875707,0.9454216668334812,0.44638828988886403,0.5872168950832499,0.8892675836733055,0.9823791500319989,0.9807971095875707,0.9454216565362841,0.3758086625838064,0.43625715607960686,0.9275820124119645,0.9803736887241783,0.9798430428906063,0.927240787197995
31,https://openreview.net/forum?id=BkNUFjR5KQ,"the authors present the interesting and important direction in searching better network architectures using the genetic algorithm. performance on the benchmark datasets seems solid. moreover, the learned insights described in section 4.4 would be very helpful for many researchers.----------------however, the overall paper needs to be polished more. there are two many typos and errors that imply that the manuscript is not carefully polished. explanations about some terms like growth rate, population, etc. are necessary for broader audience. ----------------more importantly, while some of step jumps in figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in section 4.2. thee clear explanation about that phenomena is required.----------------* details--------- please represent the blocks (e.g. 1*1conv) better. current representation is quite confusing to read. maybe proper spacing and different style of fonts may help.--------- in page 5, ""c_{m}ax"" is a typo. it should be ""c_{max}"".--------- regarding the c_max, does sum(c_max) represent (d * w)^2 where d is the total depth and w is the total indicies in each layer? if so, specifying it will help. otherwise, please explain its meaning clearly.--------- in figure 4(a), it would be better if we reuse m_{d,w} notation instead of module {d_w}.--------- please briefly explain or provide references to the terms like ""growth rate"", ""population"", and ""individuals"". --------- different mutations may favor different hyper-parameters. how the authors control the hyperparameters other than the number of epochs will be useful to know.--------- even though the sparse connection is enforced for some reasons, overfitting, variance, or any other benefits that slim structure can bring in has not been evaluated. they need to be presented to verify the hypothesis that the authors claim. the problem is of increasing practical interest and importance. ----------------the ablation study on the contribution and effects of each constituent part is a strong part of the experiment section and the paper. ----------------one major concern is about the novelty of the work. there are many similar works under the umbrella of neural architecture search who are trying to connect different building blocks (modules) to build larger cnns. one example that explicitly makes sparse connections between them is [1]. other examples of very similar works are [2,3,4].----------------the presentation of the paper can be improved a lot. in the current setup its very similar to a collection of ideas and tricks and techniques combined together. ----------------there are some typos and errors in the writing. a thorough grammatical proofreading is necessary. ----------------in conclusion there is a claim about tackling overfitting. its not well supported or discussed in the experiments. ----------------[1] shazeer, noam, et al. ""outrageously large neural networks: the sparsely-gated mixture-of-experts layer."" arxiv preprint arxiv:1701.06538 (2017).--------[2] xie, lingxi, and alan l. yuille. ""genetic cnn."" iccv. 2017.--------[3] real, esteban, et al. ""large-scale evolution of image classifiers."" arxiv preprint arxiv:1703.01041 (2017).--------[4] liu, hanxiao, et al. ""hierarchical representations for efficient architecture search."" arxiv preprint arxiv:1711.00436 (2017).","this paper proposes a genetic algorithm to search neural network architectures with locally dense and globally sparse connections. a population-based genetic algorithm is used to find the sparse, connections between dense module units. the local dense but global sparse architecture is an interesting idea, yet is not well studied in the current version, e.g. overfitting and connections with other similar architecture search methods. based on reviewers ratings (5,5,6), the current version of paper is proposed as borderline lean reject.","how the authors control the hyperparameters other than the number of epochs will be useful to know.--------- even though the sparse connection is enforced for some reasons, overfitting, variance, or any other benefits that slim structure can bring in has not been evaluated.","----------------more importantly, while some of step jumps in figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in section 4.2. thee clear explanation about that phenomena is required.----------------* details--------- please represent the blocks (e.g. 1*1conv) better.",the authors present the interesting and important direction in searching better network architectures using the genetic algorithm.,the authors present the interesting and important direction in searching better network architectures using the genetic algorithm.,the authors present the interesting and important direction in searching better network architectures using the genetic algorithm.,"----------------more importantly, while some of step jumps in figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in section 4.2. thee clear explanation about that phenomena is required.----------------* details--------- please represent the blocks (e.g. 1*1conv) better.",are necessary for broader audience.,the authors present the interesting and important direction in searching better network architectures using the genetic algorithm.,0.2222222222222222,0.0483870967741935,0.126984126984127,0.126984126984127,0.1678321678321678,0.0141843971631205,0.0839160839160839,0.0839160839160839,0.24,0.0408163265306122,0.12,0.12,0.24,0.0408163265306122,0.12,0.12,0.24,0.0408163265306122,0.12,0.12,0.1678321678321678,0.0141843971631205,0.0839160839160839,0.0839160839160839,0.0,0.0,0.0,0.0,0.24,0.0408163265306122,0.12,0.12,6.222869873046875,12.229655265808104,12.229656219482422,6.222869873046875,6.402782917022705,12.229655265808104,12.229656219482422,9.992502212524414,0.947428969547465,0.9608332821259152,0.8511086168573374,0.9761952853819906,0.8912772227566138,0.0850906491423841,0.966467441455316,0.9638055266488397,0.9128355503666996,0.966467441455316,0.9638055266488397,0.9128355503666996,0.966467441455316,0.9638055266488397,0.9128355707704513,0.9761952853819906,0.8912772227566138,0.08509087481913512,0.8102110408009672,0.9411762847612152,0.029079089888642728,0.966467441455316,0.9638055266488397,0.9128354583607339
32,https://openreview.net/forum?id=BkN_r2lR-,"the paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. the general idea is interesting and the results show improvements over previous approaches, such as cyclegan (with different initializations, pre-learned or not). the algorithm is tested on three datasets.----------------while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.----------------instead of the longer intro and related work discussion, i would prefer to see a figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. it is hard to understand sufficiently well what the formalism means without more insight.----------------also, the experiments need more details. for example, it is not clear what the numbers in table 2 mean. this paper presents an image-to-image cross domain translation framework based on generative adversarial networks. the contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved. the results show that the proposed method is superior for the task of exact correspondence identification and that an-gan rivals the performance of pix2pix with strong supervision.------------------------negatives:--------1.) the task of exact correspondence identification seems contrived. it is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing.--------2.) the supervised vs unsupervised experiment on facades->labels (table 3) is only one scenario where applying a supervised method on top of an-gans matches is better than an unsupervised method. more transfer experiments of this kind would greatly benefit the paper and support the conclusion that our self-supervised method performs similarly to the fully supervised method. ----------------positives:--------1.) the paper does a good job motivating the need for an explicit image matching term inside a gan framework--------2.) the paper shows promising results on applying a supervised method on top of an-gans matches.----------------minor comments:--------1. the paper sometimes uses l1 and sometimes l_1, it should be l_1 in all cases.--------2. discogan should have the kim et al citation, right after the first time it is used. i had to look up discogan to realize it is just kim et al.","this paper builds on top of cycle gan ideas where the main idea is to jointly optimize the domain-level translation function with an instance-level matching objective. initially the paper received two negative reviews (4,5) and a positive (7). after the rebuttal and several back and forth between the first reviewer and the authors, the reviewer was finally swayed by the new experiments. while not officially changing the score, the reviewer recommended acceptance. the ac agrees that the paper is interesting and of value to the iclr audience.","the algorithm is tested on three datasets.----------------while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.----------------instead of the longer intro and related work discussion, i would prefer to see a figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.","the algorithm is tested on three datasets.----------------while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.----------------instead of the longer intro and related work discussion, i would prefer to see a figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.",the results show that the proposed method is superior for the task of exact correspondence identification and that an-gan rivals the performance of pix2pix with strong supervision.------------------------negatives:--------1.),the paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis.,"the algorithm is tested on three datasets.----------------while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.----------------instead of the longer intro and related work discussion, i would prefer to see a figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.","the algorithm is tested on three datasets.----------------while the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.----------------instead of the longer intro and related work discussion, i would prefer to see a figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.","it is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing.--------2.)","the paper shows promising results on applying a supervised method on top of an-gans matches.----------------minor comments:--------1. the paper sometimes uses l1 and sometimes l_1, it should be l_1 in all cases.--------2.",0.3647058823529411,0.0357142857142857,0.1764705882352941,0.1764705882352941,0.3647058823529411,0.0357142857142857,0.1764705882352941,0.1764705882352941,0.2166666666666666,0.0169491525423728,0.1166666666666666,0.1166666666666666,0.1296296296296296,0.0188679245283018,0.074074074074074,0.074074074074074,0.3647058823529411,0.0357142857142857,0.1764705882352941,0.1764705882352941,0.3647058823529411,0.0357142857142857,0.1764705882352941,0.1764705882352941,0.1626016260162601,0.0,0.1138211382113821,0.1138211382113821,0.1889763779527559,0.0639999999999999,0.1417322834645669,0.1417322834645669,9.305644035339355,9.305644035339355,9.443147659301758,9.305644035339355,9.305644035339355,9.24150276184082,8.538445472717285,4.248758316040039,0.9864973415098124,0.9856617015798742,0.8818776187937597,0.9864973415098124,0.9856617015798742,0.8818775990819551,0.9748457833280637,0.9767741274952136,0.33104644856680426,0.9842110683670611,0.9835068143791088,0.9479857642837172,0.9864973415098124,0.9856617015798742,0.8818775990819551,0.9864973415098124,0.9856617015798742,0.8818776187937597,0.9405261551068462,0.9561644488207988,0.13513464862394933,0.9880565498633388,0.9877438341220424,0.9378704468843312
33,https://openreview.net/forum?id=BkeK-nRcFX,"in this paper the authors introduce a new quantity, the nonlinearity coefficient, and argue that its value at initialization is a useful predictor of test time performance for neural networks. the authors conduct a wide range of experiments over many different network architectures and activation functions to corroborate these results. the authors then extend their method to compute the local nonlinearity of activation functions instead.----------------i am a bit torn on this paper. i appreciate the direction that the authors have chosen to pursue. the topic of identifying parameters that are predictive of trainability is certainly interesting and has the potential to be quite impactful. moreover, the breadth of the experiments conducted by the authors is novel and significant. finally, i find the the overall manner in which the authors have chosen to present their data refreshingly transparent. together, this leads me to believe that the quantity proposed by the authors might be useful to researchers.----------------having said that, i am concerned by the authors exposition of the nonlinearity coefficient itself. fundamentally, my concern stems from the fact that it seems a lot of relatively ad-hoc decisions were made in the construction of the nonlinearity coefficient and an insufficiently good job was done to compare it to other measures of nonlinearity.----------------specifically, it feels like an extremely weak definition of nonlinearity to say that the linear approximation of a function fails when it produces values that lie outside of the co-domain of the function. moreover, i feel as though there is already a well defined notion of nonlinearity at a point that could be constructed by reference to the hessian (or generally by the approximation error induced by truncating the taylor series after the linear term). i would like to see some comparison between these two methods. ----------------this is made more troubling given that the correlation found by the authors is present but does not seem especially strong. for example, in fig. 2a it seems like the nonlinearity coefficient varies by at least two orders of magnitude in the inset of the figure where the test accuracy really does not seem sensitive to its value. prior work (for example, [1] from last years iclr) has shown strong correlations between the frobenius norm of the jacobian and test error (see fig. 5 and fig. 6). since the definition of the nonlinearity coefficient seems somewhat ad-hoc i would love to see a comparison between it and just looking at the jacobian norm in terms of predicting test accuracy.----------------[1] - sensitivity and generalization in neural networks: an empirical study--------roman novak, yasaman bahri, daniel a. abolafia, jeffrey pennington, jascha sohl-dickstein this paper proposes a metric to measure the ""nonlinearity"" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.----------------apart from a few problems i think this paper is well written and thorough. the contribution is solid, although not earth shattering given previous work on such metrics. there seems to be a basic error in some of the early math, although i don't think this will qualitatively affect the results in any significant way.-------------------------------------------------detailed comments by section:------------------------------------------section 3:----------------it seems like a 1/sqrt(d) factor is missing from these q_i(s_x x(i)) and q_j(s_x f(x,j)) formulas. as far as i can tell this doesn't affect def 1 because you seemed to use the correct formula there. ----------------however, the rewritten version with the traces doesn't seem to be correct. there should be a d_in factor in the denominator (inside the square root). this error seems unrelated to the other one. assuming i'm correct and that this is an error, does this affect your results in the various figures? and what is the actual final definition of nlc that you used?----------------in general, it's annoying for the reader to verify that all of these forms are equivalent. and it's fiddly enough with the sqrt(d) terms constantly disappearing and reappearing in the numerator and denominators that even you made multiple errors (as far as i can tell). i would suggest making this section more rigorous and writing out everything carefully. and you probably don't need to rewrite it in so many equivalent forms with different notation unless they are useful somehow. ----------------the use of the q and s symbols feels superfluous and counterproductive. standard notation with expectations and squares wouldn't take much more space and would be a lot clearer.------------------------section 4:----------------""we plot the relative diameter of the linearly approximable regions of the network as defined in section 3"": but you don't seem to define ""relative diameter"" there. as far as i can tell it's only defined in appendix e, and this is only mentioned in the caption of figure 1. it's impossible to interpret this result without knowing precisely what ""relative diameter"" is. if you can't afford to describe this in the main paper you should at least mention that it's a different (more expensive) way of estimating the same thing that the nlc estimates.----------------in figure 2, are the higher test errors due to the optimizer failing to lower the training error, or due to a greater generalization gap? i guess the figure 3 results suggest the latter possibility, which is surprising to me. ------------------------what does it mean to have a ""very biased output"". what does that inequality mean intuitively? should there be an absolute value on the rhs? it would be much easier to parse it if it were written in plain notation without these s and q symbols.------------------------section 6:----------------""metric also an"" -> ""metric also has an""----------------can you generate a failure case for ""correlation information"" that doesn't involve batch norm layers? i don't think the authors of those works meant for their results to deal with that.----------------note that there are actually a lot of papers going back to the 90s that discussed and proved representational benefits of depth in neural networks. i do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix d with components all equal to one except for a very large one \lambda and also multiply the input weights by d^{-1} to compensate (and have a similar function). if you look at such construction for the linear case with identity initialization of a, the nlc is sqrt((\lambda^2 + n - 1) (\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \lambda *for a linear model*. however, because of its low capacity, we would expect a linear model to have reasonable generalization. this seems to compromise the initial nlc being low as a necessary condition for reasonable generalization. --------conversely, its possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). this initialization may also be done such that the initial nlc becomes close to 1. i would not think this wouldnt necessarily result in good generalization, which seems to agree with the experimental observation. --------now given that this initial nlc is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and nlc together in the experiment section. same remark applies to the correlation between nonlinearity and nlc. this is especially concerning since in the linear case, the nlc can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. what were the architecture that resulted in small/high nlc?--------the experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper.","this paper proposes the nonlinearity coefficient (nlc), a metric which aims to predicts test-time performance of neural networks at initialization. the idea is interesting and novel, and has clear practical implications. reviewers unanimously agreed that the direction is a worthwhile one to pursue. however, several reviewers also raised concerns about how well-justified the method is: in particular, reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc. reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. these concerns make it clear that the paper needs more work before it can be published. and, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction.","since the definition of the nonlinearity coefficient seems somewhat ad-hoc i would love to see a comparison between it and just looking at the jacobian norm in terms of predicting test accuracy.----------------[1] - sensitivity and generalization in neural networks: an empirical study--------roman novak, yasaman bahri, daniel a. abolafia, jeffrey pennington, jascha sohl-dickstein this paper proposes a metric to measure the ""nonlinearity"" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.----------------apart from a few problems i think this paper is well written and thorough.","since the definition of the nonlinearity coefficient seems somewhat ad-hoc i would love to see a comparison between it and just looking at the jacobian norm in terms of predicting test accuracy.----------------[1] - sensitivity and generalization in neural networks: an empirical study--------roman novak, yasaman bahri, daniel a. abolafia, jeffrey pennington, jascha sohl-dickstein this paper proposes a metric to measure the ""nonlinearity"" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.----------------apart from a few problems i think this paper is well written and thorough.","assuming i'm correct and that this is an error, does this affect your results in the various figures?","in this paper the authors introduce a new quantity, the nonlinearity coefficient, and argue that its value at initialization is a useful predictor of test time performance for neural networks.","--------conversely, its possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0).","since the definition of the nonlinearity coefficient seems somewhat ad-hoc i would love to see a comparison between it and just looking at the jacobian norm in terms of predicting test accuracy.----------------[1] - sensitivity and generalization in neural networks: an empirical study--------roman novak, yasaman bahri, daniel a. abolafia, jeffrey pennington, jascha sohl-dickstein this paper proposes a metric to measure the ""nonlinearity"" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.----------------apart from a few problems i think this paper is well written and thorough.",same remark applies to the correlation between nonlinearity and nlc.,"since the definition of the nonlinearity coefficient seems somewhat ad-hoc i would love to see a comparison between it and just looking at the jacobian norm in terms of predicting test accuracy.----------------[1] - sensitivity and generalization in neural networks: an empirical study--------roman novak, yasaman bahri, daniel a. abolafia, jeffrey pennington, jascha sohl-dickstein this paper proposes a metric to measure the ""nonlinearity"" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.----------------apart from a few problems i think this paper is well written and thorough.",0.3776824034334763,0.1038961038961039,0.1545064377682403,0.1545064377682403,0.3776824034334763,0.1038961038961039,0.1545064377682403,0.1545064377682403,0.0779220779220779,0.0,0.0649350649350649,0.0649350649350649,0.2545454545454545,0.0981595092024539,0.1333333333333333,0.1333333333333333,0.1226993865030674,0.0372670807453416,0.1104294478527607,0.1104294478527607,0.3776824034334763,0.1038961038961039,0.1545064377682403,0.1545064377682403,0.0689655172413793,0.0139860139860139,0.0551724137931034,0.0551724137931034,0.3776824034334763,0.1038961038961039,0.1545064377682403,0.1545064377682403,10.511635780334473,7.419289112091064,11.873279571533203,10.511635780334473,10.511635780334473,6.47508716583252,10.511635780334473,8.377043724060059,0.9831714249335611,0.9857219587413745,0.8586532017799982,0.9831714249335611,0.9857219587413745,0.8586531465442175,0.41718362205227544,0.5260634779066294,0.9251930678019447,0.9770276056651835,0.9721860207936809,0.952294081406025,0.23129942390347785,0.3607910642775042,0.9335388499715974,0.9831714249335611,0.9857219587413745,0.8586532017799982,0.3718010819609388,0.6223040150684278,0.9182080110126866,0.9831714249335611,0.9857219587413745,0.8586532017799982
34,https://openreview.net/forum?id=Bkf1tjR9KQ,"the paper proposes a multi-objective search algorithm that designs resource-efficient convolutional architectures. the key idea is to maintain a population of networks and to iteratively approach the pareto front through evolution. the normal & reduction cells are searched on cifar-10 and then transferred to imagenet. the resulting architectures empirically lead to better trade-offs than other baselines.----------------pros:--------the paper is well-written and easy to comprehend.--------results are competitive against strong baselines such as nasnet.--------resource budgets are handled in a principled manner with multi-objective optimization. ----------------cons:----------------my main concerns are on the technical novelty and experimental comparison.----------------technical novelty:----------------the proposed algorithm seems highly similar to the existing multi-objective nas algorithms, especially the ones based on pareto optimality [1,2,3]. in sect 2, the authors state that the main difference from prior works such as [2] and [3] is the usage of a different and larger search space and large-scale experiments. however, both aspects are of limited technical novelty.----------------experimental comparison:----------------in sect 3.3, the authors say we noticed that the original nasnet search space can greatly benefit from extra connections from any given block. if the proposed algorithm was investigated in an enhanced version of the nasnet space, it would be unclear whether we should attribute the reported performance to the proposed multi-objective evolution or this additional search space engineering. it would be better to report the results using the original space as well for fair comparison. ----------------the main claimed contribution is a multi-objective evolutionary algorithm. to demonstrate its effectiveness, it would be necessary to compare against existent multi-objective nas strategies in the literature. most of those strategies (e.g., scalarization, weighted product method) should be straightforward to implement on top of the current search space. the current results are less convincing since the authors only compared their method against single-objective baselines (e.g. nasnet, pnas, amoebanet) which are completely unaware of additional dimensions of the desired objectives. ----------------the networks are searched on cifar-10 and then transferred to imagenet. unlike most prior works (including the ones focusing on resource-constrained nas), the authors did not the final performance of their architecture on cifar-10. it would be informative to report the cifar-10 results as well.----------------other suggestions & questions:--------the authors did not report their training setup for imagenet. it would be good to include those details to ensure the readers are informed should there are any additional augmentations.----------------uniform mutation and a crossover probability of 0.1 (sect 4.1)--------it would be better to included more details on these evolution forces for reproducibility. these are also important component of the proposed algorithm.----------------we manually select 3 architectures that we will be fully train on imagenet in section 4.2 (sect 4.1)--------i believe this part needs more clarifications since there can be a large number of architectures on the pareto front. whats the criteria for manual selection?----------------[1] elsken, thomas, jan hendrik metzen, and frank hutter. ""multi-objective architecture search for cnns."" arxiv preprint arxiv:1804.09081 (2018).--------[2] kim ye-hoon, reddy bhargava, yun sojung, and seo chanwon. nemo: neuro-evolution with multiobjective optimization of deep neural network for speed and accuracy. icml17 automl workshop, 2017.--------[3] dong, jin-dong, et al. ""dpp-net: device-aware progressive search for pareto-optimal neural architectures."" arxiv preprint arxiv:1806.08198 (2018). the paper is easy to read. the authors did a job in describing the problem, concepts, and the proposed multi-objective optimization method. the computational results are on par with nasnet-a mobile. ----------------it is good to know that we can use standard multi-objective method for neural architecture search. the implementation seems to be straightforward. the paper mainly uses existing ideas, but with some incremental improvements. it lacks novelty. ----------------the time reduction of this method on imagenet comes from transfer learning by training on cifar-10 first. as the paper admits this is not going to generalizing well. how good the method is if just using a single dateset? for cifar-10, is this method comparable with enas(https://arxiv.org/pdf/1802.03268.pdf)? the authors propose a multi-objective neural architecture search based on an evolutionary algorithm. the contradicting objective functions are optimized by ranking the candidates by pareto-dominance, replace the bottom 50% with new candidates generated by the top 50% candidates through random mutations. the multi-objective function considers classification accuracy and an approximation of the inference speed. the method is compared to mobilenet and mobile nasnet on imagenet indicating an improvement with respect to search time.----------------the authors admit that their work is incremental and a combination of existing work. furthermore, they admit that dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. the method by dong et al. requires only 8 gpu days (dvolver requires 50) yielding very similar results. why this has been ignored remains unclear.----------------the paper is not self-contained, important methodological aspects of the method are insufficiently described. i recommend at least to formally define the crowding distance. it would be also reasonable to define your objective functions already in section 3 instead of mentioning them in the caption of figure 3 and its axis labels.----------------i think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [a].----------------the comparison in table 2 is not fair. you use the swift activation function and do not report the corresponding numbers for mobilenet or mobile nasnet. ramachandran et al. (2017) report these (75% and 74.2% for nasnet and mobilenet).--------comparing the dvolver architecture with relu activations to mobilenet does not indicate any improvements.----------------you mention that most previous approaches are only keeping track of the best solution while you evolve over a population. maybe this sentence is not well written and something else is meant but now this statement is wrong.----------------[a] thomas elsken, jan hendrik metzen, frank hutter: simple and efficient architecture search for convolutional neural networks. corr abs/1711.04528 (2017)","the paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. all reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground-breaking to justify acceptance based on results alone.","maybe this sentence is not well written and something else is meant but now this statement is wrong.----------------[a] thomas elsken, jan hendrik metzen, frank hutter: simple and efficient architecture search for convolutional neural networks.",the method is compared to mobilenet and mobile nasnet on imagenet indicating an improvement with respect to search time.----------------the authors admit that their work is incremental and a combination of existing work.,the method is compared to mobilenet and mobile nasnet on imagenet indicating an improvement with respect to search time.----------------the authors admit that their work is incremental and a combination of existing work.,the paper proposes a multi-objective search algorithm that designs resource-efficient convolutional architectures.,"----------------cons:----------------my main concerns are on the technical novelty and experimental comparison.----------------technical novelty:----------------the proposed algorithm seems highly similar to the existing multi-objective nas algorithms, especially the ones based on pareto optimality [1,2,3].",the method is compared to mobilenet and mobile nasnet on imagenet indicating an improvement with respect to search time.----------------the authors admit that their work is incremental and a combination of existing work.,it would be good to include those details to ensure the readers are informed should there are any additional augmentations.----------------uniform mutation and a crossover probability of 0.1 (sect 4.1)--------it would be better to included more details on these evolution forces for reproducibility.,the authors propose a multi-objective neural architecture search based on an evolutionary algorithm.,0.0987654320987654,0.0253164556962025,0.0493827160493827,0.0493827160493827,0.2531645569620253,0.0259740259740259,0.1265822784810126,0.1265822784810126,0.2531645569620253,0.0259740259740259,0.1265822784810126,0.1265822784810126,0.2333333333333333,0.0344827586206896,0.1333333333333333,0.1333333333333333,0.2650602409638554,0.0493827160493827,0.1686746987951807,0.1686746987951807,0.2531645569620253,0.0259740259740259,0.1265822784810126,0.1265822784810126,0.1739130434782608,0.0,0.1521739130434782,0.1521739130434782,0.3,0.0689655172413793,0.1666666666666666,0.1666666666666666,8.235942840576172,11.828020095825195,10.879178047180176,8.235942840576172,7.657486438751221,8.235942840576172,9.70385456085205,7.476334571838379,0.6056872188765469,0.4628461246485049,0.8411335149844271,0.08536700389718864,0.09579722753547949,0.2099686111538292,0.08536700389718864,0.09579722753547949,0.20997011383494174,0.9652625030162918,0.9642320751858757,0.8958810479465547,0.9764208596859589,0.9735495247211816,0.9407940478599116,0.08536700389718864,0.09579722753547949,0.20996985112009559,0.8937271969208438,0.9446482440584182,0.6480315024225652,0.9167882513380784,0.9270228626705674,0.8463760747175557
35,https://openreview.net/forum?id=Bkf4XgrKvS,"this paper proposes an unsupervised hierarchical approach for learning graph representations. the proposed architecture is constructed by unrolling k-steps of a parametrized algebraic multigrid approach for minimizing the wasserstein metric between the graph and its representation. the node distance (transport cost) used in the wasserstein metric is also learned as an l2 distance between the embeddings of some graph embedding function. the approach is compared against 6 other state of the art approaches on 5 graph classification tasks, showing significant improvements 4 of them.----------the paper is reasonably well written, however, i think some of the explanations can be tightened further. especially a lot on the background of amg is not really that relevant, since the authors are not transferring technical results from amg. also, it seems like a better flow for presenting this argument might be to switch the order of sections 3.2.1 and 3.1.2. it looks like the main point is that this architecture is trying to emulate iterative coarsened residual optimization of the wasserstein metric between a graph and its representation. how the coarsening matrix is derived is more of a technical point (it looks like the results would be much more sensitive to a switch of metric than to a switch of parametrization for s). ----------the empirical results are quite intriguing. there are, however, natural and important questions left unanswered. first and foremost, how does the amount of downsampling (compression) compare between methods. how many parameters do different methods require? it would also be good to see what the baseline performance would have been without any input compression as to understand how close these approaches are to the upper bound.----------finally, i think the main issue of this paper, is left unresolved, namely, what is the point of not having supervision from the downstream task. as a user of graph representations trying to solve some problem, the only thing i would want from my representation is to capture some notion of sufficient statistics that are small enough to be efficient and allow me to solve my problem. i would not necessarily care about how well the learned representation resembles the original graph unless i believed that my downstream task was hard to evaluate and that it was very smooth in the wasserstein metric. i read the paper multiple times, trying to find any discussion on this, but it seems that the fact that an unsupervised representation is a good thing is taken for granted. a point could at least be made using the same representation for different tasks experimentally. or, perhaps, literally doing an amg-type unpacking of the downstream task itself as a comparison. this would shed light on the question of whether the iterated residuals or the choice of distance is what's driving the observed results. the paper proposes a differentiable coarsening approach for graph neural network (gnns).-----to this end, it is motivated by algebraic multigrid and optimal transport methods. ----------gnns is indeed an interesting line of research. and introducing coarsening into them, it a highly relevant step. however, there are some major downsides. first, some of the statements are a little but too strong. the paper starts with claiming that gnns are competitive to graph kernels. but then for instance----------christopher morris, martin ritzert, matthias fey, william l. hamilton, jan eric lenssen, gaurav rattan, martin grohe:-----weisfeiler and leman go neural: higher-order graph neural networks. aaai 2019: 4602-4609----------show that many (if not all) gnns are equivalently expressive as the weisefeiler-lehman (wl) graph kernel. hence, the competitiveness has to be qualified. moreover, since you also employ graph convolutional networks for coarsening, you are also in the regime of this paper. consequently, one should actually compare to wl, at least one should mention this connection. actually, given that the datasets are not that large, one should run some statistical significance test. moreover, if you check the paper above, they report much better results for patchysan on mutag, better results on protein for graph kernels, better results on imdb-b using a hierarchical gnn approach, based on ideas of higher-order wl. ----------nevertheless, indeed, the present paper shows that a differentiable pooling using wl kind of ideas is competitive to existing pooling approach. this is nice, but in the light of the work above, the novelty is unclear. this has to be clarified before publication. this paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ml tasks such as graph classification.-----although the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below:----------- in section 3.1, the coarsening method has been proposed, which is said to be achieved by finding s such that a_c = s^t a s. ----- however, a_c is usually not binary for s \in r^{n x m}, hence how to get the coarse graph g_c from a_c is not clear. please carefully explain this point.------ in the proposed method, coarse nodes should be selected beforehand. is there any guideline of how to choose them?------ in section 3.2, optimal transport is introduced and the distance between g and g_c is measured via entropic optimal transport in equation (4) or (7).----- however, in equation (4), a and b should come from the input g and g_c , and it is not clearly explained how to obtain them from the input.----- moreover, how to use the distance between g and g_c in the proposed coarsening method is also not clear. it seems that it is not used in algorithm 1.------ i do not understand why the k-step optimal transport distance is needed. since it converges to the global optimum as k becomes large, it is usually enough to set k to be large enough.------ in experiments, how is the proposed method used for graph classification?----- since the proposed method is for generating coarse graphs in an unsupervised manner, graph classification cannot be directly performed by itself.------ in addition to the above issue, to assess the effectiveness of the the proposed method, the following experiment is recommended:----- fix some classifier and compare performance of graph classification for the original graphs and for the coarse graphs.------ in the qualitative study in section 4.4, while the authors discuss coarse nodes, they are just an input from the user and results are arbitrary. hence such discussion is not informative.----------minor comments:------ what is ""x"" in equation (2)?------ i recommend to write domain for matrices when they used at the first time.","this paper presents a differentiable coarsening approach for graph neural network. it provides the empirical demonstration that the proposed approach is competitive to existing pooling approaches. however, although the paper shows an interesting observation, there are remaining novelty as well as clarity concerns. in particular, the contribution of the proposed work over the graph kernels based on other forms of coarsening such as the early work of shervashidze et al. as well as higher-order wl (pointed out by reviewer1) remains unclear. we believe the paper currently lacks comparisons and discussions, and will benefit from additional rounds of future revisions.","it would also be good to see what the baseline performance would have been without any input compression as to understand how close these approaches are to the upper bound.----------finally, i think the main issue of this paper, is left unresolved, namely, what is the point of not having supervision from the downstream task.","since it converges to the global optimum as k becomes large, it is usually enough to set k to be large enough.------ in experiments, how is the proposed method used for graph classification?----- since the proposed method is for generating coarse graphs in an unsupervised manner, graph classification cannot be directly performed by itself.------ in addition to the above issue, to assess the effectiveness of the the proposed method, the following experiment is recommended:----- fix some classifier and compare performance of graph classification for the original graphs and for the coarse graphs.------ in the qualitative study in section 4.4, while the authors discuss coarse nodes, they are just an input from the user and results are arbitrary.","the paper proposes a differentiable coarsening approach for graph neural network (gnns).-----to this end, it is motivated by algebraic multigrid and optimal transport methods.",this paper proposes an unsupervised hierarchical approach for learning graph representations.,"the paper proposes a differentiable coarsening approach for graph neural network (gnns).-----to this end, it is motivated by algebraic multigrid and optimal transport methods.","this paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ml tasks such as graph classification.-----although the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below:----------- in section 3.1, the coarsening method has been proposed, which is said to be achieved by finding s such that a_c = s^t a s. ----- however, a_c is usually not binary for s \in r^{n x m}, hence how to get the coarse graph g_c from a_c is not clear.","the paper proposes a differentiable coarsening approach for graph neural network (gnns).-----to this end, it is motivated by algebraic multigrid and optimal transport methods.","this paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ml tasks such as graph classification.-----although the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below:----------- in section 3.1, the coarsening method has been proposed, which is said to be achieved by finding s such that a_c = s^t a s. ----- however, a_c is usually not binary for s \in r^{n x m}, hence how to get the coarse graph g_c from a_c is not clear.",0.2193548387096774,0.0130718954248366,0.1419354838709677,0.1419354838709677,0.2568807339449541,0.037037037037037,0.1559633027522936,0.1559633027522936,0.272,0.1300813008130081,0.208,0.208,0.1261261261261261,0.036697247706422,0.0900900900900901,0.0900900900900901,0.272,0.1300813008130081,0.208,0.208,0.3238095238095237,0.048076923076923,0.1523809523809524,0.1523809523809524,0.272,0.1300813008130081,0.208,0.208,0.3238095238095237,0.048076923076923,0.1523809523809524,0.1523809523809524,9.233636856079102,9.37204933166504,12.412347793579102,9.192825317382812,6.169088363647461,9.372048377990724,9.233636856079102,9.372048377990724,0.9396178145923418,0.946933301830134,0.904781729193846,0.2023302845540116,0.38322345063608676,0.7951284143261269,0.9838423754906538,0.9814122576794702,0.9133862329855146,0.9647960056194688,0.9656844361448956,0.9484079505656227,0.9838423754906538,0.9814122576794702,0.9133862329855146,0.9024433652271718,0.9031870207440547,0.9332482004055961,0.9838423754906538,0.9814122576794702,0.9133862329855146,0.9024433652271718,0.9031870207440547,0.9332481686136874
36,https://openreview.net/forum?id=BkgUB1SYPS,"this paper derives lower bounds on the separation rank of a wider class of recurrent nlp models in terms of its depth and number of hidden layers, demonstrating that both the number of hidden units as well as the number of layers improves the ability of nlp networks to model context dependency. it then introduces a novel bidirectional nlp variant that is supposed to capture a good trade-off between computational cost and performance.----------the manuscript is very dense and does not follow a straight and easy-to-follow story line. in particular, the introduction of the bidirectional variant seems to substantially distract from the main story line of the paper (there is also no connection between the theoretical results to the bidirectional network). the improvements of the bidirectional models also seem to be minor, but no standard deviations for the performance results are reported.----------a clear description as to which language models are captured by the tslm model is missing. also, it is unclear how tight the bounds actually are given that no value for m (the word length) is given. finally, the title does not reflect the content of the paper (there is nothing interpretable about the network structure). the goal of the work is to quantify the dependency between contents in nlp. the method relies on parametrization of the joint probability of words in (2), and discussed some connections between the rank of the unfolded tensor t and the dependency level between two sets of words in a sentence.----------the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing. the work seems to build upon a previous work, namely, tensor space language model (tslm). but the paper does not introduce tslm in detail, making the part relevant to tslm quite inaccessible.----------the analysis of the work is based on the model in (2), which is merely an approximation for the joint probability. this is fine, but maybe this point should be more spelled out in the paper.----------the work also has an assumption that a naive bayes model in (4) always holds for a set of w_1 ... w_n. this may need some more discussion and maybe a reference. since the authors are considering a sequence, this may be related to de finetti's theorem and its extensions. but in that theorem many more assumptions are needed, eg., the rvs are exchangeable. also, it is unknown if a finite k exists.----------the proof of claim 1 is a bit trivial, if one only considers one-hot encoding. in fact, the statement and proof of claim 1 might be a bit loose. if one wishes that the svd reveals the rank k, k has to be smaller than the outer dimensions of the tensor t. this was not specified in the statement.----------the above also brings up another question: are the proofs all based on one-hot encoding of words? we know in nlp pre-trained word embeddings may be more useful. do the proofs also apply to those cases, e.g., glove or word2vec?----------did all the experiments use one-hot embedding?----------overall, the reviewer feels that the work has an interesting motivation, and the goal is meaningful. the writing is a bit hard to access and the proofs might be a bit loose (did not check all of them. but claim 1 is already a bit loose).","this paper a theoretical interpretation of separation rank as a measure of a recurrent network's ability to capture contextual dependencies in text, and introduces a novel bidirectional nlp variant and tests it on several nlp tasks to verify their analysis. ----------reviewer 3 found that the paper does not provide a clear description of the method and that a focus on single message would have worked better. reviewer 2 made a claim of several shortcomings in the paper relating to lack of clarity, limited details on method, reliance on a 'false dichotomy', and failure to report performance. reviewer 1 found the goals of the work to be interesting but that the paper was not clear, that the proofs were not rigorous enough, and clarity of experiments. the authors responded to the all the comments. the reviewers felt that their comments were still valid and did not adjust their ratings.----------overall, the paper is not yet ready in its current form. we hope that the authors will find valuable feedback for their ongoing research.",it then introduces a novel bidirectional nlp variant that is supposed to capture a good trade-off between computational cost and performance.----------the manuscript is very dense and does not follow a straight and easy-to-follow story line.,"the method relies on parametrization of the joint probability of words in (2), and discussed some connections between the rank of the unfolded tensor t and the dependency level between two sets of words in a sentence.----------the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing.","the method relies on parametrization of the joint probability of words in (2), and discussed some connections between the rank of the unfolded tensor t and the dependency level between two sets of words in a sentence.----------the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing.","this paper derives lower bounds on the separation rank of a wider class of recurrent nlp models in terms of its depth and number of hidden layers, demonstrating that both the number of hidden units as well as the number of layers improves the ability of nlp networks to model context dependency.","the method relies on parametrization of the joint probability of words in (2), and discussed some connections between the rank of the unfolded tensor t and the dependency level between two sets of words in a sentence.----------the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing.","the method relies on parametrization of the joint probability of words in (2), and discussed some connections between the rank of the unfolded tensor t and the dependency level between two sets of words in a sentence.----------the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing.",this may need some more discussion and maybe a reference.,"the method relies on parametrization of the joint probability of words in (2), and discussed some connections between the rank of the unfolded tensor t and the dependency level between two sets of words in a sentence.----------the goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing.",0.1981132075471698,0.0666666666666666,0.1415094339622641,0.1415094339622641,0.2794759825327511,0.0616740088105726,0.1746724890829694,0.1746724890829694,0.2794759825327511,0.0616740088105726,0.1746724890829694,0.1746724890829694,0.2577777777777778,0.0269058295964125,0.16,0.16,0.2794759825327511,0.0616740088105726,0.1746724890829694,0.1746724890829694,0.2794759825327511,0.0616740088105726,0.1746724890829694,0.1746724890829694,0.0327868852459016,0.0,0.0327868852459016,0.0327868852459016,0.2794759825327511,0.0616740088105726,0.1746724890829694,0.1746724890829694,5.198656558990479,5.198656558990479,11.004584312438965,5.198659420013428,11.366419792175291,5.198656558990479,5.198656558990479,2.342105865478516,0.9581306758108808,0.9687219673065821,0.7943374857053294,0.8964248283147906,0.9563313853217178,0.1856997197694783,0.8964248283147906,0.9563313853217178,0.18569985111524387,0.9696972370162329,0.9694380749530787,0.9345723261600664,0.8964248283147906,0.9563313853217178,0.1856997197694783,0.8964248283147906,0.9563313853217178,0.1856996722979655,0.9399931743318938,0.9471717809395589,0.9084580559951174,0.8964248283147906,0.9563313853217178,0.1856997197694783
37,https://openreview.net/forum?id=Bkgwp3NtDH,"this paper proposed a ""learnable"" trojan by training a neural network that takes a sample (e.g. an image) as an input and generates a programmable trigger pattern as an output. the authors argue that the proposed method can support dynamic and out scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. the authors conducted experiments on large-scale models (imagenet) in two settings: (1) outsourced training attack; (2) transfer learning attack.----------although the idea of making a backdoor attack more robust (e.g. more transferrable) and programmable is interesting, i don't think the current results fully substantiate the claimed benefits. below are my concerns:----------1. lack of performance comparison: in the outsourced training attack, the attack success rate is quite low ( the best top-1 attack rate is ~50% on vgg). in the existing literature, the backdoor attack success rate can be made nearly 100%. this makes me wonder whether the low attack success rate only occurs in the proposed attacking method. since there are no results from existing attacks, there is no way to evaluate how good the proposed attack is.----------2. how about small dataset/model? i understand that the authors want to emphasize the scalability of their attack to large models like imagenet. however, there are no comparisons on imagenet (my comment 1). the authors are suggested to compare performance on standard datasets in backdoor attack literature (e.g. cifar-10, traffic sign)----------3. it was unclear how ""dynamic"" the proposed method can be. based on the attack formulation in equation (3), in order to make the attack ""dynamic"" in terms of changing different target classes, the attackers need to train a neural trojan network for every target class, which does not seem to be dynamic to me. can the authors further justify the advantage of the dynamic feature in the proposed attack? and i have concerns about how many target classes an attacker can ""dynamically"" change. some experiments showing the number of target classes vs attack performance and clean data accuracy will be very helpful.----------4. the defense argument against detection methods is weak. unless the authors can show the proposed attack has the ability to simultaneously backdoor all possible target classes, simply arguing the attack is dynamic and thus can evade detection is not convincing, not to mention in the backdoor setting, attacker should make the first move before the defender takes action.----------*** post-rebuttal comments-----i thank the authors for the clarification. however, i feel my comments have not been fully addressed, especially on the part on justifying 50% attack success rate on vgg should be considered as significant in the considered setup. without any valid comparisons, i find it difficult to assess the contributions. in addition, the authors did not add new empirical evidence regarding my questions but mainly re-iterated the applicability of the proposed method, so i will remain my review rating.-----*** this paper proposes a general framework for constructing trojan/backdoor attacks on deep neural networks, specifically in cases where the end user plans to perform transfer learning on the backdoored classifier. to accomplish this, this work proposes the use of a trigger generator that is trained alongside the trojaned network, that allows the attacker to cause any image to be misclassified as any other. the proposed method is general, and is shown to work across a variety of datasets.----------although the threat model here is slightly less general than the standard backdoor attacks threat model (in that the adversary here also needs to be involved in training the model, and not just supplying the data), the authors do well to motivate the threat model, and the attack is sufficiently general to be an interesting security model. the method is, to the best of my knowledge, novel, and is an innovative way to launch trigger attacks even when the adversary does not know what the final classes or task will be. the evaluation is thorough enough, though the results themselves could be better. however, as this is the first attack of this kind, the results seem sufficient to warrant publication.----------my major concern with this work is readability---many of the sections are in need of significant proofreading, and there are many grammatical/word choice mistakes that make the paper somewhat difficult to read. (for example, ""p is derivative"" in section 4.2 should probably say ""p is differentiable""; the use of frontend/backend is somewhat counterintuitive, as the backend should be the what is given to victims, and the victim trains a frontend reliant on the backend). the method itself could also be presented more clearly (section 4.2 specifically could use less equations and more exposition). overall the paper requires significant written revision in order to be up to the standard of publication, but given that these concerns can be addressed in the revision period i am (weakly) recommending acceptance. this paper proposes an adaption of existing backdoor attacks, with the main goal of enabling backdoor attacks in the transfer learning setting. specifically, instead of pre-defining the trigger pattern and the target label, they train a neural network to generate different trigger patterns for different target images, so that after the source image is blended with the generated trigger pattern, it will be classified in the same way as the target image. this formulation makes it possible to generate backdoor attacks that stay effective in the transfer learning setting, when the label set of the fine-tuning task is different from the original task. they evaluate their approach using pre-trained models on imagenet, and also show transfer learning results using two smaller datasets.----------i think studying the effectiveness of backdoor attacks under the transfer learning setting is a good topic. however, i am not convinced that the proposed approach is necessary a good way to do so, and have the following questions:----------1. to train the trigger generator, do the authors only train it on the pre-trained dataset, or the images of the downstream task is also used? if training does not use the images from the downstream task at all, then it is interesting that the generator can generalize well, which may suggest that although the downstream task has a different prediction goal, the input images themselves share some similarity to the pre-trained task. could the authors provide some explanation on it?----------2. for the attack success rate, i would like to see more analysis on how the choice of the source and target images affect the attack performance. specifically, if the source and target images have the same label on the pre-trained task, but different on the downstream task, is it easier or harder to generate successful backdoors for the downstream task? similarly, what if the source and target images have different labels on the pre-trained task, but the same label on the downstream task?----------3. is it necessary to generate different trigger patterns for every different target image? is it possible to use the same trigger pattern for multiple target images, at least in the case when they look similar? in general, backdoor attacks would expect that the same trigger pattern can be re-used among different input instances.----------------------------post-rebuttal comments----------thanks for your response! however, i still think the evaluation is weak, given that the attack success rate is low, and the current version of the paper does not provide a good justification for both the design choices and the empirical results. therefore, i keep my original assessment.-----------------","this paper proposes a general framework for constructing trojan/backdoor attacks on deep neural networks. the authors argue that the proposed method can support dynamic and out-of-scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting. this paper has been very carefully discussed. while the idea is interesting and could be of interest to the broader community, all reviewers agree that it lacks of experimental comparison with existing methods for backdoor attacks on benchmark problems. the paper needs to be significantly revised before publication. i encourage the authors to improve this paper and resubmit to future conference.","however, as this is the first attack of this kind, the results seem sufficient to warrant publication.----------my major concern with this work is readability---many of the sections are in need of significant proofreading, and there are many grammatical/word choice mistakes that make the paper somewhat difficult to read.","unless the authors can show the proposed attack has the ability to simultaneously backdoor all possible target classes, simply arguing the attack is dynamic and thus can evade detection is not convincing, not to mention in the backdoor setting, attacker should make the first move before the defender takes action.----------*** post-rebuttal comments-----i thank the authors for the clarification.","however, i still think the evaluation is weak, given that the attack success rate is low, and the current version of the paper does not provide a good justification for both the design choices and the empirical results.","this paper proposed a ""learnable"" trojan by training a neural network that takes a sample (e.g. an image) as an input and generates a programmable trigger pattern as an output.","based on the attack formulation in equation (3), in order to make the attack ""dynamic"" in terms of changing different target classes, the attackers need to train a neural trojan network for every target class, which does not seem to be dynamic to me.","the authors argue that the proposed method can support dynamic and out scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting.","however, i feel my comments have not been fully addressed, especially on the part on justifying 50% attack success rate on vgg should be considered as significant in the considered setup.","unless the authors can show the proposed attack has the ability to simultaneously backdoor all possible target classes, simply arguing the attack is dynamic and thus can evade detection is not convincing, not to mention in the backdoor setting, attacker should make the first move before the defender takes action.----------*** post-rebuttal comments-----i thank the authors for the clarification.",0.2838709677419355,0.0130718954248366,0.1548387096774193,0.1548387096774193,0.3780487804878049,0.074074074074074,0.2195121951219512,0.2195121951219512,0.2253521126760563,0.0285714285714285,0.1690140845070422,0.1690140845070422,0.1481481481481481,0.0601503759398496,0.1333333333333333,0.1333333333333333,0.3243243243243243,0.0547945205479452,0.1756756756756756,0.1756756756756756,0.4122137404580153,0.3875968992248062,0.4122137404580153,0.4122137404580153,0.1333333333333333,0.0150375939849624,0.0888888888888888,0.0888888888888888,0.3780487804878049,0.074074074074074,0.2195121951219512,0.2195121951219512,14.87277603149414,12.529845237731934,14.74362850189209,12.522363662719728,6.182412624359131,7.732907295227051,12.522363662719728,6.662762641906738,0.9119667338472188,0.9298876269847438,0.8252296177466518,0.9367288761677176,0.9494146332359128,0.9311352202229959,0.7098300304933475,0.8921296049700385,0.9161884062237333,0.98021897539115,0.953712185850718,0.35771508200900126,0.9276128297729497,0.9521001775251872,0.9515861993996969,0.9696792850894596,0.970769239348891,0.8145061604536897,0.938799429003184,0.9354924425415901,0.8061301874001937,0.9367288761677176,0.9494146332359128,0.9311352202229959
38,https://openreview.net/forum?id=BklhAj09K7,"the authors studied an interesting problem of unsupervised domain adaptation when the source and the target domains have disjoin labels spaces. the paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.----------------strengths:----------------1) the proposed approach on feature transfer network was novel and interesting.--------2) the paper was very well written with a good analysis of various choices.--------3) extensive empirical analysis on multi-class settings with a traditional mnist dataset and a real-world face recognition dataset. ------------------------weakness:--------1) practical considerations addressing feature reconstruction loss needs more explanation.----------------comments:----------------the technical contribution of the paper was sound and novel. the paper considered existing work and in a good way generalizes and extends into disjoint label spaces. it was easy to read and follow, most parts of the paper including the appendix make it a good contribution. however, the reviewer has the following suggestions"" ----------------1. under the practical considerations for preventing the mode collapse via feature reconstruction, how is the reference network trained? in the equation(6) for feature reconstruction, the f_ref term maps the source and target domain examples to new feature space. what do you mean by references network trained on the label data? please clarify.----------------2. under the practical considerations for replacing the verification loss, it is said that ""our theoretical analysis suggests to use a verification--------the loss that compares the similarity between a pair of images"" - can you please cite the references to make it easier for the reader to follow. in this work, authors consider transfer learning problem when labels for the target domain is not available. unlike the conventional transfer learning, they introduce a new loss that separates examples from different domains. besides, they apply the multi-class entropy minimization to optimize the performance in the target domain. here are my concerns.--------1. the concept is not clear. for domain adaptation, we usually assume domains share the same label space. when labels are different, it can be a transfer learning problem.--------2. optimizing the verification loss is conventional for distance metric learning based transfer learning and authors should discuss more in the related work.--------3. the empirical study is not sufficient. there lacks the method of transfer learning with distance metric learning. moreover, the major improvement seems from the mcem rather than the proposed network. how about dann+mcem? i like the idea of the paper and i believe it addressing a very relevant problem. while the authors provide a good formalization of the problem and convincing demonstration of the generalization bound, the evaluation could have been better by including some more challenging experiments to really prove the point of the paper. it is surely good to present the toy example with the mnist dataset but the ethnicity domain is less difficult than what the authors claim. this is also pretty evident from the results presented (e.g., in table 3). the proposed approach provides maybe slightly better results than the state of the art but the results do not seem to be statistically significant. this is probable also due to the fact that the problem itself is made simpler by the cropped faces, no background, etc. i would have preferred to see an application domain where the improvement would be more substantial. nevertheless, i think the theoretical presentation is good and i believe the manuscript has very good potential.","this paper proposes a new solution for tackling domain adaptation across disjoint label spaces. two of the reviewers agree that the main technical approach is interesting and novel. the final reviewer asked for clarification of the problem setting which the authors have provided in their rebuttal. we encourage the authors to include this in the final version. however, there is also a consensus that more experimental evaluation would improve the manuscript and complete experimental details are needed for reliable reproduction.","while the authors provide a good formalization of the problem and convincing demonstration of the generalization bound, the evaluation could have been better by including some more challenging experiments to really prove the point of the paper.","the paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.----------------strengths:----------------1) the proposed approach on feature transfer network was novel and interesting.--------2) the paper was very well written with a good analysis of various choices.--------3) extensive empirical analysis on multi-class settings with a traditional mnist dataset and a real-world face recognition dataset.","in this work, authors consider transfer learning problem when labels for the target domain is not available.",the authors studied an interesting problem of unsupervised domain adaptation when the source and the target domains have disjoin labels spaces.,"the paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.----------------strengths:----------------1) the proposed approach on feature transfer network was novel and interesting.--------2) the paper was very well written with a good analysis of various choices.--------3) extensive empirical analysis on multi-class settings with a traditional mnist dataset and a real-world face recognition dataset.","the paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.----------------strengths:----------------1) the proposed approach on feature transfer network was novel and interesting.--------2) the paper was very well written with a good analysis of various choices.--------3) extensive empirical analysis on multi-class settings with a traditional mnist dataset and a real-world face recognition dataset.","nevertheless, i think the theoretical presentation is good and i believe the manuscript has very good potential.","the paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.----------------strengths:----------------1) the proposed approach on feature transfer network was novel and interesting.--------2) the paper was very well written with a good analysis of various choices.--------3) extensive empirical analysis on multi-class settings with a traditional mnist dataset and a real-world face recognition dataset.",0.3247863247863248,0.0695652173913043,0.188034188034188,0.188034188034188,0.2222222222222222,0.028169014084507,0.1666666666666666,0.1666666666666666,0.1855670103092783,0.0,0.1030927835051546,0.1030927835051546,0.2574257425742574,0.0606060606060606,0.1188118811881187,0.1188118811881187,0.2222222222222222,0.028169014084507,0.1666666666666666,0.1666666666666666,0.2222222222222222,0.028169014084507,0.1666666666666666,0.1666666666666666,0.1030927835051546,0.0210526315789473,0.1030927835051546,0.1030927835051546,0.2222222222222222,0.028169014084507,0.1666666666666666,0.1666666666666666,15.056907653808594,15.056907653808594,12.151015281677246,15.056907653808594,7.324065685272217,11.835077285766602,15.056907653808594,7.894437313079834,0.9695328989686581,0.9602790702006171,0.9491080871636043,0.993729006428243,0.9928251803786718,0.9438221106595293,0.9712156382500301,0.9746247717720324,0.9315265028112504,0.974096228511535,0.9748278033457215,0.880741684982281,0.993729006428243,0.9928251803786718,0.9438221106595293,0.993729006428243,0.9928251803786718,0.9438220758340903,0.9663793981465097,0.9635221084337287,0.9022171425165405,0.993729006428243,0.9928251803786718,0.9438221106595293
39,https://openreview.net/forum?id=Bkx5XyrtPS,"the motivation of this paper is training deep neural network seems to not suffer from local minima, and it tries to explain this phenomenon by showing that all local minima of deep neural network is global minima. the paper shows that for any convex differentiable loss function, a deep linear neural network has no so called spurious local minima, which to be specific, are local minima that are not global minima, as long as it is true for two-layer neural network. the motivation is that combining with existing result that no spurious local minima exists for quadratic loss in two-layer neural network, this relation connecting between two-layer and deeper linear neural network immediately implies an existing result that all local minima are global minima, removing all assumptions. the result also holds for general multi-tower linear networks. ----------overall, this paper could be an improvement of existing results. it is well written and the proof step is clear in general. however, therere some weakness need clarifications on the results, especially on the novelty. given reasonable clarifications in response, i would be willing to change my score.----------for novelty, it is unclear if the results from lemma 1 to theorem 1 and 2 are both being stated as novel results. the first part of proof of theorem 1 is obvious and straightforward, and the other direction has been used before for multiple times as claimed in the paper, what is your novelty exactly here? for the key technical claim of lemma 1, it looks like this perturbation technique already exists in (laurent & brecht, 2018), why do you claim it as a novel argument? ----------besides novelty, there are also some other unclear pieces in this paper needs clarification:-----1) is the main result which is no spurious local minima for deep neural network holds for any differentiable convex loss other than quadratic loss? how will theorem 1 help us understand the mystery of neural network? -----2) how does the result help us understand non-linear deep neural network, which is commonly use in practice?-----3) the paper should give some explanations about why the results help training neural networks. summary: ----------the paper shows a deep linear network has no spurious local minima as long as it is true for the two layer case for any convex differentiable loss. ----------comments: ----------1) i understand that there exists some work on deep linear network recently. however, they seem to be only for theoretical purpose. most of the current practical problems do not consider this kind of network for training. if it has high impact in practice, then people are starting to use it. could you please provide more reasons why we need to care about this impractical network? ----------2) it is still unclear about the contributions of the paper. why deep linear network has no spurious local minima as long as it is true for the two layer case is important? and what we can take any advantage from here? what if there exist some spurious local minima for the two layer case (which is widely true)? ----------3) the paper looks like a technical report and seems not to be ready. ----------the results are quite incremental from the existing ones. the contributions of this work to the deep learning community are still ambiguous.",paper shows that the question of linear deep networks having spurious local minima under benign conditions on the loss function can be reduced to the two layer case. this paper is motivated by and builds upon works that are proven for specific cases. reviewers found the techniques used to prove the result not very novel in light of existing techniques. novelty of technique is of particular importance to this area because these results have little practical value in linear networks on their own; the goal is to extend these techniques to the more interesting non-linear case.,summary: ----------the paper shows a deep linear network has no spurious local minima as long as it is true for the two layer case for any convex differentiable loss.,"the motivation is that combining with existing result that no spurious local minima exists for quadratic loss in two-layer neural network, this relation connecting between two-layer and deeper linear neural network immediately implies an existing result that all local minima are global minima, removing all assumptions.","given reasonable clarifications in response, i would be willing to change my score.----------for novelty, it is unclear if the results from lemma 1 to theorem 1 and 2 are both being stated as novel results.","the motivation of this paper is training deep neural network seems to not suffer from local minima, and it tries to explain this phenomenon by showing that all local minima of deep neural network is global minima.",summary: ----------the paper shows a deep linear network has no spurious local minima as long as it is true for the two layer case for any convex differentiable loss.,"the motivation of this paper is training deep neural network seems to not suffer from local minima, and it tries to explain this phenomenon by showing that all local minima of deep neural network is global minima.",could you please provide more reasons why we need to care about this impractical network?,"the motivation is that combining with existing result that no spurious local minima exists for quadratic loss in two-layer neural network, this relation connecting between two-layer and deeper linear neural network immediately implies an existing result that all local minima are global minima, removing all assumptions.",0.2539682539682539,0.1129032258064516,0.1904761904761905,0.1904761904761905,0.3034482758620689,0.0419580419580419,0.1517241379310344,0.1517241379310344,0.1954887218045112,0.015267175572519,0.1052631578947368,0.1052631578947368,0.3134328358208955,0.0606060606060606,0.1940298507462686,0.1940298507462686,0.2539682539682539,0.1129032258064516,0.1904761904761905,0.1904761904761905,0.3134328358208955,0.0606060606060606,0.1940298507462686,0.1940298507462686,0.0714285714285714,0.0,0.0535714285714285,0.0535714285714285,0.3034482758620689,0.0419580419580419,0.1517241379310344,0.1517241379310344,15.871161460876465,12.918572425842283,15.871160507202148,12.919532775878906,12.918572425842283,2.914438009262085,12.919533729553224,4.557540893554688,0.9852690745291968,0.9834780324343574,0.8860022500609298,0.969987976844364,0.9671166613517805,0.902513426805417,0.9629261665814907,0.9631463699372835,0.8983366342626652,0.981504932277154,0.9812654594032427,0.5665839706651574,0.9852690745291968,0.9834780324343574,0.8860022500609298,0.981504932277154,0.9812654594032427,0.5665837322422226,0.9634744972126914,0.945085014732239,0.9400944010795745,0.969987976844364,0.9671166613517805,0.902513426805417
40,https://openreview.net/forum?id=Bkx8JJBtDS,"this paper suggests a measure for predicting the performance of deep networks by counting number of paths. authors further evaluate this measure empirically and show that it is predictive of generalization.----------finding a measure that predicts generalizability for a given architecture is very interesting and potentially impactful since it can be used to make design choices.----------overall, i enjoyed reading this paper but i have some concerns and i hope authors can address them:----------1- definition: i think the definition make sense for layered fully connected networks but it seems like the way authors extend it to convent is not elegant. in particular, we know that convnets generalize even without weight sharing in which case they can be presented as a simple feedforward network. is it possible to generalize the definition to any feedforward network presented with a directed acyclic graph (including convnets)? another reason that this definition for convnets bothers me is that we often observe that the number of channels increases after pooling which connects #channels in each layer to the size of the image in the layer. but this definition completely ignores size of the image in the layers. another issue is that at least in the main text, the definition is provided for a layered networks and it is not obvious to me that how it can be extended to densenet and resnet.----------2- experiments: authors provide many experiments to support their claim which is great but they avoid the most direct way to evaluate the measure. i think the best way to evaluate the measure is to train many networks (layer fully connected to layered convnets) with different number of neurons and plot generalization vs #paths. this would be a very convincing experiments. many experiments provided here do not directly evaluate the measure in a convincing way.---------------i hope authors address above concerns in which case i will increase my score. title-----counting the paths in deep neural networks as a performance predictor----------review summary----------paper summary-----the paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network. the paper demonstrates a correspondence between the proposed measure and the empirical performance in a set of experiments. it is argued that more effective neural network structures can be found by maximizing the proposed measure: this is demonstrated by a novel modification of a densenet architecture that increases the proposed measure and leads to a modest performance gain.----------quality and significance-----i really enjoyed reading this paper since it aims to give a better understanding of the relation between neural network archtecture and performance, for which there is really not much theory. and while i understand the motivation behind the simple measure of path complexity proposed, i am not convinced that the measure really is very informative. the paper only provides weak empirical justification and no theoretical justification. demonstrating an improvement on cifar-10 with a relatively minor change of the densenet architecture is interesting and worth noticing. but i am simply not convinced about the explanation, and as a demonstration of the particular improvement the experiment is too limited (as the improvement in accuracy is marginal).----------clarity-----the presentation is clear and the paper is in general easy to follow. experiments are described in sufficient detail and source code is provided, which is commendable!---------------further comments----------i am not sure why you refer to number of layers etc. as ""hidden"" hyperparameters. why ""hidden""?----------""with continuous mathematics"" unclear----------""the goal of dnn models..."" this is not the goal, i think. the goal must be something along the lines of making correct classification on new data (generalization). this section describes your interpretation of how an effective nn works.----------i am not sure how your claim regarding ""increasing the dimension of the representation"" is supported by the reference (vapnik, 2013).----------""comply with many of the assumptions behind complex networks"" what you mean by this is unclear to me. is there some technical definition of ""complex network"" that i am not aware of? if so, provide a reference. (to my knowledge, the term ""complex network"" usually refers to any network with a non-trivial topology.)----------at first read, the definition in eq. 1 is confusing. what happens when layer i+1 has fewer nodes/channels than layer i? then the definition of ""paths"" seems inappropriate. consider explaining this up front.----------the claim that the number of paths quantifies quality (in some sense) is not justified well enough in my view.----------the assumption that ni+1 = ni*(1+alpha) seems unjustified.----------in fig. 2 i would have liked to see all the data from the experiments including results for different initialization and learning rates. also, in this experiment, can you justify using the same weight decay in all settings?----------""...the optimal variance of the weight initialization"" i think ""optimal"" is too strong here.----------if i read fig. 2 correctly, the optimal accuracy and z are not the same (although it seems there is some correspondence) the paper proposes a novel quantity that counts the number of path in the neural network which they argue is predictive of the performance of neural networks with the same number of parameters. the paper shows a continuous approximation z for the proposed path counting model and provides a close-form solution on how the channel number in the architecture should be chosen. the experiment investigates the correlation between z and generalization of model and also compute z on a number of popular architectures to show that z correlates with the empirical performance observed. finally, the paper proposes a small modification to the densenet architecture which increases z and shows that this change improves generalization with no extra parameters.----------i think understanding the influence of the network architecture is a valuable direction for deep learning that is currently under-explored and the approach proposed seems very reasonable. some existing works that the authors that might want to discuss are [1] and [2]. [1] proposes an architecture dependent quantity that can be optimized to improve generalization and [2] explores using different classes of random networks for neural networks. [3] proposes a quantity that predicts generalization albeit with a very different approach.----------i have a number of concerns regarding the motivation, experiments and overall writing quality, so i am currently leaning toward rejecting this paper:---------- 1. the justification for paths in section 2.1 is extremely hand-wavy. this is the core mathematical object the paper studies, and if theoretically rigorous justification is difficult, then at least a detailed heuristic justification or an illustration should be provided. the paper claims the proposed quantity provides an increased understanding of generalization but i do not see sufficient support for this claim in the current draft.---------- 2. the paper argues that neural networks is a complex network and make another loose connection to path entropy, a concept from physics. i believe making connections to physics is great, but this is not a common knowledge for the iclr community and warrants more elaboration.---------- 3. figure 2, 3 and 4 s points (especially the converged points at the the top) do not track the predicted z closely. in fact, in some cases, the networks with smaller z performs the best (figure 3). if this is due to randomness, then the experiments should be repeated for statistical significance. further, different time steps should be colored differently to increase readability.---------- 4. in my opinion, the paper positions itself to be a heuristic paper with empirical verification rather than a theory paper. as such, i think the paper needs much more empirical evidence to support the claim. for example, table 1 is missing many entries which makes comparison hard. given that in the toy models the performance sometimes does not track the prediction closely, i would like to see more results on realistic models and more thorough analysis on the results (especially when the prediction fails to explain the observed results).---------- 5. the proposed method, like the paper points out, requires non-trivial understanding of the architecture. this limits its utility. one of the largest potential i see is using it for architecture search but this need for understanding makes it difficult. is it possible to derive an algorithm to compute z for arbitrary dags? i.e. nasnet or randomly wire neural networks.---------- 6. for methods of improving z, only one possible modification is made to a single architecture (densenet). this is not very convincing evidence for the effectiveness of z. i would like to see such modification made more more architectures such as a vanilla resnet or wide resnet. some proof of universality would strengthen the argument.----------minor comments that did not affect my assessments:----- - typographical errors: inconsistent usage of double quotation, page 3 bottom feeedforward.----- - figure 5/6 look hand-drawn and are extremely confusing. i encourage the authors to remake them with either powerpoint, google slides or tikz and add more details + clarification.----------reference-----[1] efficientnet: rethinking model scaling for convolutional neural networks, tan et al. 2019-----[2] exploring randomly wired neural networks for image recognition, xie et al. 2019-----[3] predicting the generalization gap in deep networks with margin distributions, jiang et al. 2019","this paper proposed a semi-supervised few-shot learning method, on top of prototypical networks, wherein a regularization term that involves a random walk from a prototype to unlabeled samples and back to the same prototype. sota results were obtained in several experiments by using this method. all reviewers agreed that the novelty of the paper is not such high compared with haeusser et al. (2017) and the analysis and the experiments could be improved.","[3] proposes a quantity that predicts generalization albeit with a very different approach.----------i have a number of concerns regarding the motivation, experiments and overall writing quality, so i am currently leaning toward rejecting this paper:---------- 1. the justification for paths in section 2.1 is extremely hand-wavy.",title-----counting the paths in deep neural networks as a performance predictor----------review summary----------paper summary-----the paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network.,"the paper claims the proposed quantity provides an increased understanding of generalization but i do not see sufficient support for this claim in the current draft.---------- 2. the paper argues that neural networks is a complex network and make another loose connection to path entropy, a concept from physics.",this paper suggests a measure for predicting the performance of deep networks by counting number of paths.,title-----counting the paths in deep neural networks as a performance predictor----------review summary----------paper summary-----the paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network.,"it is argued that more effective neural network structures can be found by maximizing the proposed measure: this is demonstrated by a novel modification of a densenet architecture that increases the proposed measure and leads to a modest performance gain.----------quality and significance-----i really enjoyed reading this paper since it aims to give a better understanding of the relation between neural network archtecture and performance, for which there is really not much theory.","finally, the paper proposes a small modification to the densenet architecture which increases z and shows that this change improves generalization with no extra parameters.----------i think understanding the influence of the network architecture is a valuable direction for deep learning that is currently under-explored and the approach proposed seems very reasonable.",title-----counting the paths in deep neural networks as a performance predictor----------review summary----------paper summary-----the paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network.,0.2419354838709677,0.0327868852459016,0.1612903225806451,0.1612903225806451,0.2956521739130435,0.0353982300884955,0.1565217391304348,0.1565217391304348,0.3064516129032258,0.0163934426229508,0.1451612903225806,0.1451612903225806,0.1739130434782609,0.0222222222222222,0.1521739130434782,0.1521739130434782,0.2956521739130435,0.0353982300884955,0.1565217391304348,0.1565217391304348,0.3355704697986577,0.0272108843537415,0.1610738255033557,0.1610738255033557,0.328125,0.0952380952380952,0.1875,0.1875,0.2956521739130435,0.0353982300884955,0.1565217391304348,0.1565217391304348,9.074983596801758,11.36534309387207,15.073848724365234,11.365345001220703,7.977019309997559,9.01500415802002,11.365341186523438,10.476489067077637,0.02598872007974505,0.04463082331519511,0.9241006286419767,0.9912642945245251,0.9881935734641741,0.9597525942633628,0.7142420106431079,0.7282196581922545,0.67474885411313,0.9714195693855879,0.9725002229731586,0.9403120623024294,0.9912642945245251,0.9881935734641741,0.9597525942633628,0.9772785202444509,0.9794379845083251,0.9292699237088649,0.7535711157443159,0.7682301271100455,0.9351648660564271,0.9912642945245251,0.9881935734641741,0.9597525942633628
41,https://openreview.net/forum?id=By-7dz-AZ,"the authors consider the metrics for evaluating disentangled representations. they define three criteria: disentanglement, informativeness, and completeness. they learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights. experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.----------------i think that defining metrics for evaluating the degree of disentanglement in representations is great problem to look at. overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me. ----------------a second limitation of the work is the reliance on a ""true"" set of disentangled factors. we generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors. could the authors perhaps comment on how well these metrics would work in the semi-supervised case?----------------overall, i would say this is somewhat borderline, but i could be convinced to argue for acceptance based on the other reviews and the author response. ----------------minor commments:----------------- tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption. ----------------- it is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. the paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data. ----------------*quality* --------the problem addressed is surely relevant in general terms. however, the contributed framework did not account for previously proposed metrics (such as equivariance, invariance and equivalence). within the experimental results, only two methods are considered: although info-gan is a reliable competitor, pca seems a little too basic to compete against. the choice of using noise-free data only is a limiting constraint (in [chen et al. 2016], info-gan is applied to real-world data). --------finally, in order to corroborate the quantitative results, authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix.----------------*clarity*--------the explanation of the theoretical framework is not clear. in fact, figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping. but, then, the authors missed to clarify how the definitions of d_i and c_j translate this requirement into math. --------also, the criterion of informativeness of section 2 is split into two sub-criteria in section 3.3, namely test set nrmse and zero-shot nrmse: such shift needs to be smoothed and better explained, possibly introducing it in section 2.----------------*originality*--------the paper does not allow to judge whether the three proposed criteria are original or not with respect to the previously proposed ones of [goodfellow et al. 2009, lenc & vedaldi 2015, cohen & welling 2014, jayaraman & grauman 2015]. ----------------*significance*--------the significance of the proposed evaluation framework is not fully clear. the initial assumption of considering factors of variations related to graphics-generated data undermines the relevance of the work. actually, authors only consider synthetic (noise-free) data belonging to one class only, thus not including the factors of variations related to noise and/or different classes.----------------pros: --------the problem faced by the authors is interesting----------------cons:--------the criteria of disentanglement, informativeness & completeness are not fully clear as they are presented.--------the proposed criteria are not compared with previously proposed ones - equivariance, invariance and equivalence [goodfellow et al. 2009, lenc & vedaldi 2015, cohen & welling 2014, jayaraman & grauman 2015]. thus, it is not possible to elicit from the paper to which extent they are novel or how they are related..--------the dataset considered is noise-free and considers one class only. thus, several factors of variation are excluded a priori and this undermines the significance of the analysis.--------the experimental evaluation only considers two methods, comparing info-gan, a state-of-the-art method, with a very basic pca.------------------------**final evaluation**--------the reviewer rates this paper with a weak reject due to the following points.--------1) the novel criteria are not compared with existing ones [goodfellow et al. 2009, lenc & vedaldi 2015, cohen & welling 2014, jayaraman & grauman 2015].--------2) there are two flaws in the experimental validation:-------- 2.1) the number of methods in comparison (infogan and pca) is limited.-------- 2.2) a synthetic dataset is only considered.----------------the reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed. ----------------**evaluation after authors' rebuttal**--------the reviewer has read the responses provided by the authors during the rebuttal period. in particular, with respect to the highlighted points 1 and 2, point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper. despite the same level of clarification has not been reached for what concerns point 2, the proposed framework (although still limited in relevance due to the lack of more realistic settings) can be useful for the community as a benchmark to verify the level of disentanglement than newly proposed deep architectures can achieve. finally, by also taking into account the positive evaluation provided by the fellow reviewers, the rating of the paper has been risen towards acceptance. ****--------i acknowledge the author's comments and improve my score to 7.--------****----------------summary:--------the authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.--------the basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.--------the authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.--------the paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.----------------significance:--------the proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.----------------clarity:--------the paper is lucidly written and very understandable.----------------quality:--------the authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.--------a few comments:--------1. how do the authors propose to deal with multimodal true latent factors? what if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data?--------2. scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?--------3. the actual sources of variation are interpretable and explicit measurable quantities here. however, oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z (for instance the sentiment of a scene) even when these factors are known. how do the authors propose to move past narrow definitions of factors of variation and handle more complex variables? arguably, disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well-behaved and relatively easy to quantify since they relate to image formation physics.--------4. for a paper introducing a formal experimental framework and metrics or evaluation i find that the paper is light on experiments and evaluation. i would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here, especially a variational autoencoder, beta-vae and so on. furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science.----------------novelty:--------previous papers like ""beta-vae"" (higgins et al. 2017) and ""bayesian representation learning with oracle constraints"" by karaletsos et al (iclr 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does. it would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework.",the paper proposes evaluation metrics for quantifying the quality of disentangled representations. there is consensus among reviewers that the paper makes a useful contribution towards this end. authors have addressed most of reviewers' concerns in their response.,"they learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights.","****--------i acknowledge the author's comments and improve my score to 7.--------****----------------summary:--------the authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.--------the basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.--------the authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.--------the paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.----------------significance:--------the proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.----------------clarity:--------the paper is lucidly written and very understandable.----------------quality:--------the authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.--------a few comments:--------1. how do the authors propose to deal with multimodal true latent factors?","****--------i acknowledge the author's comments and improve my score to 7.--------****----------------summary:--------the authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.--------the basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.--------the authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.--------the paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.----------------significance:--------the proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.----------------clarity:--------the paper is lucidly written and very understandable.----------------quality:--------the authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.--------a few comments:--------1. how do the authors propose to deal with multimodal true latent factors?",the authors consider the metrics for evaluating disentangled representations.,"****--------i acknowledge the author's comments and improve my score to 7.--------****----------------summary:--------the authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.--------the basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.--------the authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.--------the paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.----------------significance:--------the proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.----------------clarity:--------the paper is lucidly written and very understandable.----------------quality:--------the authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.--------a few comments:--------1. how do the authors propose to deal with multimodal true latent factors?","actually, authors only consider synthetic (noise-free) data belonging to one class only, thus not including the factors of variations related to noise and/or different classes.----------------pros: --------the problem faced by the authors is interesting----------------cons:--------the criteria of disentanglement, informativeness & completeness are not fully clear as they are presented.--------the proposed criteria are not compared with previously proposed ones - equivariance, invariance and equivalence [goodfellow et al. 2009, lenc & vedaldi 2015, cohen & welling 2014, jayaraman & grauman 2015].",how do the authors propose to move past narrow definitions of factors of variation and handle more complex variables?,"****--------i acknowledge the author's comments and improve my score to 7.--------****----------------summary:--------the authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.--------the basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.--------the authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.--------the paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.----------------significance:--------the proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.----------------clarity:--------the paper is lucidly written and very understandable.----------------quality:--------the authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.--------a few comments:--------1. how do the authors propose to deal with multimodal true latent factors?",0.1690140845070422,0.0289855072463768,0.1408450704225352,0.1408450704225352,0.1714285714285714,0.0411522633744856,0.1224489795918367,0.1224489795918367,0.1714285714285714,0.0411522633744856,0.1224489795918367,0.1224489795918367,0.3478260869565217,0.0909090909090909,0.2173913043478261,0.2173913043478261,0.1714285714285714,0.0411522633744856,0.1224489795918367,0.1224489795918367,0.1565217391304348,0.0176991150442477,0.0869565217391304,0.0869565217391304,0.1785714285714285,0.0,0.1428571428571428,0.1428571428571428,0.1714285714285714,0.0411522633744856,0.1224489795918367,0.1224489795918367,9.80728530883789,11.893868446350098,17.964752197265625,11.893868446350098,8.951192855834961,11.893868446350098,11.893868446350098,7.909330368041992,0.919002686630615,0.9452185429024766,0.0344245894134012,0.7932321984819639,0.7675344835201345,0.9274727930101528,0.7932321984819639,0.7675344835201345,0.9274727930101528,0.9509830907485256,0.9562921015376405,0.7431482641792392,0.7932321984819639,0.7675344835201345,0.9274727930101528,0.5960014978021149,0.8521399927863518,0.4502389868003046,0.19188682495868778,0.3405239417689885,0.9133821199716421,0.7932321984819639,0.7675344835201345,0.9274727930101528
42,https://openreview.net/forum?id=By3v9k-RZ,"the paper presents an interesting framework for babi qa. essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences). the proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index. while the argument makes sense, it is not clear to me why one cannot simply index the original text. the additional encode/decode mechanism seems to introduce unnecessary noise. the framework does include several components and techniques from latest recent work, which look pretty sophisticated. however, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.----------------pros:-------- 1. an interesting framework for babi qa by encoding sentence to n-grams----------------cons:-------- 1. the overall justification is somewhat unclear-------- 2. the approach could be over-engineered for a special, lengthy version of babi and it lacks evaluation using real-world data this paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (""n-grams"") which can be queried efficiently. the authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate ngms on five of the 20 babi tasks. i am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for qa. however, i have some concerns about the specific implementation and model discussed here. how much of the proposed approach is specific to getting good results on babi (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small rnns, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose qa model for natural language? addressing some of these issues would likely prevent scaling to millions of (real) sentences, as the scalability is reliant on programs being efficiently executed (by simple string matching) against a knowledge storage. the paper is missing a clear analysis of ngm's limitations... the examples of knowledge storage from babi in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simple. in its current form, i am borderline but leaning towards rejecting this paper.----------------other questions:--------- is ""n-gram"" really the most appropriate term to use for the symbolic representation? n-grams are by definition contiguous sequences... the authors may want to consider alternatives.--------- why focus only on extractive qa? the evaluations are only conducted on 5 of the 20 babi tasks, so it is hard to draw any conclusions from the results as to the validity of this approach. can the authors comment on how difficult it will be to add functions to the list in table 2 to handle the other 15 tasks? or is ngm strictly for extractive qa?--------- beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny rnn sizes used in the paper. how long does the encoding time take with 10 million sentences?--------- need more detail on the programmer architecture, is it identical to the one used in liang et al., 2017? the authors propose the n-gram machine to answer questions over long documents. the model first encodes the document via tuple extraction. an autoencoder objective is used to produce meaningful tuples. then, the model generates a program, based on the extracted tuple collection and the question, to find an answer.----------------i am very disappointed in the authors' choice of evaluation, namely babi - a toy, synthetic task long abandoned by the nlp community because of its lack of practicality. if the authors would like to demonstrate question answering on long documents, they have the luxury of choosing amongst several large scale, realistic question answering datasets such as the stanford question answering dataset or triviaqa.--------beyond the problem of evaluation, the model the authors propose does not provide new ideas, and rather merges existing ones. this, in itself, is not a problem. however, the authors decline to cite many, many important prior work. for example, the tuple extraction described by the authors has significant prior work in the information retrieval community (e.g. knowledge base population, relation extraction). the idea of generating programs to query over populated knowledge bases, again, has significant related work in semantic parsing and program synthesis. question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact i believe matt gardner wrote his entire thesis on this topic). finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years. non of these areas, with the exception of semantic parsing, are addressed by the author. with sufficient knowledge of related works from these areas, i find that the authors' proposed method lacks proper evaluation and sufficient novelty.","i am a big fan of this idea, but i agree with the reviewers that evaluating this idea on babi (which was originally created from a small set of rules and primitives) discounts quite a bit of what is being claimed here. one of the future directions mentioned by the authors (""investigating whether the proposed n-gram representation is sufficient for natural languages"") should have been included even with a negative result, which would've increased the significance significantly.","then, the model generates a program, based on the extracted tuple collection and the question, to find an answer.----------------i am very disappointed in the authors' choice of evaluation, namely babi - a toy, synthetic task long abandoned by the nlp community because of its lack of practicality.","however, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.----------------pros:-------- 1. an interesting framework for babi qa by encoding sentence to n-grams----------------cons:-------- 1. the overall justification is somewhat unclear-------- 2. the approach could be over-engineered for a special, lengthy version of babi and it lacks evaluation using real-world data this paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (""n-grams"") which can be queried efficiently.",the authors propose the n-gram machine to answer questions over long documents.,the paper presents an interesting framework for babi qa.,"however, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.----------------pros:-------- 1. an interesting framework for babi qa by encoding sentence to n-grams----------------cons:-------- 1. the overall justification is somewhat unclear-------- 2. the approach could be over-engineered for a special, lengthy version of babi and it lacks evaluation using real-world data this paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (""n-grams"") which can be queried efficiently.","however, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.----------------pros:-------- 1. an interesting framework for babi qa by encoding sentence to n-grams----------------cons:-------- 1. the overall justification is somewhat unclear-------- 2. the approach could be over-engineered for a special, lengthy version of babi and it lacks evaluation using real-world data this paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (""n-grams"") which can be queried efficiently.","finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years.","however, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.----------------pros:-------- 1. an interesting framework for babi qa by encoding sentence to n-grams----------------cons:-------- 1. the overall justification is somewhat unclear-------- 2. the approach could be over-engineered for a special, lengthy version of babi and it lacks evaluation using real-world data this paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (""n-grams"") which can be queried efficiently.",0.2857142857142857,0.0483870967741935,0.1269841269841269,0.1269841269841269,0.3372093023255814,0.0705882352941176,0.1627906976744186,0.1627906976744186,0.1304347826086956,0.0444444444444444,0.108695652173913,0.108695652173913,0.0681818181818181,0.0,0.0454545454545454,0.0454545454545454,0.3372093023255814,0.0705882352941176,0.1627906976744186,0.1627906976744186,0.3372093023255814,0.0705882352941176,0.1627906976744186,0.1627906976744186,0.0631578947368421,0.0,0.0421052631578947,0.0421052631578947,0.3372093023255814,0.0705882352941176,0.1627906976744186,0.1627906976744186,11.386676788330078,11.386676788330078,11.174373626708984,11.386676788330078,7.055078029632568,7.651506423950195,11.386676788330078,6.213345527648926,0.4810374713290448,0.5646523859036141,0.9187773347166657,0.9647223421490756,0.9547728599944147,0.21930580637743147,0.9639562882870787,0.9694781350450452,0.9200379344295554,0.967882014261625,0.9692916224756547,0.8213440128044931,0.9647223421490756,0.9547728599944147,0.21930580637743147,0.9647223421490756,0.9547728599944147,0.21930580637743147,0.06787141120543973,0.06777382069252633,0.9208530401198689,0.9647223421490756,0.9547728599944147,0.21930580637743147
43,https://openreview.net/forum?id=By5e2L9gl,"this paper presents a novel layer-wise optimization approach for learning cnn with piecewise linear nonlinearities. the proposed approach trains piecewise linear cnns layer by layer and reduces the sub-problem into latent structured svm, which has been well-studied in the literature. in addition, the paper presents improvements of the bcfw algorithm used in the inner procedure. overall, this paper is interesting. however, unfortunately, the experiment is not convincing. ----------------pros:----------------- to my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.--------- the paper is well-written and easy to follow. ----------------cons:----------------- although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. this makes this work less compelling. -------- --------- the test accuracy performance on cifar-10 reported in the paper doesn't look right. the accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. for example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. also, cifar-10 is a relatively small dataset, ----------------other comments:----------------- if i understand correctly, bcfw only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). therefore, when putting them together, the cccp procedure may not monotonically decrease as the inner procedure is only solved approximately. the authors should add this note when they discuss the properties of their algorithm. a layer wise optimization for cnns with relu activations and max-pooling is proposed and shown to correspond to a series of latent structured svm problems. using cccp style optimization a monotonic decrease of the overall objective function can be guaranteed.----------------summary:----------------i think the discussed insights are very interesting but not presented convincingly. firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. in summary, i think the paper requires some more attention to form a compelling story.----------------quality: i think some of the techniques could be described more carefully to better convey the intuition. at times apples are compared to oranges, e.g., back propagation is contrasted with cccp.--------clarity: some of the derivations and intuitions could be explained in more detail.--------originality: the suggested idea is reasonable albeit heuristics are required.--------significance: since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.----------------details:----------------1. i think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.----------------2. in spirit similar is work by b. amos and j. kolter, input-convex deep networks (https://openreview.net/pdf?id=rovma279bsvnm0j1ipnn) iclr workshop track 2016; which should probably be mentioned.----------------3. i think the comparison between backprop and the discussed cccp approach is not really appropriate. note that backprop is a mechanism to compute gradients and is not at all related to any optimization technique/algorithm. this means that backprop combined with armijo line-search would for example result in convergence guarantees. hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next is according to my opinion superficial.----------------4. more justification regarding the argument that search for the step-size is a disadvantage seems necessary. there is evidence that noise introduced by mini-batches and inaccurate/no line search is actually beneficial. in contrast the proposed cccp procedure may converge pre-maturely to a local optimum close to the initialization. since mini-batches are used no guarantees are available in any case. hence i think additional evidence for the fact that such convergence guarantees dont result in premature stopping seems necessary.----------------5. the observation 1 required to convert the layer wise procedure into an svm seems rather ad-hoc. note that you can easily construct examples where eq. (9) and eq. (7) differ significantly.----------------6. i personally think the experimental evaluation is conducted on examples that are too small and some results are obvious. e.g., lw-svm always improves over the solution of the sgd algorithm. if the authors were to use backtracking line search they could also improve upon lw-svm. this paper proposes a new approaches for optimizing the objective of cnns. the proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of cnn while fixing the parameters in other layers. the key insight of this paper is that, for a large class of cnns, the optimization problem at a particular can be formulated as optimizing a piecewise linear (pl) function. this pl function optimization happens to be the optimization problem commonly encountered in latent structural svm. this connection allows this paper to borrows ideas from the latent structural svm literature, in particular concave-convex procedure, to learn the parameters of cnns.----------------overall, the paper is well-written. traditional, cnns and structural svms are almost two separate research communties. the connection of cnns to latent structural svm is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.----------------of course, the proposed method also has some limitations. 1) it is limited to layer-wise optimization. nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning cnns. when you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. it is not clear to me how the loss/gain balances each other. 2) this paper focues on improving the optimization of cnn objective. however, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). although the sgd with backprop in standard cnn learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).----------------the experiment is a bit weak.--------1) only cifar10 is used. this is a very small dataset by today's standard, while cnns are typically used in large-scale datasets, such as imagenet. it is not clear whether the conclusions of this paper still hold when applied on imagenet.----------------2) this paper only compares with a crippled variant of sgd (without batch normalization, dropout, etc). although this paper mentions that the reason is that it wants to focus on optimization. but i mentioned earlier, sgd is not designed to purely obtain the best solution that optimizes the objective, the goal of sgd is to reasonably optimize the objective, while preventing overfitting. so the comparison to sgd purely in terms of the optimization is that meaningful in the first place.","the authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. the proposed approach trains piecewise linear convnets layer by layer, reduces the sub-problem into latent structured svm. reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. while the reviewers haven't updated explicitly their reviews, i believe the changes made should have been sufficient for them to do so. thus, i recommend this paper be accepted.","hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next is according to my opinion superficial.----------------4.","although the sgd with backprop in standard cnn learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).----------------the experiment is a bit weak.--------1) only cifar10 is used.",this paper proposes a new approaches for optimizing the objective of cnns.,this paper presents a novel layer-wise optimization approach for learning cnn with piecewise linear nonlinearities.,this paper presents a novel layer-wise optimization approach for learning cnn with piecewise linear nonlinearities.,"although the sgd with backprop in standard cnn learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).----------------the experiment is a bit weak.--------1) only cifar10 is used.","traditional, cnns and structural svms are almost two separate research communties.","although the sgd with backprop in standard cnn learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).----------------the experiment is a bit weak.--------1) only cifar10 is used.",0.1240310077519379,0.0,0.1240310077519379,0.1240310077519379,0.2275449101796407,0.0242424242424242,0.1197604790419161,0.1197604790419161,0.1666666666666666,0.0425531914893617,0.0833333333333333,0.0833333333333333,0.3,0.2448979591836734,0.26,0.26,0.3,0.2448979591836734,0.26,0.26,0.2275449101796407,0.0242424242424242,0.1197604790419161,0.1197604790419161,0.0421052631578947,0.021505376344086,0.0421052631578947,0.0421052631578947,0.2275449101796407,0.0242424242424242,0.1197604790419161,0.1197604790419161,12.205490112304688,16.749237060546875,16.749237060546875,12.205490112304688,6.26841402053833,13.25582790374756,12.205490112304688,10.589162826538086,0.6303202608465209,0.7556577336621959,0.6714863247791669,0.22573499820836274,0.34965175645322144,0.853756270185968,0.8877801762785299,0.906817848384043,0.9052447412029523,0.9765627179015045,0.9698336273640625,0.894467959863681,0.9765627179015045,0.9698336273640625,0.8944681061080472,0.22573499820836274,0.34965175645322144,0.853756270185968,0.145512353028597,0.19728288453898477,0.6593690125021577,0.22573499820836274,0.34965175645322144,0.8537563385090472
44,https://openreview.net/forum?id=ByGUFsAqYm,"the authors conjecture that convolutional downsampling is an underlying mechanism behind sample memorization in over-parameterized convolutional autoencoders. they claim that this effect leads the system to converge to a low-rank solution in contrast to the theoretically possible identity mapping. they support their claim with numerical experiments on linear and non-linear convolution autoencoders.----------------strengths:--------- the authors develop their idea in close connection to commonly used architectures.----------------weaknesses:--------- the main statements concern the architecture; however, the experiments do not account for the many confounding factors such as initialization or the chosen optimizer. the paper itself states on page 4 that the results depend on the initialization and cite gunasekar et al. in the conclusion for an analogy, which, however, explores the implicit regularization effect of a gradient descent optimizer.--------- there is no clear and proved statement despite the suggestive mathematical nature of the writing (conjecture, proposition). the claimed 'proof' of the proposition is conducted via experiment. in light of the above mentioned confounding factors, the current phrasing of the proposition will not allow a formal proof as it is unclear what the system 'linear network ds' even is.--------- the boundary between conjectured and inferred statements is very vague. for example, the meaning of 'prefers to learn a point map' is unclear.----------------overall, the exposition is insufficient in supporting the conjectured effect. the methodology could be strengthened in two directions:--------1) experimentally: designing numerical experiments that exclude confounding factors and surface the conjectured effect--------2) theoretically: abstracting the idea into a clear mathematical statement that can be proved----------------i encourage the authors to extend their work for submission to a future venue. summary:---------the authors investigates downsampling as one method by which autoencoding cnns may memorize data. the theoretical motivation provided concentrates on linear cnns. they show that downsampling linear cnns tent to learn a point-map of the training data, even though (under certain initializations) they are capable of learning identity maps. however, non-downsampling linear cnns learn identity maps. given enough data however, the authors claim that the downsampling cnn will learn the identity map.----------------strengths:---------+ authors present a good exploration of how linear cnns memorize data when they do downsampling. --------+ a theoretical prediction of the amount of training data needed to counteract data memorization for downsampling linear cnns is provided, ""our conjecture also implies that when training a linear downsampling cnn on images of size 3  224  224, which corresponds to the input image size for vgg and resnet (he et al. (2016), simonyan & zisserman (2015)), the number of linearly independent training examples needs to be at least 3  224  224 = 153, 228 before the network can learn the identity function."" ----------------weaknesses:---------+ not enough theoretical proof is provided to support the hypothesis. which would be fine but some key experiments are missing to make the paper empirically rigorous.--------++ would be good to see experiments that illustrate the predication that a certain amount of data would allow for learning identity maps, both for linear and non-linear cnns.--------++ in the non-linear cnn setting, i'd like to see the same early-stopping experiment done for linear cnns whose results are in fig. 3. i don't see any obvious theoretical reason why that result form fig. 3 must extend to the non-linear setting. --------+ initializations are pointed to as effecting the type of function the network learns. the authors give an example of a hand-designed initialization that allows a downsampling linear cnn to learn the identity map but they don't explain how they arrived at this initialization, or its properties that make it a good initialization. in general however, i think it's alright to assign exploration of effect of initialization to future work, since it seems like a non-trivial task.--------+ it is mentioned that ""the results are not observed for linear networks when using kaiming initialization,"" which i read to mean the downsampling linear cnns with kaiming initialization learn the identity map, not point-map. if this is true, it seems like a vital point and should be included in discussions of future work.----------------recommendation: i think this could be a better short paper. there are some interesting contributions, but maybe not enough for a full length paper. for a full length paper, some further exploration of _why_ downsampling leads to (if indeed there is a causality) data memorization is needed.----------------minor stuff:---------citation ""gunasekar et al."" is missing year (conclusions section) the paper tries to provide an explanation for a memorization phenomenon observed in convolutional autoencoders. in the case of memorization, the autoencoder always outputs the same fixed image for any input image, even when the input image is random noise. the authors provide an empirical analysis that connects such a phenomenon to strides in convolutional layers of the autoencoder. then, a possible theoretical explanation is given in the form of conjecture with some empirical evidence.----------------the paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.----------------- please elaborate on how different initializations influence memorization effect. currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.--------- having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to appendix.--------- the comment after the proposition section is not very clear. what does it mean that the proposition does not imply that a_x must obtain rank which is given in the conjecture? please explain how is proposition providing any theoretical support for conjecture then.----------------- minor comments--------1. 2000 iteration -> 2000 iterations--------2. the text says network nd trained on frog image while the following next sentence says that the network reconstructed the digit 3. please clarify.--------3. network nd reconstructed the digit 3 with a training loss of 10^-4 and network nd with loss 10^-2. it seems that one of these should be network d.--------4. (with downsamling) -> (with downsampling)","this paper studies the question of memorization within overparametrised neural networks. specifically, the authors conjecture that memorization is linked to the downsampling operators present in many convolutional autoencoders. all reviewers agreed that this is an interesting question that deserves further analysis. however, they also agreed that in its current form, the paper lacks mathematical and experimental rigor. in particular, the paper does not follow the basic mathematical standards of proving any stated proposition/theorem, instead mixing empirical with mathematical proofs. the ac fully agrees with the points raised by reviewers, and therefore recommends rejection at this point, encouraging the authors to address these important points before resubmitting their work.",the methodology could be strengthened in two directions:--------1) experimentally: designing numerical experiments that exclude confounding factors and surface the conjectured effect--------2) theoretically: abstracting the idea into a clear mathematical statement that can be proved----------------i encourage the authors to extend their work for submission to a future venue.,"--------+ a theoretical prediction of the amount of training data needed to counteract data memorization for downsampling linear cnns is provided, ""our conjecture also implies that when training a linear downsampling cnn on images of size 3  224  224, which corresponds to the input image size for vgg and resnet (he et al. (2016), simonyan & zisserman (2015)), the number of linearly independent training examples needs to be at least 3  224  224 = 153, 228 before the network can learn the identity function.""","given enough data however, the authors claim that the downsampling cnn will learn the identity map.----------------strengths:---------+ authors present a good exploration of how linear cnns memorize data when they do downsampling.",the authors conjecture that convolutional downsampling is an underlying mechanism behind sample memorization in over-parameterized convolutional autoencoders.,"given enough data however, the authors claim that the downsampling cnn will learn the identity map.----------------strengths:---------+ authors present a good exploration of how linear cnns memorize data when they do downsampling.","given enough data however, the authors claim that the downsampling cnn will learn the identity map.----------------strengths:---------+ authors present a good exploration of how linear cnns memorize data when they do downsampling.","--------+ a theoretical prediction of the amount of training data needed to counteract data memorization for downsampling linear cnns is provided, ""our conjecture also implies that when training a linear downsampling cnn on images of size 3  224  224, which corresponds to the input image size for vgg and resnet (he et al. (2016), simonyan & zisserman (2015)), the number of linearly independent training examples needs to be at least 3  224  224 = 153, 228 before the network can learn the identity function.""","--------+ a theoretical prediction of the amount of training data needed to counteract data memorization for downsampling linear cnns is provided, ""our conjecture also implies that when training a linear downsampling cnn on images of size 3  224  224, which corresponds to the input image size for vgg and resnet (he et al. (2016), simonyan & zisserman (2015)), the number of linearly independent training examples needs to be at least 3  224  224 = 153, 228 before the network can learn the identity function.""",0.2264150943396226,0.0509554140127388,0.1761006289308176,0.1761006289308176,0.2,0.0106382978723404,0.1368421052631579,0.1368421052631579,0.1702127659574468,0.0287769784172661,0.0992907801418439,0.0992907801418439,0.1732283464566929,0.08,0.1259842519685039,0.1259842519685039,0.1702127659574468,0.0287769784172661,0.0992907801418439,0.0992907801418439,0.1702127659574468,0.0287769784172661,0.0992907801418439,0.0992907801418439,0.2,0.0106382978723404,0.1368421052631579,0.1368421052631579,0.2,0.0106382978723404,0.1368421052631579,0.1368421052631579,12.584211349487305,12.584213256835938,14.376503944396973,8.705724716186523,7.731361865997314,12.584211349487305,8.705724716186523,8.705724716186523,0.9788410038914768,0.9787894810343281,0.9360109762210389,0.9825754024236545,0.8889268301245121,0.10336073605683288,0.9803695877474045,0.976757667991131,0.7285688182179697,0.9762319567629083,0.9741036137492479,0.9403327258596005,0.9803695877474045,0.976757667991131,0.7285695496790575,0.9803695877474045,0.976757667991131,0.7285691324312615,0.9825754024236545,0.8889268301245121,0.10336091557271794,0.9825754024236545,0.8889268301245121,0.10336094978703016
45,https://openreview.net/forum?id=Byg3y3C9Km,"overall this is an important piece of work that deserves publication at iclr. i recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.----------------# quality----------------the hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by langevin dynamics versus simply using a neural network to regress shape from sequence. they construct a flexible deep energy model where the sequence and structure dependent parts are separated in such a way that fast rollouts are possible. they also adapt the learning algorithm to ensure that long rollouts can be carried out and present a clever trick for integrating internal coordinates efficiently on a gpu. ----------------the only criticism in terms of quality of work is that it somewhat lacks putting in context with results from the larger community, for example how well does the model compare in terms of speed and accuracy with co-evolutionary approaches? i realise it will not be possible to give a completely fair like to like comparison, but it will help readers put the results in context if they understood, for example, what the average tm score for casp12 results was, as summarized in this paper for example: https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25423. similarly, it would be useful to compare the baseline - at least qualitatively - with the results from alquraishi et. al. whose model seems very similar in spirit.----------------# clarity----------------i think in terms of clarity, the paper could be improved a little to take into account the audience of iclr. in particular:----------------* it may be useful to add a sentence of how profiles have been found to improve secondary structure prediction greatly. currently the text makes it sound as though they constitute a sort of 'data augmentation', whereas in my opinion they add information compared to the sequence alone. in fact a brief explanation of the importance of homology might help the reader understand the relevance of the hierarchical approach taken to splitting the training set.----------------* fig. 2 caption. could add some information to explain what panel b is showing. i think this would go a long way to explain why both cartesian and internal coordinates are important.----------------* fig. 4 second panel. the x axis should be labeled fraction or be numbered 0-100.----------------* fig 4. caption. the figure does not have a caption explaining what the graphs are showing. this would be a good place to explain that the colors refer to test sets that overlap with the training set in the full cath code (black), overlap only in the cat code (orange) etc. i admit i had found the explanation of the test/train/validation split rather confusing. it is not clear what the validation set is used for, i.e. which hyper-parameters have been tuned on it etc.----------------* the nature of the loss. the appendix does a good job in describing each term in the loss function, but does not explain how the empirical loss function and the log-likelihood terms are mixed together. ----------------# originality----------------the work is original and is references the relevant literature. this paper presents an end-to-end differentiable model (nemo) for protein structure prediction. i found this paper very interesting and the idea of training the network through the sampling procedure promising. the authors present the challenges and techniques (damping, lyapunov regularization etc) in detail.----------------the paper is clearly written, however the description of the method can be confusing. this stems in part from the many components of the network as well as the fact that the protein is represented using various coordinate systems and features, so that it is not easy to follow which applies at each stage. fig. 6 in the appendix helps, however it would be better to have a (perhaps more concise) overview in the main text.----------------in the evaluation, the nemo method is compared to a baseline approach using rnns. while nemo trained on profile features performs best, the baseline is trained on sequences only. however, it outperforms the nemo model trained on sequence-only in every category. therefore, it would be interesting to see whether nemo outperforms a baseline trained on profile features. otherwise, i am not certain whether i can follow the conclusion that ""nemo generalizes more effectively"". beyond that, it would be interesting to see some generated atomic substructures from the imputation network, in particular an analysis of how diverse the generated atom positions are and whether they depend on the local environment.----------------overall, i appreciate the general idea and find the proposed approach very interesting. the contribution could have been stronger with a more detailed evaluation and better presentation. post-rebuttal revision: the authors have adressed my concerns sufficiently. the paper still has issues with presentation, and weak comparisons to earlier methods. however, the field is currently rapidly developing, and comparing to earlier works is often difficult. i believe the langevin-based prediction is a significant and clever contribution. i'm raising my score to 6.--------------------------------------the paper proposes an end-to-end neural architecture for learning protein structures from sequences. the problem is highly important. the method proposes to use a langevin simulator to fold the protein in silico from some initial state, proposes numerous tricks for the optimisation, and proposes neural networks to extract information from both the sequence and the fold state (energy function). the system works on internal coordinates, which are conditioned and integrated on the fly. the method seems to perform very well, improving upon their baseline model considerably.----------------in spite of the paper being an outstanding work, i have two criticisms about the accessibility and impact of the paper on the broader iclr audience. in its current form and complexity, the paper feels accessible mostly to a narrow audience.----------------first, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc. the amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. the 10+ page appendix is critical for understanding (for instance, the appendix reveals that msa is used to generate more data), and even with it the method is difficult to grasp as a whole. this paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture. i also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way. ----------------second, the introduction lists numerous competing methods both on the protein modelling side and on the mcmc vs optimisation side. the paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. right now its unclear what is state-of-the-art in general. no bigger context of protein folding is given either, for instance, how well the method fares against purely alignment based approaches, or against purely physics-based simulators. finally, the experimental section poorly describes how all the pieces of the system affect the final predictions. the discussion on the exploding gradients and dampening is excellent however. the only baseline is one with the simulator replaced by an rnn. there does not seem to be any running time analyses. as such, it is hard to interpret the current system, and it feels like a black box.","this paper presents a differentiable simulator for protein structure prediction that can be trained end-to-end. it makes several contributions to this research area. particularly training a differentiable sampling simulator could be of interest to a wider community. the main criticism comes from the clarity for the machine learning community and empirical comparison with the state-of-the-art methods. the authors' feedback addressed a few confusions in the description, and i recommend the authors to further polish the text for better readability. r4 argues that a good comparison with the state-of-the-art method in this field would be difficult and the comparison with an rnn baseline is rigorously carried out. after discussion, all reviewers agree that this paper deserves a publication at iclr.",they also adapt the learning algorithm to ensure that long rollouts can be carried out and present a clever trick for integrating internal coordinates efficiently on a gpu.,i recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.----------------# quality----------------the hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by langevin dynamics versus simply using a neural network to regress shape from sequence.,this paper presents an end-to-end differentiable model (nemo) for protein structure prediction.,overall this is an important piece of work that deserves publication at iclr.,"in its current form and complexity, the paper feels accessible mostly to a narrow audience.----------------first, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc.",i recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.----------------# quality----------------the hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by langevin dynamics versus simply using a neural network to regress shape from sequence.,this paper presents an end-to-end differentiable model (nemo) for protein structure prediction.,"the method seems to perform very well, improving upon their baseline model considerably.----------------in spite of the paper being an outstanding work, i have two criticisms about the accessibility and impact of the paper on the broader iclr audience.",0.1677419354838709,0.0392156862745098,0.1032258064516129,0.1032258064516129,0.3414634146341463,0.1083743842364532,0.1951219512195121,0.1951219512195121,0.1702127659574468,0.1007194244604316,0.1134751773049645,0.1134751773049645,0.1285714285714286,0.0289855072463768,0.0999999999999999,0.0999999999999999,0.1676646706586826,0.0242424242424242,0.1197604790419161,0.1197604790419161,0.3414634146341463,0.1083743842364532,0.1951219512195121,0.1951219512195121,0.1702127659574468,0.1007194244604316,0.1134751773049645,0.1134751773049645,0.2289156626506024,0.024390243902439,0.1204819277108433,0.1204819277108433,15.276092529296877,7.081125259399414,9.229493141174316,15.276092529296877,5.943155288696289,7.702261924743652,10.306638717651367,7.702261924743652,0.9576775718793287,0.9666256290971049,0.8836128055952951,0.9830054355960505,0.9814474363102326,0.9210829466907291,0.9572484991495334,0.9660311482754669,0.9227814730372226,0.9690979042450465,0.9682110426560675,0.9282082584553271,0.8384621836939807,0.8443085568643627,0.9253804824615894,0.9830054355960505,0.9814474363102326,0.9210829466907291,0.9572484991495334,0.9660311482754669,0.922781259844431,0.5187205736118121,0.616761453172883,0.8517990931530727
46,https://openreview.net/forum?id=Byiy-Pqlx,"the neural turing machine and related external memory models have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. in particular, the ntm, dnc and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.----------------the ntm, which is the most relevant to this work, uses a differentiable version of a turing machine tape. the controller outputs a kernel which softly shifts the head, allowing the machine to read and write sequences. since this soft shift typically smears the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.----------------the premise of this work is to notice that while the ntm emulates a differentiable version of a turing tape, there is no particular reason that one is constrained to follow the topology of a turing tape. instead they propose memory stored at a set of points on a manifold and shift actions which form a lie group. in this way, memory points can have have different relationships to one another, rather than being constrained to z.----------------this is mathematically elegant and here they empirically test models with the shift group r^2 acting on r^2 and the rotation group acting on a sphere.----------------overall, the paper is well communicated and a novel idea.----------------the primary limitation of this paper is its limited impact. while this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. instead, it is likely to make an already complex and slow model such as the ntm even slower. in general, it would seem memory topology is problem specific and should therefore be learned rather than specified.----------------the baseline used for comparison is a very simple model, which does not even having the sharpening (the ntm approach to solving the problem of head distributions becoming smeared). there is also no comparison with the successor to the ntm, the dnc, which provides a more general approach to linking memories based on prior memory accesses.----------------minor issues:--------footnote on page 3 is misleading regarding the dnc. while the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.--------figures on page 8 are difficult to follow. *** paper summary ***----------------this paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. it then proposes a framework in which any lie group as the addressing space. experiments on algorithmic tasks are reported.----------------*** review summary ***----------------this paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. its proposal provide a generic scheme to build addressing mechanisms. when comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. ----------------*** detailed review ***----------------the paper reads well, has appropriate relevance to related work. the unified presentation of memory augmented networks is clear and brings unity to the field. the proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. i do not appreciate that the growing memory is not mentioned as a drawback. it should be stressed and a discussion on the impact it has on efficiency/scalability is needed.","the paper presents a lie-(group) access neural turing machine (lantm) architecture, and demonstrates it's utility on several problems. pros: reviewers agree that this is an interesting and clearly-presented idea. overall, the paper is clearly written and presents original ideas. it is likely to inspire further work into more effective generalizations of ntms. cons: the true impact and capabilities of these architectures are not yet clear, although it is argued that the same can be said for ntms. the paper has been revised to address some ntm features (sharpening) that were not included in the original version. the purpose and precise definition of the invnorm have also been fixed.","when comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical.","in particular, the ntm, dnc and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.----------------the ntm, which is the most relevant to this work, uses a differentiable version of a turing machine tape.","in particular, the ntm, dnc and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.----------------the ntm, which is the most relevant to this work, uses a differentiable version of a turing machine tape.",the neural turing machine and related external memory models have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures.,*** paper summary ***----------------this paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation.,"since this soft shift typically smears the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.----------------the premise of this work is to notice that while the ntm emulates a differentiable version of a turing tape, there is no particular reason that one is constrained to follow the topology of a turing tape.",*** paper summary ***----------------this paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation.,"in particular, the ntm, dnc and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.----------------the ntm, which is the most relevant to this work, uses a differentiable version of a turing machine tape.",0.1021897810218978,0.0,0.0875912408759124,0.0875912408759124,0.2597402597402597,0.0131578947368421,0.1428571428571428,0.1428571428571428,0.2597402597402597,0.0131578947368421,0.1428571428571428,0.1428571428571428,0.1617647058823529,0.0298507462686567,0.1323529411764706,0.1323529411764706,0.1304347826086956,0.0,0.1014492753623188,0.1014492753623188,0.2906976744186046,0.0117647058823529,0.1395348837209302,0.1395348837209302,0.1304347826086956,0.0,0.1014492753623188,0.1014492753623188,0.2597402597402597,0.0131578947368421,0.1428571428571428,0.1428571428571428,12.485078811645508,8.617033004760742,13.49452781677246,16.36727523803711,6.613275051116943,16.367273330688477,16.36727523803711,8.617033958435059,0.9463649140476811,0.9514210965816204,0.9286250780838055,0.9734279214368505,0.9751943937417061,0.8360072512502911,0.9734279214368505,0.9751943937417061,0.8360075368249342,0.9724025875631938,0.9695143674852946,0.9143209276603967,0.9802551005675162,0.9769629290304276,0.9371152311505998,0.9532658664144426,0.963901328962773,0.9395250912465132,0.9802551005675162,0.9769629290304276,0.9371152102042446,0.9734279214368505,0.9751943937417061,0.8360074300697998
47,https://openreview.net/forum?id=BylTta4YvB,"the paper empirically evaluates different variants of wgan (with weight clipping, gradient penalty, c-transform, and the generalized c-transform under entropy relaxation). the experiments, mainly performed over three datasets (mnist, cifar10, celeba), are designed to evaluate how well the wasserstein distance is approximated, how much these approximations depend on batch sizes, and how good are the obtained generative models.----------i find the presentation of the different background work and models to be excellent, especially for someone who's not expert on wgans like me. however, they may want to check the writing, like the sentence just after (17) or the penalization term between (18) and (19).----------the contributions of the paper are experimental. the authors argue that they obtain a surprising observation, which is that ""the method best approximating the wasserstein distance does not produce the best looking images in the generative setting "". -----however, the goodness of the approximation is measured with (24), which the authors called ""subjective error"". i think the authors may want to comment more on this measure, which seems to favor the different transforms.-----also, the quality of the generative models seems to strongly depend on the architectures used in wgan. the authors' conclusions are based on dcgan. however, the results obtained with simple mlp and presented in the appendix have not the same clear distinction as with dcgan.----------overall, although i liked the presentation very much, i feel the experimental results may be a bit too light for a publication in a venue such as iclr. [summary]-----this paper provides an empirical evaluation of commonly used discriminator training strategies in estimating the wasserstein distance between distributions. the paper finds that methods motivated from optimal transport theory, e.g. c-transform and (c,\eps)-transform, perform better in evaluating the wasserstein distance than methods commonly used in wgan practice such as weight clipping and gradient penalty. however, when deployed in wgans as the discriminator training strategy, these methods do not generate images as high-quality as the gradient penalty method. ----------[pros]-----the question considered in this paper, i.e. how well does various discriminator strategies (as proxies of the infeasible all 1-lipschitz discriminator) perform for *evaluating* the wasserstein distributions, is important for strengthening our understanding of generative models. the main result that methods that are better at computing w may not be better at generating images is interesting, and agrees with the theoretical insight (e.g. arora et al. 2017) that *non-parametric* minimization of w may not be a good explanation for the generative power of wgans.----------[cons]-----i have concerns about the specific setting of mini-batch distance considered in this paper, in that whether it is really a sensible task (and can say anything about computing w) given the curse of dimensionality of estimating w from samples. the paper did not really discuss this issue, and from my own thoughts i dont think the task avoids this issue. ----------from my understanding, the approximation experiment does the following:-----(1) set (\mu, \nu) to be a random split of a dataset and consider them to be the populations. lets let f_\star denote the (ground truth) optimal discriminator between (\mu, \nu).-----(2) train discriminators (f_wc, f_gp, f_c, f_ceps) (using the different algorithms) from training batches from (\mu, \nu).-----(3) evaluate <f, \mu_l - \nu_l> where (\mu_l, \nu_l) are fresh test batches from (\mu, \nu) and f is one of the above trained discriminators.-----in comparison, the ground truth computes w(\mu_l, \nu_l) = <f_l, \mu_l, \nu_l> from the pot package (though not necessarily through explicitly computing f_l.)----------the issue with this is that we expect the method to perform well if f ~= f_l, which can be achievable if all the f_ls are similar (and hopefully theyre all approximately equal to f_\star.) however i dont think this is true -- as (\mu_l, \nu_l) are samples, and because of the curse of dimensionality, we should expect the f_ls to be quite different from each other. (otherwise if theyre really just ~= f_\star, then we can use standard concentration to show the w(\mu_l, \nu_l) ~= w(\mu, \nu), which we know is not true from curse of dim.)----------given this, i dont think the task of comparing <f, \mu_l, \nu_l> with the pot results really says anything about their power in computing w. i would be glad though to hear back from the authors to see if my understanding is accurate, and adjust my evaluation from there. this paper has studied the efficiency and stability of computing the wasserstein metric through its dual formulation under weight clipping, gradient penalty, c-transform and (c- )-transform. the results show that (c- )-transform and c-transform give more estimation of the wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.----------the paper is well written and the experiment section is extensive. however, it is more like an extended experiment report to me which is very valuable but lacks sufficient technical novelty expected at iclr.----------another comment is that when the authors mentioned ""... are compared to ground truth values d_ground computed by pot"", there needs more explanations on what that library actually does to compute the wasserstein distance to make the paper self-contained, e.g. what exact algorithms it uses as there are also dozens of different algorithms implemented in that library.----------in section 3.1, the authors state that ""it tends to converge within couple of iterations of the symmetric sinkhorn-knopp algorithm. for efficiency, we approximate these terms with one sinkhorn-knopp iteration"". what is the extent of sacrifice in accuracy due to this approximation? the authors should provide more evidences to justify the approximation.","there is insufficient support to recommend accepting this paper. generally the reviewers found the technical contribution to be insufficient, and were not sufficiently convinced by the experimental evaluation. the feedback provided should help the authors improve their paper.","this paper has studied the efficiency and stability of computing the wasserstein metric through its dual formulation under weight clipping, gradient penalty, c-transform and (c- )-transform.","the results show that (c- )-transform and c-transform give more estimation of the wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.----------the paper is well written and the experiment section is extensive.","the experiments, mainly performed over three datasets (mnist, cifar10, celeba), are designed to evaluate how well the wasserstein distance is approximated, how much these approximations depend on batch sizes, and how good are the obtained generative models.----------i find the presentation of the different background work and models to be excellent, especially for someone who's not expert on wgans like me.","the paper empirically evaluates different variants of wgan (with weight clipping, gradient penalty, c-transform, and the generalized c-transform under entropy relaxation).","the results show that (c- )-transform and c-transform give more estimation of the wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.----------the paper is well written and the experiment section is extensive.","the results show that (c- )-transform and c-transform give more estimation of the wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.----------the paper is well written and the experiment section is extensive.","however, they may want to check the writing, like the sentence just after (17) or the penalization term between (18) and (19).----------the contributions of the paper are experimental.","the results show that (c- )-transform and c-transform give more estimation of the wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.----------the paper is well written and the experiment section is extensive.",0.1538461538461538,0.0317460317460317,0.1538461538461538,0.1538461538461538,0.1999999999999999,0.0,0.1555555555555555,0.1555555555555555,0.26,0.0204081632653061,0.14,0.14,0.1967213114754098,0.0,0.0983606557377049,0.0983606557377049,0.1999999999999999,0.0,0.1555555555555555,0.1555555555555555,0.1999999999999999,0.0,0.1555555555555555,0.1555555555555555,0.2985074626865672,0.0,0.208955223880597,0.208955223880597,0.1999999999999999,0.0,0.1555555555555555,0.1555555555555555,8.647331237792969,8.647331237792969,12.860695838928224,8.647331237792969,7.0782012939453125,10.793224334716797,8.647331237792969,4.503436088562012,0.29185868747092975,0.35124325892160746,0.8936212247592388,0.8591047217821246,0.8704278574413425,0.9098717387479384,0.9792426672408601,0.9741269493453876,0.9252081607152917,0.9749967145102214,0.9739521872645411,0.8866890988977426,0.8591047217821246,0.8704278574413425,0.9098714068238904,0.8591047217821246,0.8704278574413425,0.9098717387479384,0.9526504681212641,0.945781170595087,0.8368598885270248,0.8591047217821246,0.8704278574413425,0.9098714068238904
48,https://openreview.net/forum?id=BylaUTNtPS,"this paper draws inspiration from physical world and considers an independent mechansim among recurrent modules. the authors apply the proposed rim to several relatively simple tasks and show some advantages.----------in general, i like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck. it is essentially to combine some environment prior into the model design. it makes senses to me that rims will work better in environments that objects and background are nearly independent and only interact with each other when collision happens. rims share similar to spirits with capsule networks, and its recurrent cells serve somewhat similar role to capsules. such independent mechanism, selective activation and sparse communication is very inspiring and is indeed a potentially very useful way of modeling the physical world.----------for the model itself, i appreciate its simplicity, but i also have some concerns.----------1) for the selective activation of rims, the number of activated rims is a hyperparameter and needs to be pre-defined. according to your experiments, i believe you need tune this hyperparameter a little bit in order to obtain the best performance. first of all, the design of a fixed number of activated rims does not seem to be reasonable and is also highly dependent on your task. i believe the framework will be more interesting if the model can determine this number automatically.----------2) i find it quite interesting that the top-down attention in selective rim activation is corresponding to the states of these recurrent cells. i am wondering what if you do not select these top k activation and directly train it using the entire distribution of the soft attention output?----------for the experiments, i think they all serve the purpose of showing the advanatges of rims quite well, except that thery are relatively easy task. however, it is still interesting to see that rims obtain significant gains over some baselines. some of the details can be made more clear, such as loss function and evaluation metrics in every task. it is sometimes difficult to find what loss function you are using. i suggest the authors make the experiments more self-contained in the main paper, such that authors do not need frequently scroll down to the appendix and check the details. this paper proposes a neural network architecture consisting of multiple independent recurrent modules that interact sparingly. these independent modules are not all used simultaneously, a subset of them is active at each time step. this subset of active modules is chosen through an attention mechanism. the idea behind this architecture is that it would allow the different modules to specialize in different mechanisms and that would allow compositionality. the empirical results suggest that the proposed approach is able to generalize better than traditional architectures (which all have the implicit assumption that all processes interact).----------this paper is well-written and it provides a very thorough empirical analysis of the proposed idea. because it is not in my area of expertise im not confident that i can assess its novelty or its relationship to other existing approaches.----------in terms of presentation, i recommend the authors to enlarge some of the figures in the paper (e.g., i cant read the small box in figure 1) and to not use citations as nouns (e.g., the mechanisms of this attention mechanism follow (vaswani et al., 2017; santoro et al., 2018), with the ). i would also like to point out that although fairly different in how they tackle the problem, the work of arjovsky et al. (2019) seems to be related to this one.----------three questions i believe were not answered in the paper are:----------1) how is the performance related to the total number of subsystems (and the number of *active* ones). i can only see results related to that in table 1, but the variation in the number of modules is pretty small (4-6). the results also dont give any indication whether we want to have more modules active at each time step, if theres a sweet spot, etc. it is said that the method seems to be robust to this choice but this claim is made because it performs similarly for the values 5 and 6 if i recall correctly.----------2) is there any incentive in this architecture for a module to not simply give up? i mean, the modules are not necessarily incentivized to be used as often as possible, so could it be the case that a module learns to set its weights to zero?----------3) would it make sense to present baseline results for an architecture that uses attention? it seems to me that lstm was often the baseline of choice but rims have two important components: multiple lstms and an attention mechanism. could the attention mechanism be explaining some of the results we are seeing?----------finally, despite the very long appendix, i feel there are important details missing with respect to the empirical setup, at least in the atari experiments which im more familiar with. was stochasticity used, that is, sticky actions (machado et al., 2018)? moreover, for how long was ppo (and rims-ppo) trained in terms of number of frames? finally, id recommend the authors to include a table with the actual average (and standard deviation) performance in each atari games. it is really hard to know how well a method is doing by just squinting at learning curves. it is hard to know if the results are significant without a notion of variance.---------------references:----------martn arjovsky, lon bottou, ishaan gulrajani, david lopez-paz: invariant risk minimization. corr abs/1907.02893 (2019)----------marlos c. machado, marc g. bellemare, erik talvitie, joel veness, matthew j. hausknecht, michael bowling: revisiting the arcade learning environment: evaluation protocols and open problems for general agents. j. artif. intell. res. 61: 523-562 (2018)------------------------------------>>> update after rebuttal: i stand by my score after the rebuttal. ----------unfortunately i'm not an expert in this area and i don't feel confident in having a very strong opinion about this paper, willing to fight for its acceptance. i also agree with concerns raised by other reviewers. as i stated in the discussion with the authors, the clarifications and additional experiment does improve the paper a bit. this work seems to propose an alternative to general rnn so that the dynamics of sequential data can be better captured. the work is based on a hypothesis that a causal process can be modeled by ""independent"" modules and sparse interactions.----- -----the paper is written in very fluent english, but the style is less technical. the impression is that the philosophical arguments and the machine learning realization have a gap in between. there is no much rigorous mathematical definition or derivation to back up the entire development. the mathematical symbols are a bit loosely defined. for example, it is unclear to the reviewer if h is a scalar or a vector.----------the rim idea seems to be derived from some ideas from the ``'causality community' . but the authors did not elaborate how significant will this structure change the state of art. in particular, reading the abstract or the introduction does not shed much light on what are the challenges of now the ml community is facing, and how this proposed rim idea is going to help. this may have been obvious to the authors, but spelling them out may help the reviewer/readers to understand the contribution. the related work section helped a bit, but still unclear.----------the reviewer feels that the paper stands at a high level in general, but lacks concrete examples/applications for general readers to appreciate the significance. perhaps trying to re-organize this part could greatly help the readability.----------the mathematical descriptions in 2.2, 2.3 and 2.4 are very hard to follow. there is no cost function (for training) around, but there are discussions of 'gradient'. gradient of which function?","this paper has, at its core, a potential for constituting a valuable contribution. however, there was a shared belief among reviewers (that i also share) that the paper still has much room for improvement in terms of presentation and justification of the claims. i hope that the authors will be able to address the feedback they received to make this submission get where it should be.","corr abs/1907.02893 (2019)----------marlos c. machado, marc g. bellemare, erik talvitie, joel veness, matthew j. hausknecht, michael bowling: revisiting the arcade learning environment: evaluation protocols and open problems for general agents.","such independent mechanism, selective activation and sparse communication is very inspiring and is indeed a potentially very useful way of modeling the physical world.----------for the model itself, i appreciate its simplicity, but i also have some concerns.----------1) for the selective activation of rims, the number of activated rims is a hyperparameter and needs to be pre-defined.","such independent mechanism, selective activation and sparse communication is very inspiring and is indeed a potentially very useful way of modeling the physical world.----------for the model itself, i appreciate its simplicity, but i also have some concerns.----------1) for the selective activation of rims, the number of activated rims is a hyperparameter and needs to be pre-defined.",this paper draws inspiration from physical world and considers an independent mechansim among recurrent modules.,"the authors apply the proposed rim to several relatively simple tasks and show some advantages.----------in general, i like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck.","the authors apply the proposed rim to several relatively simple tasks and show some advantages.----------in general, i like the idea of making recurrent cells operate with nearly independent transition dynamics and interact only sparingly through the attention bottleneck.",i also agree with concerns raised by other reviewers.,"such independent mechanism, selective activation and sparse communication is very inspiring and is indeed a potentially very useful way of modeling the physical world.----------for the model itself, i appreciate its simplicity, but i also have some concerns.----------1) for the selective activation of rims, the number of activated rims is a hyperparameter and needs to be pre-defined.",0.0606060606060606,0.0,0.0404040404040404,0.0404040404040404,0.288,0.032520325203252,0.16,0.16,0.288,0.032520325203252,0.16,0.16,0.0987654320987654,0.0253164556962025,0.074074074074074,0.074074074074074,0.2095238095238095,0.0194174757281553,0.0952380952380952,0.0952380952380952,0.2095238095238095,0.0194174757281553,0.0952380952380952,0.0952380952380952,0.08,0.0273972602739726,0.0533333333333333,0.0533333333333333,0.288,0.032520325203252,0.16,0.16,13.84181308746338,13.84181308746338,11.583043098449709,10.68124771118164,4.01026725769043,10.68124771118164,10.68124771118164,1.934329867362976,0.3417204522970203,0.33226891736073955,0.926676727281817,0.9682175370376279,0.9697409761378133,0.9250490898324656,0.9682175370376279,0.9697409761378133,0.9250490898324656,0.9587572084927235,0.9573612664966423,0.896605719886114,0.9583644748133641,0.9518014882547059,0.9021805386271303,0.9583644748133641,0.9518014882547059,0.9021806443953527,0.44880873859038917,0.6541928294589527,0.8458827352800239,0.9682175370376279,0.9697409761378133,0.9250492183606138
49,https://openreview.net/forum?id=Byxv2pEKPH,"this paper provides a new method to deal with the zero gradient problem of relu (all input values are smaller than zero) when bn is removed, called farkas layer. this farkas layer concatenates one positive value to guarantee that, at least one neuron is active in every layer to reduce the challenge for optimization. compared with the method without bn, farkas layer shows better results on cifar-10 and cifar-100.----------though, i still have several concerns:-----1. the proposed farkas layer is too simple and seems not work well. with bn, the farkasnet does not show significant improvements than the traditional resnet with bn on cifar-10, cifar-100 and imagenet.-----without bn, though farkasnet shows significant improvements than resnet. but farkasnet without bn cannot achieve comparable performance with farkasnet with bn. with deeper networks, the performance further goes down, which really downgrade the rating of this paper.-----in fixup, resnet w/o bn with mixup can achieve comparable performance with resnet with bn on imagenet. could farkasnet further improve the performance in the setting of resnet w/o bn with mixup and fixup init? this would much more improve the application value of the proposed farkasnet.----------2. for the results of cifar-10 and cifar-100, the error bar should be added to make the results more convincing. ----------3. figure 7, the training curves seem weird. why the training error goes up in some stages? the authors propose a new `normalization' approach called farkas layer for improving the training of neural networks. the main idea is to augment each layer with an extra hidden unit so that at least one hidden unit in each layer will be active. this is achieved by making the extra hidden unit dependent on the rest of the units in the layer, so that it will become active if the rest are inactive, and they name it after farkas' lemma in linear programming. this avoids the gradient becoming zero when all the units in a layer are dead. ----------the empirical results show that this normalization method is effective, and improves the training of deep resnets when no batch normalization is used. the accuracies on cifar10 and cifar100 are improved with the use of farkas layers. unfortunately it still cannot beat or replace batch normalization. when batch normalization is used, the benefit of using this farkas layer becomes marginal (tables 1 and 2). ----------i am also not completely satisfied with the authors' explanation on why farkas' layers work. the authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead' (or gradient becomes very small) when farkas' layer is not used. there could be other reasons why the layer helps, other than keeping some units in a layer active. ----------overall i think the idea is novel and interesting, but the improvement is not big enough to replace existing normalization methods that makes this paper slightly below the acceptance threshold in my opinion.","this paper proposes a new normalization scheme that attempts to prevent all units in a relu layer from being dead. the experimental results show that this normalization can effectively be used to train deep networks, though not as well as batch normalization. a significant issue is that the paper does not sufficiently establish that their explanation for the success of farkas layer is valid. for example, do networks usually have layers with only inactive units in practice?","this paper provides a new method to deal with the zero gradient problem of relu (all input values are smaller than zero) when bn is removed, called farkas layer.","the authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead' (or gradient becomes very small) when farkas' layer is not used.","compared with the method without bn, farkas layer shows better results on cifar-10 and cifar-100.----------though, i still have several concerns:-----1. the proposed farkas layer is too simple and seems not work well.","this paper provides a new method to deal with the zero gradient problem of relu (all input values are smaller than zero) when bn is removed, called farkas layer.","with bn, the farkasnet does not show significant improvements than the traditional resnet with bn on cifar-10, cifar-100 and imagenet.-----without bn, though farkasnet shows significant improvements than resnet.","the authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead' (or gradient becomes very small) when farkas' layer is not used.",could farkasnet further improve the performance in the setting of resnet w/o bn with mixup and fixup init?,"compared with the method without bn, farkas layer shows better results on cifar-10 and cifar-100.----------though, i still have several concerns:-----1. the proposed farkas layer is too simple and seems not work well.",0.2452830188679245,0.0576923076923076,0.1698113207547169,0.1698113207547169,0.3130434782608695,0.0530973451327433,0.1217391304347826,0.1217391304347826,0.247787610619469,0.036036036036036,0.1238938053097345,0.1238938053097345,0.2452830188679245,0.0576923076923076,0.1698113207547169,0.1698113207547169,0.1481481481481481,0.0188679245283018,0.0925925925925925,0.0925925925925925,0.3130434782608695,0.0530973451327433,0.1217391304347826,0.1217391304347826,0.1041666666666666,0.0,0.0833333333333333,0.0833333333333333,0.247787610619469,0.036036036036036,0.1238938053097345,0.1238938053097345,12.14872932434082,12.467073440551758,14.120113372802734,12.14872932434082,14.12011432647705,17.622514724731445,17.622514724731445,9.06460189819336,0.9756445424773285,0.9753731379202177,0.6981925472919056,0.920973732626087,0.9407912465428315,0.8245355840007358,0.9769447196935616,0.9736735697198347,0.9054973721249252,0.9756445424773285,0.9753731379202177,0.6981926456959461,0.9721927386804353,0.9710819424164905,0.00415679835026797,0.920973732626087,0.9407912465428315,0.8245352964304397,0.9729632654355501,0.9727573965571428,0.23674631828100812,0.9769447196935616,0.9736735697198347,0.9054975115756073
50,https://openreview.net/forum?id=ESG-DMKQKsD,"summary: well written paper with solid experiments on an extension of two prior works. this is likely of interest, good quality and i recommend to accept the paper at iclr. there are no extensions that i would propose to include in this version.-----quality: good quality, well written paper, easy to follow and sufficiently detailed. content is on-topic for iclr and of interest to a general audience.-----clarity: the paper is very well written, all details on the model, the training procedures, experiments are included. this paper is well polished and was easy to read and follow. the main assumptions are stated early on, problem definition is well stated, and the goal of the experiments are clearly stated before going into their discussion. the appendix provides additional results and details. nice paper to read, thanks for putting in the effort.-----originality: this paper is a combination of two tasks, combining two prior works to one system. the original part is the research question on whether the two tasks should share a common feature space and whether the results improve by the network model. so this is an interesting paper, i would assume there is quite an audience that is interested in this topic. without doubt the model is well constructed and trained, so there is also value in the construction.-----i have not seen the task of (few shot) recognition and visual reconstruction seen so far. this paper is a good extension of hologan and has some novel points.-----conditional version of hologan. this is a simple extension but useful and serves the purpose.-----combination of view synthesis and recognition. the flow of the architecture is well explained and leads to empirical improvements over each task in separation. more architecture choices would be possible, an evaluation of different backbones is included but not of other network combinations.-----experimental results are sufficient, on established dataset, there is no novelty in the application.-----significance: for both tasks (view synthesis and reconstruction) there are stronger models. the authors claim that other models could be combined in their setup, i agree, but the empirical results are below state-of-the-art. but this is definitely a step in the right direction and i believe there is an interested audience for this finding and it is likely that the construction inspires future work. there are some extensions that would go beyond the paper, such as more challenging data, images with more than one object, and combination with even more vision tasks.-----update and final recommendation. i still recommend acceptance of the submission. the paper is well written, results stand on its own and the numbers improve in the way described. in light of the missing comparisons to other works pointed out by the fellow reviewers i have lowered my score because i think better calibrates with the significance of the work. combination of downstream tasks is not novel but this combination i have not seen and so even bearing similarity with other approaches the paper still stands on its own. thanks to the reviewers and authors for their responses. the authors propose a model for joint few-shot recognition and novel-view synthesis. as shown in figure 2, the model consists of two modules: view synthesis module and recognition module. the authors claim that these modules can help each other to become better. the two modules are trained jointly on each task alternatively.-----pros:-----as both the justifications and experiments show, these two modules can help each other to be better.-----given the few-shot setting, the help from another task is meaningful.-----the writing is clear, and makes me easy to follow.-----cons:-----the novel-view synthesis quality is bad, so it can barely used for any other purposes besides assisting few-shot recognition module. in other words, the few-shot recognition result should be the only final output, and the novel-view synthesis output should only be considered as an intermediate result. so i think that the position of novel-view synthesis should not be lifted as high as the few-shot recognition, and this work should be dedicated to the few-shot recognition with a narrowed scope from ""joint few-shot recognition and novel-view synthesis"" to ""few-shot recognition"".-----based on the above point, then i doubt whether it is necessary to synthesize the pixels of other views, because the pixel quality is bad. synthesizing the intermediate feature maps should be more realistic in this case, because the pixels are mainly for human (reviewers), but the featuremaps are mainly for model (recognition module). human does not make the final few-shot recognition result better, but the recognition module does.-----the experiments only show the fine-grained recognition results, e.g., fine-grained recognition for birds or cars. given a single category (bird or car), the view synthesis quality is so bad, so i doubt if this module can be used for general recognition task involving many categories simultaneously. in that case, the novel-view synthesis module might make no sense at all. if that's the case, the scope of this work should be further narrowed from ""few-shot recognition"" to ""few-shot fine-grained recognition"".-----based on above points, i suggest rejecting this paper. this paper proposes a ""feedback-based bowtie network"" fbnet for joint generative synthesis via a gan-based framework (specifically hologan) and few-shot fine-grained recognition. the key idea of this work is to supervise both networks jointly via feedback mechanisms between the two, which helps to improve both tasks: image synthesis and few-shot recognition. the authors propose to use the synthesis network for synthesizing augmented images and additional losses computed by the image classification network along with conditional generation to improve the quality of the synthesized images.-----pros: the authors consider a new setting of coupling hologan with a downstream image recognition network and jointly training the two together. they show improvements both in image synthesis quality and image recognition by their approach. the experimental section is fairly thorough with many analyses and results presented.-----cons:-----the proposed work's idea of coupling the synthesis hologan network with a downstream visual learning task though a feedback mechanism between the two is not novel. this approach was previously introduced in the work mustikovela et al., ""self-supervised viewpoint learning from image collections"", cvpr 2020, which bears much resemblance in approach to the current work, but trains for a different downstream task of viewpoint estimation versus few-shot categorial classifications. mustikovela et al. also employ a conditional synthesis and various similar task-specific losses to jointly supervise both networks. the authors of this work should clearly cite this prior work and reframe the novelty of their approach in relation to it.-----the paper lacks comparisons to existing sota few-shot learning techniques that employ strategies for hallucinating additional data/features for the training classes in the few-shot training settings, e.g., wang et al., 2018 and zhang et al., 2018. is the proposed approach better than the previous few-shot learning approached that hallucinate data per class?-----what dataset was the high resolution recognition network trained on?-----post rebuttal: i thank the authors for their response. i am mostly satisfied with the authors' response to my (and other reviewers') concerns about properly citing prior works that jointly consider coupled image generation with downstream tasks and reframing the novelty of their work in juxtaposition to them. i would like to point out, however, that the authors' statement in the rebuttal ""(2) we achieve bi-directional feedback while this work only implements the feedback from viewpoint estimation task to the generative network."" is technically incorrect. the viewpoint estimation network in mustikovela et al. is directly trained with images generated by the synthesis network under various viewpoints and hence it also achieves bi-directional feedback much like this current work. the authors should clearly re-frame their novelty and make this correction in the final version if accepted.-----nevertheless, i do feel that this work adds to the body of literature on joint conditional synthesis coupled with downstream vision tasks by (a) showing improvements in the quality of image synthesis achieved by considering downstream tasks and (b) by showing improvements in few-shot learning versus prior methods where only features are hallucinated, and (c) considering other applications beyond viewpoint estimation. hence its contribution is above the acceptance threshold. i will maintain my previous rating. this paper presents a new dual-task of joint few-shot recognition and novel synthesis. the main idea of this paper is to learn a shared generative model across the dual-task to boost the performances of both tasks. to achieve this, bowtie networks are employed to jointly learn geometric and semantic representations with a feedback loop. the proposed method is evaluated on fine-grained recognition datasets.-----pros:-----this paper is well-written and easy-to-follow.-----the idea of jointly learning the generative model and the proposed feedback loop is well-motivated.-----the experiments on evaluating the view synthesis module is comprehensive and the results are promising.-----cons:-----i think the contribution of this paper on the technical side is somehow weak. the recognition model and view synthesis modules are both adapted from existing works. the major contribution of this paper should be the bowtie architecture with the feedback loop, but this has also been well-studied in the literature.-----during the evaluation of the recognition model, the proposed method should also be compared with other state-of-the-art methods on the same datasets other than only the variants of the proposed method. although the authors claim that this is not the major goal of this paper, i think this is vital since one major benefit claimed by the author is that jointly learning the generative model can improve both tasks.-----in table 3, it is interesting to see that there is a performance gap when using simple (resnet-18) and deeper (resnet-50) models. are there any intuitive explanations for this gap?-----overall, i like the main idea of the proposed method which learns a shared generative model across the dual-task. however, there are some concerns about the technical contributions and experiments. i will be happy to increase my rating if these concerns can be addressed in the rebuttal period.-----update after author feedback: i thank the authors for their reply. the authors have addressed all of my concerns. therefore, i increased my final rating.","this paper uses an extension of hologan for few shot recognition and novel view synthesis. all but one reviewer gave a final rating of accept. these reviewers were concerned that the submitted version of this work had not adequately placed this work in context with prior art. however, during the discussion these concerns seem to have been addressed sufficiently. the most negative reviewer was not impressed by the quality of the generated images; however these are relatively new methods and the few shot recognition aspect of this work is also part of the contribution. accounting for all reviews and the discussion the ac recommends accepting this work as a poster.",the experimental section is fairly thorough with many analyses and results presented.-----cons:-----the proposed work's idea of coupling the synthesis hologan network with a downstream visual learning task though a feedback mechanism between the two is not novel.,"the authors of this work should clearly cite this prior work and reframe the novelty of their approach in relation to it.-----the paper lacks comparisons to existing sota few-shot learning techniques that employ strategies for hallucinating additional data/features for the training classes in the few-shot training settings, e.g., wang et al., 2018 and zhang et al., 2018. is the proposed approach better than the previous few-shot learning approached that hallucinate data per class?-----what dataset was the high resolution recognition network trained on?-----post rebuttal: i thank the authors for their response.",the authors propose a model for joint few-shot recognition and novel-view synthesis.,summary: well written paper with solid experiments on an extension of two prior works.,"the authors should clearly re-frame their novelty and make this correction in the final version if accepted.-----nevertheless, i do feel that this work adds to the body of literature on joint conditional synthesis coupled with downstream vision tasks by (a) showing improvements in the quality of image synthesis achieved by considering downstream tasks and (b) by showing improvements in few-shot learning versus prior methods where only features are hallucinated, and (c) considering other applications beyond viewpoint estimation.",the proposed method is evaluated on fine-grained recognition datasets.-----pros:-----this paper is well-written and easy-to-follow.-----the idea of jointly learning the generative model and the proposed feedback loop is well-motivated.-----the experiments on evaluating the view synthesis module is comprehensive and the results are promising.-----cons:-----i think the contribution of this paper on the technical side is somehow weak.,but this is definitely a step in the right direction and i believe there is an interested audience for this finding and it is likely that the construction inspires future work.,"so i think that the position of novel-view synthesis should not be lifted as high as the few-shot recognition, and this work should be dedicated to the few-shot recognition with a narrowed scope from ""joint few-shot recognition and novel-view synthesis"" to ""few-shot recognition"".-----based on the above point, then i doubt whether it is necessary to synthesize the pixels of other views, because the pixel quality is bad.",0.1999999999999999,0.0,0.1066666666666666,0.1066666666666666,0.2980769230769231,0.0485436893203883,0.173076923076923,0.173076923076923,0.1612903225806451,0.0983606557377049,0.1290322580645161,0.1290322580645161,0.1129032258064516,0.0327868852459016,0.096774193548387,0.096774193548387,0.3052631578947368,0.0425531914893617,0.1789473684210526,0.1789473684210526,0.2954545454545454,0.0804597701149425,0.1818181818181818,0.1818181818181818,0.1985815602836879,0.014388489208633,0.1418439716312056,0.1418439716312056,0.3152173913043478,0.1208791208791208,0.1739130434782608,0.1739130434782608,7.9930830001831055,6.471397399902344,13.059926986694336,7.578473091125488,6.252903938293457,5.170223236083984,3.2474985122680664,3.5273170471191406,0.5026400011253832,0.5710325621101672,0.8827611150948151,0.671336035786443,0.6848331034985913,0.9151455781509268,0.974021209748001,0.9734977466622756,0.8891319034768052,0.9633428455645026,0.9652475258615827,0.8968700530442479,0.551600427997564,0.7805923253280892,0.914435384909938,0.30137891512913106,0.3039391288267076,0.8534363742989243,0.9339668073453986,0.9282074417568703,0.7730945470965785,0.87439654350401,0.888462321237787,0.9416213234357311
51,https://openreview.net/forum?id=ErrNJYcVRmS,"summary: the authors consider federated learning setting and how to defend the overall learning task against malicious clients and a semi-honest centralized server. though there are known ways to prevent attacks, they suffer from a large error in the estimator and also do not preserve privacy of updates since the server sees them in the clear in order to adjust for error. this paper proposes a sharding technique and use of the estimator method whose error does not depend on the number of dimensions as previous work.-----novelty:-----the sharding approach allows the proposed method to use masking-like techniques to avoid the server seeing values of individual clients. that is, the server only sees aggregates in the shard-----the paper proposes a different tradeoff in terms of the error of the estimate compared to mechanisms in related work. in particular, the error depends on the proportion of malicious clients.-----overall it is a very nicely written and presented paper. i would suggest that the authors expand evaluation section to compare performance of the methods besides accuracy. that is how many rounds each algorithm takes, total communication cost and computation time of each approach.-----also i was not clear if one needs to make assumptions on the knowledge of the proportion of malicious clients in order to carry out the algorithm (alg 2). would that be known or there would be a known upper bound?-----please state if there is an assumption on non collusion between the server and the clients. the paper proposes a simple protocol to allow for both robust mean estimation and secure aggregation by sharding users, applying secure aggregation between shards and then doing robust mean estimation on the means returned by each shard.-----pros:-----the proposed method is simple.-----experiments suggest the proposed method is more robust to various attacks than competitors.-----cons:-----generally the paper requires further editing as there are several places where the descriptions could be more clear (e.g. ""achieves optimal or sub-optimal performance"").-----in all theorems it should be noted whether this result is by the authors or whether it is from steinhardt (2018). any theorems which are quoted should be fully attributed, and any which are novel should be accompanied by formal proofs.-----it is quite unclear what value the discussion of creating iid shards brings. non-iid data between clients is a concern in federated learning, and it can cause issues such as diverging model parameters when each client takes multiple steps of gd locally. however this issue is not solved by sharding the users (as any averaging takes place after all local updates are made), nor is it clear if this solves any other issues. the authors should identify what value having these iid shards brings. additionally it is unclear that the lindeberg clt is required here, as the shards draw directly from the mixture distribution induced by the clients-----di-----distributions.-----the paper would benefit from a much stronger explanation of the value of the iid-ness of the shards and more exploration of the value added by sharding.","the paper considers federated learning in the presence of malicious clients and a semi-honest centralized server. the authors provide a novel secure aggregation technique (i.e. split the clients into shards, and securely aggregate each shards updates, and the estimating things based on the updates from different shards) to protect clients from the server. furthermore, an important property of the proposed protocol is that the estimation error is (provably) dimension-free against byzantine malicious clients. the paper is well-written.-----the reviewers had a number of concerns many of which were addressed during the rebuttal phase. there was also another round of discussion after the rebuttal phase. overall, the reviewers felt that there are still some issues that need to be resolved (see the updated reviews--the main issues are: (i) the assumption of non-collusion between the server and the clients, (ii) assumptions and analysis of the non-iid case, and (iii) comparing to attacks that are specifically targeted against the baselines). i believe that once these issues are addressed, the paper will provide an important contribution to the area of federated learning.","that is, the server only sees aggregates in the shard-----the paper proposes a different tradeoff in terms of the error of the estimate compared to mechanisms in related work.","the paper proposes a simple protocol to allow for both robust mean estimation and secure aggregation by sharding users, applying secure aggregation between shards and then doing robust mean estimation on the means returned by each shard.-----pros:-----the proposed method is simple.-----experiments suggest the proposed method is more robust to various attacks than competitors.-----cons:-----generally the paper requires further editing as there are several places where the descriptions could be more clear (e.g. ""achieves optimal or sub-optimal performance"").-----in all theorems it should be noted whether this result is by the authors or whether it is from steinhardt (2018).","non-iid data between clients is a concern in federated learning, and it can cause issues such as diverging model parameters when each client takes multiple steps of gd locally.",summary: the authors consider federated learning setting and how to defend the overall learning task against malicious clients and a semi-honest centralized server.,"the paper proposes a simple protocol to allow for both robust mean estimation and secure aggregation by sharding users, applying secure aggregation between shards and then doing robust mean estimation on the means returned by each shard.-----pros:-----the proposed method is simple.-----experiments suggest the proposed method is more robust to various attacks than competitors.-----cons:-----generally the paper requires further editing as there are several places where the descriptions could be more clear (e.g. ""achieves optimal or sub-optimal performance"").-----in all theorems it should be noted whether this result is by the authors or whether it is from steinhardt (2018).","the paper proposes a simple protocol to allow for both robust mean estimation and secure aggregation by sharding users, applying secure aggregation between shards and then doing robust mean estimation on the means returned by each shard.-----pros:-----the proposed method is simple.-----experiments suggest the proposed method is more robust to various attacks than competitors.-----cons:-----generally the paper requires further editing as there are several places where the descriptions could be more clear (e.g. ""achieves optimal or sub-optimal performance"").-----in all theorems it should be noted whether this result is by the authors or whether it is from steinhardt (2018).","though there are known ways to prevent attacks, they suffer from a large error in the estimator and also do not preserve privacy of updates since the server sees them in the clear in order to adjust for error.","the paper proposes a simple protocol to allow for both robust mean estimation and secure aggregation by sharding users, applying secure aggregation between shards and then doing robust mean estimation on the means returned by each shard.-----pros:-----the proposed method is simple.-----experiments suggest the proposed method is more robust to various attacks than competitors.-----cons:-----generally the paper requires further editing as there are several places where the descriptions could be more clear (e.g. ""achieves optimal or sub-optimal performance"").-----in all theorems it should be noted whether this result is by the authors or whether it is from steinhardt (2018).",0.2046511627906976,0.0657276995305164,0.1302325581395348,0.1302325581395348,0.2698961937716262,0.0766550522648083,0.1730103806228373,0.1730103806228373,0.1395348837209302,0.0187793427230046,0.0651162790697674,0.0651162790697674,0.1818181818181818,0.0966183574879227,0.124401913875598,0.124401913875598,0.2698961937716262,0.0766550522648083,0.1730103806228373,0.1730103806228373,0.2698961937716262,0.0766550522648083,0.1730103806228373,0.1730103806228373,0.1696428571428571,0.036036036036036,0.0982142857142857,0.0982142857142857,0.2698961937716262,0.0766550522648083,0.1730103806228373,0.1730103806228373,10.63124656677246,10.63124656677246,12.175500869750977,10.63124656677246,6.800427436828613,5.398859977722168,10.63124656677246,8.9785737991333,0.9819627179693715,0.9740584012536401,0.2974447735589625,0.9927063560503411,0.9784365003168909,0.49541783241376475,0.9400471956955172,0.9584126735808488,0.9340577584063099,0.9714628958144905,0.9720915213678541,0.9173342859136885,0.9927063560503411,0.9784365003168909,0.49541783241376475,0.9927063560503411,0.9784365003168909,0.49541783241376475,0.9438473998626121,0.9588765702548048,0.7772537431231845,0.9927063560503411,0.9784365003168909,0.49541783241376475
52,https://openreview.net/forum?id=F-mvpFpn_0q,"the paper proposes 2 new benchmarks for rapid task solving (rts), that evaluate rl agents on the ability to memorize past experiences and learn to plan to solve new tasks in different environments rapidly. the paper also proposes episodic planning networks (epn), an rl method that replaces a weighted sum and multi-layer peceptrons in memory networks with self-attention. the proposed epns proved to significantly outperform baseline methods with memory and an lstm method without memory. the paper formulates rts as an extension of meta-reinforcement learning framework, where in addition to optimizing over a distribution of tasks, the objective is to also optimize over a distribution over environments.-----i think the work proposed in the paper is an important step towards developing general agents that learn to adapt quickly. to that end, it would be beneficial for the authors to release their code to benchmark new and older methods.-----i have a few clarification questions.-----how is the reward defined for each task in the 2 benchmarks? is it 1 for reaching the goal and 0 otherwise? given that the 2 domains are defined in terms of relations between entities (connections between symbols in the memory&planning game and neighborhoods in the other) how would relational and symbolic rl methods perfom compared to the baselines and the proposed epn? i believe these would provide a fairer baseline performance compared to the lstm used. the authors propose a non-parametric memory based on the transformer/self-attention architecture to learn over tasks that require planning from previously experienced tasks. they describe this style of learning as a form of meta reinforcement learning where individual episodes are a collection of tasks in the same environment and individual environments are sampled from a 'meta' environment. this is very similar in spirit to the rl-squared framework of duan et al. 2017.-----as is typical of meta-learning environments, there is an outer loop and an inner loop. the outer loop is composed of environments sampled from a 'meta' environment and within each sampled environment are tasks sampled with different initial states and goals within that environment.-----unlike the baselines in this paper, the proposed method utilizes a non-parametric memory that stores all (action, prev. state, observation) tuples that were experienced within an episode. this allows a model to leverage experiences from previous tasks to accomplish the current goal. the unique contribution presented in this work lies in using self-attention to integrate these past experiences.-----furthermore, results are demonstrated on a toy environment and a street view environment to demonstrate scalability to harder tasks.-----the authors convincingly show that the proposed episodic memory architecture:-----learns from experiences from past tasks by improving as time progresses within an episode-----learning is improved when the self-attention memory is allowed more recurrent iterations-----inner-loop learning is accomplished in the absence of gradient updates-----further improvements:------are multiple episodes present within each minibatch or are parameter updates computed from minibatches extracted from single episodes? is this method sensitive to how the episodes/tasks are presented during training? in many settings, it is not be possible to have all environments available to sample from at any given training iteration due to limited computation or data resources.------there is little to no discussion about the computational resources necessary as compared to the baseline methods described in this paper. the authors do provide some information in the supplementary, but these tradeoffs are likely significant enough to warrant some discussion in the main text. there is clearly a computational complexity difference between the self-attention mechanism and baseline models like the lstm.------how important is tuning the timescale of the environment in this work? how does the computational complexity and model performance scale with course and finer grains of timescale?-----duan, yan, et al. ""rl-----2-----: fast reinforcement learning via slow reinforcement learning."" arxiv preprint arxiv:1611.02779 (2016). the paper is presenting a transformer-based architecture for episodic memory-based sequential decision making. the authors are interested in solving rts and perform experiments that are focused on the case of mapless navigation. the paper is insisting on the capacity to seamlessly coupling the exploration and planning parts of the overall task. the authors are using a very pertinent dataset of google street maps for mapless navigation but seem to basically claim the superiority of transformer against lstm based policy in this particular setting which does not seem surprising. a large quantity of work has been recently done on this task [1] [2] [3] one remark is that the planning phase of the algorithm is completely embedded into the model which can be problematic depending on the nature of the task, especially regarding the possible need for decision explainability. so, beyond parametric variation of tasks, as presented here, a good improvement could be to evaluate this approach on non-parametric variations of tasks as proposed in the manipulation framework of the stanford meta world [4].-----refs [1] gated path planning networks, 2018, salakhutdinov and al [2] learning to explore using active neural slam, 2019, salakhutdinov and al [3] semantic curiosity for active visual learning, 2020, chaplot and al [4] meta-world: a benchmark and evaluation for multi-task and meta reinforcement learning, 2019, levine and al","the paper proposes the challenge of rapid task-solving in unfamiliar environments and presents an approach to achieve this called episodic planning networks -- a non-parametric memory based on the transformer architecture to learn tasks that require planning from previously experienced tasks, following a form of meta-rl. the problem and approach are compelling, with strong empirical results. the paper is well-written and is an exciting contribution. this is a clear accept.-----in response to the initial reviews, the authors updated their paper to improve the formalization and address other concerns in the reviews, which were viewed favorably by the reviewers as a good improvement. based on the reviewer discussions, the work could still be placed better in context with respect to other literature.",the authors are using a very pertinent dataset of google street maps for mapless navigation but seem to basically claim the superiority of transformer against lstm based policy in this particular setting which does not seem surprising.,"the unique contribution presented in this work lies in using self-attention to integrate these past experiences.-----furthermore, results are demonstrated on a toy environment and a street view environment to demonstrate scalability to harder tasks.-----the authors convincingly show that the proposed episodic memory architecture:-----learns from experiences from past tasks by improving as time progresses within an episode-----learning is improved when the self-attention memory is allowed more recurrent iterations-----inner-loop learning is accomplished in the absence of gradient updates-----further improvements:------are multiple episodes present within each minibatch or are parameter updates computed from minibatches extracted from single episodes?",the authors propose a non-parametric memory based on the transformer/self-attention architecture to learn over tasks that require planning from previously experienced tasks.,"the paper proposes 2 new benchmarks for rapid task solving (rts), that evaluate rl agents on the ability to memorize past experiences and learn to plan to solve new tasks in different environments rapidly.","so, beyond parametric variation of tasks, as presented here, a good improvement could be to evaluate this approach on non-parametric variations of tasks as proposed in the manipulation framework of the stanford meta world [4].-----refs [1] gated path planning networks, 2018, salakhutdinov and al [2] learning to explore using active neural slam, 2019, salakhutdinov and al [3] semantic curiosity for active visual learning, 2020, chaplot and al [4] meta-world: a benchmark and evaluation for multi-task and meta reinforcement learning, 2019, levine and al","so, beyond parametric variation of tasks, as presented here, a good improvement could be to evaluate this approach on non-parametric variations of tasks as proposed in the manipulation framework of the stanford meta world [4].-----refs [1] gated path planning networks, 2018, salakhutdinov and al [2] learning to explore using active neural slam, 2019, salakhutdinov and al [3] semantic curiosity for active visual learning, 2020, chaplot and al [4] meta-world: a benchmark and evaluation for multi-task and meta reinforcement learning, 2019, levine and al","the paper formulates rts as an extension of meta-reinforcement learning framework, where in addition to optimizing over a distribution of tasks, the objective is to also optimize over a distribution over environments.-----i think the work proposed in the paper is an important step towards developing general agents that learn to adapt quickly.","the unique contribution presented in this work lies in using self-attention to integrate these past experiences.-----furthermore, results are demonstrated on a toy environment and a street view environment to demonstrate scalability to harder tasks.-----the authors convincingly show that the proposed episodic memory architecture:-----learns from experiences from past tasks by improving as time progresses within an episode-----learning is improved when the self-attention memory is allowed more recurrent iterations-----inner-loop learning is accomplished in the absence of gradient updates-----further improvements:------are multiple episodes present within each minibatch or are parameter updates computed from minibatches extracted from single episodes?",0.1604938271604938,0.0124999999999999,0.0864197530864197,0.0864197530864197,0.3755458515283842,0.026431718061674,0.1572052401746725,0.1572052401746725,0.2933333333333333,0.2297297297297297,0.2799999999999999,0.2799999999999999,0.2389937106918238,0.0636942675159235,0.1635220125786163,0.1635220125786163,0.3018867924528302,0.0476190476190476,0.1320754716981131,0.1320754716981131,0.3018867924528302,0.0476190476190476,0.1320754716981131,0.1320754716981131,0.3016759776536313,0.0790960451977401,0.1675977653631284,0.1675977653631284,0.3755458515283842,0.026431718061674,0.1572052401746725,0.1572052401746725,7.703494071960449,7.703494071960449,14.108694076538086,7.324324131011963,4.343867778778076,7.639098644256592,7.324324131011963,11.633143424987791,0.17837840286573176,0.2866112166031171,0.8961492897876573,0.9779921436964,0.9821456694776035,0.8988267033476457,0.965214372342466,0.962810546008706,0.8430219295738489,0.9714036925336111,0.9638903415410363,0.9103206312710952,0.4771489077415851,0.6324436986514809,0.7816690101681539,0.4771489077415851,0.6324436986514809,0.7816690101681539,0.9830168093680139,0.9810796590678933,0.9470337962767159,0.9779921436964,0.9821456694776035,0.8988267033476457
53,https://openreview.net/forum?id=FUdBF49WRV1,"this work considers the limitation of graph neural networks that cannot consider the directions. it proposed directional graph networks to overcome this limitation. the concerns for this work are as below:-----i do not quite understand the motivation of this work. in section 2.1, the authors explain that a big limitation of current gnn methods is that they cannot process directions. this is not case. in most message passing gnns, they can rely on the adjacency matrix during message passing process. if a specific direction is desired, it is feasible to modify or design the adjacency matrix accordingly. this fact is also true on the grid graphs. thus, i dont quite buy the motivation of this work. the authors need to clarify this motivation and illustrate why the direction limitation is not resolvable by manipulating the adjacency matrix. -----the data augmentation is proposed in section 2.7. the authors claimed that the advantage of the proposed method is that it does not influence the data but applied on kernels. however, there may be some issues here for changing kernels. in this work, the kernel is acting as a kind of message passing directions on the graph. the message passing patterns are an important part of the graph. if the patterns or connections are changed, the graph will change consequently. from this perspective, if the proposed method changes the kernel and modify graph connections, this cannot be considered as a strict data augmentation since the modified graphs can have totally different properties as the original ones. thus, i would like the authors to provide a clarification on this point.-----the number of datasets used in the experimental parts is quite limited. these datasets are not commonly used in the community. the authors may want to clarify this and add more datasets for comprehensive evaluations. this paper provides a theoretical framework that allows to directional convolutional kernels in any graph, e.g., generalize cnns on an n-dimensional grid. in the framework, gradients of the eigenvectors of the graph laplacian are used to define directions on the graph.-----the proposed method is well-motivated and seems to be theoretically justified (i did not fully understand the details and check the proofs). my main concerns are:-----the theoretical development is difficult to follow. the proposed method is not clearly described. it is hard for readers to find in the paper what are the steps of the proposed algorithm and understand how it works. it would be better to describe the algorithm step by step.-----i have some doubts about the practical value of the proposed method because it requires eigen-decomposition of the laplacian matrices. though the authors provide complexity analysis in the appendices, it would be more informative to provide a runtime comparison with sota methods such as the vanilla gcn.-----================ post rebuttal =============================================================-----thank the authors for the updates.-----in the latest version, the algorithm flow is clearly stated in figure 1, and now i can understand how the algorithm works. the authors also reported additional results on running time in the latest version, which are informative.-----here is what i think after reading the paper again.-----this paper proposes a novel idea. defining directions on graphs is not a well-addressed problem in current gnn models, and using the gradients of the low-frequency eigenvectors of the laplacian to define directions seems novel and interesting to me. -----the insight and analysis are not clear. section 2.4 is still difficult to follow after the updates. more importantly, i am not sure about the correctness of the theorems and corollaries. ---------- the k-walk distance is supposed to reflect the difficulty of passing information between two nodes, and a larger distance means more difficulty. in the paper the k-walk distance is defined as the average number of times that a k-step random walk from one node to hit another (formal definition given in page 18), which really puzzles me, because frequent visits indicate ease of message passing. did the authors confuse hitting probabilities with hitting times? summary: the authors propose a convolution as a message passing of node features over edges where messages are aggregated weighted by a ""direction"" edge field. furthermore, the authors propose to use the gradients of laplace eigenfunctions as direction fields. presumably, the aggregation is done with different direction fields derived from the laplace eigenfunctions with lowest eigenvalues, which are then linearly combined with learnable parameters. doing so allows their graph network to behave more like a conventional cnn, in which the kernels have different parameters for signals from different directions. the authors achieve good results on several benchmarks. furthermore, the authors prove that their method reduces to a conventional cnn on a rectangular grid and have theoretical results that suggest that their method suffers less from the ""over-smoothing"" and ""over-squashing"" problems.-----strong points: the proposal is highly novel. it is a simple and scalable modification to conventional graph nets that shows a strong performance increase in the benchmarks. the paper is mostly clearly written. the theoretical analyses contribute to the understanding of the work.-----weak points:-----it is unclear how parameters are used in their model. i presume that they apply the directional derivative with several edge fields and then linearly combine with learnable parameters, but this is not stated explicitly. furthermore, for different graphs with different spectra, how are the parameter shared? simply by their order?-----as the authors note at the end of sec 2.4, the laplace eigenvectors can not uniquely be identified, only the eigenspace of a certain eigenvalue. for example, this means that when lanczos method is applied to two isomorphic graphs in different orderings, different eigenvectors can be returned. the authors propose to overcome the arbitrary-ness of the sign by taking the absolute value after each dictional derivative. still, when an eigenvector is used from a degenerate eigenspace, the method appears not equivariant to node re-orderings. the authors state that in their datasets, the first non-trivial eigenvector is always non-degenerate, but the experiments also use higher eigenvectors. are these also non-degenerate? or is that model not equivariant? this also means that when using the proposed method on a square grid, which the authors do on cifar10, the method is not equivariant to node re-orderings.-----recommendation: i recommend to accept this paper, as it proposes a simple to use method to build more powerful graph nets. in spite of my concerns about equivariance, it appears to perform well in relevant benchmarks.-----opportunities for improvement:-----the paper could be improved by more directly addressing the concerns about equivariance. are some of the proposed models indeed not equivariant on certain graphs? do we care that the model is not always equivariant?-----it would be interesting to see if the authors could prove that the resulting model is more expressive than a conventional graph networks, for example by comparing theoretically or experimentally to the expressiveness of weisfeiler-lehman tests.-----post rebuttal-----my previous rating still applies. if accepted, i encourage the authors to more clearly state in the final version that their method is not applicable (without additional - and arguably inelegant - random augmentation) to graphs with degenerate eigenvalues and in particular symmetric graphs. the necessity of taking the absolute value to ensure invariance to the sign of the eigenvector should also be more clearly stated. i share reviewer #1's concerns about corollaries 2.5 and 2.6. these should be clarified or removed from the final version. nevertheless, i think the novelty of the approach justifies acceptance. it is a simple modification that may bring the expressive power of graph networks closer to that of pixel cnns.","the main merit of the paper is to try to address some important issues about gnn, e.g. expressivity power and data augmentation, from a novel perspective and using well grounded mathematical tools. unfortunately, however, this novel perspective is also introducing some confusion about its meaning in the context of graphs. in fact, it is not clear how, in the general case, direction as introduced in the paper makes sense, especially when considering the data augmentation approach. moreover, although well grounded mathematical tools are used, proofs of theorems, as well as justification of related corollaries, are not sufficiently clear to guarantee their correctness.-----in summary, a potentially interesting contribution that needs more work to better clarify motivations, grounding to common graph concepts, better presentation of the theoretical results.","in the paper the k-walk distance is defined as the average number of times that a k-step random walk from one node to hit another (formal definition given in page 18), which really puzzles me, because frequent visits indicate ease of message passing.","this also means that when using the proposed method on a square grid, which the authors do on cifar10, the method is not equivariant to node re-orderings.-----recommendation: i recommend to accept this paper, as it proposes a simple to use method to build more powerful graph nets.","this also means that when using the proposed method on a square grid, which the authors do on cifar10, the method is not equivariant to node re-orderings.-----recommendation: i recommend to accept this paper, as it proposes a simple to use method to build more powerful graph nets.",this work considers the limitation of graph neural networks that cannot consider the directions.,"summary: the authors propose a convolution as a message passing of node features over edges where messages are aggregated weighted by a ""direction"" edge field.","this also means that when using the proposed method on a square grid, which the authors do on cifar10, the method is not equivariant to node re-orderings.-----recommendation: i recommend to accept this paper, as it proposes a simple to use method to build more powerful graph nets.",it is hard for readers to find in the paper what are the steps of the proposed algorithm and understand how it works.,"this also means that when using the proposed method on a square grid, which the authors do on cifar10, the method is not equivariant to node re-orderings.-----recommendation: i recommend to accept this paper, as it proposes a simple to use method to build more powerful graph nets.",0.161849710982659,0.023391812865497,0.1156069364161849,0.1156069364161849,0.2711864406779661,0.0114285714285714,0.135593220338983,0.135593220338983,0.2711864406779661,0.0114285714285714,0.135593220338983,0.135593220338983,0.1267605633802817,0.0285714285714285,0.0845070422535211,0.0845070422535211,0.1045751633986928,0.0,0.0784313725490196,0.0784313725490196,0.2711864406779661,0.0114285714285714,0.135593220338983,0.135593220338983,0.1721854304635761,0.0536912751677852,0.1059602649006622,0.1059602649006622,0.2711864406779661,0.0114285714285714,0.135593220338983,0.135593220338983,6.40509033203125,6.2614336013793945,15.242157936096191,6.40509033203125,3.190607786178589,6.40509033203125,6.405091762542725,5.823832035064697,0.8426986613160257,0.8695407248645823,0.9278357884690861,0.44212019893130056,0.5509723002725888,0.9145854895534636,0.44212019893130056,0.5509723002725888,0.9145854418536268,0.9505811939548231,0.9540635172814315,0.8201186491621079,0.8401780027154749,0.8800220938882514,0.8376929465784703,0.44212019893130056,0.5509723002725888,0.9145854895534636,0.9202383682866084,0.9392768637194289,0.9425735937744596,0.44212019893130056,0.5509723002725888,0.9145854895534636
54,https://openreview.net/forum?id=Gu5WqN9J3Fn,"summary of the paper: this paper proposes a shape generation methods where shapes are represented by a set of parametric patches. sets of patches from the same category will follow certain constrained defined by templates. the templates are generated from some abstract representation of the category of interested (i.e. sets of cubbies). the authors evaluate the method qualitatively on sketch-to-shape task, and show that the methods allows generating meshes thats editable, interpretable, and with certain level of details.-----strength:-----the representation of parametric patches have several advantages compared to prior works (whose patches are represented by neural networks). it can create sparse, compact, interpretable, and editable shapes that can be directly imported into many industrial design software.-----the parametric patches allow computing cd without uniformly sampling from the surface.-----weakness:-----the papers evaluation of the method is very limited. even though the representation has many inherit advantages, i still expect the authors to demonstrate the methods advantage in different tasks (not only sketch-modeling), different datasets (not just limited to rather small categories like mugs), and across baselines with different representations (such as implicit representation, etc)-----a major weakness of the methods comes from the template generation. it occurs to me that the paper requires human user to pre-defined templates using a set of cuboids for each set of shapes before it can start generate. this makes it a little bit hard for the method to capture all topological variation of the category (which the author also mentioned in the last section). with this regard, the method requires additional information compared many prior methods (for example, implicit methods that can handle many topology and good details, and it doesnt take a template).-----there are potentially missing related works. some of which i think the paper should compare with as baselines:-----[a] nash, charlie, et al. ""polygen: an autoregressive generative model of 3d meshes."" arxiv preprint arxiv:2002.10880 (2020). [b] deng, zhantao, et al. ""better patch stitching for parametric surface reconstruction."" arxiv preprint arxiv:2010.07021 (2020).-----while the papers idea has its value as the representation could be useful for editing in many industrial software, but it requires more sophisticated evaluation (quantitative evaluation, more baselines, more tasks) in order to show-case the effectiveness of the representation. with that, i do not recommend the paper for iclr. this paper presents a method that leverages parametric surface patches as the fundamental representation in the task of shape modeling and reconstruction. this method requires a pre-generated template for each shape category. several losses are specially designed to regularize the generation of the surface patches. empirical results have demonstrated the performance of the proposed method in sketch-based shape reconstruction and 3d shape interpolation.-----pros:-----the paper shows some good results on reconstructing some simple shapes, such as bottles, mugs, bathtubs, etc.-----the idea of using parametric surface patches may reduce the parameter space that a network has to search and could potentially lead to smoother surfaces.-----cons:-----it is not clear to me why such representation is advantageous over conventional polygonal mesh-based representation.-----though the parametric surface patch can typically lead to smoother reconstruction, it may also suffer from the incapability of capturing fine geometric details.-----similar to the mesh-based approach, the patch-based method still has to fight similar challenges, e.g. avoiding the intersections, undesirable shape distortions, etc.-----editing-wise, the reconstructed polygonal mesh can be easily converted to parametric surface patches to ease the following editing workflow, by registering the pre-generated template of coon patches to the reconstructed shape.-----there are no quantitative evaluations in the paper. also, the visual comparisons with other approaches are very limited -- only one or two examples are presented. due to the lack of evaluations, it is difficult to determine the effectiveness of the proposed approach.--------- final rating ---------the revised version of the paper with additional experiments has addressed my concern on the limited advantage over prior deep 3d representation. fig.6 in the revised version has shown the potential of the proposed approach in generating directly usable representation in downstream applications, including cad design and manufacturing. hence, i would change my rating to positive.-----however, the paper still lacks quantitative and qualitative evaluations. i hope it could be properly addressed in the final version. the paper proposes a self-supervised method to fit a template (represented as a union of coons patches) to a certain 2d sketch. it derives a way to build a proper template, uses a network to predict the patches' parameters, and proposes a union of different losses. the qualitative results of the method are shown in several different objects.-----pros-----losses: i appreciated the effort of the authors in formulating an ensemble of losses for this specific representation. i think they will be useful for future works with similar representations.-----versatility: this representation permits working with a large variety of different classes of objects (also disconnected ones, but not topological changes), at an arbitrary resolution and predicts a small set of values.-----the paper does not clearly state if the code and the data will be made publicly available, but since the code is attached to the submission i assume this is in the will of the authors, and this is also a contribution.-----finally, the paper is well organized and i like the presentations.-----cons-----quantitative analysis: the only quantitative evaluation is reported in the supplementary material. i think this is the major weakness of the paper. i would suggest having more experiments to analyze the performance of the method. in particular, the ablation is a critical part since many losses are involved; i would suggest providing some quantitative measures, for example showing how each loss improves a particular aspect (e.g. the distance from the ground truth, the difference between normals, the intersection area, ...).-----i would also suggest highlighting the output patches on some qualitative results.-----minor fixes:-----equation 4, fix the pedix of-----u[0,1]2-----equation 13 (sup.mat), fix the integral limits locations-----i think ""p2p-net: bidirectional point displacement net for shape transform""(yin et al., 2018) worth a mention since solves (among the others) a similar problem-----pre-rebuttal rating i am favorable to accept the paper because it provides several tools that will be useful for future works, and i think it has interest points for the computer graphics community. for the acceptance i would recommend adding some more quantitative analysis, both for ablation and for the shown results.-----final-rating-----i read the other reviews and authors' replies carefully.-----first of all, i would acknowledge the effort of the authors in replying to reviewers' concerns.-----about my points, i agree with other reviewers that the quantitative analysis is a bit limited (i think it is also the main criticism), but the introduced ablation and the new figure 6 (numerically comparing against other sota methods) are convincing. the authors solved also my other concerns, and so i raise my score; i think the study of this representation is interesting, and well fit the audience of iclr.-----thanks to the author for their availability, and best of luck with their work!","description: the paper presents a patch-based 3d representation of man-made shapes that can be computed with deep learning and used directly in existing cad applications. this representation is based off a deformable parametric template with coons patches. results in sketch-based modelling tasks shows comparable results with stoa-----strengths:-----the patch-based representation provide several advantages: compact, sparse, interpretable, consistent and easily editable.-----can infer the right template, and thus does not require manually created templates-----weaknesses-----limited evaluation restrained to mostly sketch-based modelling, and missing evaluation against a few stoa methods-----the paper has introduced a very impactful new representation for 3d shapes and has strong technical novelty. i recommend, as reviewers have suggested, more in-depth quantitative evaluation (against other work and ablation studies)","due to the lack of evaluations, it is difficult to determine the effectiveness of the proposed approach.--------- final rating ---------the revised version of the paper with additional experiments has addressed my concern on the limited advantage over prior deep 3d representation.","empirical results have demonstrated the performance of the proposed method in sketch-based shape reconstruction and 3d shape interpolation.-----pros:-----the paper shows some good results on reconstructing some simple shapes, such as bottles, mugs, bathtubs, etc.-----the idea of using parametric surface patches may reduce the parameter space that a network has to search and could potentially lead to smoother surfaces.-----cons:-----it is not clear to me why such representation is advantageous over conventional polygonal mesh-based representation.-----though the parametric surface patch can typically lead to smoother reconstruction, it may also suffer from the incapability of capturing fine geometric details.-----similar to the mesh-based approach, the patch-based method still has to fight similar challenges, e.g. avoiding the intersections, undesirable shape distortions, etc.-----editing-wise, the reconstructed polygonal mesh can be easily converted to parametric surface patches to ease the following editing workflow, by registering the pre-generated template of coon patches to the reconstructed shape.-----there are no quantitative evaluations in the paper.","the authors evaluate the method qualitatively on sketch-to-shape task, and show that the methods allows generating meshes thats editable, interpretable, and with certain level of details.-----strength:-----the representation of parametric patches have several advantages compared to prior works (whose patches are represented by neural networks).",summary of the paper: this paper proposes a shape generation methods where shapes are represented by a set of parametric patches.,"empirical results have demonstrated the performance of the proposed method in sketch-based shape reconstruction and 3d shape interpolation.-----pros:-----the paper shows some good results on reconstructing some simple shapes, such as bottles, mugs, bathtubs, etc.-----the idea of using parametric surface patches may reduce the parameter space that a network has to search and could potentially lead to smoother surfaces.-----cons:-----it is not clear to me why such representation is advantageous over conventional polygonal mesh-based representation.-----though the parametric surface patch can typically lead to smoother reconstruction, it may also suffer from the incapability of capturing fine geometric details.-----similar to the mesh-based approach, the patch-based method still has to fight similar challenges, e.g. avoiding the intersections, undesirable shape distortions, etc.-----editing-wise, the reconstructed polygonal mesh can be easily converted to parametric surface patches to ease the following editing workflow, by registering the pre-generated template of coon patches to the reconstructed shape.-----there are no quantitative evaluations in the paper.","empirical results have demonstrated the performance of the proposed method in sketch-based shape reconstruction and 3d shape interpolation.-----pros:-----the paper shows some good results on reconstructing some simple shapes, such as bottles, mugs, bathtubs, etc.-----the idea of using parametric surface patches may reduce the parameter space that a network has to search and could potentially lead to smoother surfaces.-----cons:-----it is not clear to me why such representation is advantageous over conventional polygonal mesh-based representation.-----though the parametric surface patch can typically lead to smoother reconstruction, it may also suffer from the incapability of capturing fine geometric details.-----similar to the mesh-based approach, the patch-based method still has to fight similar challenges, e.g. avoiding the intersections, undesirable shape distortions, etc.-----editing-wise, the reconstructed polygonal mesh can be easily converted to parametric surface patches to ease the following editing workflow, by registering the pre-generated template of coon patches to the reconstructed shape.-----there are no quantitative evaluations in the paper.",the paper proposes a self-supervised method to fit a template (represented as a union of coons patches) to a certain 2d sketch.,"empirical results have demonstrated the performance of the proposed method in sketch-based shape reconstruction and 3d shape interpolation.-----pros:-----the paper shows some good results on reconstructing some simple shapes, such as bottles, mugs, bathtubs, etc.-----the idea of using parametric surface patches may reduce the parameter space that a network has to search and could potentially lead to smoother surfaces.-----cons:-----it is not clear to me why such representation is advantageous over conventional polygonal mesh-based representation.-----though the parametric surface patch can typically lead to smoother reconstruction, it may also suffer from the incapability of capturing fine geometric details.-----similar to the mesh-based approach, the patch-based method still has to fight similar challenges, e.g. avoiding the intersections, undesirable shape distortions, etc.-----editing-wise, the reconstructed polygonal mesh can be easily converted to parametric surface patches to ease the following editing workflow, by registering the pre-generated template of coon patches to the reconstructed shape.-----there are no quantitative evaluations in the paper.",0.188235294117647,0.0238095238095238,0.1058823529411764,0.1058823529411764,0.3288590604026846,0.081081081081081,0.1610738255033557,0.1610738255033557,0.3163841807909604,0.0342857142857142,0.135593220338983,0.135593220338983,0.16,0.0135135135135135,0.0933333333333333,0.0933333333333333,0.3288590604026846,0.081081081081081,0.1610738255033557,0.1610738255033557,0.3288590604026846,0.081081081081081,0.1610738255033557,0.1610738255033557,0.1842105263157895,0.0266666666666666,0.1184210526315789,0.1184210526315789,0.3288590604026846,0.081081081081081,0.1610738255033557,0.1610738255033557,10.586620330810549,10.586620330810549,16.520557403564453,10.586620330810549,7.238980770111084,12.415815353393556,10.586620330810549,12.197151184082031,0.503442676299077,0.6149365663632711,0.8174557674622766,0.9438391994301117,0.9649920331110191,0.4085817769697677,0.9591174008034632,0.9618174101443951,0.8983041467089635,0.9704388314003881,0.9664858221733172,0.9341017221095189,0.9438391994301117,0.9649920331110191,0.4085817769697677,0.9438391994301117,0.9649920331110191,0.4085817769697677,0.3351220168023548,0.5056730735988536,0.9231552056909872,0.9438391994301117,0.9649920331110191,0.4085817769697677
55,https://openreview.net/forum?id=H1GLm2R9Km,"this paper attempts to learn layers of nns greedily one at a time by using kernel machines as nodes instead of standard nonlinearities. the paper is well-written and was an interesting read, despite being notation heavy. ----------------i think the interpretability claims have some merits but are over-stated. furthermore, the expressive power of universal approximation through kernels holds only asymptotically. so i am not sure if the authors can claim equivalence in expressive powers to more traditional nns theoretically. i have some additional questions about the paper, and i am reserving my recommendation on this paper till the authors answer them. ----------------1) since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \tau * norm(weights) ? what is the benefit of explicitly talking about gaussian complexities and delineating theorem 4.2 when the same can be achieved by writing a much simpler form? lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since lemma c.1 follows easily, and again could be simplified a lot by just using the regularized cost function. am i missing something here?------------------------2) lemma 4.3 assumes separability (since c should be > a for \tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). why are these assumptions reasonable ? i understand that the empirical evaluation presented do justify the methodology, but i am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. ----------------minor :--------below def 4.1 ""to a standard normal distribution "" should be ""according to p"".--------some typos, please proof read e.g. spelling error ""represnetation "". summary: the paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. the theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. some small-scale experiments are provided.----------------evaluation: i have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. i did not check the proofs in the appendix so i might have missed some critical info or have not fully understood the experimental set-up.----------------- interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). whether bp or any layer-wise training schemes is used, isn't the goal is to get s_{l-1} to the state where s_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?--------- function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. it's unclear to me what is x here -- input to the network or output of the previous layer? this also has a sum over all training points, so is training kmlps in a layer-wise fashion more efficient than traditional kernel methods? --------- training scheme: what is the order of layers being trained? input to output or output to input? i'm slightly hazy on how to obtain f^{(l-1)}(s) to compute g_{l-1}. --------- the intuition of layer-wise optimality: on page 4, the paper states that ""the global min of r_l wrt s_{l-1} can be explicitly identified prior to any training"" but intuitively this must condition on some known function/function class f^(l). could you please enlighten me on this?--------- the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. what are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?--------- vanishing gradients: i'm not clear how layer-wise training can avoid this issue - could you please explain this?--------- some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation","the reviewers mostly raised two concerns regarding the paper: a) why this algorithm is more interpretability than bp (which is just gradient descent); b) the exposition of the paper is somewhat confusing at various places; c) the lack of large-scale experiment results to show this is practically relevant. in the ac's opinion, a principled kernel-based approach can be counted as interpretable, and there the ac would support the paper if a) is the only concern. however, c) seems to be a serious concern since the paper doesn't seem to have experiments beyond fashion mnist (e.g., cifar is pretty easy to train these days) and doesn't have experiments with convolutional models. based on c), the ac decided that the paper is not quite ready for acceptance.","what are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?--------- vanishing gradients: i'm not clear how layer-wise training can avoid this issue - could you please explain this?--------- some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation","summary: the paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks.",so i am not sure if the authors can claim equivalence in expressive powers to more traditional nns theoretically.,this paper attempts to learn layers of nns greedily one at a time by using kernel machines as nodes instead of standard nonlinearities.,so i am not sure if the authors can claim equivalence in expressive powers to more traditional nns theoretically.,"whether bp or any layer-wise training schemes is used, isn't the goal is to get s_{l-1} to the state where s_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?--------- function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points.",input to output or output to input?,"summary: the paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks.",0.1129943502824858,0.0,0.0564971751412429,0.0564971751412429,0.1317365269461078,0.0242424242424242,0.1317365269461078,0.1317365269461078,0.0933333333333333,0.0,0.04,0.04,0.1168831168831169,0.0,0.0779220779220779,0.0779220779220779,0.0933333333333333,0.0,0.04,0.04,0.24,0.0101010101010101,0.1399999999999999,0.1399999999999999,0.0289855072463768,0.0,0.0289855072463768,0.0289855072463768,0.1317365269461078,0.0242424242424242,0.1317365269461078,0.1317365269461078,7.262369155883789,17.910808563232422,15.693342208862305,6.786962985992432,8.91757583618164,17.910808563232422,6.786963939666748,0.7591249346733093,0.274313996179406,0.487598482150872,0.8765725540495459,0.9667385563411435,0.9660688632765064,0.9550859299316996,0.9458225927840832,0.9321131165554979,0.8782296255065353,0.965423781831665,0.9637442069150572,0.9137623097794759,0.9458225927840832,0.9321131165554979,0.8782297781491626,0.958208088884708,0.9639985874550003,0.8932422177712004,0.9274157099274258,0.9434698245563502,0.1872758006645738,0.9667385563411435,0.9660688632765064,0.9550859299316996
56,https://openreview.net/forum?id=H1OQukZ0-,"# summary of paper--------the paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of franceschi 2017 were some estimates are warm restarted to increase the stability of the method. ----------------# summary of review--------i find the contribution to be incremental, and the validation weak. furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that i would consider necessary on an optimization-based contribution. none of my comments are fatal, but together with the incremental contribution i'm inclined as of this revision towards marginal reject. ----------------# detailed comments----------------1. the distinction between parameters and hyperparameters (section 3) should be revised. first, the definition of parameters should not include the word parameters. second, it is not clear what ""parameters of the regularization"" means. typically, the regularization depends on both hyperparameters and parameters. the real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are ""begin learnt"", just from different datasets.----------------2. in section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to maclaurin 2015. this is not correct. this approach was first proposed in domke 2012 and refined by maclaurin 2015 (as correctly mentioned in maclaurin 2015).----------------3. some quantities are not correctly specified. i should not need to guess from the context or related literature what the quantities refer to. theta_k for example is undefined (although i could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_k(lambda, theta_0) and theta_k are used).----------------4. the hypothesis are not correctly specified. many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated.----------------5. the algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this. while i do believe that projecting into a compact domain is necessary (see pedregosa 2016 assumption a3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm.----------------# minor----------------missing . after ""hypergradient exactly"".----------------""we could optimization the hyperparam-"" (typo)----------------references:-------- justin domke. generic methods for optimization-based modeling. in--------international conference on artificial intelligence and statistics, 2012. summary of the paper-------------------------------------------the paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning. the covered framework is limited to regularization parameters. these hyper-parameters, noted , are updated along the training of model parameters by relying on the generalization performance (validation error). the paper proposes a dynamical system including the dynamical update of and the update of the gradient , derivative of w.r.t. to the hyper-parameters. the main contribution of the paper is to propose a way to re-initialize at each update of and a clipping procedure of in order to maintain the stability of the dynamical system. experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach.----------------comments------------------------------ the materials of the paper sometimes may be quite not easy to follow. nevertheless the paper is quite well written.--------- the main contributions of the paper can be seen as an incremental version of (franceschi et al, 2017) based on the proposal in (luketina et al., 2016). as such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors.--------- one motivation of the approach is to fix the slow convergence of the method in (franceschi et al, 2017). the paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed.--------- the goal of the paper is to address automatically the learning of regularization parameters. unfortunately, algorithm 1 involves several other hyper-parameters (namely clipping factor , constant or ) which choices are not clearly discussed. it turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious. this fact weakens the scope of the online hyper-parameter optimization approach.--------- it may be helpful to indicate the standard deviations of the experimental results.","this paper presents an update to the method of franceschi 2017 to optimize regularization hyperparameters, to improve stability. however, the theoretical story isn't so clear, and the results aren't much of an improvement. overall, the presentation and development of the idea needs work.",the main contribution of the paper is to propose a way to re-initialize at each update of and a clipping procedure of in order to maintain the stability of the dynamical system.,# summary of paper--------the paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of franceschi 2017 were some estimates are warm restarted to increase the stability of the method.,"the real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are ""begin learnt"", just from different datasets.----------------2.",# summary of paper--------the paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of franceschi 2017 were some estimates are warm restarted to increase the stability of the method.,"the paper proposes a dynamical system including the dynamical update of and the update of the gradient , derivative of w.r.t.",# summary of paper--------the paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of franceschi 2017 were some estimates are warm restarted to increase the stability of the method.,"in--------international conference on artificial intelligence and statistics, 2012. summary of the paper-------------------------------------------the paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning.",# summary of paper--------the paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of franceschi 2017 were some estimates are warm restarted to increase the stability of the method.,0.3589743589743589,0.0263157894736842,0.2307692307692307,0.2307692307692307,0.4050632911392405,0.1038961038961039,0.2278481012658228,0.2278481012658228,0.2340425531914893,0.0,0.1276595744680851,0.1276595744680851,0.4050632911392405,0.1038961038961039,0.2278481012658228,0.2278481012658228,0.3283582089552239,0.0615384615384615,0.2388059701492537,0.2388059701492537,0.4050632911392405,0.1038961038961039,0.2278481012658228,0.2278481012658228,0.24,0.0273972602739726,0.1333333333333333,0.1333333333333333,0.4050632911392405,0.1038961038961039,0.2278481012658228,0.2278481012658228,16.20128059387207,8.955240249633789,16.20128059387207,16.20128059387207,8.107921600341797,8.259716033935547,16.20128059387207,14.29015064239502,0.9333873942219784,0.951244240897942,0.19638857911869242,0.9637454713108474,0.960456506073472,0.10874033612762916,0.9480391778780162,0.9516104135170101,0.15658412126991716,0.9637454713108474,0.960456506073472,0.1087403163650086,0.9609312946775764,0.9682161780368604,0.02232810067498383,0.9637454713108474,0.960456506073472,0.10873945548049768,0.9756345600704279,0.9739119594478401,0.5287529084588478,0.9637454713108474,0.960456506073472,0.10874033612762916
57,https://openreview.net/forum?id=H1V4QhAqYQ,"this paper describes a new method for data augmentation which is called batch augmentation. the idea is very simple -- include in your batch m augmentations of the each training sample, effectively this will increase the size of the batch by m. i have not seen a similar idea to this proposed before. as the authors show this simple technique has the potential to increase training convergence and final accuracy. several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. cifar, imagenet, ptb) and architectures (resnet, wide-resnet, densenet, mobilenets). following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers the paper shows that training with large batch size (e.g., with mxb samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models. the enlarged batch of mxb consists of multiple (i.e., b) transforms of each of the m samples from the given batch; the transform is executed by a data augmentation method such as cutout or dropout. the authors also provide a theoretical explanation for the working of the method, suggesting that the enlarged batch training decreases the gradient variance during the training of the networks.----------------the paper is well written and easy to follow. also, some interesting results are experimentally obtained such as the figures presented in figure4. nevertheless, the experimental studies are not very satisfactory in its current form.-------- --------major remarks:----------------1. in terms of regularization with transformed data in a given batch, the proposed method is related to mixup (zhang et al., mixup: beyond empirical risk minimization), adamixup (guo et al., mixup as locally linear out-of-manifold regularization), manifold mixup (verma et al., manifold mixup: learning better representations by interpolating hidden states), and agrlearn (guo et al. aggregated learning: a vector quantization approach to learning with neural networks). it would be useful for the authors to discuss how the proposed strategy differs from them or empirically show how the proposed regularization method compares to them in terms of regularization effect. for example, in mixup, adamixup and manifold mixup, the samples in a given batch will be linearly interpolated with randomly reshuffled samples of the same batch. in these sense, using them as baselines would make the contribution of the proposed method much significant. --------2. in the experiments, it seems the authors use different data augmentation methods for different datasets (except for cifar10 and cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using cutout for the mobilenet and resnet50 on the imagenet data set. --------3. regarding the experimental study, i wonder if it would be beneficial to include three variations of the proposed method. first, use baseline with the same batch size, namely bxm, but with sampling with replacement. that is, using the same batchsize as that in batch augmentation but with repeated samples. in this way, the contribution of the data augmentation in the proposed method would be much clearer. second, as suggested from the results in the ptb data in table1, using only dropout obtains very minor improvement over the baseline method. in this sense, using other data augmentation methods instead of cutout for the image tasks would make the contribution of the paper much clear. third, training the networks with the batchsize of bxm, but excluding the original data samples in the given batch would be another interesting experiment. that is, all samples of the batch in the batch augmentation are synthetic samples. ----------------minor remarks:----------------1. is the regularized model robust to adversarial attacks as suggested in mixup and manifold mixup?--------2. would it be beneficial to include various data augmentation methods for the same batch? that is, each transformed sample may come from a different data augmentation strategy.----------------==========after rebuttal===========----------------my main concern is that the paper did not clearly show where the performance improvement comes from. it may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. i think the current comparison baseline in the paper is insufficient. i did propose three comparison baselines in my initial review, but i am not satisfied with the authors' rebuttal on that.","the authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. the experiments demonstrate that this leads to better performance compared to training with small batches. however, as noted by reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. the rebuttal didnt address the reviewers' concerns, and they argue for rejection.","overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers the paper shows that training with large batch size (e.g., with mxb samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models.","nevertheless, the experimental studies are not very satisfactory in its current form.-------- --------major remarks:----------------1. in terms of regularization with transformed data in a given batch, the proposed method is related to mixup (zhang et al., mixup: beyond empirical risk minimization), adamixup (guo et al., mixup as locally linear out-of-manifold regularization), manifold mixup (verma et al., manifold mixup: learning better representations by interpolating hidden states), and agrlearn (guo et al. aggregated learning: a vector quantization approach to learning with neural networks).","that is, all samples of the batch in the batch augmentation are synthetic samples.",this paper describes a new method for data augmentation which is called batch augmentation.,"nevertheless, the experimental studies are not very satisfactory in its current form.-------- --------major remarks:----------------1. in terms of regularization with transformed data in a given batch, the proposed method is related to mixup (zhang et al., mixup: beyond empirical risk minimization), adamixup (guo et al., mixup as locally linear out-of-manifold regularization), manifold mixup (verma et al., manifold mixup: learning better representations by interpolating hidden states), and agrlearn (guo et al. aggregated learning: a vector quantization approach to learning with neural networks).","--------2. in the experiments, it seems the authors use different data augmentation methods for different datasets (except for cifar10 and cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using cutout for the mobilenet and resnet50 on the imagenet data set.","third, training the networks with the batchsize of bxm, but excluding the original data samples in the given batch would be another interesting experiment.","--------2. in the experiments, it seems the authors use different data augmentation methods for different datasets (except for cifar10 and cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using cutout for the mobilenet and resnet50 on the imagenet data set.",0.2898550724637681,0.0294117647058823,0.1594202898550724,0.1594202898550724,0.2235294117647058,0.0119047619047619,0.1058823529411764,0.1058823529411764,0.1782178217821782,0.0,0.099009900990099,0.099009900990099,0.1188118811881188,0.0,0.0594059405940594,0.0594059405940594,0.2235294117647058,0.0119047619047619,0.1058823529411764,0.1058823529411764,0.2567567567567568,0.0273972602739726,0.1486486486486486,0.1486486486486486,0.1981981981981982,0.0,0.1261261261261261,0.1261261261261261,0.2567567567567568,0.0273972602739726,0.1486486486486486,0.1486486486486486,8.451915740966797,5.904018878936768,16.90428352355957,5.904018878936768,8.570549964904785,9.330978393554688,8.451915740966797,8.818998336791992,0.9661532520757188,0.9600844625874604,0.24879093175795405,0.9708765113074015,0.871423987630275,0.395180270347078,0.9707791273322491,0.970049105737671,0.4981293495904114,0.9787875909407023,0.9782996599637025,0.946503727296797,0.9708765113074015,0.871423987630275,0.395180270347078,0.9796412406046153,0.9751186017414204,0.9094343538320172,0.9588240522257679,0.9562231695894791,0.7325274779837417,0.9796412406046153,0.9751186017414204,0.9094343538320172
58,https://openreview.net/forum?id=H1VyHY9gg,"this paper proposes a data noising technique for language modeling. the main idea is to noise a word history by using a probabilistic distribution based on n-gram smoothing techniques. the paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. may main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.----------------comments: --------- p. 3 can be seen has a way -> can be seen as a way (?)--------- p. 3. in general, the explanation about blank noising should be improved. why does it avoid overfitting on specific contexts?--------- p. 4. it would be better to provide more detailed derivations for a general form of unigram and blank noising equations.--------- p. 5, section 3.6: is there any discussions about noising either/both input and output sequences with some numbers? this would be helpful information. this paper discusses data noising as a regularization technique for language modelling as an alternative to dropout regularization. the key idea is to adapt smoothing methods from ngram language modelling in such a fashion that they can be applied to continuous language models through noise. through this motivation, the authors present noising analogies to standard discounting, as well as kneser-ney smoothing.----------------the experiments are convincing in that the smoothed (noised) models outperform their unregularized baselines.----------------my main issue with the evaluation in this paper is that there is no comparison between the noising/smoothing idea and more conventional regularizers (such as l2 and dropout) which were discussed in the paper. likewise, it would have been interesting if the techniques proposed here had been applied to stronger base models (such as the models compared with in table 2). seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in zaremba's or gal's!----------------those weaknesses aside (and i recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly. i strongly recommend the paper for acceptance.","the reviewers are reasonably supportive of this paper. the ideas presented in the paper are nice and the results are encouraging. the authors should consider, for the final version of this work, providing comparisons to other approaches on the text8 corpus (or on the 1 billion words corpus, chelba et al.).",the paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation.,"seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in zaremba's or gal's!----------------those weaknesses aside (and i recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly.",this paper proposes a data noising technique for language modeling.,this paper proposes a data noising technique for language modeling.,"seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in zaremba's or gal's!----------------those weaknesses aside (and i recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly.","may main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.----------------comments: --------- p. 3 can be seen has a way -> can be seen as a way (?)",i strongly recommend the paper for acceptance.,"seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in zaremba's or gal's!----------------those weaknesses aside (and i recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly.",0.1351351351351351,0.0277777777777777,0.1351351351351351,0.1351351351351351,0.2735042735042735,0.0347826086956521,0.1538461538461538,0.1538461538461538,0.0983606557377049,0.0338983050847457,0.0983606557377049,0.0983606557377049,0.0983606557377049,0.0338983050847457,0.0983606557377049,0.0983606557377049,0.2735042735042735,0.0347826086956521,0.1538461538461538,0.1538461538461538,0.0761904761904762,0.0,0.0571428571428571,0.0571428571428571,0.1034482758620689,0.0357142857142857,0.1034482758620689,0.1034482758620689,0.2735042735042735,0.0347826086956521,0.1538461538461538,0.1538461538461538,13.192593574523926,9.993223190307615,17.830108642578125,9.993223190307615,11.53874969482422,17.830108642578125,9.993223190307615,5.308621406555176,0.9611783823301571,0.9682081706847044,0.9411924658039793,0.949454903639502,0.9616739404191534,0.9265584619402917,0.9772626291920538,0.9757699310042385,0.9340174359592576,0.9772626291920538,0.9757699310042385,0.9340174498774093,0.949454903639502,0.9616739404191534,0.9265584619402917,0.9682501209507015,0.9758317573340707,0.05697432763355362,0.9450002617782381,0.9584641363188243,0.8056336122452417,0.949454903639502,0.9616739404191534,0.9265584895541005
59,https://openreview.net/forum?id=H1ersoRqtm,"this paper presents a structural summarization model with a graph-based encoder extended from rnn. experiments are conducted on three tasks, including generating names for methods, generating descriptions for a function, and generating text summaries for news articles. experimental results show that the proposed usage of gnn can improve performance by the models without gnn. i think the method is reasonable and results are promising, but i'd like to see more focused evaluation on the semantics captured by the proposed model (compared to the models without gnn).----------------here are some questions and suggestions:----------------- overall, i think additional evaluation should be done to evaluate on the semantic understanding aspects of the methods. concretely, the graph-based encoder has access to semantic information, such as entities. in order to better understand how this helps with the overall improvement, the authors should consider automatic evaluation and human evaluation to measure its contribution. also from fig. 3, we can see that all methods get the ""utf8 string"" part right, but it's hard to say the proposed method generates better description. ----------------- in the last table in tab. 1, why the authors don't have results for adding gnn for the pointer-generator model with coverage? structured neural summarization----------------summary:----------------this work combines graph neural networks with a sequential approach to abstractive summarization across both natural and programming language datasets. the extension of gnns is simple, but effective across all datasets in comparison to external baselines for cnn/dailymail, internal baselines for c#, and a combination of both for java. the idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide. the chosen examples (which i hope are randomly sampled; are they?) do seem to suggest the efficacy of this approach with that intuition.----------------comments:----------------should probably cite cnn/dailymail when it is first introduced as nlsummarization in section 2 like you do the other datasets.----------------can you further elaborate on how your approach is similar to and differs from that in marcheggiani et al 2017 on graph cnns for semantic role labeling, bastings et al 2017 on graph convolutional encoders for syntax-aware machine translation, and de cao et al 2018? why should one elect to go the direction of sequential gnns over the gcns of those other works, and how might you compare against them? i would like to see some kind of ablation analysis or direct comparison with similar methods if possible.----------------why would gnns hurt selfatt performance on methoddoc c# selfatt+gnn / selfatt?----------------why not add the coverage mechanism from see et al 2017 in order to demonstrate that the method does in fact surpass that prior work? i'm left wondering whether the proposed method's returns diminish once coverage is added. note: i changed my original score from 4 to 7 based on the new experiments that answer many of the questions i had about the relative performance of each part of the model. the review below is the original one i wrote before the paper changes.----------------# positive aspects of this submission----------------- the intuition and motivation behind the proposed model are well explained.----------------- the empirical results on the methodnaming and methoddoc tasks are very promising.----------------# criticism----------------- the novelty of the proposed model is limited since it is essentially adding an existing ggnn layer, introduced by li et al. (2015), on top of an existing lstm encoder. the most important novelty seems to be the custom graph representation for these sequence inputs to make them compatible with the ggnn, which should then deserve a more in-depth study (i.e. ablation study with different graph representations, etc).----------------- since you compare your model performance against alon et al. on java-small, it should be fair to report the numbers on java-med and java-large as well.----------------- the ""gnn -> lstm+pointer"" experiment results are reported on the methoddoc task, but not for methodnaming. reporting this number for methodnaming is essential to show the claimed empirical superiority of the hybrid encoder compared to gnn only.----------------- i have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons:---------------- - the comparison of the proposed model for nlsummarization against see et al. is a bit unfair, since it uses additional information through the corenlp named entity recognizer and coreference models. with the experiments listed in table 1, there is no way to know whether the increased performance is due to the hybrid encoder design or due the additional named entity and coreference information. adding the entity and coreference data in a simpler way (i.e. at the token embedding level with a basic sequence encoder) in the ablation study would very useful to answer that question.---------------- - in nlsummarization, connecting sentence nodes using a next edge can be analogous to using a hierarchical encoder, as used by nallapati et al. (""abstractive text summarization using sequence-to-sequence rnns and beyond"", 2016). ignoring the other edges of the gnn graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder?---------------- - adding the coverage decoder introduced by see et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else.----------------- how essential is the weighted averaging for graph-level document representation (gilmer et al. 2017) compared to uniform averaging?----------------- a few minor comments about writing:-------- - in table 1, please put the highest numbers in bold to improve readability-------- - on page 7, the word ""summaries"" is missing in ""the model produces natural-looking with no noticeable negative impact""-------- - on page 9, ""cove content"" should be ""core content""","this paper examines ways of encoding structured input such as source code or parsed natural language into representations that are conducive for summarization. specifically, the innovation is to not use only a sequence model, nor only a tree model, but both. empirical evaluation is extensive, and it is exhaustively demonstrated that combining both models provides the best results. the major perceived issue of the paper is the lack of methodological novelty, which the authors acknowledge. in addition, there are other existing graph-based architectures that have not been compared to. however, given that the experimental results are informative and convincing, i think that the paper is a reasonable candidate to be accepted to the conference.","ignoring the other edges of the gnn graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder?---------------- - adding the coverage decoder introduced by see et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else.----------------- how essential is the weighted averaging for graph-level document representation (gilmer et al. 2017) compared to uniform averaging?----------------- a few minor comments about writing:-------- - in table 1, please put the highest numbers in bold to improve readability-------- - on page 7, the word ""summaries"" is missing in ""the model produces natural-looking with no noticeable negative impact""-------- - on page 9, ""cove content"" should be ""core content""","ignoring the other edges of the gnn graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder?---------------- - adding the coverage decoder introduced by see et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else.----------------- how essential is the weighted averaging for graph-level document representation (gilmer et al. 2017) compared to uniform averaging?----------------- a few minor comments about writing:-------- - in table 1, please put the highest numbers in bold to improve readability-------- - on page 7, the word ""summaries"" is missing in ""the model produces natural-looking with no noticeable negative impact""-------- - on page 9, ""cove content"" should be ""core content""","reporting this number for methodnaming is essential to show the claimed empirical superiority of the hybrid encoder compared to gnn only.----------------- i have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons:---------------- - the comparison of the proposed model for nlsummarization against see et al. is a bit unfair, since it uses additional information through the corenlp named entity recognizer and coreference models.",this paper presents a structural summarization model with a graph-based encoder extended from rnn.,structured neural summarization----------------summary:----------------this work combines graph neural networks with a sequential approach to abstractive summarization across both natural and programming language datasets.,"reporting this number for methodnaming is essential to show the claimed empirical superiority of the hybrid encoder compared to gnn only.----------------- i have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons:---------------- - the comparison of the proposed model for nlsummarization against see et al. is a bit unfair, since it uses additional information through the corenlp named entity recognizer and coreference models.",experimental results show that the proposed usage of gnn can improve performance by the models without gnn.,"reporting this number for methodnaming is essential to show the claimed empirical superiority of the hybrid encoder compared to gnn only.----------------- i have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons:---------------- - the comparison of the proposed model for nlsummarization against see et al. is a bit unfair, since it uses additional information through the corenlp named entity recognizer and coreference models.",0.3319502074688796,0.0418410041841004,0.1410788381742738,0.1410788381742738,0.3319502074688796,0.0418410041841004,0.1410788381742738,0.1410788381742738,0.3913043478260869,0.0439560439560439,0.1630434782608695,0.1630434782608695,0.1538461538461538,0.03125,0.123076923076923,0.123076923076923,0.158273381294964,0.0,0.0863309352517985,0.0863309352517985,0.3913043478260869,0.0439560439560439,0.1630434782608695,0.1630434782608695,0.106060606060606,0.0307692307692307,0.0757575757575757,0.0757575757575757,0.3913043478260869,0.0439560439560439,0.1630434782608695,0.1630434782608695,9.295955657958984,10.483898162841797,10.313886642456056,8.758591651916504,8.758591651916504,9.295955657958984,9.295955657958984,13.76841640472412,0.20878848327329605,0.10704284131852036,0.24479231928172482,0.20878848327329605,0.10704284131852036,0.24479242113362032,0.28250414697985005,0.24640270016754598,0.1208019687374954,0.9666113829652874,0.9686646177159894,0.8871816879090861,0.9857999469536145,0.981548885891874,0.8767379390217654,0.28250414697985005,0.24640270016754598,0.1208018353929958,0.96767615297025,0.9652582280373183,0.7739570360090225,0.28250414697985005,0.24640270016754598,0.1208018353929958
60,https://openreview.net/forum?id=H1l-02VKPB,"this paper proposes a topology-aware pooling method on graph data, which explicitly encodes the topology information when computing ranking scores. more specifically, the proposed method uses an attention operator to compute similarity scores between each node and its neighborhood nodes, and then uses the average similarity score of each node as the ranking score in the node selection process. this topology-aware pooling technique can be applied to graph neural networks on downstream tasks such as graph classification. experimental results demonstrate the effectiveness of the proposed method, which outperform previous state-of-the-art models consistently.----------overall, this paper is well-organized and the contributions are clear. although the methodology of adding a graph connectivity term is relatively simple, it is effective to improve the performance on graph classification task and should be straightforward to be applied to other graph learning tasks. based on the above reasons, i would like to recommend a weak accept for this paper. this paper proposed a topology-aware-pooling method to generate ranking scores for each node and so the pooling (or coarsening) of the graph can be achieved by picking those nodes with higher aggregated attention scores. the ranking scores for each node are computed by taking the average of the scores of its neighbors, by doing which the authors claim that the topological information of the graph is taken into direct account in the pooling steps. ----------the proposed method indeed shares some common idea with hierarchical graph representation learning with differentiable pooling by xing et al. there, a differentiable pooling procedure is used to progressively coarsen the graph to achieve global representation, and the assignment matrix is learned in each hierarchies. this is very similar to the proposed method, since those nodes with higher attention scores are those with more neighbors, which can be considered as ``local cluster centers on a graph. in other words, both methods perform some kind of hierarchical clustering to coarsen the graph. the authors claim that an assignment matrix may cause (1) overfitting, and (2) the coarsened graph may have a different structure than the original graph, which i believe are not justified criticisms. in many cases of clustering the assignment matrix is auxiliary variables; and even if the assignment matrix is learned as a whole matrix variable, as long as the loss terms is properly defined (such as reconstruction loss in k-means), it can be learned very well because it does not need labels as a unsupervised term. second, computing the assignment matrix has more flexibility, especially considering that the coarsening step is not just to perform clustering but rather to facilitate the final classification, therefore locating ``important nodes (in terms of fully representing the graph) is not the only goal, but locating both important and discriminative nodes (i.e., those nodes that can lead to informative features for final classification) is. in this sense, learning an assignment matrix obviously has the benefit of receiving back-propagated gradients from class labels, while the proposed method does not have this flexibility in the pooling step since the neighborhood structure of the graph is fixed throughout the iterations. ----------another important observation is that the proposed method seems to connect each hierarchy directly to the final layer, which is similar to skip-connections. can this be the main reason why the performance shows improvement, as has been validated in extensive studies in the computer vision community? some ablation studies are needed to verify that the performance gains are mainly due to the key idea of tap but not due to the skip connections that have been widely used in computer vision tasks.----------some minor comments:-----how do you implement the ranking_k() function? is it differentiable?","this paper proposes to incorporate graph topology into pooling operations on graphs, to better define the notion of locality necessary for pooling. while the paper tackles an important problems, and seems to be also well-written, the reviewers agree that there are several issues regarding the contribution and empirical results that need to be addressed before this paper is ready for publication.","in this sense, learning an assignment matrix obviously has the benefit of receiving back-propagated gradients from class labels, while the proposed method does not have this flexibility in the pooling step since the neighborhood structure of the graph is fixed throughout the iterations.","----------the proposed method indeed shares some common idea with hierarchical graph representation learning with differentiable pooling by xing et al. there, a differentiable pooling procedure is used to progressively coarsen the graph to achieve global representation, and the assignment matrix is learned in each hierarchies.","----------the proposed method indeed shares some common idea with hierarchical graph representation learning with differentiable pooling by xing et al. there, a differentiable pooling procedure is used to progressively coarsen the graph to achieve global representation, and the assignment matrix is learned in each hierarchies.","this paper proposes a topology-aware pooling method on graph data, which explicitly encodes the topology information when computing ranking scores.",this paper proposed a topology-aware-pooling method to generate ranking scores for each node and so the pooling (or coarsening) of the graph can be achieved by picking those nodes with higher aggregated attention scores.,this paper proposed a topology-aware-pooling method to generate ranking scores for each node and so the pooling (or coarsening) of the graph can be achieved by picking those nodes with higher aggregated attention scores.,"although the methodology of adding a graph connectivity term is relatively simple, it is effective to improve the performance on graph classification task and should be straightforward to be applied to other graph learning tasks.",this paper proposed a topology-aware-pooling method to generate ranking scores for each node and so the pooling (or coarsening) of the graph can be achieved by picking those nodes with higher aggregated attention scores.,0.2452830188679245,0.0192307692307692,0.1509433962264151,0.1509433962264151,0.2429906542056074,0.019047619047619,0.1495327102803738,0.1495327102803738,0.2429906542056074,0.019047619047619,0.1495327102803738,0.1495327102803738,0.1927710843373493,0.074074074074074,0.1927710843373493,0.1927710843373493,0.2857142857142857,0.0416666666666666,0.2244897959183673,0.2244897959183673,0.2857142857142857,0.0416666666666666,0.2244897959183673,0.2244897959183673,0.2680412371134021,0.0421052631578947,0.1443298969072165,0.1443298969072165,0.2857142857142857,0.0416666666666666,0.2244897959183673,0.2244897959183673,11.722776412963867,11.722774505615234,14.233844757080078,10.034340858459473,7.835522651672363,10.034341812133787,11.722774505615234,9.218887329101562,0.9475914943538115,0.9623250003552078,0.9391498917534953,0.9776915630360046,0.9316266440603875,0.8379246110968024,0.9776915630360046,0.9316266440603875,0.8379246110968024,0.9851313084470658,0.9831482850677403,0.9534389357307536,0.9851405468144379,0.9827133976331606,0.9443919528247843,0.9851405468144379,0.9827133976331606,0.9443919528247843,0.9459830606712073,0.9583797811498147,0.9503409256623337,0.9851405468144379,0.9827133976331606,0.9443919528247843
61,https://openreview.net/forum?id=H1l2mxHKvr,"a new task is suggested, similarly to fsl the test is done in an episodic manner of k-shot 5-way, but the number of samples for base classes is also limited. the model is potentially pre-trained on a large scale dataset from another domain. the suggested method is applying spatial attention according to entropy criteria (or certainty) of the original classifier (from a different domain).---------------i think the suggested task is important and more realistic than the usual fsl benchmarks. i would modify it so instead of discarding mini-imagenet classes that are overlapping with places i would discard the problematic places classes. this way it will be easier to compare to standard fsl. also, i dont understand why for cub the benchmarks includes k={0,1,5} while for mini-imagenet it is k={0,20,50}, obviously k={0,1,5} are more interesting.----------as for the suggested method, i find it hard to judge since there are no strong baselines to compare against. also, the ablation study of removing the attention and/or adaptation doesnt result in a definitive conclusion. ---------------update:-----while your comments do weaken some of my concerns, i'm afraid it is not enough for changing my previous rating. i think being more careful about the benchmark definition with regards to train/test overlap and comparing to stronger baselines will help improve the paper for future submissions. this paper proposed a new realistic setting for few-shot learning that we can obtain representations from a pre-trained model trained on a large-scale dataset, but cannot access its training details. also, there may be a large domain shift between the dataset of the pre-trained model and our dataset. for the pre-trained model, they will not only use its weights but also use it to generate a spatial attention map and help the model focuses on objects of images. back to the standard few-shot classification problem, they will first adapt the model with base class samples and then adapt to novel classes.----------the proposed new setting is very meaningful since we already have many powerful pre-trained models and why not exploit its usage for few-shot learning problems. however, i doubt the novelty and effectiveness of the attention way used in the paper. the attention module helps the model focuses on the objects not the background, which is absolutely correct. but there are already some relevant studies in the missing reference large-scale long-tailed recognition in an open world, cvpr2019. also, from the results, the significant improvements come from the weights of the pre-trained model but not the attention used. is the attention way used in the paper a good way to exploit the pre-trained model for few-shot classification problems?----------also, i am curious about the dense classification used in the adaptation phase. will it achieve similar performance with finetuning using just standard loss?----------btw, according to the formatting instructions, the abstract should be limited in one paragraph.----------=========================================================-----after rebuttal:----------i thank the author for the response.----------i do see there are differences in the way of generating attention masks between the proposed work and (liu et al.). but the improvements from the attention module is not significant, especially when using all base data.----------i keep my original scores. the paper introduces a problem few-shot few-shot learning that aims to firstly transfer prior knowledge from one domain to the domain where the base training tasks reside, and then train a few-shot learning model on training tasks and apply it to novel test tasks. the two few-shot in the name refers to base training tasks and novel test tasks. in their algorithm, they use a model pre-trained on another dataset as the prior knowledge and fine-tune it on training tasks. during the test, they use the weighted average of samples representations per class as the prototype of each class, where the weight is large for samples with more discriminative prediction over pre-trained domains classes. afterward, classification is reduced to finding the nearest neighbor among the class prototypes. some experiments show that the pre-trained model can improve few-shot classification accuracy.----------my major concerns:----------1) they try to propose a new problem, but their description shows that the problem is exactly the same as what most few-shot learning works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks. ----------2) the algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre-trained model and apply the nearest neighbor classification. the so-called prototypical classifier is actually the nearest neighbor classifier since no prototypical network structure is learned in the proposed method.----------3) i would not call the weighted average as attention because it is not: the weight in attention is computed by a module with learnable parameters, while the weight in this paper is computed by the entropy of a pre-defined models output prediction. ----------4) the spatial attention only makes sense when the pre-trained domains classes can describe the main concepts appearing in the images of novel classes. this assumption is too strong since it requires class-level (rather than lower-level) relationships.----------5) the base training is not necessary in the algorithm: it is used to only fine-tuning theta and w. as the author said in the beginning of section 4.1, they can directly solve novel tasks based on the pre-trained model.----------6) the experiments show that the pre-trained model is helpful in few-shot learning, which is a known fact.----------7) the writing of this paper is very poor: a lot of typos and grammar errors, inconsistency between narratives, abuse of notations, wrong equation reference, even missing punctuations. they make the paper hard to understand.---------------------------------update:----------thanks for the authors' rebuttal! after reading their rebuttal, i still have main concerns about the novelty of the problem and the writing quality. the proposed method tends to be incremental.","this paper tackles the interesting problem of meta-learning in problem spaces where training ""tasks"" are scarce. two criticisms that seems to shared across reviewers are that (i) it is debatable how ""novel"" the space of meta learning with ""few"" tasks is, especially since there aren't established standard for how many training tasks should be available, and (ii) the paper could use more comparisons with baseline methods and ablations to understand the contributions. as an ac, i down-weight criticism (i) because i don't feel the paper has to be creating a new problem definition; it's acceptable to make advances within an existing space. however, criticism (ii) seems to remain. after conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewer's opinions on this issue, and so the paper does not have enough support to justify acceptance. the paper certainly addresses interesting issues, and i look forward to seeing a revised/improved version at another venue.","will it achieve similar performance with finetuning using just standard loss?----------btw, according to the formatting instructions, the abstract should be limited in one paragraph.----------=========================================================-----after rebuttal:----------i thank the author for the response.----------i do see there are differences in the way of generating attention masks between the proposed work and (liu et al.).","some experiments show that the pre-trained model can improve few-shot classification accuracy.----------my major concerns:----------1) they try to propose a new problem, but their description shows that the problem is exactly the same as what most few-shot learning works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks.","during the test, they use the weighted average of samples representations per class as the prototype of each class, where the weight is large for samples with more discriminative prediction over pre-trained domains classes.","a new task is suggested, similarly to fsl the test is done in an episodic manner of k-shot 5-way, but the number of samples for base classes is also limited.","the paper introduces a problem few-shot few-shot learning that aims to firstly transfer prior knowledge from one domain to the domain where the base training tasks reside, and then train a few-shot learning model on training tasks and apply it to novel test tasks.","some experiments show that the pre-trained model can improve few-shot classification accuracy.----------my major concerns:----------1) they try to propose a new problem, but their description shows that the problem is exactly the same as what most few-shot learning works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks.","in their algorithm, they use a model pre-trained on another dataset as the prior knowledge and fine-tune it on training tasks.","some experiments show that the pre-trained model can improve few-shot classification accuracy.----------my major concerns:----------1) they try to propose a new problem, but their description shows that the problem is exactly the same as what most few-shot learning works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks.",0.2162162162162162,0.009090909090909,0.1171171171171171,0.1171171171171171,0.2393162393162393,0.0344827586206896,0.1111111111111111,0.1111111111111111,0.1485148514851485,0.0,0.0693069306930693,0.0693069306930693,0.1306532663316583,0.0203045685279187,0.0603015075376884,0.0603015075376884,0.233644859813084,0.0283018867924528,0.1214953271028037,0.1214953271028037,0.2393162393162393,0.0344827586206896,0.1111111111111111,0.1111111111111111,0.1263157894736842,0.0106382978723404,0.0631578947368421,0.0631578947368421,0.2393162393162393,0.0344827586206896,0.1111111111111111,0.1111111111111111,7.957479953765869,7.404500007629394,9.943678855895996,7.957479953765869,7.186298370361328,5.91150426864624,7.957479953765869,7.458443164825439,0.9101380682132717,0.9449499491064428,0.6855007601010412,0.37420862943029304,0.4937684860696501,0.8572316107971651,0.5211364856535263,0.6571585796277826,0.38649027091142024,0.9679864993460879,0.9662077715609112,0.5275797864524997,0.9600259579176631,0.9601292029975311,0.22403836973709074,0.37420862943029304,0.4937684860696501,0.8572311467215202,0.935961442412936,0.9575153571565569,0.7814112718535313,0.37420862943029304,0.4937684860696501,0.8572311467215202
62,https://openreview.net/forum?id=H1lKNp4Fvr,"this paper presents an algorithm for stereo image matching that attempts to capture improved representations of detailed spatial structure information, in particular, by increasing the size of the receptive field. the paper shows that this leads to a major reduction in the number of model parameters (42% in one case) with comparable performance on the kitti2015 data set.----------i like the driving principle of the authors' approach (that stereo image matching relies more heavily on low-level features, and that higher level ""semantic"" features are not as critical) compelling. i would have really like to have seen the authors do some analysis of the features that they do extract, so that the reader can get a deeper insight into why their method works. the paper could be improved by providing more this kind of analysis and by adding more motivate for why low-level features are more important for stereo matching. ----------i'm concerned that the paper only present results on one, small (200+200 images) data set. the paper would be much stronger if the authors tested on more, and varied data sets.----------is it simply a network complexity issues or is there something else?----------the related work section appears to be just a laundry list of methods. the paper would be stronger if the authors provided more interpretation of the strengths and weaknesses of these methods, some insight into why they work, and why the proposed method is better.----------the authors' method claims to use a 1x1 convolution layer. is that correct? sounds like simple multiplication. explain what it different.----------the authors' reporting of their results appears muddled. they claim the error rate was ""reduced 3.4% and 1.9%"" in table 5. i could not figure out which numbers they were talking about. in most cases, the authors' method was not the best.----------minor point: the authors say ""conclusion"" when i think they mean ""oclusion"". the paper at hand argues that shallow feature extraction networks should be favored for the computer vision task of stereo matching, rather than the commonly used deep resnet backbones. to that end, a model is proposed consisting of three convolutional feature extraction layers only. as this makes it impossible to capture global context, dilated convolutions with varying dilations are applied to the extracted features in parallel and concatenated. finally, a fusion module implemented via channel attention is applied.----------i found this paper lacking in terms of contributions. while the motivation for retaining detailed feature information makes sense for the task in question, it is not clear why it requires replacing the resnet feature extraction with a very shallow network; an alternative would be to add skip-connections originating from lower levels. both dilated convolutions and the fusion module are not novel. the paper addresses the need for a fusion module in great detail. however, the insight that a large dilation on top of features computed from a limited receptive field will result in a non-continuous receptive field is a rather trivial one. hence i don't see the need for section 4.3.1 and 4.3.2, as well as the somewhat complicated deduction in appendix a. the individual modules are ablated, but what about the number of layers for feature extraction? why settle for 3 rather than 1, 2, or 4?----------the paper does compare against other methods for stereo matching, but i was wondering why the ktti leader-board excerpt does not include the better results from http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo ? the result tables are also hard to parse as bold numbers do not correspond to best performance. the conclusion claims that the proposed shallow architecture exhibits ""lower training difficulty"" but i did not see any support for this in the experiments.----------overall, i think that this paper should be rejected on the basis of insufficient contributions.","the paper proposed the use of a shallow layers with large receptive fields for feature extraction to be used in stereo matching tasks. it showed on the kitti2015 dataset this method leads to large model size reducetion while maintaining a comparable performance.----------the main conern on this paper is the lack of technical contributions:-----* the task of stereo matching is very specialized one, simply presenting the model size reduction and performance is not interesting to general readers. adding more analysis that help understanding why the proposed method helps in this particular task and for what kind of tasks a shallow feature instead a deeper one is perferred. in that way, the paper would be addressing much wider audiences. -----* the discussions on related work is not thorough enough, lacking of analysis of pros and cons between different methods.","this paper presents an algorithm for stereo image matching that attempts to capture improved representations of detailed spatial structure information, in particular, by increasing the size of the receptive field.","the paper shows that this leads to a major reduction in the number of model parameters (42% in one case) with comparable performance on the kitti2015 data set.----------i like the driving principle of the authors' approach (that stereo image matching relies more heavily on low-level features, and that higher level ""semantic"" features are not as critical) compelling.",both dilated convolutions and the fusion module are not novel.,"this paper presents an algorithm for stereo image matching that attempts to capture improved representations of detailed spatial structure information, in particular, by increasing the size of the receptive field.","the paper would be stronger if the authors provided more interpretation of the strengths and weaknesses of these methods, some insight into why they work, and why the proposed method is better.----------the authors' method claims to use a 1x1 convolution layer.","while the motivation for retaining detailed feature information makes sense for the task in question, it is not clear why it requires replacing the resnet feature extraction with a very shallow network; an alternative would be to add skip-connections originating from lower levels.","hence i don't see the need for section 4.3.1 and 4.3.2, as well as the somewhat complicated deduction in appendix a. the individual modules are ablated, but what about the number of layers for feature extraction?","the paper shows that this leads to a major reduction in the number of model parameters (42% in one case) with comparable performance on the kitti2015 data set.----------i like the driving principle of the authors' approach (that stereo image matching relies more heavily on low-level features, and that higher level ""semantic"" features are not as critical) compelling.",0.2035928143712574,0.0242424242424242,0.1197604790419161,0.1197604790419161,0.336734693877551,0.0515463917525773,0.1938775510204081,0.1938775510204081,0.0408163265306122,0.0,0.0408163265306122,0.0408163265306122,0.2035928143712574,0.0242424242424242,0.1197604790419161,0.1197604790419161,0.2681564245810056,0.0677966101694915,0.1675977653631284,0.1675977653631284,0.2430939226519337,0.0446927374301675,0.143646408839779,0.143646408839779,0.1573033707865168,0.0227272727272727,0.1011235955056179,0.1011235955056179,0.336734693877551,0.0515463917525773,0.1938775510204081,0.1938775510204081,6.137652397155762,6.266980648040772,13.111968994140623,11.011224746704102,13.111968994140623,4.124090671539307,11.011224746704102,4.785374164581299,0.9773536245892351,0.9751382275997509,0.9499185073186354,0.9609626726817194,0.9714854855478349,0.16371489938332853,0.9569307628027153,0.9594678869554228,0.8230988082964604,0.9773536245892351,0.9751382275997509,0.9499185285511875,0.9459985218971468,0.9678786945113854,0.9180164433033239,0.9450097763819045,0.9471307489280798,0.9094058636717897,0.9605811581617067,0.9534690185937206,0.11064924137620362,0.9609626726817194,0.9714854855478349,0.16371498120958855
63,https://openreview.net/forum?id=H1lMogrKDH,"i wanted to first thank the authors for their work on connecting biologically inspired cwa to deep learning. the paper overall reads nicely. ----------the authors make the assumption that reviewers are fully aware of biological terminology related to cwa, however, this may not be the case. it took a few hours of searching to be able to find definitions that could explain the paper better. i suggest the authors add these definitions explicitly to their paper (perhaps in supplementary), also an overview figure would go a long way. i enjoyed reading the paper, and before making a final decision (hence currently weak reject), i would like to know the answer to the following questions:----------1. have the authors considered the computational burden of equation 3? in short, it seems that there are two summations (one for building the probability space over measure h) and one right before e_j. this is somewhat important, if this type of neural network is presented as a competitor to affine mapped activations. ----------2. it would be nice to have some proof regarding universal approximation capabilities of cwa. in my opinion it is, but a proof would be nice (however redundant or trivial - simply use supplementary). ----------3. i was a bit confused to see cwa+bn in the table 1. in introduction, authors write but cwa networks are by definition normalized and range-limited. therefore, one can conjecture that cwa plays a normalization role in biological neural networks. therefore, i was expecting cwa+bn to work similarly as cwa for cifar10. please elaborate further on this note. ----------4. essentially, the cwa changes the definition of a layer in a neural net. do authors see a path from cwa works to cwa works better than affine?. if so, please elaborate. specifically, i am asking this question why should/must we stop using affine maps in favor of cwa?. now this may or may not be the claim of the paper. its ok if it is not; still showing competitive performance is somewhat acceptable, but certainly further insight would make the paper stronger. this paper focuses on non-spiking hudghkin-huxley model, which is different from existing works on spiking neural network-based hudghkin-huxley model. ----------there are many ways of using neuron firing model as unit to construct neural networks. they choose a specific way (mentioned above). i think the most interesting part would be the cwa method which achieves the normalization. ----------they have a fair list of literature in spiking neural networks. but i find the way they illustrate the difference between their model and other models is insufficient. they should focus on the model-wise difference, instead of focusing on whether its applied to mnist or not or whats the accuracy. ----------they dont include any other snn model in the paper for experimental comparison. they also mention a few snn works that work well on mnist in the related work section which actually have better accuracies than their model. so it is inappropriate to say this proposed method is a state-of-art neuro-inspired method, because others perform well on mnist as well, and their limited experiments only investigate mnist and cifar-10, which are less interesting generally. ----------cwa cannot outperform affine+bn. ----------overall, the idea is somehow interesting, but the experiments are weak. applying the method to mnist and cifar-10 is far from being called either interesting computer vision applications or difficult perceptual tasks. they only use perceptual in the title, but the applications are mnist and cifar-10. it feels like they want to learn something big, but they only focus on benchmark datasets. ----------they compare nothing with other snn type of model on other truly difficult perceptual tasks. this paper proposes a novel neural network architecture inspired by the analysis of a steady-state solution of the hodgkin-huxley model. using a few simplifying assumptions, the authors use conventional backpropagation to train dnn- and cnn-based models and demonstrate that their accuracies are not much lower than the state-of-the-art results.----------the paper is well-written, sufficiently detailed and understandable. derived self-normalizing conductance-weighted averaging (cwa) mechanism is interesting in itself, especially contrasting cwa results with those obtained for the non-batch-normalized networks. it is also inspiring to see that this model can be derived based on a relatively accurate biological neuron model.----------my main question is actually related to the potential impact of this study. i am curious about the implications and the ways in which these results can inspire other researchers.----------after reading the paper, i got an impression that:----------(a) from the point of view of a machine learning practitioner, these results may not be particularly impressive. they do hint at the importance of self-normalization though, which could potentially be interesting to explore further.----------(b) from the point of view of a neuroscientist, the proposed model might be too simplistic. it is my understanding, that neural systems (even at ""rest"") are inherently non-equilibrium (and i assume the presence of simple feedback loops could also dramatically change the stead-state of the system). is it possible that something similar to this ""steady-state inference"" mode could actually take place in real biological neural systems?----------(c) presented results appear to be important from the point of view of someone who wants to transfer insights from biology into the field of deep learning. but there might be an extent to what is achievable given a simple goal of optimizing a supervised accuracy of an artificial neural network trained using gradient descent (especially considering limitations imposed by hardware). i am optimistic about the prospect of knowledge transfer between these disciplines, but it is my feeling that the study of temporal dynamics, emergent spatiotemporal encodings, ""training"" process of a biological neural system, etc. have potentially much more to offer to the field of machine learning. these questions do appear to be incredibly complex though and the steady-state analysis is definitely a prerequisite.","the paper studies non-spiking hudgkin-huxley models and shows that under few simplifying assumptions the model can be trained using conventional backpropagation to yield accuracies almost comparable to state-of-the-art neural networks. overall, the reviewers found the paper well-written, and the idea somewhat interesting, but criticized the experimental evaluation and potential low impact and interest to the community. while the method itself is sound, the overall assessment of the paper is somewhat below what's expected from papers accepted to iclr, and im thus recommending rejection.","using a few simplifying assumptions, the authors use conventional backpropagation to train dnn- and cnn-based models and demonstrate that their accuracies are not much lower than the state-of-the-art results.----------the paper is well-written, sufficiently detailed and understandable.","this paper focuses on non-spiking hudghkin-huxley model, which is different from existing works on spiking neural network-based hudghkin-huxley model.",do authors see a path from cwa works to cwa works better than affine?.,i wanted to first thank the authors for their work on connecting biologically inspired cwa to deep learning.,"this paper focuses on non-spiking hudghkin-huxley model, which is different from existing works on spiking neural network-based hudghkin-huxley model.","is it possible that something similar to this ""steady-state inference"" mode could actually take place in real biological neural systems?----------(c) presented results appear to be important from the point of view of someone who wants to transfer insights from biology into the field of deep learning.",----------they have a fair list of literature in spiking neural networks.,do authors see a path from cwa works to cwa works better than affine?.,0.3787878787878788,0.2,0.2727272727272727,0.2727272727272727,0.1769911504424778,0.054054054054054,0.1238938053097345,0.1238938053097345,0.0384615384615384,0.0,0.0384615384615384,0.0384615384615384,0.0555555555555555,0.0,0.0555555555555555,0.0555555555555555,0.1769911504424778,0.054054054054054,0.1238938053097345,0.1238938053097345,0.1884057971014492,0.0,0.1159420289855072,0.1159420289855072,0.0792079207920792,0.0202020202020202,0.0594059405940594,0.0594059405940594,0.0384615384615384,0.0,0.0384615384615384,0.0384615384615384,6.018510818481445,2.23098373413086,15.541533470153809,2.2309842109680176,6.375593185424805,13.219564437866213,13.219563484191896,5.725833892822266,0.49748349533382724,0.6081431100977833,0.8858653258715213,0.9673536889870215,0.9704748016893338,0.9433634069441528,0.9657303625404599,0.9645887504795579,0.009182186112018733,0.9655900004670044,0.9658484060391058,0.9099990785737155,0.9673536889870215,0.9704748016893338,0.943363382620758,0.6119679308910989,0.7663990845066707,0.9312619006883194,0.9683190322009604,0.9719087069273531,0.8635075998589022,0.9657303625404599,0.9645887504795579,0.009182163965471028
64,https://openreview.net/forum?id=H1lNPxHKDH,"in this paper, the author analysis the (approximate) function class generated by an infinite-width network when the euclidean norm is bounded. they extend the work of savarese et al. on the univariable function by introducing the randon transform and r-norm to this problem. the authors finally prove that any function in sobolev space could be (approximately) obtained by a bounded network. the results achieved implies some generalization performance analysis and the induction error. also, according to the authors, the difference between r-norm and rkhs norm might lead to the distinct from neural networks and kernel methods.----------i would recommend accepting this paper since it might give a good insight into understanding the performance of the network beyond the traditional method. this paper gives characterization of the norm required to approximate a given multivariate function by an infinite-width two-layer neural network. an important result is the relation between radon-transform and the -norm. this paper also shows application of the norm on some special case.----------i suggest this paper being accepted because it provides new insights into the approximation theory for neural networks. the perspective of norm constraint is different from the traditional approximation theory and may serve as a good contribution to the community.----------one question is that: in section 4, the equation (19) is differentiated twice to get the equation (20) containing dirac delta. although this is intuitively correct, this seems not a strict derivation to my mathematical background. it would be great if the authors can show the strict definition and derivation presented here. the paper studies the function space regularization behavior of learning with an infinite-width relu network with a bound on the l2 norm of weights, in arbitrary dimension, extending the univariate study of savarese et al. (2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks. i thus recommend acceptance.----------a few comments:-----* is it possible to obtain precise characterizations of interpolating solutions in this setting (other than a mere representer theorem with relus), as done in savarese et al (2019, theorem 3.3) for the univariate case?----------* perhaps the results of section 5.1 should be contrasted with those of bach (2017, e.g. prop. 5), which only require ~ d/2 derivatives instead of ~ d here, albeit with stronger requirements, for essentially the same functional space (though the approximation result is obtained from an associated rkhs, which is smaller).----------* are the results on radial bump functions intended to provide insight on approximation or depth separation? what was the motivation behind this section?----------other minor comments/typos:------ after prop. 1: ""intertwining"" appears twice------ eq. (22): missing f in l.h.s.------ eq. (23): is the first minus sign needed?------ before thm. 1: point to which appendix------ section 4.1, ""in particular, this is what would happen ... d+1"": this should be further explained------ section 4.1, final paragraph, ""in order r-norm to be"": rephrase------ section 5.4, ""required norm with three layers is finite"": which norm? maybe point to a reference? also, example 5 could be explained in further detail------ section 5.5: what is an rkhs semi-norm? you'd always have ||f|| = 0 => f = 0 in an rkhs, by the reproducing property","the article studies the set of functions expressed by a network with bounded parameters in the limit of large width, relating the required norm to the norm of a transform of the target function, and extending previous work that addressed the univariate case. the article contains a number of observations and consequences. the reviewers were quite positive about this article.","(2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks.","(2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks.","(2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks.","in this paper, the author analysis the (approximate) function class generated by an infinite-width network when the euclidean norm is bounded.","(2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks.","(2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks.",what was the motivation behind this section?----------other minor comments/typos:------ after prop.,"(2019).----------the authors show that the corresponding regularization function is more or less an l1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the ""r-norm"", which is expressed via duality through the radon transform and powers of the laplacian.----------in addition, the paper provides a number of implications of this study, such as approximation results through sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.----------overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of relu networks.",0.3707865168539326,0.0681818181818181,0.2247191011235955,0.2247191011235955,0.3707865168539326,0.0681818181818181,0.2247191011235955,0.2247191011235955,0.3707865168539326,0.0681818181818181,0.2247191011235955,0.2247191011235955,0.2682926829268292,0.0,0.1707317073170731,0.1707317073170731,0.3707865168539326,0.0681818181818181,0.2247191011235955,0.2247191011235955,0.3707865168539326,0.0681818181818181,0.2247191011235955,0.2247191011235955,0.0547945205479452,0.0,0.0547945205479452,0.0547945205479452,0.3707865168539326,0.0681818181818181,0.2247191011235955,0.2247191011235955,9.731929779052734,9.731929779052734,13.144535064697266,9.731929779052734,9.731929779052734,9.731929779052734,9.731929779052734,1.3639556169509888,0.29948430638678536,0.1634942240643558,0.058682555958796734,0.29948430638678536,0.1634942240643558,0.058682555958796734,0.29948430638678536,0.1634942240643558,0.058682555958796734,0.9298954547239314,0.9282403905609897,0.899422092419108,0.29948430638678536,0.1634942240643558,0.058682555958796734,0.29948430638678536,0.1634942240643558,0.058682587519248264,0.9404676303281148,0.9252078491075763,0.019338737636409715,0.29948430638678536,0.1634942240643558,0.058682555958796734
65,https://openreview.net/forum?id=H1lPUiRcYQ,"in response to the authors' rebuttal, i have increased my ratings accordingly. i strongly encourage the authors to include those ablative study results in the work. i also strongly recommend an ablative study on importance sampling so as to provide more quantitative results, in addition to fig. 4. finally, i hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.----------------=================================--------this paper proposes several enhancements to a neural network method for computing committor functions so that it can perform better on rare events in high-dimensional space. the basic idea is using a variational formulation with dirichlet-like boundary conditions to learn a neural committor function. the authors claim to improve a previous neural network based method by i) using a clever parameterization of the neural committor function so that it approximately satisfy the boundary condition; ii) bypassing the difficulty of rare events using importance sampling; and iii) using collective variables as feature engineering.----------------generally i feel this paper is well written and easy to understand, without requiring too much background in physics and chemistry. the application is new to most people in the machine learning community. however, --------the main contributions of this paper are empirical, and i found the experiments not very convincing. here are my main concerns:----------------1. there is almost no ablation study. the parameterization of committor function satisfies the dirichlet boundary condition, which is aesthetically pleasing. however, it's unclear how much this improves the regularization used in the previous method. similarly, without importance sampling, will the results actually become worse? what changes if the collective variables are removed? there is even no comparison with the previous neural network based method on computing committor functions, though the authors cited it.----------------2. in the experiment on extended mueller potentials, authors use the fem results as the ground truth. however, it is not clear how accurate those fem solutions are. without this being clarified, it is unclear to me that the rmse and mae results in table 1 are meaningful. maybe try some simpler problem where the committor functions can be computed exactly?----------------3. in experiments the authors often argue that results will improve when networks become deeper. however, all network architectures used in the paper are narrow and shallow when viewed from the perspective of modern deep learning. if the authors want to stress this point, i would expect to see more experimental results on neural network architectures, where you vary the depth of the network and report the change of results.----------------4. ""then we use the result as the initial network to compute the committor function at t = 300k"" => did you first train a neural committor on samples of t = 800k and use its weights as initialization to the neural committor for t = 300k? please clarify this more.----------------5. finally, i think the importance sampling technique proposed in this paper can be improved by other methods, such as annealed importance sampling. the largest dimension tested in this paper is only 66, which is still fairly small in machine learning, and i don't expect the vanilla importance sampling can work in higher dimensions. the paper proposes to apply neural network to compute the committor function arose from physics, which looks an interesting application problem by employing machine learning algorithms. typically, i know very well that the bg distribution usually has multi-modes which makes the sampling difficult extremely. the authors then employ the importance sampling for possibly explore the whole variable space. it seems to me the only possible contribution is parameterize the committor function by using a neural network.--------the committor function is parametrized by using a neural network. my first concern is the training data. how would you collect the training data? it is well-known that a neural network works best when there are plenty of training data. presumably when you are collecting the data, you are basically calculate the committor function and so that you may be able to directly solve the variational problem. --------importance sampling: i would not consider the importance sampling as a big deal concerning the contribution of the paper. you could employ a series of importance distributions which could result in many more samples. have you also looked at the uniform distribution?--------the paper targets high-dimensional problem. however, in the experiments, the problems do not look really like high-dimensional problems.--------some notations need to be clarified, for example, the nabla, the delta, and as well as the dot operator. this paper presents a method to train nns as black box estimators of the commitor function for a physical, statistical mechanical, distribution. this training is performed using samples from the distribution. as the committor function is used to understand transitions between modes of the distribution, it is important that the training samples include points between modes, which are often extremely low probability. to address this concern, this paper draws mcmc samples at a high temperature, and then uses importance weights when training the committor function using these samples. overall -- this seemed like a good application paper. it applies largely off-the-shelf machine learning techniques to a problem in physics. i don't have enough background to judge the quality of the experimental results.----------------i had one major concern: the approach in this paper is motivated as a solution to estimating commitor functions in high-d. the variance of importance sampling estimates typically increases exponentially in the dimensionality of the problem, so i suspect this technique as presented would fall apart quickly if pushed to higher dimensions. all experiments are on problems with either 9 or 10 (effective) degrees of freedom, which from the ml perspective at least is quite low dimensional, and which is consistent with this exponentially poor scaling. there are likely fixes to this problem -- e.g. the authors might want to look into annealed importance sampling*.----------------more specific comments:----------------""and dislocation dynamics"" -> ""dislocation dynamics""----------------""one can easily check"" -> ""one can check"" :p----------------eq 5 -- this is very sudden deep water! especially for an ml audience. you should either give more context for the kolmogorov backward equation, or just drop it. (the kolmogorov formulation of the problem is not used later, and for an ml audience describing the task in terms of it will confuse rather than clarify.)--------what is \delta q? does that indicate the laplacian? not standard ml notation -- define.----------------similarly, define what is intended by \partial a and \partial b (boundary of the respective regions?)----------------eq. 9 -- nit -- recommend using a symbol other than rho for regularization coefficient. visually resembles p, and is rarely used this way. lambda is very common.----------------eqs 10/11 -- include some text motivation for why the definition of chi explicitly excludes the regions inside a and b.----------------eq 14: cleverly formulated!----------------eq 14 / eq 20:--------factor of 1000 is very fast! corresponds to an epsilon of o(1e-3). you need to make sure that training samples are generated in the epsilon width border around a and b, otherwise the effect of chi will be invisible when training q_theta. so it seems like epsilon should be chosen significantly larger than this. might want to include some discussion of how to choose epsilon.----------------* totally incidental to the current context, but fascinatingly, annealed importance sampling turns out to be equivalent to the jarzynski equality in nonequilibrium physics.","this paper proposes a neural network based method for computing committor functions, which are used to understand transitions between stable states in complex systems. the authors improve over the techniques of khoo et al. with a method to approximately satisfy boundary conditions and an importance sampling method to deal with rare events. this is a good application paper, introducing a new application to the ml audience, but the technical novelty is a bit limited. the reviewers see value in the paper, however scaling w.r.t. dimensionality appears to be an issue with this approach.","4. finally, i hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.----------------=================================--------this paper proposes several enhancements to a neural network method for computing committor functions so that it can perform better on rare events in high-dimensional space.","the authors claim to improve a previous neural network based method by i) using a clever parameterization of the neural committor function so that it approximately satisfy the boundary condition; ii) bypassing the difficulty of rare events using importance sampling; and iii) using collective variables as feature engineering.----------------generally i feel this paper is well written and easy to understand, without requiring too much background in physics and chemistry.","finally, i think the importance sampling technique proposed in this paper can be improved by other methods, such as annealed importance sampling.","in response to the authors' rebuttal, i have increased my ratings accordingly.","there is even no comparison with the previous neural network based method on computing committor functions, though the authors cited it.----------------2.","4. finally, i hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.----------------=================================--------this paper proposes several enhancements to a neural network method for computing committor functions so that it can perform better on rare events in high-dimensional space.","not standard ml notation -- define.----------------similarly, define what is intended by \partial a and \partial b (boundary of the respective regions?)----------------eq.","4. finally, i hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.----------------=================================--------this paper proposes several enhancements to a neural network method for computing committor functions so that it can perform better on rare events in high-dimensional space.",0.3108108108108108,0.1506849315068493,0.2027027027027027,0.2027027027027027,0.3902439024390244,0.1358024691358025,0.2073170731707317,0.2073170731707317,0.188034188034188,0.0347826086956521,0.1025641025641025,0.1025641025641025,0.0747663551401869,0.038095238095238,0.0560747663551401,0.0560747663551401,0.2051282051282051,0.1043478260869565,0.1538461538461538,0.1538461538461538,0.3108108108108108,0.1506849315068493,0.2027027027027027,0.2027027027027027,0.1196581196581196,0.0,0.0683760683760683,0.0683760683760683,0.3108108108108108,0.1506849315068493,0.2027027027027027,0.2027027027027027,15.543004989624023,2.6170685291290283,15.602447509765623,7.449287414550781,15.543004989624023,14.5764741897583,15.543004989624023,5.334639549255371,0.9444741595457029,0.9641879777089808,0.9158952724040186,0.9649156492305142,0.9644404933020487,0.9306927987214615,0.9648130853063369,0.9640330915772479,0.9548360608099471,0.9316280464769235,0.9282449356514563,0.942737887404121,0.9443032396052708,0.9412999755206255,0.24096548453208458,0.9444741595457029,0.9641879777089808,0.9158954411929168,0.488888026589961,0.6712680686491335,0.6350319457538423,0.9444741595457029,0.9641879777089808,0.9158952724040186
66,https://openreview.net/forum?id=H1lTRJBtwB,"while this paper has some interesting experiments. i am quite confused about what exactly the author are claiming is the core contribution of their work. to me the proposed approach does not seem particularly novel and the idea that hierarchy can be useful for multi-task learning is also not new. while it is possible that i am missing something, i have tried going through the paper a few times and the contribution is not immediately obvious. the two improvements in section 3.2 seem quite low level and are only applicable to this particular approach to hierarchical rl. additionally, it is very much not clear why someone, for example, would select the approach of this paper in comparison to popular paradigms like option-critic and feudal networks. ----------the authors mention that feudal approaches ""employ different rewards for different levels of the hierarchy rather than optimizing a single objective for the entire model as we do."" why reward decomposition at the lower levels is a problem instead of a feature isn't totally clear, but this criticism does not apply to option-critic models. for option-critic models the authors claim that ""rather than the additional inductive bias of temporal abstraction, we focus on the investigation of composition as type of hierarchy in the context of single and multitask learning while demonstrating-----the strength of hierarchical composition to lie in domains with strong variation in the objectives such as in multitask domains."" first of all, i should point out that [1] looked at applying option-critic in a many task setting and found both that there was an advantage to hierarchy and an advantage to added depth of hierarchy. additionally, it is well known that option-critic approaches (when unregularized) tend to learn options that terminate every step [2]. so, if you generically apply option-critic, it would in fact be possible to disentangle the inductive bias of hierarchy from the inductive bias of temporal abstraction by using options that always terminate. ----------in comparison to past frameworks, the approach of this paper seems less theoretically motivated. it certainly does not seem justified to me to just assume this framework and disregard past successful approaches even as a comparison. while the experiments show the value of hierarchy, they do not show the value of this particular method of creating hierarchy. the feeling i get is that the authors are trying to make their experiments less about what they are proposing in this paper and more about empirical insights about the nature of hierarchy overall. if this is the case, i feel like the empirical results are not novel enough to create value for the community and too tied to a particular approach to hierarchy which does not align with much of the past work on hrl. ----------[1] ""learning abstract options"". matthew riemer, miao liu, and gerald tesauro. neurips-18. -----[2] ""when waiting is not an option: learning options with a deliberation cost"" jean harb, pierre-luc bacon, martin klissarov, and doina precup. aaai-18. this paper introduces a hierarchical policy structure for use in both single task and multitask reinforcement learning. the authors then assess the usefulness of such a structure in both settings on complex robotic tasks. these tasks include the stacking and reaching of blocks using a robotic hand, as an example. in addition to carrying out these experiments on simulated robots, the authors have also carried out experiments on a real sawyer robotic arm. ----------the particular form of their hierarchical policy for the multitask case is as follows. the policy, which is conditioned on the current state and task index consists of a gaussian mixture, where the individual gaussian densities are conditioned on the state and a context variable. the weights of this mixture are then dependent on a density on this context variable, which is conditioned on the state and task index. the intuition behind this is that the weight portion, which is called the high level component identifies task specific information, while the low level policy learns general, shareable knowledge of the different problems. ----------the authors adapt the multitask policy optimisation algorithm for their use by introducing an intermediate non-parametric policy, which is derived by setting kl bounds on the policy w.r.t to a reference policy. having derived a closed-form solution to this, they go on to learn the parametric policy of interest. ----------the authors consider 3 settings of experiments. firstly, they assess the benefits of the hierarchical structure for single task settings in a simulated environment. for the most part, they find that compared to a flat policy, the hierarchical structure shows benefits only if the initial means of the high-level components are sampled to be different. while the experimental results are shown to support this, further discussion of why this is the case would have been welcome. ----------the main benefits of the hierarchical policy are shown in the multitask case, in both simulated and real situations. in fact, the authors have shown that the hierarchical case often shows major benefits in difficult, more complicated tasks (reach vs stacking for example).----------i think that the paper was very well written. it is nicely structured, with easy to read language, and without unnecessary jargon or clutter. where necessary, the relevant extra details were provided in the appendices.----------the following are some additional notes:-----1) it would have been interesting to see how the hierarchal policy faired in new tasks that were not a part of the original training set, compared to a flat multitask policy. -----2) further details about how each task is differentiated from each other in the experiments. that is, what are their different goals, which are reflected by the reward functions. ----------as such i recommend this paper to be weak accepted. the paper is rather interesting and is able to solve some difficult tasks. a combination of different learning techniques for acquiring structure and learning with asymmetric data are used. while the combination of methods is new i am not sure that this particular combination of methods to train an hrl policy is sufficiently novel. is the authors can highlight the effects and contribution of how these methods are combined to indicate this better it would be good.----------more detailed comments:------ in section 3.2 you mention a reference policy? can you provide more details on this reference policy? ------ in the paper it is mentioned that the method collects data, including the reward for each task on a single state, action transition. this assumption seems rather strong. earlier in the paper, the authors discussed the motivation for learning transferable sub-policies. in the real world, it may not be possible to collect the reward for every kind of task simultaneously.------ the first evaluation in section 4.1 uses two humanoid environments. while these environments can be considered difficult that does not seem like the multi-task type environment the method is motivated to work well on. there is little sub-task transfer in this task.------ in section 4.1 it is noted that the version that initialized with different policy means works best. how are these means initialized?------ is the pile1 collection of tasks really separate tasks? it would be good to have some more details on how these are organized. there may not be a clear definition in the community what is considered a specific task but i am not overly convinced that these ""different"" tasks are separate. most of them look like a similar version of pick and place.------ in figure 2 the hierarchical method is similar in performance on the stack and leave (pile1) set of tasks and marginally better in the pile2 set yet does far better on the cleanup2 set. while these are all simulations with multiple tasks is there some reasoning to why each method looks similar on the pile1 set of tasks?------ for the robotic tasks, it is noted that again the baseline methods do well on the ""reach"" task. it is shown that the rhpo does much better on the stack task. it would be great if the authors can describe the interesting differences between the tasks. it is not clear how difficult the stack task is and why it is largely different from reach + grasping.------ for the images on the right of figure 4, it shows a comparison between the tasks and some ""components"" are these components the states? the ""components' are not explained well in the papers.------ there is no algorithm in the main paper which makes it a little difficult to understand the operation of the learning method. for example how exactly is the learning of the two different levels compared? it seems like they are trained together. if they are they should be compared to hiro or hac. how is temporal abstraction handled between the two policy layers if they are trained together?","this paper is concerned with improving data-efficiency in multitask reinforcement learning problems. this is achieved by taking a hierarchical approach, and learning commonalities across tasks for reuse. the authors present an off-policy actor-critic algorithm to learn and reuse these hierarchical policies.----------this is an interesting and promising paper, particularly with the ability to work with robots. the reviewers did however note issues with the novelty and making the contributions clear. additionally, it was felt that the results proved the benefits of hierarchy rather than this approach, and that further comparisons to other approaches are required. as such, this paper is a weak reject at this point.","in fact, the authors have shown that the hierarchical case often shows major benefits in difficult, more complicated tasks (reach vs stacking for example).----------i think that the paper was very well written.","while these are all simulations with multiple tasks is there some reasoning to why each method looks similar on the pile1 set of tasks?------ for the robotic tasks, it is noted that again the baseline methods do well on the ""reach"" task.","where necessary, the relevant extra details were provided in the appendices.----------the following are some additional notes:-----1) it would have been interesting to see how the hierarchal policy faired in new tasks that were not a part of the original training set, compared to a flat multitask policy.",while this paper has some interesting experiments.,"in fact, the authors have shown that the hierarchical case often shows major benefits in difficult, more complicated tasks (reach vs stacking for example).----------i think that the paper was very well written.",this paper introduces a hierarchical policy structure for use in both single task and multitask reinforcement learning.,"it is nicely structured, with easy to read language, and without unnecessary jargon or clutter.","while these are all simulations with multiple tasks is there some reasoning to why each method looks similar on the pile1 set of tasks?------ for the robotic tasks, it is noted that again the baseline methods do well on the ""reach"" task.",0.1830985915492957,0.0285714285714285,0.1126760563380281,0.1126760563380281,0.2251655629139072,0.0134228187919463,0.1324503311258278,0.1324503311258278,0.2784810126582279,0.0128205128205128,0.1139240506329113,0.1139240506329113,0.0517241379310344,0.0175438596491228,0.0517241379310344,0.0517241379310344,0.1830985915492957,0.0285714285714285,0.1126760563380281,0.1126760563380281,0.1904761904761905,0.0806451612903225,0.0952380952380952,0.0952380952380952,0.0806451612903225,0.0,0.064516129032258,0.064516129032258,0.2251655629139072,0.0134228187919463,0.1324503311258278,0.1324503311258278,9.754343032836914,10.117501258850098,10.172582626342772,7.332686901092529,10.117501258850098,6.1715898513793945,7.332686901092529,2.8688108921051025,0.4320297289476803,0.5794266832828148,0.9152117110473589,0.334425916526346,0.3815601239772439,0.9004608924081164,0.7360585008164404,0.8153663965012041,0.9373147450408065,0.9614663063869286,0.9649017416890214,0.06259142320474644,0.4320297289476803,0.5794266832828148,0.9152117110473589,0.9690162953060203,0.9692179937514805,0.9421198772673464,0.2711395822299765,0.3976553173230785,0.9508980724979064,0.334425916526346,0.3815601239772439,0.9004608924081164
67,https://openreview.net/forum?id=H1lqZhRcFm,"in this paper, the authors focus on the task of learning the value function and the constraints in unsupervised case. different from the conventional classification-based approach, the proposed algorithm uses local maxima as an indicator function. the functions c and h and two corresponding generators are trained in an adversarial way. besides, the authors analyzed that the proposed algorithm is more efficient than the conventional classification-based approach, and a suitable generalization bound is given. overall, this work is theoretically complete and experimentally sufficient.--------1. the trained c and h give different predictions in most cases. as a unsupervised method, how to deal with them?--------2. in table3, why can h achieve better results when adding noise? this paper describes a new form of one-class/set beloning learning, based on definition of 4 player game:--------- classifier player (c), which is a typical one-class classifier model--------- comparator player (h), which given two instances answers if first is ""not smaller"" (wrt. set belonging) than the other--------- classifier adversary player (gc), which tries to produce hard to distinguish samples for (c)--------- comparator adversary player (gh), which tries to produce hard to classify samples for (h)--------this way authors end up with cooperative-competitive game, where c and h act cooperatively to solve the problem, while gc and gh constantly try to ""beat"" them. ----------------overall i find this paper to be interesting and worth presenting, however i strongly encourage authors to rethink the way story is presented so that it is more approachable by people who do not have much experience with viewing typical classification problems as games. in particular, one could completely avoid talking about ""sets of local maxima"" and just talk about the density estimation problem, with c being characteristic function (of belonging to the support) and h being comparator of the pdf.----------------strong points:--------- novel, multi-agent in nature, approach to one-class classification--------- proposed method build a complex system, which can be used in much wider class of problems than just classification (due to joint optimisation of classifier and comparator)--------- extensive evaluation on 4 problems--------- nice ablation study showing that most of the benefits come from pure c/gc game (on average 68.8% acc vs 65.2% of just c, and 69.8% of entire system) but that h/gh players do indeed still improve (an extra 1%). it might be interesting to investigate what exactly changed in c due to existance of h in training. are there any identifiable properties of the model that can now be analysed?----------------weak points:--------in general i believe that theoretical analysis is the weakest part of the paper, and while interesting - it is actually a minor point, and shows interesting properties, but not the ones that would guarantee anything in ""practical setup"". i would suggest ""downplaying"" this part of the paper, maybe moving most of it to the appendix. --------to be more specific:--------- theorem 1 shows that representation can be more compact, however existance of compactness does not rely imply that this particular solution can ever be learned or that it is a good thing (number of parameters is not correlated with generalisation capabilities of the model).--------- lemma 1 seems a bit redundant for the story. while it is nice to be able to show generalisation bounds in general, this paper is not really introducing new class of models (since in the end c is going to be used for actual classification), but rather training regime, and generalisations bounds do not tell us anything about the emerging dynamical system. the fact that adding v does not constrain c too much seems quite obvious, and as a result i would suggest moving this section to appendix.--------instead, if possible, the actual tricky mathematical bit for methods like this would be, in reviewers opinion, any analysis of learning dynamics of the system like this. multi-agent systems cannot be optimised with independent gradient descent in general (convergence guarantees are lost). consequently many papers focus on methods that bring these properties back (e.g. consensus optimisation or symplectic gradient ascent). it would be beneficial for the reader to spend some time discussing stability of the system proposed, even if only empirically and on small problems.----------------other remarks:--------- eq. (1) is missing \cdot--------- it could be useful to include explicit parameters dependences in (1) and (2) so that one sees how losses really define asymmetric game between the players--------- why do we need 4 players and not just 3, with gc and gh being a single player/neural network? can we consider this as another ablation?--------- given small performance gaps in table 1 can we get error estimates/confidence intervals there? deep svdd paper includes error estimates of the baseline methods--------- since training is performed in mini batch (it does not have to be decomposible over samples) shouldn't equations be based on expectations rather than sums?----------------- the reviewer feels that the paper is hard to follow. the abstract is confusing enough and raises a number of questions. the paper talks about `""local maxima"" without defining an optimization problem. what is the optimization problem are we talking about? is it a maximization problem or minimization problem? if we are dealing with a minimization problem, why do we care about maxima?----------------the first several paragraphs did not make the problem of interest clearer. but at least the fourth paragraph starts talking about training networks (the reviewer guesses this ""network"" refers to neural network, not other types network (e.g., bayesian network) arising in machine learning). this paragraph talks about random initialization for minimizing a loss function, does this mean we are considering a minimization problem's local maxima? in addition, random initialization-based neural network training algorithms like back propagation cannot guarantee giving local maxima or local minima of the problem of interest (which is the loss function for training). it is even not clear if a stationary point can be achieved. so if the method in this paper wishes to work with local maxima of an optimization problem, this may not be a proper example.----------------the next paragraph brings out a notion of value function, which is hard to follow what it is. a suggestion is to give a much more concrete example to enlighten the readers.----------------the next two paragraphs seem to be very disconnected. it is not properly defined what is x and how to obtain it. if they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?----------------since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the ""related work"" section are really related.----------------the motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.----------------overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced. this makes evaluating this work very hard.------------------------========= after author feedback =======--------after discussing with the authors through openreview, the reviewer feels that a lot of things have been clarified. the paper is interesting in its setting, and seems to be useful in different applications. the clarity can still be improved, but this might be more of a style matter. the analysis part is a bit heavy and overwhelming and not very insightful at this moment. overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ```` accept.","the paper proposes a new unsupervised learning scheme via utilizing local maxima as an indicator function. the reviewers and ac note the novelty of this paper and good empirical justifications. hence, ac decided to recommend acceptance. however, ac thinks the readability of the paper can be improved.","the fact that adding v does not constrain c too much seems quite obvious, and as a result i would suggest moving this section to appendix.--------instead, if possible, the actual tricky mathematical bit for methods like this would be, in reviewers opinion, any analysis of learning dynamics of the system like this.","if they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?----------------since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the ""related work"" section are really related.----------------the motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.----------------overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced.","in particular, one could completely avoid talking about ""sets of local maxima"" and just talk about the density estimation problem, with c being characteristic function (of belonging to the support) and h being comparator of the pdf.----------------strong points:--------- novel, multi-agent in nature, approach to one-class classification--------- proposed method build a complex system, which can be used in much wider class of problems than just classification (due to joint optimisation of classifier and comparator)--------- extensive evaluation on 4 problems--------- nice ablation study showing that most of the benefits come from pure c/gc game (on average 68.8% acc vs 65.2% of just c, and 69.8% of entire system) but that h/gh players do indeed still improve (an extra 1%).","in this paper, the authors focus on the task of learning the value function and the constraints in unsupervised case.","set belonging) than the other--------- classifier adversary player (gc), which tries to produce hard to distinguish samples for (c)--------- comparator adversary player (gh), which tries to produce hard to classify samples for (h)--------this way authors end up with cooperative-competitive game, where c and h act cooperatively to solve the problem, while gc and gh constantly try to ""beat"" them.","if they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?----------------since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the ""related work"" section are really related.----------------the motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.----------------overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced.","in table3, why can h achieve better results when adding noise?","if they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?----------------since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the ""related work"" section are really related.----------------the motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.----------------overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced.",0.26,0.0204081632653061,0.14,0.14,0.2384105960264901,0.0805369127516778,0.1986754966887417,0.1986754966887417,0.2093023255813953,0.0352941176470588,0.1395348837209302,0.1395348837209302,0.3283582089552239,0.0307692307692307,0.1791044776119402,0.1791044776119402,0.1111111111111111,0.0,0.0925925925925925,0.0925925925925925,0.2384105960264901,0.0805369127516778,0.1986754966887417,0.1986754966887417,0.0344827586206896,0.0,0.0344827586206896,0.0344827586206896,0.2384105960264901,0.0805369127516778,0.1986754966887417,0.1986754966887417,8.801935195922852,5.622074604034424,7.371906280517578,8.801935195922852,5.688992023468018,7.592244625091553,8.801935195922852,4.971415996551514,0.9079833358823285,0.9335339688546546,0.9127738014280408,0.13309749914771205,0.27427470884744176,0.7235434727577053,0.9662538394272883,0.9642959131032646,0.69084920843069,0.9645950209674634,0.9666606284439094,0.9304032372720631,0.9348781555951052,0.963631995620776,0.0012578861199441561,0.13309749914771205,0.27427470884744176,0.7235428006161803,0.9624025902204968,0.9600499977549454,0.2645597980732653,0.13309749914771205,0.27427470884744176,0.7235440142946488
68,https://openreview.net/forum?id=H1wgawqxl,"summary:----------------the paper introduces a parametric class for non linearities used in neural networks. the paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.----------------significance:----------------the paper introduces a nice idea, and present nice experimental results. however i find the theoretical analysis not very informative, and distractive from the main central idea of the paper. -------- --------a more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. ------------------------comments: ----------------- are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?----------------- if all weights are tied across units and layers. one question that would be interesting to study , if there is an optimal non linearity. ----------------- how different is the non linearity learned if the hidden units are normalized or un-normalized. in other words how does the non linearity change if you use or don't use batch normalization? ----------------- does normalization affect the conclusion that polynomial basis fail? this paper describes an approach to learning the non-linear activation function in deep neural nets. this is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients. authors use fourier basis in the paper. a theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.----------------the main question i have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them. thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me. or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?----------------another question - in the two stage training process for cnns, when relu activation is replaced by npfc(l,t), is the npfc(l,t) activation initialized to approximate relu, or is it initialized using random coefficients?----------------few minor corrections/questions:--------- pg 2.   the interval [-l+t, l+t]  should be   the interval [-l+t, l-t]   ?--------- pg 2., equation for f(x), should it be  (-l+t) i \pi x / l  in both sin and cos terms, or without  x  ?--------- theorem 4.2   some algorithm \eps-uniformly stable  remove the word algorithm--------- theorem 4.5. sgm undefined","the authors propose a nonparametric regression approach to learn the activation functions in deep neural networks. the proposed theoretical analysis, based on stability arguments, is quite interesting. experiments on mnist and cifar-10 illustrate the potential of the approach. reviewers were somewhat positive, but preliminary empirical evidence on small datasets makes this contribution better suited for the workshop track.","or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?----------------another question - in the two stage training process for cnns, when relu activation is replaced by npfc(l,t), is the npfc(l,t) activation initialized to approximate relu, or is it initialized using random coefficients?----------------few minor corrections/questions:--------- pg 2.   the interval [-l+t, l+t]  should be   the interval [-l+t, l-t]   ?--------- pg 2., equation for f(x), should it be  (-l+t) i \pi x / l  in both sin and cos terms, or without  x  ?--------- theorem 4.2   some algorithm \eps-uniformly stable  remove the word algorithm--------- theorem 4.5. sgm undefined","a theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.----------------the main question i have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.","a theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.----------------the main question i have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.",summary:----------------the paper introduces a parametric class for non linearities used in neural networks.,"the paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.----------------significance:----------------the paper introduces a nice idea, and present nice experimental results.","or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?----------------another question - in the two stage training process for cnns, when relu activation is replaced by npfc(l,t), is the npfc(l,t) activation initialized to approximate relu, or is it initialized using random coefficients?----------------few minor corrections/questions:--------- pg 2.   the interval [-l+t, l+t]  should be   the interval [-l+t, l-t]   ?--------- pg 2., equation for f(x), should it be  (-l+t) i \pi x / l  in both sin and cos terms, or without  x  ?--------- theorem 4.2   some algorithm \eps-uniformly stable  remove the word algorithm--------- theorem 4.5. sgm undefined","however i find the theoretical analysis not very informative, and distractive from the main central idea of the paper.","a theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.----------------the main question i have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.",0.1477272727272727,0.0,0.125,0.125,0.2797202797202797,0.0709219858156028,0.1538461538461538,0.1538461538461538,0.2797202797202797,0.0709219858156028,0.1538461538461538,0.1538461538461538,0.1643835616438356,0.028169014084507,0.136986301369863,0.136986301369863,0.247191011235955,0.0689655172413793,0.1797752808988764,0.1797752808988764,0.1477272727272727,0.0,0.125,0.125,0.1794871794871795,0.0526315789473684,0.1794871794871795,0.1794871794871795,0.2797202797202797,0.0709219858156028,0.1538461538461538,0.1538461538461538,9.979425430297852,16.107158660888672,15.683650970458984,10.079463958740234,9.979425430297852,10.079463958740234,10.079463958740234,12.128578186035156,0.6694267262882462,0.6894430757415024,0.4655841274417715,0.9371319953928248,0.9398690600983062,0.9187402805606116,0.9371319953928248,0.9398690600983062,0.9187403010962909,0.9587543479171384,0.9579129342690559,0.9470729897817333,0.9709381505617821,0.9639865223445836,0.8342931778210171,0.6694267262882462,0.6894430757415024,0.4655841274417715,0.9210530070517962,0.9052310682783158,0.8879945004498951,0.9371319953928248,0.9398690600983062,0.9187403147867443
69,https://openreview.net/forum?id=H1xJjlbAZ,"the key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (simonyan et al, 2013), integrated gradient (sundararajan et al, 2017), and deeplift ( shrikumar et al, 2016) ) have large variation while predicting same output. thus the authors claim that one has to be careful about using feature importance maps.----------------pro: the paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.----------------cons:--------the main problem i have with the paper is that there is no precise definition of what constitutes the stable feature importance map. the examples in the paper seem to be cherry picked to illustrate dramatic effects. the experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs. the paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior. the authors study cases where interpretation of deep learning predictions is extremely fragile. they systematically characterize the fragility of several widely-used feature-importance interpretation methods. in general, questioning the reliability of the visualization techniques is interesting. regarding the technical details, the reviewer has the following comments: ----------------- what's the limitation of this attack method?----------------- how reliable are the interpretations? ----------------- the authors use spearman's rank order correlation and top-k intersection as metrics for interpretation similarity. ----------------- understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. the authors showed that across the test images, they were able to perturb the ordering of the training image influences. i am wondering how this will be used and evaluated in medical imaging setting.",the paper tries to show that many of the state-of-the-art interpretability methods are brittle and do not provide consistent stable explanations. the authors show this by perturbing (even randomly) the inputs so that the differences are imperceptible to a human observer but the interpretability methods provide completely different explanations. although the output class is maintained before and after the perturbation it is not clear to me or the reviewers why one shouldn't have different explanations. the difference in explanations can be attributed to the fragility of the learned models (highly non-smooth decision boundaries) rather than the explanation methods. this is a critical point and has to come out more clearly in the paper.,the paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior.,thus the authors claim that one has to be careful about using feature importance maps.----------------pro: the paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.----------------cons:--------the main problem i have with the paper is that there is no precise definition of what constitutes the stable feature importance map.,"the authors showed that across the test images, they were able to perturb the ordering of the training image influences.","the key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (simonyan et al, 2013), integrated gradient (sundararajan et al, 2017), and deeplift ( shrikumar et al, 2016) ) have large variation while predicting same output.","the key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (simonyan et al, 2013), integrated gradient (sundararajan et al, 2017), and deeplift ( shrikumar et al, 2016) ) have large variation while predicting same output.",thus the authors claim that one has to be careful about using feature importance maps.----------------pro: the paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.----------------cons:--------the main problem i have with the paper is that there is no precise definition of what constitutes the stable feature importance map.,"in general, questioning the reliability of the visualization techniques is interesting.",thus the authors claim that one has to be careful about using feature importance maps.----------------pro: the paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.----------------cons:--------the main problem i have with the paper is that there is no precise definition of what constitutes the stable feature importance map.,0.1780821917808219,0.0138888888888888,0.1095890410958904,0.1095890410958904,0.2613636363636363,0.0459770114942528,0.1249999999999999,0.1249999999999999,0.144927536231884,0.0588235294117647,0.144927536231884,0.144927536231884,0.1818181818181818,0.0122699386503067,0.1212121212121212,0.1212121212121212,0.1818181818181818,0.0122699386503067,0.1212121212121212,0.1212121212121212,0.2613636363636363,0.0459770114942528,0.1249999999999999,0.1249999999999999,0.0775193798449612,0.0157480314960629,0.0775193798449612,0.0775193798449612,0.2613636363636363,0.0459770114942528,0.1249999999999999,0.1249999999999999,13.82558250427246,11.708192825317385,11.708192825317385,13.82558250427246,7.74906587600708,7.02199125289917,13.82558250427246,7.671947479248047,0.9815112338041214,0.9813722029757489,0.9233326926980655,0.9885032574916975,0.9842993280157929,0.9155377209047858,0.9788218505175214,0.9838254260925215,0.8398076427172765,0.9841387119682036,0.9685415758089677,0.5723473151241176,0.9841387119682036,0.9685415758089677,0.5723475402604008,0.9885032574916975,0.9842993280157929,0.9155376663336354,0.9712807807172743,0.9756965094114189,0.9179547344738842,0.9885032574916975,0.9842993280157929,0.9155376458694557
70,https://openreview.net/forum?id=H5B3lmpO1g,"this paper uses several different techniques in il and rl to improve performance on 6d robot grasping. it uses an expert planner omg to collect initial data for bc as well as for online il via dagger. the uses ddpg to further train as well as fine tune on new unlabeled objects.-----the topic is very relevant and of current interest as more real world applications will need more than just 2d grasping that bin-picking has addressed and related work is sufficiently discussed.-----the technical contribution seems weak as the paper mostly explores known methods and well-known 'trade-tricks' (goal conditioning or loss on goal) towards a grasping centric problem which is also heavily explored as part of various rl tasks in literature. the main weakness of the work however is the lack of clear motivation for why such a complicated procedure is necessary compared to the expert planner already being used - the experiments aren't designed to address this question.-----using a planner as an expert for il is common practice and i don't think counts as a major contribution as presented at the end of the introduction.-----the bulk of il experiments focus on what input representation is helpful. while it is not surprising that 3d inputs like point clouds would be better for 6d grasping these are more suitable as ablations than main experiments investigating the proposed method itself, compared to other sota approaches learning based or otherwise.-----in several places 'contact-rich' and 'different dynamics' is motivated without clear explanation early on until the experiments identified what the setup was. the former does not seem to be well explored in the experiments, '...especially in those contact-rich scenarios...'. aren't all grasping problems contact rich (unless only reaching to a pre-grasp is being considered) or were there some new scenarios constructed to specifically study the relative effects of contact?-----results in table 1 and figure 4 present mean statistic from 3-5 runs. this seems small, variance bands should be shown in figure 4 to see if the small number of sample are sufficient to capture the full picture.-----the problems studied could be addressed by planar grasping as well. complex setups with clutter, etc would better motivate if the presented approach is able to scale to scenarios where 6d grasping is necessary.-----other comments:-----does not including the bc loss for some samples in the batch (or between training iteration) cause any discontinuities or make learning unstable, as the loss landscape discontinuously changes?-----[update] thank you for the responses and clarifications. i appreciate the additional experiments in the real world and comparisons with the open-loop policy. novelty still remains a concern however; in using a planner as an il expert, it isn't clear what was challenging to adopt this strategy for the grasping problem and qualifies as a significant contribution. additionally, the experiments to study 'contact-rich' and 'different dynamics' problems is unclear; the experiments don't indicate what aspects of the proposed method address these challenges and are able to do so with vision/depth-only feedback (no tactile); also the evaluation in simulation alone is insufficient to study such scenarios. i have updated my score accordingly. this paper tackles the task of closed loop 6-dof grasping of objects in simulation. the learned policy is a closed-loop policy, in that the gripper pose is continuously adjusted as the gripper approaches the object. the paper employs a combination of imitation learning, reinforcement learning, and auxiliary losses for training this policy. the policy operates upon information from point clouds as observed from a wrist-mounted camera.-----strengths: the paper tackles the important and relevant problem of closed-loop 6dof grasping. the proposed solution makes sound choices: a) uses fused point-clouds to represent the state, b) use of expert behavior for imitation, c) use of auxiliary losses for training. the paper also does systematic experiments in simulation to judge the importance of the different components.-----shortcomings: while the problem is interesting and important, and the proposed approach is sound, the experiments have entirely been done in simulation. past work on this topic has studied this problem in the real world. in particular, the paper focuses on the design of closed-loop policies. closed-loop policies are more relevant when there is noise in the motion of the robot, or there are hard to predict dynamics arising from the interaction of the gripper with the object. these are precisely the aspects that should be studied in the real world, as they are hard to model in order to study in simulation. thus, it is not clear to me as to what aspects of this paper will be applicable to the study of this problem in the real world.-----my second concern is about novelty over past work. all aspects of the technical approach of the paper have been studied in the past. use of imitation learning and rl together has been studied (eg: dapg rajeswaran et al.), use of auxiliary rewards has been studied (eg: unreal jaderberg et al.), use of hindsight experience replay (eg: andrychowicz et al.). thus, i am not sure what is the precise technical contribution made in the paper.-----thus in summary, while the paper tackles an interesting and important problem, the problem has only been studied in simulation which makes the application less interesting. at the same time, proposed approach is largely a combination of known techniques in the literature.-----update: i thank the authors for providing clarifications and additional experiments, in particular the comparison to open-loop grasping (sota grasp detection method from mousavian et al.). i still find the technical novelty of the paper limited.","the paper got mixed reviews ranging from 5 to 7. the main concerns of the reviewers were the missing novelty as the paper combines different well known methods for a given problem, so there is no big algorithmic contribution. the presented pipeline for closed-loop grasping using imitation learning from a planner, dagger and subsequent deep rl with td3 is a straightforward, but sound and intuitive combination of algorithms to address the problem of closed loop grasping. the presented results and ablation studies also motivate these algorithmic choices. in the rebuttal the authors addressed most concerns regarding the experiments (missing comparisons to open-loop grasping and real world experiments), but more real world experiments would be necessary to evaluate the effectiveness of the approach.-----this is a borderline paper were i unfortunately have to recommend rejection due to the missing algorithmic contribution, a major requirement for iclr. the paper would however fit very well to a robotics conference and the authors are encouraged to resubmit the paper the venues such as rss or corl.","novelty still remains a concern however; in using a planner as an il expert, it isn't clear what was challenging to adopt this strategy for the grasping problem and qualifies as a significant contribution.",the uses ddpg to further train as well as fine tune on new unlabeled objects.-----the topic is very relevant and of current interest as more real world applications will need more than just 2d grasping that bin-picking has addressed and related work is sufficiently discussed.-----the technical contribution seems weak as the paper mostly explores known methods and well-known 'trade-tricks' (goal conditioning or loss on goal) towards a grasping centric problem which is also heavily explored as part of various rl tasks in literature.,"additionally, the experiments to study 'contact-rich' and 'different dynamics' problems is unclear; the experiments don't indicate what aspects of the proposed method address these challenges and are able to do so with vision/depth-only feedback (no tactile); also the evaluation in simulation alone is insufficient to study such scenarios.",this paper uses several different techniques in il and rl to improve performance on 6d robot grasping.,the uses ddpg to further train as well as fine tune on new unlabeled objects.-----the topic is very relevant and of current interest as more real world applications will need more than just 2d grasping that bin-picking has addressed and related work is sufficiently discussed.-----the technical contribution seems weak as the paper mostly explores known methods and well-known 'trade-tricks' (goal conditioning or loss on goal) towards a grasping centric problem which is also heavily explored as part of various rl tasks in literature.,the main weakness of the work however is the lack of clear motivation for why such a complicated procedure is necessary compared to the expert planner already being used - the experiments aren't designed to address this question.-----using a planner as an expert for il is common practice and i don't think counts as a major contribution as presented at the end of the introduction.-----the bulk of il experiments focus on what input representation is helpful.,"the proposed solution makes sound choices: a) uses fused point-clouds to represent the state, b) use of expert behavior for imitation, c) use of auxiliary losses for training.","additionally, the experiments to study 'contact-rich' and 'different dynamics' problems is unclear; the experiments don't indicate what aspects of the proposed method address these challenges and are able to do so with vision/depth-only feedback (no tactile); also the evaluation in simulation alone is insufficient to study such scenarios.",0.1818181818181818,0.0096618357487922,0.1052631578947368,0.1052631578947368,0.2671755725190839,0.0461538461538461,0.1450381679389313,0.1450381679389313,0.247787610619469,0.0178571428571428,0.1327433628318584,0.1327433628318584,0.1047120418848167,0.0,0.06282722513089,0.06282722513089,0.2671755725190839,0.0461538461538461,0.1450381679389313,0.1450381679389313,0.3241106719367589,0.0637450199203187,0.1739130434782609,0.1739130434782609,0.1182266009852216,0.0,0.0788177339901477,0.0788177339901477,0.247787610619469,0.0178571428571428,0.1327433628318584,0.1327433628318584,6.991496562957764,10.73513126373291,14.205793380737305,10.73513126373291,7.299222469329834,5.558374404907227,5.558374404907227,6.41170597076416,0.9198839349008924,0.9476085392126585,0.01687004627976881,0.9605281375622682,0.9685071195316346,0.009525756122397071,0.9681744183512003,0.9567216840866392,0.921141526174187,0.9684664233065323,0.9733397222102179,0.90109940251169,0.9605281375622682,0.9685071195316346,0.009525772726602467,0.9451251672136837,0.9625217259655389,0.9297294173649018,0.9453034170660042,0.9547907761754751,0.9252285670050201,0.9681744183512003,0.9567216840866392,0.9211415124479646
71,https://openreview.net/forum?id=H92-E4kFwbR,"summary-----this paper tackles the problem of adversarial training for the image classification task. it proposed a novel adversarial training method called composite adversarial training (cat) against combined attacks constructed by multiple perturbations. first, cat is based on the composite adversarial attacks, in which the attackers explore different sources of perturbations. second, cat leverages the composite adversarial attacks as the inner loop for optimization during the training. the experimental evaluations have been focused on comparing the proposed cat with existing robust training methods including adversarial training with pgd attacks, avg, max (tramer and boneh, 2019), and msd (maini et al. 2020) on mnist and cifar-10 classification benchmarks.-----comments-----this paper studies an important problem in adversarial machine learning. the paper is well-motivated with novel technical contributions (section 3.1) supported by reasonably designed experiments. however, reviewer feels the submission in the current form is a borderline case mainly due to mixed or inconclusive experimental results.-----w1: the clean accuracy of cat (table 1 - 4, first row, last column) is significantly worse than methods such as avg & max and msd, especially on cifar-10 where the accuracy drops 20+% (i assume the state-of-the-art model has 90+% accuracy for the 10-way classification on cifar-10). this seems to be a major weakness of the proposed method. reviewer understands the tradeoff between clean accuracy and accuracy under attack, but not sure how much value it is given the proposed defense method sacrifices too much on the clean accuracy. what makes it worse, this is just the performance drop of 10-way classification on cifar dataset. reviewer is worried if this gap is even more significant on cifar-100 or imagenet (w/ 1000 classes). it would be good to have some ablation studies.-----w2: besides the drop on clean accuracy, reviewer fails to see a clear winner between msd and cat (see the last two columns in table 1 and table 2). cat seems to be more robust to composite attacks but not as robust as msd on other attacks. such comparisons are missing in section 4.2 (pixel perturbation and spatial transformations). it would be good to comment on this.-----w3: it would be good to report the computational cost (e.g., number of iterations in optimization, training time) of the proposed composite training method and explain how it is compared to the existing methods.-----minor1 (applied for all the tables): it would be good to mention each row is a different attack method and each column is a different defense (robust training) method. it is not crystal clear at the first glance. summary-----this paper proposes adversarial training with a novel threat model. specifically, the authors propose to compose multiple adversaries, such as the ones based on l_p norm and spatial transform, in a predefined order to create a strong adversary in the adversarial training. the paper empirically demonstrated that the composite adversary is effective against previous adversarial defense mechanisms. it also demonstrated that the proposed adversarial training can lead to the classifier robust against the composite attack as well as the individual or union of multiple adversaries.-----pros-----the composite adversary seems to be novel and effective in terms of both adversarial attack and adversarial training.-----the paper is generally well-written and easy to read.-----the experiment results convey comprehensive evaluation and analysis. i especially enjoyed that it covers various attack scenarios, such as the ones with unseen attacks and composite attack with a random order, etc.-----concerns & suggestions-----it is not clear why the composited thread model can be stronger than individual or union attacks as claimed by the authors. if there are some theoretical justifications/proofs, it would be interesting to see such discussions (e.g., the composited attack consistently leads to higher classification loss (inner maximization of adversarial training objective)).-----although i appreciate authors for their comprehensive experiments, the current results are based on fairly small and easy datasets and it would be still interesting to see the results on more complex datasets such as cifar-100 or mini-imagenet.-----it is unclear how exactly the l_p attacks are implemented. in section 4.1, the authors mentioned various methods for l_p attacks, such as pgd, fgsn, c&w, deepfool, salt&pepper, etc., but it is unclear how they are actually used in the experiments, for instance in table 1 and 2.-----it would be clear if authors add constraints on total attack budget on eq.(8)-----table 5 & 6: please clarify that the rows are the adversarially-trained models and columns are threats. it is confusing since rows and columns are different from the previous tables.-------- post rebuttal update ---------the authors successfully addressed my initial concerns regarding more analysis and experiments on a larger dataset. therefore, i keep my rating weak accept. the authors propose a method for dealing with composite adversarial attacks, which are defined as a sequence of perturbation operators each applying some constrained perturbation to the output of the previous operator. their method models the composed adversarial examples-----x-----as the sum of the unperturbed example with a series of perturbations-----i-----which maximize the estimator's loss. they compare their results to other existing adversarial training methods against multiple types of adversarial attacks.-----pros:-----interesting idea, seems like a very natural continuation of existing work-----good experimental design, results are reasonably thorough-----some results are encouraging-----cons:-----explanation of method (cat) is somewhat lacking. it's not clear to me exactly what their method does differently than the baselines explained in the background.-----results are mixed with discussion focusing almost entirely on the positive parts. for example, cat consistently performs significantly worse than baselines on ""clean accuracy"" and worse than one or more baselines on other singular attacks (see tables 1,2,3,4).-----results in section 5.2 lack explanation (i.e. what do the table columns/rows actually mean)-----minor formatting issues-----overall, i think the central problem that the authors are trying to solve is important and their work makes a reasonable contribution towards the solution. despite the apparent mixed results, this paper should be a candidate for acceptance.-----additional comments for the authors:-----it would be helpful to provide references for the definitions of ""robust accuracy"" and ""clean accuracy""; i'm sure these are metrics that have been defined and used in prior work but this can sometimes make it difficult for outside readers to find where they are rigorously defined.-----as mentioned in the cons, you should make it more clear what the reader should be looking for in the tables. reading just by the accuracy scores, it seems like cat often performs worse or about the same as baselines in multiple experiments.-----table captions should be above, not below, the table. this particularly problematic with table 4/figure 4 where the table caption looks like the title of figure 4.-----as mentioned before, equation 8 does not (for me) satisfactorily explain what cat actually does.-----in equation 8,-----i-----appears in the constraint but not in the expression; perhaps you meant to write:-----x=argmaxx(m)(f(x(m)+i,y)-----the distinction between the different indexing notations-----xi-----and-----x(i)-----is not always clear-----it's not clear what the notation means in tables 5, 6, and 7 and how it relates to ""ordering"" of perturbations. summary:-----this paper proposed an interesting new form of adverserial attack (composite adversarial attack) as well as an algorithm to defend this form of attack (cat). the new form of attack is constructed as a composition of different individual perturbation models including pixel perturbation and spatial transformations. the cat is proposed to defend both individual attack and composite attack by penalising the maximum accuracy loss during the sequential generation process of a composite attack perturbation. empirical experiments comparing the proposed algorithm under both individual and composite attacks are conducted on benchmark datasets against baseline methods. the proposed cat outperformed the baselines under composite attacks. further analysis and discussion on different variations of composite attack as well as cat are also presented with possible future exploration directions.-----pros:-----the paper is well structured in general and easy to understand.-----the idea of composite attack is interesting and meaningful to the neural network adversarial attacking area.-----the proposed method improves the network robustness under composite attack.-----the detailed analysis on composite attacks is valuable.-----cons: my main concern with the paper is the general performance of the proposed algorithm and the fairness of the comparison.-----while the paper claims outstanding performance on individual perturbation model attacks, it is not always true across the two dataset. and the proposed algorithm always presents a lower clean accuracy in most of the experiment settings by a relatively large margin. there seems to be a clear tradeoff between the clean accuracy and the robustness towards a more aggressive attack (composite attack). the result limits the strength of the algorithm.-----i am concerned about the fairness of the comparison against baseline methods like max/ avg. since the paper used pretrained models from previous work, max/avg baseline models are trained based on eq(2)(3) and evaluated under the composite attack. in this case, the underlying perturbation space considered in eq(2)(3) is different from (smaller than) the one in composite attack. (e.g. true maximum perturbation will not never be considered when training these models)-----another question is: what does alpha mean for baseline methods during training? is alpha used to rescale the perturbation during baseline training or not? if not, then figure 4 presents very limited information since alpha is an unfair information available to the proposed model. if yes, then isnt the whole experiment a scaling version of the main results?-----other comments:-----i would move the introduction of spatial transformation perturbation to section 2 as it is part of the fundamentals.-----some details of baselines in appendix a should be moved to the main text to provide a more self-contained experiment section. e.g. how the baseline models are trained.-----it would be nice to bold the best performance number in the tables.-----post-rebuttal-----i would like to thank the authors for their efforts to improve the methods and the draft. part of my concerns was resolved. for clean accuracy, cat-r did provide a better trade-off. however, it is improved after the submission deadline, it can't be counted into the original contribution in theory. for the concern that the comparison to the baseline presents unfairness as the proposed method was designed for the composite attack with a larger perturbation space, i think the author agrees with my point to some extend. i decided to keep my original score deal to the remaining weakness in the paper.","i thank authors and reviewers for discussions. reviewers found the paper (specially the cat-r method proposed in the rebuttal period) interesting but there are some remaining concerns about the significance of the results and experiments. given all, i think the paper still needs a bit of more work before being accepted. i encourage authors to address comments raised by the reviewers to improve their paper.-----ac",it is confusing since rows and columns are different from the previous tables.-------- post rebuttal update ---------the authors successfully addressed my initial concerns regarding more analysis and experiments on a larger dataset.,"further analysis and discussion on different variations of composite attack as well as cat are also presented with possible future exploration directions.-----pros:-----the paper is well structured in general and easy to understand.-----the idea of composite attack is interesting and meaningful to the neural network adversarial attacking area.-----the proposed method improves the network robustness under composite attack.-----the detailed analysis on composite attacks is valuable.-----cons: my main concern with the paper is the general performance of the proposed algorithm and the fairness of the comparison.-----while the paper claims outstanding performance on individual perturbation model attacks, it is not always true across the two dataset.",it proposed a novel adversarial training method called composite adversarial training (cat) against combined attacks constructed by multiple perturbations.,summary-----this paper tackles the problem of adversarial training for the image classification task.,"further analysis and discussion on different variations of composite attack as well as cat are also presented with possible future exploration directions.-----pros:-----the paper is well structured in general and easy to understand.-----the idea of composite attack is interesting and meaningful to the neural network adversarial attacking area.-----the proposed method improves the network robustness under composite attack.-----the detailed analysis on composite attacks is valuable.-----cons: my main concern with the paper is the general performance of the proposed algorithm and the fairness of the comparison.-----while the paper claims outstanding performance on individual perturbation model attacks, it is not always true across the two dataset.",it proposed a novel adversarial training method called composite adversarial training (cat) against combined attacks constructed by multiple perturbations.,this seems to be a major weakness of the proposed method.,"further analysis and discussion on different variations of composite attack as well as cat are also presented with possible future exploration directions.-----pros:-----the paper is well structured in general and easy to understand.-----the idea of composite attack is interesting and meaningful to the neural network adversarial attacking area.-----the proposed method improves the network robustness under composite attack.-----the detailed analysis on composite attacks is valuable.-----cons: my main concern with the paper is the general performance of the proposed algorithm and the fairness of the comparison.-----while the paper claims outstanding performance on individual perturbation model attacks, it is not always true across the two dataset.",0.2424242424242424,0.0206185567010309,0.1414141414141414,0.1414141414141414,0.284090909090909,0.0344827586206896,0.1818181818181818,0.1818181818181818,0.1162790697674418,0.0,0.0697674418604651,0.0697674418604651,0.1234567901234567,0.0,0.0987654320987654,0.0987654320987654,0.284090909090909,0.0344827586206896,0.1818181818181818,0.1818181818181818,0.1162790697674418,0.0,0.0697674418604651,0.0697674418604651,0.1794871794871795,0.0263157894736842,0.0769230769230769,0.0769230769230769,0.284090909090909,0.0344827586206896,0.1818181818181818,0.1818181818181818,20.261287689208984,11.587728500366213,13.916040420532228,11.587728500366213,4.7341156005859375,20.261287689208984,11.587728500366213,5.333357810974121,0.41527667157943904,0.6552827517261233,0.8599922720558224,0.2635749769507982,0.35290761031866635,0.7039429766482865,0.9573058530052171,0.961646321836489,0.6581439088846867,0.972197568577882,0.9700611607175537,0.9486208718739048,0.2635749769507982,0.35290761031866635,0.7039429766482865,0.9573058530052171,0.961646321836489,0.6581448429871005,0.9558635379864828,0.954436851636093,0.9315175231271882,0.2635749769507982,0.35290761031866635,0.7039426190005713
72,https://openreview.net/forum?id=HJcSzz-CZ,"this paper proposes to extend the prototypical network (nips17) to the semi-supervised setting with three possible --------strategies. one consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the --------assigned pseudo-labels. another is able to deal with the case of distractors i.e. unlabeled samples not beloning to--------any of the known categories. in practice this second solution is analogous to the first, but a general 'distractor' class--------is added. finally the third technique learns to weight the samples according to their distance to the original prototypes.----------------these strategies are evaluated in a particular semi-supervised transfer learning setting: the models are first trained --------on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting--------multiple times a large dataset), then they are used on a final target task with again few labeled data and large --------unlabeled samples but beloning to a different set of categories.----------------+ the paper is well written, well organized and overall easy to read--------+/- this work builds largely on previous work. it introduces only some small technical novelty inspired by soft-k-means--------clustering that anyway seems to be effective.--------+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of--------semantic relatedness between the source and the target sets----------------few notes and questions--------1) why for the omniglot experiment the table reports the error results? it would be better to present accuracy as for the other tables/experiments--------2) i would suggest to use source and target instead of train and test -- these two last terms are confusing because--------actually there is a training phase also at test time.--------3) although the paper indicate that there are different other few-shot methods that could be applicable here, --------no other approach is considered besides the prothotipical network and its variants. an further external reference --------could be used to give an idea of what would be the experimental result at least in the supervised case. this paper is an extension of the prototypical network which will be published in nips 2017. the classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode. the paper solves a new semi-supervised situation, which is more close to the setting of the real world, with an extension of the prototype network. sufficient implementation detail and analysis on results.----------------however, this is definitely not the first work on semi-supervised formed few-shot learning. there are plenty of works on this topic [r1, r2, r3]. the authors are advised to do a thorough survey of the relevant works in multimedia and computer vision community. -------- --------another concern is that the novelty. this work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data. ----------------the experiments are also not enough. not only some other works such as [r1, r2, r3]; but also the other nave baselines should also be compared, such as directly nearest neighbor classifier, logistic regression, and neural network in traditional supervised learning. additionally, in the 5-shot non-distractor setting on tiered imagenet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset?----------------[r1] videostory: a new multimedia embedding for few-example recognition and translation of events, in acm mm, 2014----------------[r2] transductive multi-view zero-shot learning, ieee tpami 2015----------------[r3] video2vec embeddings recognize events when examples are scarce, ieee tpami 2014","the paper extends the earlier work on prototypical networks to semi-supervised setting. reviewers largely agree that the paper is well-written. there are some concerns on the incremental nature of the paper wrt to the novelty aspect but in the light of reported empirical results which show clear improvement over earlier work and given the importance of the topic, i recommend acceptance.","additionally, in the 5-shot non-distractor setting on tiered imagenet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset?----------------[r1] videostory: a new multimedia embedding for few-example recognition and translation of events, in acm mm, 2014----------------[r2] transductive multi-view zero-shot learning, ieee tpami 2015----------------[r3] video2vec embeddings recognize events when examples are scarce, ieee tpami 2014","finally the third technique learns to weight the samples according to their distance to the original prototypes.----------------these strategies are evaluated in a particular semi-supervised transfer learning setting: the models are first trained --------on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting--------multiple times a large dataset), then they are used on a final target task with again few labeled data and large --------unlabeled samples but beloning to a different set of categories.----------------+ the paper is well written, well organized and overall easy to read--------+/- this work builds largely on previous work.","this paper is an extension of the prototypical network which will be published in nips 2017. the classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode.",this paper proposes to extend the prototypical network (nips17) to the semi-supervised setting with three possible --------strategies.,"finally the third technique learns to weight the samples according to their distance to the original prototypes.----------------these strategies are evaluated in a particular semi-supervised transfer learning setting: the models are first trained --------on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting--------multiple times a large dataset), then they are used on a final target task with again few labeled data and large --------unlabeled samples but beloning to a different set of categories.----------------+ the paper is well written, well organized and overall easy to read--------+/- this work builds largely on previous work.","finally the third technique learns to weight the samples according to their distance to the original prototypes.----------------these strategies are evaluated in a particular semi-supervised transfer learning setting: the models are first trained --------on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting--------multiple times a large dataset), then they are used on a final target task with again few labeled data and large --------unlabeled samples but beloning to a different set of categories.----------------+ the paper is well written, well organized and overall easy to read--------+/- this work builds largely on previous work.",the authors are advised to do a thorough survey of the relevant works in multimedia and computer vision community.,"finally the third technique learns to weight the samples according to their distance to the original prototypes.----------------these strategies are evaluated in a particular semi-supervised transfer learning setting: the models are first trained --------on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting--------multiple times a large dataset), then they are used on a final target task with again few labeled data and large --------unlabeled samples but beloning to a different set of categories.----------------+ the paper is well written, well organized and overall easy to read--------+/- this work builds largely on previous work.",0.1690140845070422,0.0285714285714285,0.1126760563380281,0.1126760563380281,0.3151515151515151,0.0736196319018404,0.1818181818181818,0.1818181818181818,0.2641509433962264,0.0576923076923076,0.169811320754717,0.169811320754717,0.2716049382716049,0.1265822784810126,0.2222222222222222,0.2222222222222222,0.3151515151515151,0.0736196319018404,0.1818181818181818,0.1818181818181818,0.3151515151515151,0.0736196319018404,0.1818181818181818,0.1818181818181818,0.1951219512195122,0.0249999999999999,0.1463414634146341,0.1463414634146341,0.3151515151515151,0.0736196319018404,0.1818181818181818,0.1818181818181818,7.738184928894043,7.738184928894043,13.623821258544922,7.738184928894043,7.409911632537842,8.49833869934082,7.738184928894043,3.872866153717041,0.6917259414469105,0.7066569027884245,0.875163408276428,0.9810347875942996,0.9772624519586458,0.9321739239278953,0.9632621707194701,0.9651342666618198,0.46497271149359903,0.974488321068079,0.9726774061265928,0.8996479580270634,0.9810347875942996,0.9772624519586458,0.9321739239278953,0.9810347875942996,0.9772624519586458,0.9321739239278953,0.9375009809241367,0.9474069266546031,0.9318637283955781,0.9810347875942996,0.9772624519586458,0.9321739239278953
73,https://openreview.net/forum?id=HJeRveHKDH,"in this paper, the authors propose a new class of programs they call programming puzzles. the authors argue that this class of programs is ideal for helping learn ai systems to reason. the second contribution of the paper is an adaptive method of puzzle generation inspired by gan-like generation that can generate a diverse and difficult set of programs. the paper shows that the generated puzzles are reasonably difficult to solve (using the time to solve as a measure of difficulty) and reasonably diverse. ----------i found the paper well-written and easy to understand. the methodology to generate programs is convincing. i am not sure time to solve is the best way to measure the complexity of the program, but it seems a reasonable proxy. did the authors study if the program length is correlated to the time to execute? if the correlation holds, then can complex programs not be created by simply having a bias towards longer programs? that would be a strong baseline to compare against. ----------i may have missed something, but i understand that only the guided solver is trainable. if that is the case, then why do we see increase in solving time for other solvers (table 1). in only the case of the guided solver can the generator adaptively increase the complexity of the programs. ----------overall, i feel that the paper puts forth an interesting class of programs. but there are some gaps in the evaluation and the baselines. i am also not sure how this class of programs can help advance artificial reasoning. ----------feedback:------ the authors should provide more references to the solvers used in section 5. ------ the paper ends abruptly. a summary/conclusion would be useful. ----------i am not an expert in this area, and i am willing to revise my recommendation if the authors can address these issues. this paper proposes a method for generating hard puzzles with a trainable puzzle solver. this is an interesting and important problem which sits at the intersection of symbolic and deep learning based ai. the approach is largely gan inspired, where the neural solver takes the role of a discriminator, and the generator is trained with reinforce instead of plain gradient descend.----------although i'm not an expert in this area, i have found this paper well written and easy to follow. the problem is well motivated, and the approach is sensible. as this is a novel problem, the paper also defines their own metric, namely the average time taken to solve the puzzle by given solvers, and the diversity of generated puzzles. it is nice to see that the generator indeed learns to generate puzzles that are significantly harder than random counterparts, while maintaining reasonable diversity. although i think these are convincing results, my question to the authors is: have you tried or considered other ways of evaluating the generated puzzles? e.g., if you train the guided search solver on the generated puzzles and evaluate it on a random set of puzzles, would you see an improvement? i think this would be interesting to see, which can serve as an alternative evaluation metric.----------my other comments are regarding the experiment section:-----1. it would be useful to provide references to the solvers used, both in the adversarial training phase and the evaluation phase, if there is any.-----2. more details of the training process would also be valuable. e.g., the training time and stability, common failure modes if any.----------minors:-----1. figure f3 should be s.count(""a"")==1000 and s.count(""aa"")==0 -----2. first sentence under fig 1, one is give -> one is given-----3. figure 5, f2: 2**(x**2)) == 16 -> 2**(x**2) == 16","the authors introducing programming puzzles as a way to help ai systems learn about reasoning. the authors then propose a gan-like generation algorithm to generate diverse and difficult puzzles.----------this is a very novel problem and the authors have made an interesting submission. however, at least 2 reviewers have raised severe concerns about the work. in particular, the relation to existing work as pointed by r2 was not very clear. further, the paper was also lacking a strong empirical evaluation of the proposed ideas. the authors did agree with most of the comments of the reviewers and made changes wherever possible. however, some changes have been pushed to future work or are not feasible right now. ----------based on the above observations, i recommended that the paper cannot be accepted now. the paper has a lot of potential and i would strongly encourage a revised submission addressing the questions/suggestions made by the reviewers.","the approach is largely gan inspired, where the neural solver takes the role of a discriminator, and the generator is trained with reinforce instead of plain gradient descend.----------although i'm not an expert in this area, i have found this paper well written and easy to follow.",the second contribution of the paper is an adaptive method of puzzle generation inspired by gan-like generation that can generate a diverse and difficult set of programs.,"in this paper, the authors propose a new class of programs they call programming puzzles.","in this paper, the authors propose a new class of programs they call programming puzzles.",the second contribution of the paper is an adaptive method of puzzle generation inspired by gan-like generation that can generate a diverse and difficult set of programs.,"as this is a novel problem, the paper also defines their own metric, namely the average time taken to solve the puzzle by given solvers, and the diversity of generated puzzles.","i think this would be interesting to see, which can serve as an alternative evaluation metric.----------my other comments are regarding the experiment section:-----1. it would be useful to provide references to the solvers used, both in the adversarial training phase and the evaluation phase, if there is any.-----2.",the second contribution of the paper is an adaptive method of puzzle generation inspired by gan-like generation that can generate a diverse and difficult set of programs.,0.2178217821782178,0.0099999999999999,0.1188118811881188,0.1188118811881188,0.2197802197802197,0.0666666666666666,0.1098901098901098,0.1098901098901098,0.1183431952662721,0.0359281437125748,0.0710059171597633,0.0710059171597633,0.1183431952662721,0.0359281437125748,0.0710059171597633,0.0710059171597633,0.2197802197802197,0.0666666666666666,0.1098901098901098,0.1098901098901098,0.2162162162162162,0.0546448087431694,0.1513513513513513,0.1513513513513513,0.2048780487804878,0.0098522167487684,0.1170731707317073,0.1170731707317073,0.2197802197802197,0.0666666666666666,0.1098901098901098,0.1098901098901098,9.974942207336426,14.79526710510254,18.125431060791016,14.79526710510254,8.88217544555664,18.125431060791016,14.795266151428224,8.192314147949219,0.9399014322772729,0.9516561168109564,0.9149765708302571,0.9660208577592444,0.9608870965909326,0.9323456197307597,0.9662247451511354,0.9655768292516151,0.9371512912819724,0.9662247451511354,0.9655768292516151,0.9371513331763084,0.9660208577592444,0.9608870965909326,0.9323456197307597,0.9270544188885551,0.9402036799488029,0.921484330488024,0.9597600843424147,0.9705030560304843,0.682222536610665,0.9660208577592444,0.9608870965909326,0.9323456197307597
74,https://openreview.net/forum?id=HJezF3VYPB,"this paper introduces an unsupervised federated domain adaptation (ufda) problem and proposes a new model called federated adversarial domain adaptation (fada) to transfer the knowledge learned from distributed source domains to an unlabeled target domain. this paper uses a dynamic attention mechanism by leveraging the gap statistics to transfer distributed source knowledge. this paper also proposes a method to disentangle the domain-invariant features from domain-specific features, using adversarial training. moreover, a theoretical generalization bound for ufda is derived. an extensive empirical evaluation is performed on ufda vision and linguistic benchmarks.----------this paper should be rejected because the total pipeline seems ad-hoc except for optimizing the weight of the source domain in the attention mechanism. although the derivation of generalization bound for fda in sec.3 is excellent, it only demonstrates the importance of the weight . this result is trivial if we assume to have the same source domain as the target and utterly unrelated source domain to the target domain. it seems that proving why minimizing the gap statistics contributes to fada is more essential in the dynamic attention mechanism. because representation disentanglement has no relation with the derived theory, it would be better to clarify whether this method is original or not.----------in the ufda setting, the reviewer has doubts about whether it is realistic that the source node has a rich labeled data assuming our smartphones. also, the assumption that the system cannot access the source data but must access all source feature seems a significant limitation in terms of privacy issues and communication cost between the target node and the source nodes.----------it is unclear what is the final target classifier. if the target can access the teaching signal (e.g., labels or tags) in the source domains, it would be better to mention whether this situation violates the assumption the authors raised or not.----------minor comments-----1) what is t(p, q, \theta) in the section of representation disentanglement?----------2) what is c_s in eq.6? c_{s_i}?----------3) in fig.3, it is not proper to discuss the size of intra-class variance by just looking at the figures because the t-sne is a non-linear mapping. it is better to show quantitative scores, such as the value of the fisher criterion. the authors present a novel algorithm for dealing with domain adaptation in the setting of federated learning (classification, specifically). that is, they tackle the issue of learning a model on a new domain when access to the data points used in training the source models is not possible due to privacy constraints. the approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain.----------the authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains. this bound shows that the weighted sum of divergences in the symmetric difference hypothesis space controls the generalization error, so the authors aim at deriving feature representations and using aggregation weights that ensure this weighted sum is small. ----------the authors use a novel dynamic attention model to get the aggregation weights: they cluster the features in the target domain, and measure how much the intra-cluster variation decreases when information from a given source domain is incorporated. the aggregation weights for the model updates on the target domain are then weighed using a softmax transform of these contribution weights. ----- -----the motivation up through and including section 3 is clear, the theoretical results are presented clearly, but the model details in section 4 are unclear:------ in the dynamic attention mechanism, how does one a priori choose the number of clusters in computing the gap statistics, and what is the impact? ------ the notation in the federated adversarial alignment section is unclear: what *exactly* are the model coefficients theta that are being updated?------ the statement ""optimize following objective"" is made several times. this is ambiguous, and should be corrected to ""miminize"" following objective.------ the representation disentanglement process is intricate, and only vaguely addressed. how does one fit the neural net and use (8)? where is the l2 reconstruction loss balanced with the mutual information? the vagueness of this section means algorithm 1 is not well-specified.----------the experiments are reasonable, and compare to baseline domain adaptation methods.----------the problem considered is of interest, and the approach is novel and interesting. however, the algorithm is not described in sufficient detail. after reading the paper, and spending considerable time rereading section 4, i still do not understand how algorithm 1 is implemented in practice. for that reason i lean towards reject. i will update my score if the authors clarify the details of algorithm 1. ----------comments:------ the symmetric difference hypothesis space is incorrectly called the hdeltah divergence in section 3 the paper proposed and studied the unsupervised federated domain adaption problem, which aims to transfer knowledge from source nodes to a new node with different data distribution. to address the problem, a federated adversarial domain adaption (fada) algorithm is introduced in the paper. the key idea of the algorithm is to update the target model by aggregating the gradients from source nodes, and also leverage adversarial adaption techniques to reduce the discrepancy between source features and target features. overall, the problem studied in the paper is interesting, theoretical analysis on the error bound is provided in the paper, and the effectiveness of the proposed method has been validated in various datasets. although the technical contributions of the paper are solid, i still have several concerns about it.-----1. the proposed algorithm is not described very clearly in section 4. according to the paper, di is used to identify the domain from the output of gi and gt and align the features from those domains, then how is it related to the disentanglement in eq 6. also in eq 6, symbol c_s was not introduced in the previous context, which makes it confusing to understand this objective.----------2. it would be better if the author(s) can provide some complexity analysis of the proposed algorithm.----------3. the paper still contains some typos and unresolved reference issues.","this paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement.----------reviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. while many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. ac believes that the updated version is acceptable.----------hence i recommend acceptance.","because representation disentanglement has no relation with the derived theory, it would be better to clarify whether this method is original or not.----------in the ufda setting, the reviewer has doubts about whether it is realistic that the source node has a rich labeled data assuming our smartphones.","the approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain.----------the authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains.","the approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain.----------the authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains.",this paper introduces an unsupervised federated domain adaptation (ufda) problem and proposes a new model called federated adversarial domain adaptation (fada) to transfer the knowledge learned from distributed source domains to an unlabeled target domain.,this paper introduces an unsupervised federated domain adaptation (ufda) problem and proposes a new model called federated adversarial domain adaptation (fada) to transfer the knowledge learned from distributed source domains to an unlabeled target domain.,this paper introduces an unsupervised federated domain adaptation (ufda) problem and proposes a new model called federated adversarial domain adaptation (fada) to transfer the knowledge learned from distributed source domains to an unlabeled target domain.,this paper introduces an unsupervised federated domain adaptation (ufda) problem and proposes a new model called federated adversarial domain adaptation (fada) to transfer the knowledge learned from distributed source domains to an unlabeled target domain.,"the approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain.----------the authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains.",0.1999999999999999,0.0468749999999999,0.123076923076923,0.123076923076923,0.2516556291390728,0.0268456375838926,0.1456953642384106,0.1456953642384106,0.2516556291390728,0.0268456375838926,0.1456953642384106,0.1456953642384106,0.2905982905982905,0.1043478260869565,0.2393162393162393,0.2393162393162393,0.2905982905982905,0.1043478260869565,0.2393162393162393,0.2393162393162393,0.2905982905982905,0.1043478260869565,0.2393162393162393,0.2393162393162393,0.2905982905982905,0.1043478260869565,0.2393162393162393,0.2393162393162393,0.2516556291390728,0.0268456375838926,0.1456953642384106,0.1456953642384106,17.005084991455078,17.005084991455078,17.005084991455078,7.44022274017334,7.223791599273682,7.44022274017334,7.44022274017334,17.005084991455078,0.9539858800446861,0.9608683511083799,0.07918107983798935,0.9696227237061633,0.9725309757731455,0.8942261126467549,0.9696227237061633,0.9725309757731455,0.8942260930166492,0.9806496944311898,0.9748687514489787,0.9391651945867913,0.9806496944311898,0.9748687514489787,0.9391652785557941,0.9806496944311898,0.9748687514489787,0.9391652785557941,0.9806496944311898,0.9748687514489787,0.9391651945867913,0.9696227237061633,0.9725309757731455,0.8942261126467549
75,https://openreview.net/forum?id=HJgLlgBKvH,"# readability review----------i am a complete outsider to the field of theoretical analysis of model parallelism, although i'm well familiar with deep-----learning and backpropagation in general. thus, i'm going to review this paper in terms of readability and writing flow-----for an outsider rather than provide domain-specific feedback.----------the intent of the paper is clear for an outsider. the proposed solution is not.-----at a high level, my understanding is that the work proposes to parallelize backpropagation by updating multiple-----blocks/layers of a model in parallel, which requires sometimes using ""stale"" gradients/activations from previous-----timesteps as the ""real"" ones have not been computed yet. this is communicated reasonably well in section 1. however, the-----specific parallelization insight is never explained at a high level and lost in notation (especially for a reader-----unfamiliar with the recomputation technique). i appreciated figure 3 that attempted to illustrate the process on an-----example with k=3, but it was not self-sufficient to communicate the intuition. figure 3 makes use of the concepts-----defined in section 3.3, and thus illustrates algorithm 2 rather than explains its high-level ideas. without a high-level-----intuition, by the time the reader gets to section 3.3, they are already lost.----------layer-wise staleness is introduced in section 3.1 but is only used in theorems 1-2. it doesn't seem to have an effect on-----the actual algorithm, only on its analysis. as such, it's confusing when introduced. instead, i'd recommend starting-----section 3 with introducing timestamps and a notion of possible computation across timestamps (especially explaining how-----it differs from vanilla backpropagation). this understanding is crucial for section 3.2, but the notation t is not.----------figure 1 needs a number of improvements to adequately present the state of the field for an outsider on a first reading:-----* k (number of blocks/layers) is not defined in section 1, only later in section 2, but figure 1 uses it.-----* column 1, ""method"", needs citations to link it to the literature without having to read the main text.-----* not all of the methods mentioned in the main text appear in the figure.-----* it is unclear whether all the computation boundaries must be aligned. for example, in vanilla bp the ""forward"" phase----- at k=0 must finish before the ""forward"" phase k=1 starts. is it also true when, say, in fr the ""forward"" phase at k=1----- ends right when the ""backward"" phase at k=0 begins, or is it just an artifact of how the figure is drawn?----- more generally, the figure shows the time when each phase happens but doesn't show the dependency graph between phases.----------why does algorithm 1 appear? it's just vanilla bp.----------the colors in figures 4-5 are difficult to distinguish, especially different shades of blue or red.----------i struggle to see why dsp would improve the test accuracy of a network aside from just making the training faster. the-----networks in table 3 go through the same number of updates, correct? dsp uses noisier gradients for its updates, which-----might act as a regularizer, but this is just speculation. one could try to test it by adding random noise to the true-----gradients of bp (with magnitude similar to the expected magnitude of the bp-dsp divergence at a corresponding epoch). if-----it similarly improves the performance of that network, that might be evidence towards this hypothesis. in general,-----gradient noise does not necessarily improve the network performance unless tuned to the architecture/task.----------in addition, the numbers in table 3 must be listed with their corresponding standard deviation across multiple runs.-----otherwise it's difficult to ascertain whether the dsp improvement is real or thanks to a lucky seed.----------spelling:------ section 3.1: ""...the data is forwarded...""----------in summary, i have to give this paper a weak reject score for readability. i'm happy to increase it if my concerns are-----addressed in a new pdf version during the discussion period. summary: this paper proposes an approach for model parallelism without forward or backward locking (i.e., layers dont have to wait to perform computation). they maintain forward and backward queues per block of layers, where each block of layers is on a different device. each block pops activations from the previous blocks forward output queue, computes the output activations and pushes into its forward output queue. then it pops the gradient queue from the next block, computes gradients of its parameters and the corresponding input, updates its parameters, and pushes the input gradient into the queue. this technique introduces gradient staleness. they evaluate resnets on cifar-10, cifar-100, and imagenet.----------comments:-----* there are a number of prior works that explore the same idea of pipelining forward and backward calculations that are not cited and compared [1, 2, 3]. the novelty of this work is low given these works.-----* this paper is poorly written. it is overly notation heavy and does not give a clear description of how the method works. the diagrams only add to the confusion instead of making it easier to understand. it takes a long time to understand what ends up being a very simple method.-----* the baselines in this paper are weak. the resnet-164 gets 92.8%, but the original resnet paper [4] has a resnet-101 that gets 93.6% accuracy. this seems like a nitpicking detail, but the the problem with asynchronous training is not that it can diverge, but that the performance is often lower. without having a strong baseline, its hard to tell whether the stale gradients are affecting performance. the imagenet baselines are similarly concerning.-----* no gpipe comparison in the experiments. furthermore, the gpipe speedup comparison is unfair because these seem like two separate implementations of a training pipeline. its hard to tell whether this approach will have speedups over gpipe if the same codebase is used.----------in conclusion, the lack of comparison with prior work, poor writing, and weak experimental results leads to a suggestion of rejection.----------[1] performance analysis of a pipelined backpropagation parallel algorithm. alain petrowski, gerard dreyfus, and claude girault-----[2] ampnet: asynchronous model-parallel training for dynamic neural networks. alexander l. gaunt, matthew a. johnson, maik riechert, daniel tarlow, ryota tomioka, dimitrios vytiniotis, sam webster-----[3] pipedream: fast and efficient pipeline parallel dnn training. aaron harlap, deepak narayanan, amar phanishayee, vivek seshadri, nikhil devanur, greg ganger, phil gibbons-----[4] deep residual learning for image recognition. kaiming he, xiangyu zhang, shaoqing ren, jian sun this paper proposes a solution to improve the efficiency of distributed cnn training, which is an important and practical problem in accelerated training using multiple devices. the paper presents diversely stale parameters (dsp) that applies different staleness to the blocks of cnn layers, which this work refers to as layer-wise staleness. each block saves the input data while using it to compute initial activations in the block, making the output data of the block that will be taken by the next block. the saved input data is used once again: when the next block returns the error gradient, the activations within the block are recomputed from the saved input data that matches the error gradient's timestamp, and the error gradient for this block is computed with these recomputed activations and the next block's error gradient. the recomputation uses the current set of parameters that has been updated since the initial computation; the recomputed activations may differ from the initial activations. this design breaks data dependencies between forward and backward computation and creates more opportunities to overlap computation and communication of different layers. the paper analyzes the convergence property of dsp, and compares the empirical performance of dsp with standard back propagation (bp), the decoupled neural interface, the features replay, and gpipe.----------i enjoyed reading this paper since it is written well, but i am not certain whether this paper should be accepted because the paper currently does not answer the following questions that are relevant to the main contributions of this paper.----------1. would it be possible to explain why the proposed solution does not maintain a queue for parameters? such a queue could allow computing h = f_k(h_k^{n_k-m_k}; x_k^{n_k-m_k}) instead of h = f_k(h_k^{n_k-m_k}; x_k) in algorithm 2. this makes the recomputation of activations in the backward pass the same as those in the forward pass, which makes it simpler to reason about convergence. for cnns, the parameters of layers are much smaller than the activation of layers, especially with large batches enabled by pipelining, so the memory size of a parameter queue may not be large compared to the total memory size of block-level queues. a similar idea was explored in prior work: pipedream [sosp'19]; https://arxiv.org/abs/1806.03377.----------2. how good is time-to-accuracy with dsp on a large dataset such as imagenet? on cifar-10, the test accuracy is often better when some amount of staleness exists (as shown in sparsification-based gradient compression papers), which makes it unsurprising to see better accuracy on dsp than on bp. however, this result is less common on imagenet, indicating that the empirical performance gain could also be due to the choice of the dataset, not just due to the training algorithm. in fact, figure 5 (after magnification) seems to show that dsp has worse accuracy and loss compared to bp's on imagenet. the paper does not state the precise numbers, and makes it difficult for me to compare the time-to-accuracy of dsp with bp and fr. in addition, dsp introduces some staleness to parameter updates while gpipe adds no staleness to parameter updates, so it would be also interesting to see whether the small speedup difference between dsp (x2.7) and gpipe (x2.2) still translates into a time-to-accuracy difference.----------3. what would be the practical benefit of overlapping the mini-batch forward and the mini-batch recomputation? if such overlapping is possible, it implies that the mini-batch recomputation alone was underutilizing the device memory as the forward computation still requires some memory; in other words, when not overlapping these stages, it might be possible to use a larger batch size or a different layer block size, which may improve computational efficiency and save all-reduce traffic. since this paper tries to make a conscious decision on optimizing the memory use of distributed training, it would make the paper stronger if it examines the potentially increased memory use while overlapping forward and recomputation stages and explains why overlapping these stages is better than using the maximum amount of memory for individual stages.","this paper presents a upper bound on the curvature of a deep network. after the discussion, the author has addressed some concerns of reviwers, but the results are not very strong, there is some limitation on the applications. there is no strong support for this paper. due to the high standard of iclr, the acceptance of the paper need strong results in terms of theory or experiments.","its hard to tell whether this approach will have speedups over gpipe if the same codebase is used.----------in conclusion, the lack of comparison with prior work, poor writing, and weak experimental results leads to a suggestion of rejection.----------[1] performance analysis of a pipelined backpropagation parallel algorithm.","the saved input data is used once again: when the next block returns the error gradient, the activations within the block are recomputed from the saved input data that matches the error gradient's timestamp, and the error gradient for this block is computed with these recomputed activations and the next block's error gradient.","its hard to tell whether this approach will have speedups over gpipe if the same codebase is used.----------in conclusion, the lack of comparison with prior work, poor writing, and weak experimental results leads to a suggestion of rejection.----------[1] performance analysis of a pipelined backpropagation parallel algorithm.","# readability review----------i am a complete outsider to the field of theoretical analysis of model parallelism, although i'm well familiar with deep-----learning and backpropagation in general.","each block pops activations from the previous blocks forward output queue, computes the output activations and pushes into its forward output queue.","in general,-----gradient noise does not necessarily improve the network performance unless tuned to the architecture/task.----------in addition, the numbers in table 3 must be listed with their corresponding standard deviation across multiple runs.-----otherwise it's difficult to ascertain whether the dsp improvement is real or thanks to a lucky seed.----------spelling:------ section 3.1: ""...the data is forwarded...""----------in summary, i have to give this paper a weak reject score for readability.","dsp uses noisier gradients for its updates, which-----might act as a regularizer, but this is just speculation.","the saved input data is used once again: when the next block returns the error gradient, the activations within the block are recomputed from the saved input data that matches the error gradient's timestamp, and the error gradient for this block is computed with these recomputed activations and the next block's error gradient.",0.208695652173913,0.0176991150442477,0.1391304347826087,0.1391304347826087,0.2131147540983607,0.0166666666666666,0.1475409836065573,0.1475409836065573,0.208695652173913,0.0176991150442477,0.1391304347826087,0.1391304347826087,0.1473684210526316,0.021505376344086,0.1263157894736842,0.1263157894736842,0.0449438202247191,0.0,0.0449438202247191,0.0449438202247191,0.2535211267605634,0.0285714285714285,0.1408450704225352,0.1408450704225352,0.1176470588235294,0.0,0.0705882352941176,0.0705882352941176,0.2131147540983607,0.0166666666666666,0.1475409836065573,0.1475409836065573,6.605580806732178,-1.4847218990325928,16.302230834960938,2.617715358734131,7.997169494628906,7.997169494628906,2.617715835571289,5.775526523590088,0.3173498744739248,0.4078930110047811,0.7567525939625724,0.7395158008395781,0.8348038732781013,0.9332047331066468,0.3173498744739248,0.4078930110047811,0.7567523184834132,0.9763528787365153,0.9703675688482049,0.8999642896314812,0.5157667628383815,0.5879359653110416,0.45688493615273157,0.7437109712029207,0.7705188165884966,0.9361414996231695,0.9530501133123447,0.9591767416608773,0.9029472626653642,0.7395158008395781,0.8348038732781013,0.9332047966931986
76,https://openreview.net/forum?id=HJlU-AVtvS,"this paper examined the spectrum of nngp and ntk kernels and answer several questions about deep networks using both analytical results and experimental evidence:-----* are randomly initialized and trained deep networks biased to simple functions?-----* how does this change with depth, activation function, and initialization?----------all studies are conducted on a space of inputs that is a boolean cube. the input distribution is assumed to be uniform. though it is argued in section 3 that the results also generalize to uniform distributions on spheres and isotropic gaussian distributions. although this boolean cube setting is followed from previous works on the same topic, it does limit the scope of the paper. discussions on how this assumption relates to practical problems are missing from the paper.----------putting aside the limitations of restricting the input distributions on boolean cubes (and other similar choices), i really like the paper, which demonstrates the powerfulness of spectral analysis. i also found that many analytical results (e.g., computing eigenvalues of a kernel operator with respect to uniform distributions on a boolean cube) in the paper are highly nontrivial to derive, which adds to the value of the paper. these results might seem restricted in terms of deep network theory because of the assumptions on input distributions, but i do believe the methods used can be of interest to a wider audience.----------some questions:-----* in figure 1, the 10^4 boolean function samples are sorted according to frequency (rank). what precisely is the frequency (rank) here? it shouldn't be the frequency that corresponds to the eigendecomposition because each function sample could always have multiple components with different frequencies.-----* in figure 1b, the y-axis is described as normalized eigenvalues, which seems different from degree k fractional variance defined in the next section. the degree k fractional variance is the sum of all normalized eigenvalues for degree k eigenfunctions. is this difference intended or it is a mistake?-----* is the ground truth degree k polynomial used in experiments defined somewhere in the paper?----------on writing and clarity. overall i find this paper well-written and a pleasure to read. some minor issues are-----* the definition of ""neural kernels"" seems unnecessary and a bit sudden. it would be helpful to include the definition of phi just after eq. (2) for ck and ntk.-----* for introducing boolean analysis and fourier series, it might be better to include the formula that explicit shows the expansion f(x) = \sum_{s} f^p(s) x_s(x) before introducing theorem 3.1. aiming to resolve the question whether and why deep networks are biased towards simple functions, this paper gives a spectral analysis on neural networks' conjugate kernel(ck) and neural tangent kernel(ntk) on boolean cube. the eigenfunctions are identified and the eigenvalues are shown computable in polynomial time. another main contribution of this paper is showing that the simplicity bias exists at least in a weak sense.----------i believe that this paper should be weakly rejected because it made more claims than what it can show in that the analysis doesn't work in real space, and the authors did not really show the simplicity bias. the following are my detailed comments.----------first, the whole analysis is based on boolean cube. although the paper has shown empirically that in high dimension the uniform binary distribution is close enough to the uniform sphere distribution, it doesn't suffice to substitute boolean cube for sphere in real space. the spectral analysis in this paper is heavily due to working on boolean cube. the boolean cube is finite, which guarantees any inner-product kernel function can be diagonalized by finite many monomial functions, and there are only o(d) different eigenvalues, which enables efficient computation. these techniques are not easy to be transferred to real space. the experiment shows that the first five eigenvalues in boolean cube, sphere, gaussian is close, but key problems here are first, in practive the dimension could be smaller and second, sphere and gaussian have infinitely many eigenvalues while boolean cube has eigenvalues. the experiment cannot really justify that all eigenvalues are close (only first several are shown), not to mention the tail eigenvalues over the first -th.----------even if we assume that boolean cube is a reasonable choice, we should notice the goal of computing eigenvalues is to eventually show the inductive bias toward 'simple functions'. however, the authors failed to show it at least from the following perspectives:-----1) this paper did not show the trend of eigenvalues, but only the weak version of, for example, . in the limiting case, it is more reasonable to fix dimension rather than the degree .-----2) working on boolean cube leads to limited complexity. the most complicated base function is restricted to where . so the weak simplicity bias theorem actually only describes the relation among finite eigenvalues.-----3) no optimization arguments appear in this paper. based on the spectral analysis, it is not rigorous enough to claim the networks are biased to simple functions, given that the target function consists of simple multilinear monomial functions. ----------since the boolean spectra is not a reliable measure, the further experiments under such a measure is therefore put under doubt.----------to summarize, this paper definitely contains some rigorous analysis which i appreciate, but it made some claims that are not verified. more importantly, the boolean cube is not the appropriate domain which is hard to generalize to real space and the simplicity bias theorem in this paper is to some extent weak. therefore, i suggest rejecting this paper in its current form.","the authors develop a spectral analysis on the boolean cube for the neural ""conjugate kernel"" (ck) and ""tangent kernel"" (ntk). the analysis sheds light into inductive biases of neural networks, such as whether they are biased to simple functions. ----------this work contains rigorous analysis and theory which is useful for further discussions. however, the theory and insights do not feel complete. one important drawback is that the analysis is limited by the boolean cube setting; this also means that it is more difficult to link theory to practical scenarios. this has been discussed a lot during the rebuttal and among reviewers. empirical validation has attempted to deal with these concerns, but it would be useful to have this validation coming from theory, or at least have further relevant theoretical insights. this could happen by further building on the theorem provided in the rebuttal for eigenvalue behavior when d is large.","these results might seem restricted in terms of deep network theory because of the assumptions on input distributions, but i do believe the methods used can be of interest to a wider audience.----------some questions:-----* in figure 1, the 10^4 boolean function samples are sorted according to frequency (rank).","(2) for ck and ntk.-----* for introducing boolean analysis and fourier series, it might be better to include the formula that explicit shows the expansion f(x) = \sum_{s} f^p(s) x_s(x) before introducing theorem 3.1. aiming to resolve the question whether and why deep networks are biased towards simple functions, this paper gives a spectral analysis on neural networks' conjugate kernel(ck) and neural tangent kernel(ntk) on boolean cube.","another main contribution of this paper is showing that the simplicity bias exists at least in a weak sense.----------i believe that this paper should be weakly rejected because it made more claims than what it can show in that the analysis doesn't work in real space, and the authors did not really show the simplicity bias.","this paper examined the spectrum of nngp and ntk kernels and answer several questions about deep networks using both analytical results and experimental evidence:-----* are randomly initialized and trained deep networks biased to simple functions?-----* how does this change with depth, activation function, and initialization?----------all studies are conducted on a space of inputs that is a boolean cube.","(2) for ck and ntk.-----* for introducing boolean analysis and fourier series, it might be better to include the formula that explicit shows the expansion f(x) = \sum_{s} f^p(s) x_s(x) before introducing theorem 3.1. aiming to resolve the question whether and why deep networks are biased towards simple functions, this paper gives a spectral analysis on neural networks' conjugate kernel(ck) and neural tangent kernel(ntk) on boolean cube.","another main contribution of this paper is showing that the simplicity bias exists at least in a weak sense.----------i believe that this paper should be weakly rejected because it made more claims than what it can show in that the analysis doesn't work in real space, and the authors did not really show the simplicity bias.","these results might seem restricted in terms of deep network theory because of the assumptions on input distributions, but i do believe the methods used can be of interest to a wider audience.----------some questions:-----* in figure 1, the 10^4 boolean function samples are sorted according to frequency (rank).","(2) for ck and ntk.-----* for introducing boolean analysis and fourier series, it might be better to include the formula that explicit shows the expansion f(x) = \sum_{s} f^p(s) x_s(x) before introducing theorem 3.1. aiming to resolve the question whether and why deep networks are biased towards simple functions, this paper gives a spectral analysis on neural networks' conjugate kernel(ck) and neural tangent kernel(ntk) on boolean cube.",0.19,0.0,0.1,0.1,0.3466666666666667,0.1255605381165919,0.1333333333333333,0.1333333333333333,0.2211538461538461,0.0388349514563106,0.1153846153846153,0.1153846153846153,0.2392344497607655,0.038647342995169,0.1435406698564593,0.1435406698564593,0.3466666666666667,0.1255605381165919,0.1333333333333333,0.1333333333333333,0.2211538461538461,0.0388349514563106,0.1153846153846153,0.1153846153846153,0.19,0.0,0.1,0.1,0.3466666666666667,0.1255605381165919,0.1333333333333333,0.1333333333333333,4.916928768157959,9.525188446044922,13.764986038208008,9.525188446044922,9.03317928314209,4.916928768157959,9.525188446044922,9.03317928314209,0.9653406463042056,0.9636136628458148,0.9458887705844073,0.95628550335872,0.9597253490416341,0.8465075623698459,0.9546595152148148,0.9540464384663135,0.7691230708304534,0.9856082329786273,0.9853031547251363,0.9064610931285063,0.95628550335872,0.9597253490416341,0.8465072299629488,0.9546595152148148,0.9540464384663135,0.7691228667905999,0.9653406463042056,0.9636136628458148,0.945888798177426,0.95628550335872,0.9597253490416341,0.8465072299629488
77,https://openreview.net/forum?id=HJlY0jA5F7,"the paper proposes a new evaluation metric for generative adversarial networks and shows that it is better aligned with human judgment than fid. the metric is based on a domain-specific encoder to extract features of the image rather than imagenet inception network and a class-aware frechet distance which makes a gaussian mixture assumption for the extracted features rather than a simple gaussian assumption for fid. the paper shows an advantage for the new metric vs the others by constructing examples where fid fails while the proposed metric doesn't. although this is an interesting finding, it is not a breakthrough in the sense that a domain-specific representation is expected to be better behaved than the features of the inception classifier and using a gaussian mixture would be an obvious step after fid. moreover, other metrics don't even rely on any assumption on the features distributions [1,2], so i would expect them to behave at least as well as the proposed metric. ------------------------[1] :m. arjovsky, s. chintala, l. bottou, wasserstein gan--------[2] :m. binkowski, d. j. sutherland, m. arbel, and a. gretton. demystifying mmd gans. the authors study the task of sample-based quantitative evaluation applied to gans. the authors suggest multiple modifications to existing evaluation pipelines: (1) instead of embedding the samples in the inceptionnet feature space, train a domain-specific encoder. if labeled data is available, add a cross-entropy loss to the encoder training objective so that the class can be predicted. (2) instead of fitting a single gaussian in the feature space, fit a gmm instead. this should allow for a more fine-grained class-aware distance between the (empirical) distributions. ----------------pro: --------attempt to attack a critical issue in generative modeling. good overview of competing approaches.--------several ablation studies of evaluation measures and the behavior of fid with respect to the representation space.--------the ideas make sense on a conceptual level, albeit suffering from major practical concerns.----------------con:--------- clarity can be improved (e.g. use of double negatives as in the top of page 3), the same arguments repeated multiple (>3) times (i.e. deficiencies of fid and is, etc.), many statements which should be empirically tested are stated as folklore (last paragraph on page 3). in general the paper merits another polishing pass (mode != model, last paragraph in section 3, unmatch, etc.).--------- why would a vae capture a good feature space? it is known that the tradeoff between what is stored in the latent space versus the discriminator *completely* depends on the power of the discriminator -- if the discriminator is flexible enough it can just learn the marginal distribution and ignore the latent code. hence, this subtle issue will likely undermine the entire model comparison.--------- using the predictive distribution as a soft label for cafd. interesting idea, but why would one have access to labels in the first place? why wouldn't one use a conditional gan if we already have labels? secondly, why would the modes necessarily correspond to classes?--------- stated issues with fid: why would you expect fid to be resistant to such drastic transformations as blocking out a significant proportion of pixels with blocks? this is a *major* change in the underlying distribution. the fact that humans can fill in this gap should have nothing to do with the quality of the underlying model. arguably, you can also hide one eye, the nose and the mouth and still judge the sample as good.----------------the ideas presented in this paper are conceptually interesting. however, given the drawbacks discussed above i cannot recommend the acceptance of this work.","the paper proposes a novel sample based evaluation metric which extends the idea of fid by replacing the latent features of the inception network by those of a data-set specific (v)ae and the fid by the mean fid of the class-conditional distributions. furthermore, the paper presents interesting examples for which fid fails to match the human judgment while the new metric does not. all reviewers agree, that while these ideas are interesting, they are not convinced about the originality and significance of the contribution and believe that the work could be improved by a deeper analysis and experimental investigation.","although this is an interesting finding, it is not a breakthrough in the sense that a domain-specific representation is expected to be better behaved than the features of the inception classifier and using a gaussian mixture would be an obvious step after fid.",the metric is based on a domain-specific encoder to extract features of the image rather than imagenet inception network and a class-aware frechet distance which makes a gaussian mixture assumption for the extracted features rather than a simple gaussian assumption for fid.,the metric is based on a domain-specific encoder to extract features of the image rather than imagenet inception network and a class-aware frechet distance which makes a gaussian mixture assumption for the extracted features rather than a simple gaussian assumption for fid.,the paper proposes a new evaluation metric for generative adversarial networks and shows that it is better aligned with human judgment than fid.,the metric is based on a domain-specific encoder to extract features of the image rather than imagenet inception network and a class-aware frechet distance which makes a gaussian mixture assumption for the extracted features rather than a simple gaussian assumption for fid.,"good overview of competing approaches.--------several ablation studies of evaluation measures and the behavior of fid with respect to the representation space.--------the ideas make sense on a conceptual level, albeit suffering from major practical concerns.----------------con:--------- clarity can be improved (e.g. use of double negatives as in the top of page 3), the same arguments repeated multiple (>3) times (i.e. deficiencies of fid and is, etc.",the paper shows an advantage for the new metric vs the others by constructing examples where fid fails while the proposed metric doesn't.,the metric is based on a domain-specific encoder to extract features of the image rather than imagenet inception network and a class-aware frechet distance which makes a gaussian mixture assumption for the extracted features rather than a simple gaussian assumption for fid.,0.2328767123287671,0.0416666666666666,0.136986301369863,0.136986301369863,0.2602739726027397,0.0416666666666666,0.1643835616438356,0.1643835616438356,0.2602739726027397,0.0416666666666666,0.1643835616438356,0.1643835616438356,0.224,0.08130081300813,0.16,0.16,0.2602739726027397,0.0416666666666666,0.1643835616438356,0.1643835616438356,0.2339181286549707,0.0473372781065088,0.1637426900584795,0.1637426900584795,0.2380952380952381,0.0806451612903225,0.1746031746031746,0.1746031746031746,0.2602739726027397,0.0416666666666666,0.1643835616438356,0.1643835616438356,7.194619178771973,10.95160675048828,13.322381019592283,10.95160675048828,8.498639106750488,10.95160675048828,10.95160675048828,13.6148099899292,0.9586462825368696,0.9572892970127822,0.8840608448233365,0.9627070483117955,0.962485312537508,0.6739994278945345,0.9627070483117955,0.962485312537508,0.6739999442677022,0.9747616925744174,0.9704275038823247,0.8967919428070751,0.9627070483117955,0.962485312537508,0.6739998846609666,0.966611914858561,0.9320344749693232,0.17693997839898168,0.9686844015397956,0.9651160949701959,0.9026175010523766,0.9627070483117955,0.962485312537508,0.6739998846609666
78,https://openreview.net/forum?id=HJlnC1rKPB,"this paper studies the recent application of attention based transformer networks for image classification tasks and asks the question as to the similarity of functions learned by these attention networks with the standard convolutional networks. ----------first the paper theoretically proves that a multi head self attention layer (appropriately defined for a 3 dimensional input) can represent a convolutional filter. the proof is based on constructing weights for the attention layers that results in a convolution operation. this construction uses rather crucially the relative positional encodings for the self attention layer. the paper claims that the results can be extended to other forms of positional encodings. ----------it looks like the construction is correct as far as i can tell. one caveat is that, it looks like, the weights of the attention layer need to be arbitrarily large (\alpha in lemma 2) to exactly represent the convolution layer. i think this is not possible to avoid for exact representation. a comment on this after the results will be nice.----------finally the paper presents experiments on the cifar10 dataset. the paper shows that the multihead attention units in the lower layers learn to attend on grid like structures on pixels, similar to a conv filter. i find the experiments to be nicely complementing the theoretical results, even though they are limited to the cifar10 dataset.----------overall i think this paper takes a nice step towards understanding the similarities and differences between the attention and conv layers, and i suggest acceptance.----------minor:-----first sentence in intro raise -> rise. the paper shows both theoretically and in practice that self-attention can learn to act as convolutions. the main intuition is that every attention head can learn to attend individually to a given relative offset around each pixel. given enough heads (k**2) such a layer can imitate a convolution with kernel size (k,k). this leads to the conclusion that self-attention is at least as powerful as cnns are. this fact has been acknowledged by (at least part of) the community for a while (following a similar intuition) but as far as i know has never been formalized. hence, although incremental i consider this an important contribution. the derivation of quadratic relative encoding is a nice theoretical construction. experiments show improvements over learned relative attention, however, experiments are merely conducted on cifar.----------finally, even though the contributions are somewhat marginal and the experiments are not quite enough to establish the new relative attention mechanism as being superior, i like this paper and consider its contributions valuable. the message of the paper deserves a larger audience and i therefore lean to accept despite some shortcomings.","this paper studies the relationship between attention networks such as transformers and convolutional networks. the paper shows that a special case of attention can be cast as convolution. however this link depends on using relative positional embeddings and generalization to other encodings are not given in the paper. the reviewers found the results correct, but we caution that the writing should better reflect the caveats of the approach.","the paper shows that the multihead attention units in the lower layers learn to attend on grid like structures on pixels, similar to a conv filter.","experiments show improvements over learned relative attention, however, experiments are merely conducted on cifar.----------finally, even though the contributions are somewhat marginal and the experiments are not quite enough to establish the new relative attention mechanism as being superior, i like this paper and consider its contributions valuable.","experiments show improvements over learned relative attention, however, experiments are merely conducted on cifar.----------finally, even though the contributions are somewhat marginal and the experiments are not quite enough to establish the new relative attention mechanism as being superior, i like this paper and consider its contributions valuable.",this paper studies the recent application of attention based transformer networks for image classification tasks and asks the question as to the similarity of functions learned by these attention networks with the standard convolutional networks.,"experiments show improvements over learned relative attention, however, experiments are merely conducted on cifar.----------finally, even though the contributions are somewhat marginal and the experiments are not quite enough to establish the new relative attention mechanism as being superior, i like this paper and consider its contributions valuable.","experiments show improvements over learned relative attention, however, experiments are merely conducted on cifar.----------finally, even though the contributions are somewhat marginal and the experiments are not quite enough to establish the new relative attention mechanism as being superior, i like this paper and consider its contributions valuable.",this paper studies the recent application of attention based transformer networks for image classification tasks and asks the question as to the similarity of functions learned by these attention networks with the standard convolutional networks.,"experiments show improvements over learned relative attention, however, experiments are merely conducted on cifar.----------finally, even though the contributions are somewhat marginal and the experiments are not quite enough to establish the new relative attention mechanism as being superior, i like this paper and consider its contributions valuable.",0.2340425531914893,0.108695652173913,0.1489361702127659,0.1489361702127659,0.3103448275862069,0.0350877192982456,0.1551724137931034,0.1551724137931034,0.3103448275862069,0.0350877192982456,0.1551724137931034,0.1551724137931034,0.3495145631067961,0.1188118811881188,0.2524271844660194,0.2524271844660194,0.3103448275862069,0.0350877192982456,0.1551724137931034,0.1551724137931034,0.3103448275862069,0.0350877192982456,0.1551724137931034,0.1551724137931034,0.3495145631067961,0.1188118811881188,0.2524271844660194,0.2524271844660194,0.3103448275862069,0.0350877192982456,0.1551724137931034,0.1551724137931034,6.403513908386231,6.403513908386231,8.793275833129883,6.403513908386231,10.070534706115724,6.403513908386231,6.403513908386231,8.793275833129883,0.971351039668453,0.9747049179287226,0.8385996971555375,0.9655180147030761,0.9690751101287324,0.5165864402307067,0.9655180147030761,0.9690751101287324,0.5165860229771405,0.9821188670169775,0.9798320648804539,0.946126616688418,0.9655180147030761,0.9690751101287324,0.5165863364116514,0.9655180147030761,0.9690751101287324,0.5165864402307067,0.9821188670169775,0.9798320648804539,0.9461266833428884,0.9655180147030761,0.9690751101287324,0.5165863364116514
79,https://openreview.net/forum?id=HJx54i05tX,"building on the recent progresses in the analysis of random high-dimensional statistics problem and in particular of message passing algorithm, this paper analyses the performances of weighted tied auto-encoder. technically, the paper is using the state evolution formalism. in particular the main theorem uses the analysis of the multi-layer version of these algorithm, the so-called state evolution technics, in order to analyse the behaviour of optimal decoding in weight-tied decoder. it is based on a clever trick that the behaviour of the decoding is similar to the one of the reconstruction on a multilayer estimation problem. this is a very orginal use of these technics.----------------the results are 3-folds: (i) a deep analysis of the limitation of weight-tied dae, in the random setting, (ii) the demonstration of the sensitivity to perturbations and (iii) a clever method for initialisation that to train a dae.----------------pro: a rigorous work, a clever use of the recent progresses in rigorous analysis of random neural net, and a very deep answer to interesting questions, and --------con: i do not see much against the paper. a minor comment: the fact that the dae is ""weight-tied"" is fundamental in this analysis. it actually should be mentioned in the title! this work applies infinite width limit random network framework (a.k.a. mean field analysis) to study deep autoencoders when weights are tied between encoder and decoder. random network analysis allows to have exact analysis of asymptotic behaviour where the network is infinitely deep (but width taken to infinite first). this exact analysis allows to answer some theoretical questions from previous works to varying degrees of success. ----------------building on the techniques from poole et al (2016) [1], schoenholz et al (2017) [2], the theoretical analysis to deep autoencoder with weight tied encoder/decoder shows interesting properties. the fact that the network component are split into encoder/decoder architecture choice along with weight tying shows various interesting phase of network configuration. ----------------main concern with this work is applicability of the theoretical analysis to real networks. the autoencoding samples on mnist provided in the appendix at least visually do not seem to be a competitive autoencoder (e.g. blurry and irrelevant pixels showing up). ----------------also the empirical study with various schemes is little hard to parse and digest. it would be better to restructure this section so that the messages from theoretical analysis in the earlier section can be clearly seen in the experiments.----------------the experiments done on fixed learning rate should not be compare to other architectures in terms of training speed as learning rates are sensitive to the architecture choice and speed may be not directly comparable. ----------------questions/comments--------- without weight tying the whole study is not much different from just the feedforward networks. however, as noted by the authors vincent et al (2010) showed that empirically autoencoders with or without weight tying performs comparably. what is the benefit of analyzing more complicated case where we do not get a clear benefit from? ----------------- many auto encoding networks benefit from either bottleneck or varying the widths. the authors regime is when all of the hidden layers grows to infinity at the same order. would this limit capture interesting properties of autoencoders?----------------- when analysis is for weight tied networks, why is encoder and decoder assume to have different non-linearity? it does show interesting analysis but is it a practical choice? from this work, would you recommend using different non-linearities?----------------- it would be interesting to see how this analysis is applied to denoising autoencoders [3], which should be straightforward to apply similar to dropout analysis appeared in schoenholz et al [2].----------------[1] ben poole, subhaneil lahiri, maithra raghu, jascha sohl-dickstein, and surya ganguli. exponential--------expressivity in deep neural networks through transient chaos. in advances in neural information--------processing systems, pp. 33603368, 2016.--------[2] s.s. schoenholz, j. gilmer, s. ganguli, and j. sohl-dickstein. deep information propagation. 5th international conference on learning representations, 2017.--------[3] pascal vincent, hugo larochelle, isabelle lajoie, yoshua bengio, and pierre-antoine manzagol. stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion. journal of machine learning research, 11(dec):33713408, 2010.","this paper analyzes random auto encoders in the infinite dimension limit with an assumption that the weights are tied in the encoder and decoder. in the limit the paper is able to show the random auto encoder transformation as doing an approximate inference on data. the paper is able to obtain principled initialization strategies for training deep autoencoders using this analysis, showing the usefulness of their analysis. even though there are limitations of paper such as studying only random models, and characterizing them only in the limit, all the reviewers agree that the analysis is novel and gives insights on an interesting problem.","building on the recent progresses in the analysis of random high-dimensional statistics problem and in particular of message passing algorithm, this paper analyses the performances of weighted tied auto-encoder.","this is a very orginal use of these technics.----------------the results are 3-folds: (i) a deep analysis of the limitation of weight-tied dae, in the random setting, (ii) the demonstration of the sensitivity to perturbations and (iii) a clever method for initialisation that to train a dae.----------------pro: a rigorous work, a clever use of the recent progresses in rigorous analysis of random neural net, and a very deep answer to interesting questions, and --------con: i do not see much against the paper.","however, as noted by the authors vincent et al (2010) showed that empirically autoencoders with or without weight tying performs comparably.","building on the recent progresses in the analysis of random high-dimensional statistics problem and in particular of message passing algorithm, this paper analyses the performances of weighted tied auto-encoder.","----------------building on the techniques from poole et al (2016) [1], schoenholz et al (2017) [2], the theoretical analysis to deep autoencoder with weight tied encoder/decoder shows interesting properties.","this is a very orginal use of these technics.----------------the results are 3-folds: (i) a deep analysis of the limitation of weight-tied dae, in the random setting, (ii) the demonstration of the sensitivity to perturbations and (iii) a clever method for initialisation that to train a dae.----------------pro: a rigorous work, a clever use of the recent progresses in rigorous analysis of random neural net, and a very deep answer to interesting questions, and --------con: i do not see much against the paper.","----------------building on the techniques from poole et al (2016) [1], schoenholz et al (2017) [2], the theoretical analysis to deep autoencoder with weight tied encoder/decoder shows interesting properties.",random network analysis allows to have exact analysis of asymptotic behaviour where the network is infinitely deep (but width taken to infinite first).,0.2686567164179104,0.0606060606060606,0.1343283582089552,0.1343283582089552,0.3723404255319149,0.064516129032258,0.202127659574468,0.202127659574468,0.1290322580645161,0.0,0.064516129032258,0.064516129032258,0.2686567164179104,0.0606060606060606,0.1343283582089552,0.1343283582089552,0.2121212121212121,0.0153846153846153,0.1212121212121212,0.1212121212121212,0.3723404255319149,0.064516129032258,0.202127659574468,0.202127659574468,0.2121212121212121,0.0153846153846153,0.1212121212121212,0.1212121212121212,0.1587301587301587,0.0,0.0952380952380952,0.0952380952380952,6.328453063964844,7.947897911071777,8.144946098327637,6.328453063964844,8.144946098327637,6.906635284423828,6.78266716003418,7.947898387908935,0.9611392453412166,0.9586401410376477,0.8958805106406136,0.975682645946549,0.9779386152305521,0.2556530906975889,0.9126828718943895,0.9034772257162628,0.75651376959681,0.9611392453412166,0.9586401410376477,0.8958803184729203,0.9852582320085911,0.978433521352246,0.905366021426415,0.975682645946549,0.9779386152305521,0.2556530906975889,0.9852582320085911,0.978433521352246,0.9053660473039488,0.9301065722781512,0.9483649004219322,0.6554505812545645
80,https://openreview.net/forum?id=HkLXCE9lx,"the problem of maximising total discounted reward across multiple trials of an unknown mdp (but sampled from a known distribution, e.g. multi-arm bandit) can be formulated as a pomdp problem. a single episode of the pomdp consists of multiple episodes of interaction with the underlying mdp during which the agent should efficiently explore and integrate information about the mdp to minimize reward across the whole trial.----------------using this observation (which is not original to this work), the authors compare using an existing rl algorithm (trpo) to solve pomdps against classic regret minimization methods on two classic tasks: multi-armed bandits and tabular mdps. additionally, they also demonstrate their approach can scale to a visual navigation task.----------------while the comparison with classic regret minimization problems is useful, this paper has several weaknesses. it seems very confusing to introduce a new name (rl^2) for an existing class of algorithms (essentially any rl method for solving pomdps). this terminology and the paper structure obscures to relationship between this and prior work.----------------the comparison with prior work training rnns is, while improved from the previous version, still lacking. the distinction the authors make, that prior work focussed on memory aspect instead of fast rl, seems somewhat arbitrary. the visual navigation task is conceptually identical to the water maze experiment [heess et al, 2015] or labyrinth navigation [mnih et al, 2016]. these prior tasks require more than just memory, the also requires meta-learning such as exploration and demonstrate fast rl with the agent able to improve dramatically after a single episode.----------------the introduction mentions the need for the use of priors on the environment to create agents which learn quickly and suggests that prior work in deeprl is data inefficient. yet, the recently prior work (e.g. previous paragraph) focussed on pomdps demonstrated one-shot learning once trained (in the fast rl task to use the authors terminology).----------------although the discussion highlights the potential for new algorithms and architectures which are structured to improve performance at these multi-episode tasks, no new algorithms or architectures are introduced.----------------unfortunately, because of the limited contribution, poor comparisons with prior work and confusing terminology this paper is not suitable for iclr without substantial revision. the paper proposes to use rl methods on sequences of episodes instead of single episodes. the underlying idea is the problem of 'learning to learn', and the experimental protocol proposed here allows one to understand how a neural network-based rl model can keep memory of past episodes in order to improve its ability to solve a particular problem. experiments are made on bandit problems, but also on maze problems and show the interesting properties of such an approach, particularly on the maze problem where the agent seems to learn to first explore the maze, and then to exploit its knowledge to quickly find the goal. ----------------the paper is based on a very simple and natural idea which is acutally a good point. i really like the idea, and also the experiment on the maze which is very interesting. experiments on bandits problem are less interesting since meta-learning models have been already proposed in the bandit problem with interesting results and the proposed model does not really bring additionnal information. my main concerns is based on the fact that the paper never clearly formally defines the problem that it attempts to solve. so, between the intuitive idea and the experimental results, the reader does not understand what exactly the learning problem is, what is its impact and/or to which concrete application it belongs to. from my point of view, the article clearly lacks of maturity and does not bring yet a strong contribution to the field. ----------------good:--------* interesting experimental setting--------* simple and natural idea--------* nice maze experiments and model behaviour----------------bad:--------* no real problem defined, only an intuition is given. is it really useful ? for which problems ? what is the performance criterion one wants to optimize ? ...--------* bandit experiments do not really bring relevant informations","hi, i really enjoyed reading the paper. it is definitely a good contribution to the learning to learn set of works. i have a clarification question. what kind of assumptions are made about the distribution over the mdps ? is it assumed that all the support mdps share state spaces or action spaces ? are any other assumptions made ?","yet, the recently prior work (e.g. previous paragraph) focussed on pomdps demonstrated one-shot learning once trained (in the fast rl task to use the authors terminology).----------------although the discussion highlights the potential for new algorithms and architectures which are structured to improve performance at these multi-episode tasks, no new algorithms or architectures are introduced.----------------unfortunately, because of the limited contribution, poor comparisons with prior work and confusing terminology this paper is not suitable for iclr without substantial revision.","yet, the recently prior work (e.g. previous paragraph) focussed on pomdps demonstrated one-shot learning once trained (in the fast rl task to use the authors terminology).----------------although the discussion highlights the potential for new algorithms and architectures which are structured to improve performance at these multi-episode tasks, no new algorithms or architectures are introduced.----------------unfortunately, because of the limited contribution, poor comparisons with prior work and confusing terminology this paper is not suitable for iclr without substantial revision.",for which problems ?,"the problem of maximising total discounted reward across multiple trials of an unknown mdp (but sampled from a known distribution, e.g. multi-arm bandit) can be formulated as a pomdp problem.","a single episode of the pomdp consists of multiple episodes of interaction with the underlying mdp during which the agent should efficiently explore and integrate information about the mdp to minimize reward across the whole trial.----------------using this observation (which is not original to this work), the authors compare using an existing rl algorithm (trpo) to solve pomdps against classic regret minimization methods on two classic tasks: multi-armed bandits and tabular mdps.","a single episode of the pomdp consists of multiple episodes of interaction with the underlying mdp during which the agent should efficiently explore and integrate information about the mdp to minimize reward across the whole trial.----------------using this observation (which is not original to this work), the authors compare using an existing rl algorithm (trpo) to solve pomdps against classic regret minimization methods on two classic tasks: multi-armed bandits and tabular mdps.","the problem of maximising total discounted reward across multiple trials of an unknown mdp (but sampled from a known distribution, e.g. multi-arm bandit) can be formulated as a pomdp problem.","experiments are made on bandit problems, but also on maze problems and show the interesting properties of such an approach, particularly on the maze problem where the agent seems to learn to first explore the maze, and then to exploit its knowledge to quickly find the goal.",0.2318840579710144,0.0,0.1159420289855072,0.1159420289855072,0.2318840579710144,0.0,0.1159420289855072,0.1159420289855072,0.0,0.0,0.0,0.0,0.1573033707865168,0.0,0.0898876404494382,0.0898876404494382,0.2307692307692307,0.03125,0.1538461538461538,0.1538461538461538,0.2307692307692307,0.03125,0.1538461538461538,0.1538461538461538,0.1573033707865168,0.0,0.0898876404494382,0.0898876404494382,0.2115384615384615,0.0588235294117647,0.1153846153846153,0.1153846153846153,13.5447416305542,13.5447416305542,13.701112747192385,8.692132949829102,8.692132949829102,7.748717308044434,8.616392135620117,13.7011137008667,0.9592786496868095,0.8128512814934774,0.02041591332684245,0.9592786496868095,0.8128512814934774,0.02041589429042901,0.8952839110120606,0.9196847557063256,0.1641628409931791,0.9692894617318566,0.923555191070845,0.13506116168421922,0.9716750865588277,0.9705944765499422,0.8933235983339078,0.9716750865588277,0.9705944765499422,0.8933237175452811,0.9692894617318566,0.923555191070845,0.13506117444926966,0.9614599670793637,0.9641258046710361,0.888309421291897
81,https://openreview.net/forum?id=Hke4_JrYDr,"this paper provides a depth learning architecture (global-local structure) from two images. it claims that sota depth can be estimated from the supervision of a very sparse ground truth by leveraging the optical flow information between two images. in the experiments, it shows superior performance than other baseline methods such as eigen's network and dispnet. ----------pros:-----1: the paper is well written and motivations are clearly explained. -----2: the architecture proposed is reasonable and generate good results, since it accept the ground truth scale from sparse map and relative dense matching cues from optical flow, where implicitly relative camera pose is from global module. ---------------cons:-----1`: it is a fairly standard network design similar to demon[25], where motion network for global pose and local dense network for local matching.----------1: the claim of robustness to camera intrinsics is not solved in principle but due to training using ground truth from multiple dataset . it still suffer from depth motion confusion if there is no ground truth depth guidance when testing. it is also unfair for comparison of this metric with unsupervised approach where a universal intrinsic is assumed. ----------2: i think the comparison might not be fair since the baselines are all single image estimation networks, while the approach has two images, where disparities are serving as a strong cue for depth. other possible architectures such as flow net, pwc net, demon[25] and stereo networks such as (gc-net or psm-net) should be considered since these networks are more focus on feature matching. ----------3: even for single image network, eigen's method is not sota, the author may consider (1) as one of the baseline, etc.----------(1) ""deeper depth prediction with fully convolutional residual networks"" this paper proposed a novel global-local network, which can be trained with extremely sparse ground truth, to predict dense depth. though widely applied on the task of segmentation, the use of only uncalibrated input and extremely sparse label for depth estimation is novel. by incorporating optical flow and decoupling global and local modules, this pipeline aligns well with disparity-based geometric methods. the paper is generally well-written and easy to follow. ---------------here are some concerns:----------some important details are missing in order to reproduce the results. for example, the forward pipelines of the global module and the local module are covered with only few sentences. the authors should expand the section of methodology and give better formulations of their pipeline.----------how much does the quality of input optical flow affect the results? also, the author claimed the network has the opportunity to correct for small inaccuracies or outliners in the optical flow input, could you show any result related to this claim? (e.g. flawed optical flow but good output)----------can you try to incorporate the optical flow module and fine-tune for better results? ----------are there some failure cases, and some visualizations on the filter banks of the local network?----------=========================================================-----after rebuttal:----------i thank the author for the response. ----------since there is a possibly more realistic setting that we train the model with dense depth annotations we currently have and use very sparse annotations to adaptive to new environments where we are hard to gain dense annotations. also, the optical flow module relies on extra annotations to train. i will keep my origin scores. this paper presents a method for learning to predict a depth map given a pair of images. training is done is an unsupervised way except for a small set of image locations for which the absolute 3d locations are known. the optical flow between the 2 images is computed automatically. a network is trained to predict the camera motion between the 2 images from the images and the optical flow. from this global motion, convolutional filters are predicted, in order to transform the optical flow into a depth map.----------the use of image locations with known 3d locations is motivated in the paper to ""simulate"" the knowledge of depth coming from a haptic system.----------i have several concerns about this paper.----------* my main concern is that the paper compares against only a few papers (1 from 2014, 1 from 2016, and one more recent from 2019) while the literature is extremely vast, and visually, the results seem far from the state-of-the art. see for example:----------huangying zhan, ravi garg, chamara saroj weerasekera, kejie li, harsh agarwal, and ian reid. unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. in cvpr, 2018.----------anurag ranjan, varun jampani, kihwan kim, deqing sun, jonas wulff, and michael j black. competitive collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. in cvpr, 2019.----------chaoyang wang, jose miguel buenaposada, rui zhu, and simon lucey. learning depth from monocular videos using-----direct methods. in cvpr, 2018.----------in fact, these papers have simpler requirements as they do not need 2 images at run-time, nor 3d data at training time. i thought for a moment that the advantage of the paper was to be able to predict an absolute motion and an absolute depth (this is not impossible if the method is able to estimate the scale from known objects, as it is suggested from the introduction). this was however incorrect as the text says in the middle of section 4.1 "" we resolve the inherent scale ambiguity by normalizing the depth values such that the norm of the translation vector between the two views is equal to 1"".----------* the network used to predict optical flow was trained with a large amount of supervised data. as image matching is the most difficult task, it is difficult to claim that the method is unsupervised.----------* the motivation for having sparse measurements is to ""simulate"" haptic. this is fine for me in principle, however haptic measurements would probably have large errors, while it seems that the experiments use ground truth values for these points.----------one minor remark: end of section 4.4: what is a ""179% error reduction""? how is computed the percentage?","this paper proposes a deep network architecture for learning to predict depth from images with sparsely depth-labeled pixels. ----------this paper was subject to some discussion, since the authors felt that the approach was interesting and the problem-well motivated. some of the concerns about experimental evaluation (especially from r1) were resolved due the author's rebuttal, but ultimately the reviewers felt the paper was not yet ready for publication.","this was however incorrect as the text says in the middle of section 4.1 "" we resolve the inherent scale ambiguity by normalizing the depth values such that the norm of the translation vector between the two views is equal to 1"".----------* the network used to predict optical flow was trained with a large amount of supervised data.","----------3: even for single image network, eigen's method is not sota, the author may consider (1) as one of the baseline, etc.----------(1) ""deeper depth prediction with fully convolutional residual networks"" this paper proposed a novel global-local network, which can be trained with extremely sparse ground truth, to predict dense depth.",a network is trained to predict the camera motion between the 2 images from the images and the optical flow.,this paper provides a depth learning architecture (global-local structure) from two images.,"-----2: the architecture proposed is reasonable and generate good results, since it accept the ground truth scale from sparse map and relative dense matching cues from optical flow, where implicitly relative camera pose is from global module.","----------3: even for single image network, eigen's method is not sota, the author may consider (1) as one of the baseline, etc.----------(1) ""deeper depth prediction with fully convolutional residual networks"" this paper proposed a novel global-local network, which can be trained with extremely sparse ground truth, to predict dense depth.","though widely applied on the task of segmentation, the use of only uncalibrated input and extremely sparse label for depth estimation is novel.","----------3: even for single image network, eigen's method is not sota, the author may consider (1) as one of the baseline, etc.----------(1) ""deeper depth prediction with fully convolutional residual networks"" this paper proposed a novel global-local network, which can be trained with extremely sparse ground truth, to predict dense depth.",0.3125,0.0476190476190476,0.15625,0.15625,0.3089430894308942,0.0991735537190082,0.1300813008130081,0.1300813008130081,0.2444444444444444,0.0454545454545454,0.1999999999999999,0.1999999999999999,0.1927710843373493,0.0246913580246913,0.144578313253012,0.144578313253012,0.1682242990654205,0.0,0.0934579439252336,0.0934579439252336,0.3089430894308942,0.0991735537190082,0.1300813008130081,0.1300813008130081,0.1720430107526881,0.0,0.086021505376344,0.086021505376344,0.3089430894308942,0.0991735537190082,0.1300813008130081,0.1300813008130081,10.76247215270996,11.449831008911133,12.24349594116211,10.76247215270996,7.234536170959473,7.622333526611328,10.76247215270996,5.156189441680908,0.09591344563410437,0.1662311535743868,0.8236220517732125,0.9740750189686181,0.9722724605302472,0.6625820366502668,0.9543238597281778,0.9712558054790347,0.04864036933599107,0.9767083688473158,0.9759609600464452,0.9099014637995542,0.972894160128449,0.9697914053427806,0.701320273945772,0.9740750189686181,0.9722724605302472,0.6625820366502668,0.9359561213365839,0.951141509934067,0.9137418730365475,0.9740750189686181,0.9722724605302472,0.6625819569323262
82,https://openreview.net/forum?id=Hkeh21BKPH,"in the paper, the authors present a new algorithm for training neural networks used in an automated theorem prover using theorems with or without proofs as training data. the algorithm casts this training task as a reinforcement learning problem, and employs curriculum learning and the proximal policy optimization algorithm to find appropriate neural network parameters, in particular, those that make the prover good at finding long proofs. the authors also propose a new dataset for theorems and proofs for a simple equational theory of arithmetic, which is again suitable for improving (via learning) and testing the ability of the prover for finding long proofs. the proposed prover is tested against existing theorem provers, and for the authors' dataset, it outperforms those provers.----------i found it difficult to make up my mind on this paper. on the one hand, the paper tackles an interesting problem of improving an automated theorem prover via learning, in particular, its ability for finding long nontrivial proofs. also, i liked a qualitative analysis of the failure of the curriculum learning for tackling hard tasks in the paper. on the other hand, i couldn't quite make me excited with the dataset used to test the prover in the paper. the dataset seems to consist of easy variable-free equational formulas about arithmetic that can be proved by evaluation. of course, i may be completely wrong about the value of the dataset. also, if the dataset includes variables and other propositional logic formulas, such as disjunction, negation and conjunction, so that the prover can be applied to any formulas from peano arithmetic via skolemization, i would be much more supportive for the paper. another thing that demotivated me is that i couldn't find the discussion about the subtleties in using curriculum learning and ppo for the theorem-proving task in the paper. what are the possible design choices? why does the authors' choice work better than others? ----------i added a few minor comments below.----------* abstract, p1: ""significantly outperforms previous learning-based"". when i read the experimental result section, i couldn't quite get this sense of huge improvement of the proposed approach over the existing provers. specifically, from table 4, i can see flop performs better than rlcop, but i wasn't sure that the improvement was that significant (especially because rlcop might not have given a chance to be tuned to the type of questions used to train flop -- i may be wrong here). i suggest you to add some further explanation so that a reader can share your sentiment and excitement on the improvement brought by your technique.----------* p2: the related work section is great. i learned a lot by reading it. thanks.----------* p4: i think that you used the latex citation command incorrectly in ""learning resnick ... chen (2018)""-----and ""features kaliszyk ... kaliszyk et al. (2015a; 2018)"".----------* p6: discount factor) parameters related ===> discount factor), parameters related ----------* p8: a a well ==> a well this paper uses reinforcement learning for automated theorem proving. the proposed method aims to generalize the short proofs to longer proofs with similar structure. experiments were run to compare the performance of curriculum learning with the ones without curriculum.----------overall the paper attempts to explain clearly the original contribution of the proposed approach, which is using curriculum learning in rl based proof guidance. however, i am not convinced about the how compelling the results are in support of the claim. the main arguments to bolster my decision are as follows.----------i am familiar with rl and curriculum learning but not so much with connection tableau calculus. the description given in the paper seems to be insufficient and confusing for readers with limited knowledge in this area. a step-by-step explanation with a toy example might have done the job nicely. without such a clear understanding of the calculus, it gets hard to appreciate the merits of the results.----------some claims of the paper are not clearly validated by the reported experimental results. for example:------ in table 8, curriculum learning is worse in some cases and better in others. what to conclude from such a report?------ in experiment 3, curriculum learning tends to find shorter proofs. isn't that contrary to the focus of the paper? ------ in table 3, curriculum learning performs lot worse than the other method. what is to be inferred from such a report?----------it would be nice if there was a clear explanation of the role of curriculum in the learning algorithm. for example, in algorithm 1, how is line 8 helping in overall objective of learning longer proofs? if one advances curriculum, one takes lesser number of proof steps according to stored proofs. how does that help in the learning? does it imply 'less memorizing' with advancement of curriculum? overall:----------i don't work on atp and am not particularly well suited to review this paper, but i am-----slightly inclined to accept for the following reasons.----------1. it's an important problem and not much work is done on it.-----2. creating new environments is hard, valuable work that is (imo) insufficiently incentivized in our community,-----and i think accepting papers that do this work is a good policy.-----3. the experiments, to my super-inexperienced eye, seem well designed and like they address-----obvious questions readers would have.----------however, i have no idea if e.g. the baselines used in this paper are reasonable, so-----i would appreciate someone with more experience on that topic weighing in.---------------some limitations of the paper:----------1. i'm not actually convinced there's much that's methodologically new here. -----it seems like mostly an application of existing rl techniques to an atp environment.----------2. the writing is not particularly clear, and could use substantial editing (but this-----is something that could be fixed during the discussion period).----------3. the focus on robinson arithmetic seems kind of limiting.-----though i am sympathetic to the reasoning given in the paper, it's unclear to me (again, as a non-expert),-----that the techniques that work in this context will actually work in the context considered by e.g. [1]----------detailed comments:-----> in the training set, all numbers are 0 and 1 and this approach works more often.-----you have this sentence twice in different places.----------> it is insightful to compare...-----not a very idiomatic use of insightful?---------------[1] deep network guided proof search.","this paper proposes a curriculum-based reinforcement learning approach to improve theorem proving towards longer proofs. while the authors are tackling an important problem, and their method appears to work on the environment it was tested in, the reviewers found the experimental section too narrow and not convincing enough. in particular, the authors are encouraged to apply their methods to more complex domains beyond robinson arithmetic. it would also be helpful to get a more in depth analysis of the role of the curriculum. the discussion period did not lead to improvements in the reviewers scores, hence i recommend that this paper is rejected at this time.","on the one hand, the paper tackles an interesting problem of improving an automated theorem prover via learning, in particular, its ability for finding long nontrivial proofs.","experiments were run to compare the performance of curriculum learning with the ones without curriculum.----------overall the paper attempts to explain clearly the original contribution of the proposed approach, which is using curriculum learning in rl based proof guidance.",how does that help in the learning?,"in the paper, the authors present a new algorithm for training neural networks used in an automated theorem prover using theorems with or without proofs as training data.","thanks.----------* p4: i think that you used the latex citation command incorrectly in ""learning resnick ... chen (2018)""-----and ""features kaliszyk ... kaliszyk et al. (2015a; 2018)"".----------* p6: discount factor) parameters related ===> discount factor), parameters related ----------* p8: a a well ==> a well this paper uses reinforcement learning for automated theorem proving.","the algorithm casts this training task as a reinforcement learning problem, and employs curriculum learning and the proximal policy optimization algorithm to find appropriate neural network parameters, in particular, those that make the prover good at finding long proofs.","overall:----------i don't work on atp and am not particularly well suited to review this paper, but i am-----slightly inclined to accept for the following reasons.----------1.","experiments were run to compare the performance of curriculum learning with the ones without curriculum.----------overall the paper attempts to explain clearly the original contribution of the proposed approach, which is using curriculum learning in rl based proof guidance.",0.208955223880597,0.0454545454545454,0.1044776119402985,0.1044776119402985,0.2602739726027397,0.0138888888888888,0.136986301369863,0.136986301369863,0.087719298245614,0.0178571428571428,0.0526315789473684,0.0526315789473684,0.1481481481481481,0.0300751879699248,0.0888888888888888,0.0888888888888888,0.1666666666666666,0.0389610389610389,0.0769230769230769,0.0769230769230769,0.2328767123287671,0.0416666666666666,0.1643835616438356,0.1643835616438356,0.1617647058823529,0.0298507462686567,0.1176470588235294,0.1176470588235294,0.2602739726027397,0.0138888888888888,0.136986301369863,0.136986301369863,9.576020240783691,7.76572322845459,14.654914855957031,6.557538032531738,11.293781280517578,7.349388122558594,6.557538032531738,3.623077392578125,0.934441161973577,0.9423763274628992,0.9152011379392518,0.9573179578418338,0.9685069619260842,0.7846436789077319,0.14490935107508018,0.36914641249348684,0.6903066347545297,0.9690347019256391,0.9703128202075555,0.9166249734151272,0.969236195051809,0.8910834149663457,0.36979816690859363,0.9355694059251808,0.9507649745373816,0.9271984759206058,0.8454346387230736,0.8240264307630603,0.6692716440731683,0.9573179578418338,0.9685069619260842,0.7846436789077319
83,https://openreview.net/forum?id=Hkemdj09YQ,"summary of the paper:--------this paper proposed rectgrad, a gradient-based attribution method that tries to avoid the problem of noise in the attribution map. further, authors hypothesize that noise is caused by the network carrying irrelevant features, as opposed to saturation, discontinuities, etc as hypothesized by related papers. ----------------the paper is well written and easy to read through. ----------------strengths:--------- formally addresses a hitherto unanswered question of why saliency maps are noisy. this is an important contribution.--------- rectgrad is easy to implement.----------------questions for authors:--------- since the authors are saying that the validity of their hypothesis is trivial, it would be nice to have this statement supported by more quantitative, dataset-wide analyses on the feature map and training dataset occlusion tests. for e.g., what percentage of the test dataset shows attributions on the 10x10 occluded patch? --------- how does rectgrad compare with simply applying a final threshold on other attribution maps? how do the results on training data and feature occlusion change after such a threshold is applied? how do results on adversarial attacks change?--------- could this method generalize to non-relu networks? --------- premise that auxiliary objects in the image are part of the background is not necessarily true. for instance, the hand in lighter is clearly important to know that the flame is from a lighter and not from a candle or some other form of fire. similarly, the leaves in the frog example. --------- (optional) as shown in (https://openreview.net/forum?id=b1xeyhcctq) gradients on relu networks overlook the bias term. in the light of this, what is the authors take on whether a high bias-attribution is the cause for the noisy gradient-attribution? --------- (optional) in some sense, rectgrad works because layers closer to the input may capture more focussed features than layers close to input which may activate features spread out all over the image. it would be interesting to see if rectgrad works for really small networks such as mobilenet (https://arxiv.org/abs/1801.04381) where such an explicit hierarchy of features may not be there. this paper studies how to better visually interpret a deep neural network. it proposes a new method to produce less noisy saliency maps, named rectgrad. rectgrad thresholds gradient during backprop in a layer-wise fashion in a similar manner to a previous work called guided backprop. the difference is that guided backprop employs a constant threshold, i.e. 0, while rectgrad uses an adaptive threshold based on a percentile hyper-parameter. the paper is well-written, including a comprehensive review of previous related works, an meaningful meta-level discussion for motivation, and a clear explanation of the proposed method. ----------------one of my biggest concern is regarding the experiment and evaluation section. conclusions are drawn based on the visualization of a few saliency maps. i am not sure how much i can trust these conclusions as the conclusions are drawn in a handy-wavy manner the examples are prone to cherry-picking and . for example, this is the conclusion in the adversarial attack paragraph: we can conclude that rectified gradient is equally or more class sensitive than baseline attribution methods. as pointed out by the paper, the conclusion can be drawn from figure 8 in the main paper and figure 10 in appendix a.1. however, the proposed method tends to produce a saliency map with higher sparsity, therefore the difference may appear more apparent. it is stretching to conclude that it is more class sensitive without further quantitative validation. ----------------evaluation appears to be a common concern to the work on saliency maps. the existing quantitative evaluation in the paper seems disconnected to the visual nature of saliency maps. concretely, when can we say one saliency map looks better than another? since this paper claims to produce less noisy saliency maps, what does it mean quantitatively? is it true that it produces less pixels on the background? if so, can we evaluate it with foreground-background segmentation annotation to prove that point? though how to evaluate saliency maps remains an open question, i feel some discussion on this paper would make the paper more insightful. in the paper, the authors proposed a new saliency map method, based on some empirical observations about the cause of noisy gradients.--------specifically, through experiments, the authors clarified that the noisy gradients are due to irrelevant information propagated in the forward pass in dnn. because the backpropagation follows the same pass, irrelevant feature are conveyed back to the input, which results in noisy gradients.--------to avoid noisy gradients, the authors proposed a new backpropagation named rectified gradient (rectgrad). in rectgrad, the backward pass is filtered out if the product of the forward signal and the backward signal are smaller than a threshold. the authors claim that, with this modification in backpropagation, the gradients get less noisy.--------in some experiments, the authors presented that rectgrad can produce clear saliency maps.----------------i liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing. the experiments are designed well to support the claim.--------here, i would like to point out, that noisy gradients in occluded images may be because of the convolutional structures. each filter in convolution layer is trained to respond to certain patterns. because the same filter is used for each of subimages, some filters can be activated occasionally on occluded parts. i think this does not happen if the network is densely connected without convolutional structures. the trained dense connection will be optimized to remove the effects of occluded parts. hence, for such networks, the gradient will be zeros for occluded parts.----------------the second half of the paper (sec.4 and 5) are not very much convincing to me.--------below, i raise several concerns.----------------1. there is no justification on the definition of rectgrad: why rl = i(al * rl > t) r(l+1)?--------the authors presented rl = i(al * rl > t) r(l+1) as rectgrad, that can filter out irrelevant passes. however, there is no clear derivation of this formula: the definition suddenly appears. if the irrelevant forward passes are causes of noisy gradients, the modification rl = i(al > t) r(l+1) seems to be more natural to me. it is also a natural extension to the relu backward pass rl = i(al > 0) r(l+1). why we need to filter out negative signals in backward pass?----------------2. the experimental results are less convincing: is rectgrad truly good?--------in sec.5.2, the authors presented saliency maps only on a few images, and claimed that they look nicely. however, it is not clear that those ""nicely looking"" saliency map are truly good ones. i expect the authors to put much efforts on quantitative comparisons rather than qualitative comparisons, so that we can understand that those ""nicely looking"" saliency maps are truly good ones.--------sec.5.3 presents some quantitative comparisons, however, the reported sensitivity and roar/kar on rectgrad are not significant. the authors mentioned that this may be because of the sparsity of rectgrad. however, if the sparsity is the harm, the underlying observations of rectgrad may have some errors. i think the current manuscript has an inconsistency between the fundamental idea (based on empirical observations) and the performance of rectgrad.----------------[minor concern]--------in sec.5, the authors frequently refer to the figures in appendix. i think the main body of the paper should be self-contatined. i therefore think that some of the figures related to main results should appear in the main part.","the main goal of the submission is to figure out a way to produce less ""noisy"" saliency maps. the rectgrad method uses some thresholding during backprop, like guided backprop. the visuals of the proposed method are good, but the reviewers rightfully point out that evaluating whether the proposed method is any good is not obvious. the roar/kar results are perhaps not telling the whole story (and the authors claim that rectgrad is not expected to get a high roar score, but i would like to see this developed more in a further version of this work). generally, i feel like there was a healthy back and forth between authors and r3 on the main concerns of this work. i agree that the mathematical justification for rectgrad seems not fully developed. given all of these concerns, at this point i cannot support acceptance of this work at iclr.","the paper is well-written, including a comprehensive review of previous related works, an meaningful meta-level discussion for motivation, and a clear explanation of the proposed method.","the authors claim that, with this modification in backpropagation, the gradients get less noisy.--------in some experiments, the authors presented that rectgrad can produce clear saliency maps.----------------i liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing.","since this paper claims to produce less noisy saliency maps, what does it mean quantitatively?","summary of the paper:--------this paper proposed rectgrad, a gradient-based attribution method that tries to avoid the problem of noise in the attribution map.","because the backpropagation follows the same pass, irrelevant feature are conveyed back to the input, which results in noisy gradients.--------to avoid noisy gradients, the authors proposed a new backpropagation named rectified gradient (rectgrad).","the authors claim that, with this modification in backpropagation, the gradients get less noisy.--------in some experiments, the authors presented that rectgrad can produce clear saliency maps.----------------i liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing.","in the light of this, what is the authors take on whether a high bias-attribution is the cause for the noisy gradient-attribution?","the authors claim that, with this modification in backpropagation, the gradients get less noisy.--------in some experiments, the authors presented that rectgrad can produce clear saliency maps.----------------i liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing.",0.1477272727272727,0.0344827586206896,0.1022727272727272,0.1022727272727272,0.2842639593908629,0.0717948717948718,0.1522842639593908,0.1522842639593908,0.0981595092024539,0.062111801242236,0.0736196319018404,0.0736196319018404,0.1618497109826589,0.0116959064327485,0.1040462427745664,0.1040462427745664,0.1648351648351648,0.0111111111111111,0.0989010989010989,0.0989010989010989,0.2842639593908629,0.0717948717948718,0.1522842639593908,0.1522842639593908,0.1860465116279069,0.0352941176470588,0.1046511627906977,0.1046511627906977,0.2842639593908629,0.0717948717948718,0.1522842639593908,0.1522842639593908,12.00971221923828,8.636795043945312,13.007783889770508,12.00971221923828,7.655685901641846,12.447165489196776,12.00971221923828,10.45753288269043,0.9337912149277661,0.9469159840289374,0.9593260892603843,0.7426779131777624,0.7912976210864464,0.9349989350785974,0.957487709070561,0.9577646239414229,0.9285486674694905,0.9680952090679746,0.9666768001149479,0.9295491703949278,0.6323155271265558,0.8815436685175958,0.8249210141535466,0.7426779131777624,0.7912976210864464,0.934998963726447,0.9539180361818648,0.9471256908289483,0.9001532399532716,0.7426779131777624,0.7912976210864464,0.9349989350785974
84,https://openreview.net/forum?id=Hkexw1BtDr,"the paper proposes a deep rl approach called auto-deferring policy (adp) to learning a policy for constructing solutions for the maximum independent set (mis) problem. rather than constructing a solution one variable per episode step, the policy can make decisions about multiple variables per step, as well as defer decisions to later steps. at each step the mis constraints of a valid solution are checked and decisions that violate the constraints are reverted, and any additional decisions that are automatically implied by the decisions so far and the mis constraints are taken. the policy and value function are parameterized as a graph convolutional network to make use of the graph structure of the problem. a diversity regularizer that encourages the policy to generate diverse final solutions is used for training. results show that the approach is able to match the objective value of the state-of-the-art solvers on several datasets, and is able to outperform s2v-dqn on several datasets when neither approach is trained on them.----------pros:------ extensive experiments have been done on several datasets, both synthetic and real. the ablation and generalization experiments are particularly valuable for getting insight into the performance of the algorithm.------ comparison to cplex and redumis strengthens the results.------ the paper is reasonably well-written and easy to follow. figures 2 and 3 are especially useful for quickly understanding key ideas.----------cons:------ the update and clean-up phases inject significant domain knowledge about mis into the policy. at a high level the idea is similar to unit propagation in boolean satisfiability (sat) solvers or domain propagation in mixed integer programming (mip) solvers, i.e., the hard constraints of the problem can be used to make decisions in the search for a solution. both sat and mip solvers have been shown to rely on it to significantly improve their search. improvements over s2v-dqn could be due to this extra built-in domain knowledge, rather than any improvements related to learning. for a fairer comparison, either this knowledge should be removed from adp as an additional ablation study, or it should somehow be given to s2v-dqn as well. without such a comparison, it is not clear that the improvements are really due to the new ideas like auto-deferring, diversity regularizer, etc.------ a comparison to li, chen, and koltun, neurips18 is needed since they have shown strong results for mis and they have made the code available online. although their approach is supervised learning, it would still be useful to assess an rl approachs performance relative to sl.---------------additional comments:------ it is not clear what encourages the policy to defer decisions, rather than making all the {0,1} decisions for all variables in one step or a small number of steps. does this behaviour of the policy arise naturally via training, or does some regularization need to be applied to ensure that the policy doesnt prematurely fix all variables? for example, as a way to avoid/reduce variance in the reward signal, the policy may learn to fix all variables in just one step -- does this issue arise?----------- the abstract says ""the reported performance of our generic drl scheme is also comparable with that of the state-of-the-art solvers specialized for mis, e.g., adp outperforms them for some graphs with millions of vertices."" can you please point out the specific results in the paper that this sentence is referring to?----------- comparison to cplex: the paper mentions that we observe that our algorithm outperforms the cplex solver on er-(100, 200) and er-(400, 500) datasets, while consuming a smaller amount of time, and it is remarkable to observe that adp outperforms the cplex solver on both datasets under reasonably limited time. note that cplex not only optimizes the objective, but also proves a bound on the objective, while adp only does the former. so this is not a fair comparison. a fairer comparison would be to set cplex hyperparameters to give higher priority to optimizing the objective and then measure the time cplex took to first reach a particular objective value rather than the time to solve an instance. it can be the case that a given objective value is achieved quickly but then proving a bound requires much longer. also there is no mention of the optimality gap used as a stopping criterion for cplex.----------- one potential advantage of making one decision per step is that the credit assignment problem may be simpler compared to making many decisions simultaneously. it would be insightful to explore this tradeoff in more detail.----------- the acronym adp is somewhat well-known in the optimization community as approximate dynamic programming (http://adp.princeton.edu/), so it would be helpful to use a different one. the paper introduces auto-deferring policies (adps) for deep reinforcement learning (rl). adps automatically stretching or shrinking their decision process, in particular, deciding whether to finalize the value of each vertex at the current stage or defer to determine it at later stages. adps are evaluated on maximum independent set problems. ----------the paper is in principle well written and structured. some statements of the paper appear a little bit too strong. for instance, saying that deep rl approaches ""can automatically learn the design of a good solver without using any sophisticated knowledge or hand-crafted heuristic specialized for the target problem"" is misleading as thee designer of the rl setup is putting a lot of knowledge into the design. likewise the statement ""without any human guidance"" is not true, at least at the current stage. it would be great to acknowledge this by softening this statement. ----------the basic idea is also fine, learning to expand or not nodes in the current fringe of the combinatorial solver. however, being an informed outsider, i would like to understand more why encoding np-hard problem using an (discrete, finite) mdp, which is rather efficient to solve, is a good idea. moreover, while the focus on mis is justified in the paper, showing the (potential) benfit on other tasks such as tsp is useful to convince the reader that adps apply across different problem classes. without, it is not clear whether adps work fine on other problem classes (even if one may expect that this is the case). ----------anyhow, the main idea of implementing two independent agents/networks to implement a reward signal that rewards deviation is interesting. moreover, the experimental results show that the approach works. it actually manages to be on par with redumis. however, it does not really improve upon this well-known heuristic. hence some experiments across different problems would really be beneficial. without, it is just interesting to see that rl can also come up with good heuristics but the existing heuristic already works pretty well.----------to sum up, a very nice idea that shows promise but experiments on other problems is missing for a complete picture. also some statements should be soften.","this paper proposes a new way to formulate the design of the deep reinforcement learning that automatically shrinks or expands decision processes.----------the paper is borderline and all reviewers appreciate the paper and gives thorough reviews. however, it not completely convince that it is ready publication. ----------rejection is recommended. this can become a nice paper for next conference by taking feedback into account.","for example, as a way to avoid/reduce variance in the reward signal, the policy may learn to fix all variables in just one step -- does this issue arise?----------- the abstract says ""the reported performance of our generic drl scheme is also comparable with that of the state-of-the-art solvers specialized for mis, e.g., adp outperforms them for some graphs with millions of vertices.""","can you please point out the specific results in the paper that this sentence is referring to?----------- comparison to cplex: the paper mentions that we observe that our algorithm outperforms the cplex solver on er-(100, 200) and er-(400, 500) datasets, while consuming a smaller amount of time, and it is remarkable to observe that adp outperforms the cplex solver on both datasets under reasonably limited time.","although their approach is supervised learning, it would still be useful to assess an rl approachs performance relative to sl.---------------additional comments:------ it is not clear what encourages the policy to defer decisions, rather than making all the {0,1} decisions for all variables in one step or a small number of steps.",the paper proposes a deep rl approach called auto-deferring policy (adp) to learning a policy for constructing solutions for the maximum independent set (mis) problem.,the paper proposes a deep rl approach called auto-deferring policy (adp) to learning a policy for constructing solutions for the maximum independent set (mis) problem.,the paper proposes a deep rl approach called auto-deferring policy (adp) to learning a policy for constructing solutions for the maximum independent set (mis) problem.,"at a high level the idea is similar to unit propagation in boolean satisfiability (sat) solvers or domain propagation in mixed integer programming (mip) solvers, i.e., the hard constraints of the problem can be used to make decisions in the search for a solution.",the paper proposes a deep rl approach called auto-deferring policy (adp) to learning a policy for constructing solutions for the maximum independent set (mis) problem.,0.2153846153846153,0.03125,0.1538461538461538,0.1538461538461538,0.2748091603053434,0.0465116279069767,0.1374045801526717,0.1374045801526717,0.2586206896551724,0.0175438596491228,0.1034482758620689,0.1034482758620689,0.2247191011235954,0.0689655172413793,0.1573033707865168,0.1573033707865168,0.2247191011235954,0.0689655172413793,0.1573033707865168,0.1573033707865168,0.2247191011235954,0.0689655172413793,0.1573033707865168,0.1573033707865168,0.2407407407407407,0.0188679245283018,0.1481481481481481,0.1481481481481481,0.2247191011235954,0.0689655172413793,0.1573033707865168,0.1573033707865168,15.836832046508787,15.836832046508787,15.836830139160156,6.538174629211426,8.733057022094727,11.616329193115234,15.836832046508787,5.767519950866699,0.9654590608242593,0.9703707572775397,0.8807723419259613,0.9714802189352901,0.9654137205703621,0.9248826333754856,0.8482231121634053,0.944870925493644,0.8904281061509154,0.9717234291743836,0.9714266316104546,0.8717967845440289,0.9717234291743836,0.9714266316104546,0.8717967325794982,0.9717234291743836,0.9714266316104546,0.8717964269065516,0.9388292333804659,0.9526563703769007,0.9479337788249159,0.9717234291743836,0.9714266316104546,0.871796546119044
85,https://openreview.net/forum?id=HkfPSh05K7,"the paper proposes a multi-document extractive machine reading model and algorithm. the model is composed of 3 distinct parts. first, the document retriever and the document reader that are states of the art modules. then, the paper proposes to use a ""multi-step-reasoner"" which learns to reformulate the question into its latent space wrt its current value and the ""state"" of the machine reader.----------------in the general sense, the architecture can be seen as a specific case of a memory network. indeed, the multi-reasoner step can be seen as the controller update step of a memory network type of inference. the retriever is the attention module and the reader as the final step between the controller state and the answer prediction.----------------the authors claim the method is generic, however, the footnote in section 2.3 mentioned explicitly that the so-called state of the reader assumes the presence of a multi-rnn passage encoding. furthermore, this section 2.3 gives very little detailed about the ""reinforcement learning"" algorithms used to train the reasoning module.----------------finally, the experimental section, while giving encouraging results on several datasets could also have been used on qangoroo dataset to assess the multi-hop capabilities of the approach. furthermore, very little details are provided regarding the reformulation mechanism and its possible interpretability. the authors improve a retriever-reader architecture for open-domain qa by iteratively retrieving passages and tuning the retriever with reinforcement learning. they first learn vector representations of both the question and context, and then iteratively change the vector representation of the question to improve results. i think this is a very interesting idea and the paper is generally well written.----------------i find some of the description of the models, methods and training is lacking detail. for example, their should be more detail on how reinforce was implemented; e.g. was a baseline used?----------------i am not sure about the claim that their method is agnostic to the choice of machine reader, given that the model needs access to internal states of the reader and their limited results on bidaf.----------------the presentation of the results left a few open questions for me:---------------- - it is not clear to me which retrieval method was used for each of the baselines in table 2.-------- - why does table 2 not contain the numbers obtained by the drqa model (both using the retrieval method from the drqa method and their method without reinforcement learning)? that would make their improvements clear.-------- - moreover, for triviaqa their results and the cited baselines seem to all perform well below to current top models for the task (cf. https://competitions.codalab.org/competitions/17208#results).-------- - i would also like to see a better analysis of how the number of steps helped increase f1 for different models and datasets. the presentation should include a table with number of steps and f1 for different step numbers they tried. (figure 2 is lacking here.)-------- - in the text, the authors claim that their result shows that natural language is inferior to 'rich embedding spaces'. they base this on a comparison with the aqa model. there are two problems with this claim: 1) the two approaches 'reformulate' for different purposes, retrieval and machine reading, so they are not directly comparable. 2) both approaches use a 'black box' machine reading model, but the authors use drqa as the base model while aqa uses bidaf. indeed, since the authors have an implementation of their model that uses bidaf, an additional comparison based on matched machine reading models would be interesting.--------- generally, it would be great to see more detailed results for their bidaf-based model as well. this paper introduces a new framework to interactively interact document retriever and reader for open-domain question answering. while retriever-reader framework was often used for open-domain qa, this bi-directional interaction between the retriever and the reader is novel and effective because--------1) if the retriever fails to retrieve the right document at the first step, the reader can give a signal to the retriever so that the retriever can recover its mistake at the next step--------2) the idea of `reader state` from the reader to the retriever is new--------3) the retriever use question-independent representation of paragraphs, which does not require different representation depending on the question and makes the framework easily scalable.----------------strengths--------1) the idea of multi-step & bi-directional interaction between the retriever and the reader is novel enough (as mentioned above). the paper contains enough literature studies on existing retriever-reader framework in open-domain setting, and clearly demonstrates how their framework is different from them.--------2) the authors run the experiments on 4 different dataset, which supports the argument about the frameworks effectiveness.----------------weakness--------1) the authors seem to highlight multi-step `reasoning`, while it is not `reasoning` in my opinion. multi-step reasoning refers to the task which you need evidence from different documents, and/or you need to find first evident to find the second evidence from a different document. i dont think the dataset here are not multi-step reasoning dataset, and the authors seem not to claim it either. therefore, i recommend using another term (maybe `multi-step interaction`?) instead of `multi-step reasoning`.--------2) while the idea of multi-step interaction and how it benefits the overall performance is interesting, the analysis is not enough. figure 3 in the paper does not have enough description  for example, i got the left example means step 2 recovers the mistake from step 1, but what does the right example mean?----------------questions on result comparison--------1) on triviaqa (both open and full), the authors mentioned the result is on hidden test set  did you submit it to the leaderboard? i dont see the same numbers on the triviaqa leaderboard. also, the authors claim they are sota on triviaqa, but there are higher numbers on the leaderboard (which are submitted prior to the iclr deadline).--------2) there are other published papers with higher result on quasar-t, searchqa and triviaqa (such as https://aclanthology.info/papers/p18-1161/p18-1161 and https://arxiv.org/abs/1805.08092) which the authors did not compare with.--------3) in section 4.2, is there a reason for the specific comparison to aqa (5th line), though aqa is not sota on searchqa? i dont think it means latent space is better than natural language space. they are totally different model and the only intersection is they contains interaction between two submodules.--------4) in section 5, the authors mentioned their framework outperforms previous sota by 15% margin on triviaqa, but what is that? i dont see 15% margin in table 2.----------------marginal comments:--------1) if i understood correctly, `triviaqa-open` and `triviaqa-full` in the paper are officially called `triviaqa-full` and `open-domain triviaqa`. how about changing the term for readers to better understand the task? also, in section 4, the authors said triviaqa-open is larger than web/wiki setting, but to my knowledge, this setting is part of the wiki setting.--------2) it would be great if the authors make the capitalization consistent. e.g. em, quasar-t, bidaf. also, the authors can use em instead of `exact match` after they mentioned em refers to exact match in section 4.2.----------------overall comment--------the idea in the paper is interesting, and their model and experiments are concrete. my only worries is that the terms in the paper are confusing and performance comparison are weak. i would like to update the score when the authors update the paper.------------------------update 11/27/2018--------thanks for the authors for updating the paper. the updated paper have more clear comparisons with other models, with more & stronger experiments with the additional dataset. also, the model is claimed to perform multi-step interaction rather than multi-step reasoning, which clearly resolves my initial concern. the analysis, especially ablations in varying number of iterations, was helpful to understand how their framework benefits. i believe these make the paper stronger along with its initial novelty in the framework. in this regard, i vote for acceptance.","pros: - novel idea for multi-step qa which rewrites the query in embedding space - good comparison with related work - reasonable evaluation and improved results cons: there were concerns about missing training details, insufficient evaluation, and presentation. these have been largely addressed in revision and i am recommending acceptance.","then, the paper proposes to use a ""multi-step-reasoner"" which learns to reformulate the question into its latent space wrt its current value and the ""state"" of the machine reader.----------------in the general sense, the architecture can be seen as a specific case of a memory network.","while retriever-reader framework was often used for open-domain qa, this bi-directional interaction between the retriever and the reader is novel and effective because--------1) if the retriever fails to retrieve the right document at the first step, the reader can give a signal to the retriever so that the retriever can recover its mistake at the next step--------2) the idea of `reader state` from the reader to the retriever is new--------3) the retriever use question-independent representation of paragraphs, which does not require different representation depending on the question and makes the framework easily scalable.----------------strengths--------1) the idea of multi-step & bi-directional interaction between the retriever and the reader is novel enough (as mentioned above).","indeed, since the authors have an implementation of their model that uses bidaf, an additional comparison based on matched machine reading models would be interesting.--------- generally, it would be great to see more detailed results for their bidaf-based model as well.",the paper proposes a multi-document extractive machine reading model and algorithm.,"also, the authors claim they are sota on triviaqa, but there are higher numbers on the leaderboard (which are submitted prior to the iclr deadline).--------2) there are other published papers with higher result on quasar-t, searchqa and triviaqa (such as https://aclanthology.info/papers/p18-1161/p18-1161 and https://arxiv.org/abs/1805.08092) which the authors did not compare with.--------3) in section 4.2, is there a reason for the specific comparison to aqa (5th line), though aqa is not sota on searchqa?","while retriever-reader framework was often used for open-domain qa, this bi-directional interaction between the retriever and the reader is novel and effective because--------1) if the retriever fails to retrieve the right document at the first step, the reader can give a signal to the retriever so that the retriever can recover its mistake at the next step--------2) the idea of `reader state` from the reader to the retriever is new--------3) the retriever use question-independent representation of paragraphs, which does not require different representation depending on the question and makes the framework easily scalable.----------------strengths--------1) the idea of multi-step & bi-directional interaction between the retriever and the reader is novel enough (as mentioned above).","2) both approaches use a 'black box' machine reading model, but the authors use drqa as the base model while aqa uses bidaf.","while retriever-reader framework was often used for open-domain qa, this bi-directional interaction between the retriever and the reader is novel and effective because--------1) if the retriever fails to retrieve the right document at the first step, the reader can give a signal to the retriever so that the retriever can recover its mistake at the next step--------2) the idea of `reader state` from the reader to the retriever is new--------3) the retriever use question-independent representation of paragraphs, which does not require different representation depending on the question and makes the framework easily scalable.----------------strengths--------1) the idea of multi-step & bi-directional interaction between the retriever and the reader is novel enough (as mentioned above).",0.1666666666666666,0.0212765957446808,0.1458333333333333,0.1458333333333333,0.1294117647058823,0.0119047619047619,0.0705882352941176,0.0705882352941176,0.1333333333333333,0.0,0.0666666666666666,0.0666666666666666,0.1,0.0,0.0666666666666666,0.0666666666666666,0.1617647058823529,0.0,0.088235294117647,0.088235294117647,0.1294117647058823,0.0119047619047619,0.0705882352941176,0.0705882352941176,0.028169014084507,0.0,0.028169014084507,0.028169014084507,0.1294117647058823,0.0119047619047619,0.0705882352941176,0.0705882352941176,5.93257474899292,3.8715531826019287,13.76333236694336,5.93257474899292,11.50026035308838,7.952589511871338,5.93257474899292,7.349237442016602,0.9603977010186027,0.9536107093922259,0.9271688913129041,0.935652088911062,0.9461277089875954,0.9371044380661667,0.9285724477123675,0.9458674600332577,0.9437326307087133,0.9706344021481839,0.9702319081926719,0.819041339937843,0.6381958318375938,0.7545785918099228,0.9130434099092061,0.935652088911062,0.9461277089875954,0.9371044380661667,0.9529321122782851,0.9551234508774697,0.8999641198767903,0.935652088911062,0.9461277089875954,0.9371044380661667
86,https://openreview.net/forum?id=HkgDTiCctQ,"this paper proposes a framework for few-sample knowledge distillation of convolution neural networks. the basic idea is to fit the output of the student network and that of the teacher network layer-wisely. such a regression problem is parameterized by a 1x1 point-wise convolution per layer (i.e. minimizing the fitting objective over the parameters of 1x1 convolutions). the author claims such an approach, called fskd, is much more sample-efficient than previous works on knowledge distillation. besides, it is also fast to finish the alignment procedure as the number of parameters is smaller than that in previous works. the sample efficiency is confirmed in the experiments on cifar-10, cifar-100 and imagenet with various pruning techniques. in particular, fskd outperforms the fitnet and fine-tuning by non-trivial margins if only small amount of samples are provided (e.g. 100).----------------here are some comments:----------------1. what exactly does absorb mean? is it formally defined in the paper?----------------2. we do not optimize this loss all together using sgd due to that too much hyper-parameters need tuning in sgd. i dont understand (1) why does sgd require too much hyper-parameters tuning and (2) if not sgd, what algorithm do you use? ----------------3. according to the illustration in 3.3, the algorithm looks like a coordinate decent that optimizing l over one q_j at a time, with the rest fixed. however, the sentence until we reach the last block in the student-net means the algorithm only runs one iteration, which i suspect might not be sufficient to converge.----------------4. it is also confusing to use the notation sgd+fskd v.s. fitnet+fskd, as it seems sgd and fitnet are referring to the same type of terminology. however, sgd is an algorithm, while fitnet is an approach for neural network distillation. ----------------5. while i understand the training of student network with fskd should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient? ----------------6. i assume the top-1 accuracies of teacher networks in figure 4 are the same as table 2 and 3, i.e. 93.38% and 72.08% for cifar-10 and cifar-100 respectively. then the student networks have much worse performance (~85% for cifar-10 and ~48% for cifar-100) than the teachers. so does it mean fskd is not good for redesigned student networks?----------------7. while most of the experiments are on cifar10 and cifar100, the abstract and conclusion only mention the results of imagenet. why? in this paper, an efficient re-training algorithm for neural networks is proposed. the essence is like hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. the core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. ----------------the proposed method named fksd is simple yet achieves good performance. also, it performs well with a few samples, which is desirable in terms of time complexity. ----------------the downside of this paper is that there is no clear explanation of why the fksd method goes well. for me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. of course, learning 1x1 conv is easier than learning original conv because of a few parameters. however, it also restricts the representation power so we cannot say which one is always better. do you have any hypothesis of why 1x1 conv works so well?--------------------------------minor:----------------the operator * in (1) is undefined.----------------what does the boldface in tables of the experiments mean? i was confused because, in table 1, the accuracy achieved by fksd is in bold but is not the highest one.","the paper considers the problem of knowledge distillation from a few samples. the proposed solution is to align feature representations of the student network with the teacher by adding 1x1 convolutions to each student block, and learning only the parameters of those layers. as noted by reviewers 1 and 2, the performance of the proposed method is rather poor in absolute terms, and the use case considered (distillation from a few samples) is not motivated well enough. reviewers also note the method is quite simplistic and incremental.","----------------5. while i understand the training of student network with fskd should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient?","----------------5. while i understand the training of student network with fskd should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient?","however, sgd is an algorithm, while fitnet is an approach for neural network distillation.",this paper proposes a framework for few-sample knowledge distillation of convolution neural networks.,"since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples.","----------------5. while i understand the training of student network with fskd should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient?","----------------3. according to the illustration in 3.3, the algorithm looks like a coordinate decent that optimizing l over one q_j at a time, with the rest fixed.","----------------5. while i understand the training of student network with fskd should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient?",0.2241379310344827,0.0526315789473684,0.1896551724137931,0.1896551724137931,0.2241379310344827,0.0526315789473684,0.1896551724137931,0.1896551724137931,0.0792079207920792,0.0,0.0594059405940594,0.0594059405940594,0.198019801980198,0.0404040404040404,0.1188118811881188,0.1188118811881188,0.2385321100917431,0.0186915887850467,0.146788990825688,0.146788990825688,0.2241379310344827,0.0526315789473684,0.1896551724137931,0.1896551724137931,0.1379310344827586,0.0175438596491228,0.1034482758620689,0.1034482758620689,0.2241379310344827,0.0526315789473684,0.1896551724137931,0.1896551724137931,10.810140609741213,5.571424961090088,13.159585952758787,10.810138702392578,10.810140609741213,9.982982635498049,10.810140609741213,4.045103073120117,0.9735551972657372,0.9706937370990074,0.8002087750926414,0.9735551972657372,0.9706937370990074,0.8002087750926414,0.950965023850575,0.9494767489093932,0.9066058346027603,0.9707294722851387,0.9697064920287278,0.9358671851959948,0.8604967538473601,0.9502266403398161,0.8563259255038285,0.9735551972657372,0.9706937370990074,0.8002087750926414,0.965205880605884,0.9649274618942989,0.17455952451325804,0.9735551972657372,0.9706937370990074,0.8002087750926414
87,https://openreview.net/forum?id=HkgxheBFDS,"as an extension of recent developments on adversarial attacks and defenses, this paper proposes a simple but effective technique called undersensitivity on machine comprehension task, where the input question is changed but the prediction does not change when it should be. they use two linguistically informed tricks; pos and ner, to produce the perturbations. in addition to that, several techniques are developed for reducing the adversarial search spaces (eq 1 and 3) and controlling the level of undersensitivity (eq 2). ----------in general, the paper is very well written and clear to read. the formulation of the problem is very straightforward, too. i enjoyed reading the overall paper, especially the experimental results, which provides lots of insights about the techniques. the proposed techniques are simple but they are well-executed in the experiment with reasonable justification. please find my detailed comments below. ---------------method. -----i appreciate the simplicity of the proposed models with clear motivations. also, validation of the approaches is well-executed in the experiment.----------i like the idea of linguistically-controlled perturbations using pos and ner. however, there might be many other ways to control it: for example, parsing a sentence using a constituency parser and replacing each phrase with corresponding synonyms/antonyms using wordnet might be interesting. or, based on the parse, negating the verb might be another way to try. i would expect more linguistically-informed perturbations like these, and i could find some of them from (kang et al 2018, ebrahimi et al., 2018). also, adding a couple of them in the experiment might be interesting to understand the underlying logic of the perturbations. ----------one major concern of the proposed approach is the sub-optimality by the pre-trained rc model. the undersensitivity (eq 2) and adversarial search (eq 3) are calculated by the probability scores predicted by the pre-trained models. this means that producing the new sample x is only based on the correctness of the pre-trained model on new samples generated, which sounds to be unreliable. moreover, using the samples produced by this sub-optimal model may be very limited to produce samples under the sub-optimal space of questions. i wonder how the authors tackle this issue in the experiment. ----------experiment-----adversarial attacks should show how an existing system is fragile to be attacked, but at the same time augmenting or adversarially training with them needs to improve its generalization power of the system against the attacks. however, many of the adversarial attack papers mostly focus on the former but not the latter part. in this work, authors showed a result of adversarial training/augmentation but its generalization power on original task (i.e., hasans case) was not that powerful. the unbiased data setup is interesting but still did not provide any insights about generalization from the adversaries. it would be more convincing to see how this generalization from adversarial attacks can take benefits from bit different tasks such as open-end reading comprehension as a perspective of data augmentation. ----------i see no comparison with other attacking/defending methods in tables 3 and 4. adding the recent models (ebrahimi et al., 2018, wallance et al., 2019) may help understand how the proposed models are more effective than other techniques. the paper proposes a framework for evaluating the sensitivity of a qa model to perturbations in the input. the core of the idea is that one can replace content words (i.e. named entities and nouns) in questions in such a way that makes qa models more confident of their original answer (despite, presumably, the question now being unanswerable). replacements are constructed by mining equivalence classes in squad data (i.e. all words w/ pos = noun are one set). depending on how many such substitutions are searched over (and whether multiple are applied), one can find at least one such failure in about 50% of cases, on a bert model trained on squad2. the paper also proposes a simple mitigation technique: an objective that modifies a given qa example with all possible substitutions and trains for ""no answer"" (or alternatively substitutions which break the system). results demonstrate that performance on squad2 is roughly unchanged while the success rate of the attack is significantly decreased.----------while the idea of forming such equivalence sets is very interesting, my concern with the paper is both in terms of impact and experimental methodology. ----------impact: the method is essentially a data augmentation approach over a fixed list of words. this isn't very different than what was proposed in https://arxiv.org/pdf/1804.06876.pdf and https://arxiv.org/pdf/1807.11714.pdf . while there are some nice nuggets in the analysis, in particular that model confidence is a factor for the attack, i'm not sure anything very novel is being proposed. ----------experimental methodology: other works in this vein explicitly create a split between counterfactual examples evaluated at train vs at test. the methodology proposed here requires a search where there isn't a clear split between what aspects of the search are allowed at train vs test. in doing counterfactual data augmentation, it is possible the model observes most elements of the search that will be evaluated at test time, making it almost inevitable that the search will be less successful after the model is trained. a simple solution would be splitting the equivalence sets into train/test. i was not able to confirm whether or not this happened from the paper.----------that being said, the paper did evaluate on lewis&fan( https://openreview.net/pdf?id=bkx0rja9tx ) 's bias training simulation, which i appreciate, but i was disappointed that (a) the results from lewis&fan were not included for comparison, and when compared the augmentation method proposed here works much worse, in some settings, than generative based training. this paper studies undersensitivity of the neural models for reading comprehension. first, the neural model is trained on reading comprehension tasks with unanswerable questions. then, they add perturbations to the input to turn an answerable question into an unanswerable question, using two methods, pos tag based and named entity based. then, they search for adversarial attacks to find perturbations that the model still predicts the same prediction with even a higher probability. experiments show that the error rate (attack success rate) is high, over 0.9 with pos tag based method and over 0.5 with named entity based method. finally, this paper shows data augmentation and adversarial training for this perturbation help the model to be more robust, especially in a biased data scenario.----------the contribution of this paper is clear to me: it is one of the first studies which investigates undersensitivity of the model when the input text after the perturbation is complete (e.g. in contrast to feng et al 2018 and other related work where the perturbation causes the input text to be incomplete).----------the weakness of this paper is:-----1) the observations are somewhat obvious: it is hard to expect the model to always assign lower probabilities to the original answer when, for example, the named entity in the question is replaced to entities with the same type. also, i think the observation could be more interesting if the adversarial attack works across different models.-----2) table 2 shows that the perturbation does not always work; especially with pos based method, only half of cases work. how many samples were used for this analysis? is there a breakdown of the error rate (attack success rate) showing that the rate is still significant for valid perturbations? i think it is significant since perturbations seem to cause invalid attack with a pretty high probability.----------despite the weakness, i think this paper demonstrates comprehensive studies on this focused area and is worth to be published in iclr overall.","the paper investigates the sensitivity of a qa model to perturbations in the input, by replacing content words, such as named entities and nouns, in questions to make the question not answerable by the document. experimental analysis demonstrates while the original qa performance is not hurt, the models become significantly less vulnerable to such attacks. reviewers all agree that the paper includes a thorough analysis, at the same time they all suggested extensions to the paper, such as comparison to earlier work, experimental results, which the authors made in the revision. however, reviewers also question the novelty of the approach, given data augmentation methods. hence, i suggest rejecting the paper.","results demonstrate that performance on squad2 is roughly unchanged while the success rate of the attack is significantly decreased.----------while the idea of forming such equivalence sets is very interesting, my concern with the paper is both in terms of impact and experimental methodology.","finally, this paper shows data augmentation and adversarial training for this perturbation help the model to be more robust, especially in a biased data scenario.----------the contribution of this paper is clear to me: it is one of the first studies which investigates undersensitivity of the model when the input text after the perturbation is complete (e.g. in contrast to feng et al 2018 and other related work where the perturbation causes the input text to be incomplete).----------the weakness of this paper is:-----1) the observations are somewhat obvious: it is hard to expect the model to always assign lower probabilities to the original answer when, for example, the named entity in the question is replaced to entities with the same type.","as an extension of recent developments on adversarial attacks and defenses, this paper proposes a simple but effective technique called undersensitivity on machine comprehension task, where the input question is changed but the prediction does not change when it should be.","as an extension of recent developments on adversarial attacks and defenses, this paper proposes a simple but effective technique called undersensitivity on machine comprehension task, where the input question is changed but the prediction does not change when it should be.","experiments show that the error rate (attack success rate) is high, over 0.9 with pos tag based method and over 0.5 with named entity based method.","finally, this paper shows data augmentation and adversarial training for this perturbation help the model to be more robust, especially in a biased data scenario.----------the contribution of this paper is clear to me: it is one of the first studies which investigates undersensitivity of the model when the input text after the perturbation is complete (e.g. in contrast to feng et al 2018 and other related work where the perturbation causes the input text to be incomplete).----------the weakness of this paper is:-----1) the observations are somewhat obvious: it is hard to expect the model to always assign lower probabilities to the original answer when, for example, the named entity in the question is replaced to entities with the same type.","they use two linguistically informed tricks; pos and ner, to produce the perturbations.","finally, this paper shows data augmentation and adversarial training for this perturbation help the model to be more robust, especially in a biased data scenario.----------the contribution of this paper is clear to me: it is one of the first studies which investigates undersensitivity of the model when the input text after the perturbation is complete (e.g. in contrast to feng et al 2018 and other related work where the perturbation causes the input text to be incomplete).----------the weakness of this paper is:-----1) the observations are somewhat obvious: it is hard to expect the model to always assign lower probabilities to the original answer when, for example, the named entity in the question is replaced to entities with the same type.",0.2467532467532467,0.0394736842105263,0.1298701298701298,0.1298701298701298,0.4017094017094016,0.0948275862068965,0.2051282051282051,0.2051282051282051,0.1721854304635761,0.0134228187919463,0.0927152317880794,0.0927152317880794,0.1721854304635761,0.0134228187919463,0.0927152317880794,0.0927152317880794,0.1159420289855072,0.0294117647058823,0.0579710144927536,0.0579710144927536,0.4017094017094016,0.0948275862068965,0.2051282051282051,0.2051282051282051,0.08130081300813,0.0,0.048780487804878,0.048780487804878,0.4017094017094016,0.0948275862068965,0.2051282051282051,0.2051282051282051,10.440482139587402,7.271381378173828,14.108006477355955,10.440482139587402,5.827949047088623,14.108006477355955,10.440482139587402,8.846719741821289,0.05926759871109456,0.07564866826329111,0.7968922622222522,0.5182287372378186,0.39489540487618574,0.5904426597398338,0.9710770405972148,0.9641984895603258,0.9398300563371336,0.9710770405972148,0.9641984895603258,0.9398300176537128,0.3056584784846158,0.45565183830240624,0.8770867535325843,0.5182287372378186,0.39489540487618574,0.5904426597398338,0.9466244433528419,0.9574075893957107,0.813693833613151,0.5182287372378186,0.39489540487618574,0.5904420635319712
88,https://openreview.net/forum?id=HklSeREtPB,"## overview-----this paper studies whether a recurrent neural network trained to solve a particular task (integration of angular velocity to generate head direction). given trained recurrent networks that solve the task, the paper analyzes these using a number of different methods (visualizing tuning curves, perturbation experiments, and varying input statistics). overall, i think this is a nice paper that shows the power of using artificial networks as a model system to answer neuroscientifically motivated questions. in particular, i found the perturbation analyses particularly illuminating. these kinds of experiments are only possible in these artificial networks, and can highlight/guide future biological experiments.----------## major comments------ ""units with minimal tuning to both variables are discarded from further analysis"" -- how many units end up being discarded? the reason i ask is because the number of discarded neurons affects this statement: ""therefore, units in the trained rnn could be mapped on to the biological head direction system both in terms of general functional architecture and detailed tuning properties."". if a large number of neurons are discarded, then the statement should be amended to say that ""a fraction of units in the trained rnn"" can be mapped onto the hd system. however, my understanding is that many biological neurons have tuning properties that are hard to classify, perhaps the discarded neurons could map on to these previously uncharacterized neurons?------ the paper demonstrates a number of qualitiative similarities between artificial and biological networks (e.g. both contain neurons tuned for hd or av). it would be even more compelling if, wherever possible, these comparisons were made to be quantitative.------ ""we conjecture that ring units in the trained rnn serve to maintain a stable activity bump in the absence of inputs"". i think a cool direct test of this idea would be to use the techniques in barak & sussillo 2013 (find fixed points of the recurrent network dynamics, and analyze the linearized system at those fixed points) to identify the ring attractor structure. in particular, numerical auto-differentiation can be used to automatically identify these points (c.f. the methods in https://arxiv.org/abs/1907.08549). using these tools, can one find the ring attractor hidden in these networks?----------## minor comments/questions------ the layout in fig 2a (and fig 5c, 5f, and 5i) is a little misleading. if i understand correctly, the axes labels apply to each individual panel (which shows tuning for a particular neuron). however, since the labels extend across many panels, it looks as if the panels themselves are organized according to angular velocity and head direction, which doesn't make sense.------ missing citation towards the end of pg. 3------ consider setting the panels in fig 5a-h to have the same axes limits, for easier comparison. this paper examines head direction representations in a rnn. the rnn was trained to report current head direction using initial head direction and angular velocity information as inputs. the authors compare the representations in the rnn to the representations found in the head direction systems of mice and flies. they find that the representations and connectivity matrices of the rnn recapitulate many aspects of the real brains head direction systems. these include the presence of cells tuned only for head direction (ring cells) and cells with combined head direction and angular velocity tuning (shift cells). these cells tile the space in a functional torus structure, as revealed by t-sne, which resembles the structures seen in real brains. similarly, the connectivity profiles and functional role of these cell types matches what is observed in the brains. altogether, these results demonstrate that rnns optimized for this task can capture both the functional and anatomical aspects of the systems found in animals. this validates the use of rnns for studying these systems. in my opinion, it also suggests that information about the structure of brains could inform strong priors for anns.----------i think this is a great paper. it was a pleasure to read, the data was very clear, and the results very interesting. it should be accepted in my opinion. i have only a few minor notes for improvement.----------- per my last note in the summary, it would be nice if the authors made the paper a little bit more relevant for machine learners by discussing how this information could potentially inform novel neural network architectures for navigation.----------- theres a missing reference on page 3, second last paragraph.----------- figure 5 a,d, and g would be easier to interpret if they all had the same x-range as g (that way, you could see the narrower ranges and wouldnt have to notice that the scales were different).----------- why have figure s8 and fig 3 separate? arent they nearly identical? fig s8 isnt that much bigger, why not just make it fig. 3? the authors train rnns to integrate motion cues, to determine the head direction of a simulated animal. they then investigate the tuning properties of the units in the rnn, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent.----------similarities are quite strong: the rnn learns to reproduce the ring attractor seen in the fly, where head direction (hd) cells excite other hd cells with similar direction preference and inhibit those with dissimilar preference. shifter cells are also observed, that integrate angular velocity cues and ""shift"" the representation of head direction: these show an intuitive asymmetric connectivity profile. ----------i can't quite decide what to make of this paper. one the one hand, the recapitulation of the biological circuit in the rnn is fairly compelling. on the other hand, other models (admittedly with hand-tuned connectivity) yield quite similar findings. so my overall opinion comes down to the question of: how big a deal is it that the rnn learned this connectivity, instead of it being designed by a modeller like xiao-jing wang?----------i have a few specific suggestions:----------1) it would be useful as a comparison to duplicate the analysis of unit tuning, connectivity, etc., in untrained networks. recent work (arxiv:1909.10116 [q-bio.nc]) shows that, even in untrained randomly-connected rnns, some units have (by chance) tuning, and that tuning depends on the connectivity between units in a manner that could somewhat resemble the attractor models.----------2) there were important details that i couldn't find. for example, how many of the rnn units receive the external inputs? is it all of them? is that realistic?----------3) i question the approach (first paragraph of sec. 3.1) of only analyzing the units with strong hd and/or av selectivity. presumably the mouse shows some units with weak tuning, and investigating the connectivity of the weakly-tuned units in the rnn (e.g., their connectivity to each other, and to hd and av units) could be informative.----------one analysis to consider: ablate the nonselective units after training, and run the rnn with no retraining. does the head direction tracking function remain intact or degrade? based on some previous work with feedforward nets (arxiv:1803.06959 [stat.ml]), i'd guess that the function could be seriously degraded by removing nonselective units. that modelling result could make a strong prediction for experiments: inactivation of nonselective neurons (e.g., using archaerhodopsin), would impair function.","this paper studies properties that emerge in an rnn trained to report head direction, showing that several properties in natural neural circuits performing that function are detected. -----all reviewers agree that this is quite an interesting paper. while there are some reservations as to the value of letting a property of interest emerge as opposed to simply hand-coding it in, this approach is seen as powerful and valuable by many people, in that it suggests a higher plausibility that the emerging properties are actually useful when optimizing for that function -- a claim which hand-coding would not make possible. reviewers have also provided valuable suggestions and requests for clarifications, and authors have responded by improving the presentation and providing more insights.-----overall, this is a solid contribution that will be of interest to the part of the iclr audience that is interested in biological systems.","the reason i ask is because the number of discarded neurons affects this statement: ""therefore, units in the trained rnn could be mapped on to the biological head direction system both in terms of general functional architecture and detailed tuning properties."".","they then investigate the tuning properties of the units in the rnn, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent.----------similarities are quite strong: the rnn learns to reproduce the ring attractor seen in the fly, where head direction (hd) cells excite other hd cells with similar direction preference and inhibit those with dissimilar preference.","they then investigate the tuning properties of the units in the rnn, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent.----------similarities are quite strong: the rnn learns to reproduce the ring attractor seen in the fly, where head direction (hd) cells excite other hd cells with similar direction preference and inhibit those with dissimilar preference.",## overview-----this paper studies whether a recurrent neural network trained to solve a particular task (integration of angular velocity to generate head direction).,"they then investigate the tuning properties of the units in the rnn, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent.----------similarities are quite strong: the rnn learns to reproduce the ring attractor seen in the fly, where head direction (hd) cells excite other hd cells with similar direction preference and inhibit those with dissimilar preference.","they then investigate the tuning properties of the units in the rnn, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent.----------similarities are quite strong: the rnn learns to reproduce the ring attractor seen in the fly, where head direction (hd) cells excite other hd cells with similar direction preference and inhibit those with dissimilar preference.",does the head direction tracking function remain intact or degrade?,"they then investigate the tuning properties of the units in the rnn, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent.----------similarities are quite strong: the rnn learns to reproduce the ring attractor seen in the fly, where head direction (hd) cells excite other hd cells with similar direction preference and inhibit those with dissimilar preference.",0.2245989304812834,0.0216216216216216,0.1176470588235294,0.1176470588235294,0.2285714285714285,0.0384615384615384,0.1238095238095238,0.1238095238095238,0.2285714285714285,0.0384615384615384,0.1238095238095238,0.1238095238095238,0.1420118343195266,0.0479041916167664,0.0946745562130177,0.0946745562130177,0.2285714285714285,0.0384615384615384,0.1238095238095238,0.1238095238095238,0.2285714285714285,0.0384615384615384,0.1238095238095238,0.1238095238095238,0.0512820512820512,0.0129870129870129,0.0384615384615384,0.0384615384615384,0.2285714285714285,0.0384615384615384,0.1238095238095238,0.1238095238095238,5.9531779289245605,5.9531779289245605,10.990301132202148,5.9531779289245605,7.173851013183594,5.9531779289245605,5.9531779289245605,5.206729888916016,0.926082549750339,0.9560627739281524,0.5956899340926535,0.4507314656586684,0.6041770270960583,0.9412353114989355,0.4507314656586684,0.6041770270960583,0.9412352624091841,0.9670138331903435,0.9615704103387682,0.8049876641772493,0.4507314656586684,0.6041770270960583,0.9412352869540591,0.4507314656586684,0.6041770270960583,0.9412353114989355,0.34291305824281765,0.5519283151968335,0.78048957125764,0.4507314656586684,0.6041770270960583,0.9412352624091841
89,https://openreview.net/forum?id=HklY120cYm,"after reading other reviews and author comments, i have raised my rating to a 6. my main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments). however, i'm not against the paper as an interesting finding in and of itself. it would be great if the authors (or interested members of the research community) may analyze how general-purpose their proposals are (e.g., of gaussian base distribution) and how extensive the results are on tts benchmarks.----------------------------------overall, i very much like the direction this paper pursues. however, the content doesn't substantiate their two claimed contributions. i highly recommend the authors either back up their claims in more detail, or center their work in terms of the result and less so about the ideas (which at the moment, are not convincing to use outside of this specific setup).----------------the authors propose two contributions:----------------1. they build on parallel wavenet which uses distillation by minimizing a kl divergence from a logistic iaf as a student to a mixture of logistic af as a teacher. instead, they simply use gaussians which has a closed-form kl divergence and makes training during distillation significantly simpler. because of stability problems, they also add 1. a penalty term to discourage the original loss from dividing by a standard deviation close to zero; and 2. converting van den oord et al. (2018)'s average power loss penalty to a frame-level loss penalty.----------------their choice of gaussians requires a restriction on the likelihood, and they show one result arguing the likelihood choice doesn't make much of a difference. this result comprises 4 human-evaluated numbers, with a fixed architecture and training hyperparameters of their choice. unfortunately, i'm not convinced. can the authors provide more compelling evidence? if the authors argue this is one of their main contributions, i find that lack of a more comprehensive empirical or theoretical study disconcerting.----------------similarly, while i like that using gaussian kls makes the distillation objective in closed-form, there isn't evidence indicating the benefit. the one result (the 4 numbers above) are conflated by both the change in model as well as utilizing the closed-form loss. the same goes for their one result (2 numbers) comparing forward to reverse kl.----------------2. they ""propose the first text-to-wave neural architecture for tts, which can be trained from scratch in an end-to-end--------manner."" i'm not an expert on speech so i can't accurately assess the novelty here. however, it would be nice to show these results independent of the other proposed changes.----------------writing-wise, the paper was clear, although potentially too packed with background information. as a expert on generative models, most of sections 1-3 are already well-known and could be made more concise by referencing past works for more details. they add various details (such as the architecture notes at the end of 3.1) which should be better placed elsewhere to tease out what the important changes are in this paper. paper summary:----------------the paper presents two distinct contributions in text-to-speech systems:--------a) it describes a method for distilling a gaussian wavenet into a gaussian inverse autoregressive flow that uses an analytically computed kl between their conditionals.--------b) it presents a text-to-speech system that is trained end-to-end from text to waveforms.----------------technical quality:----------------the distillation method presented in the paper is technically correct. the evaluation is based on mean opinion score and seems to follow good practices.----------------the paper makes three claims:--------a) a wavenet with gaussian conditionals can model speech waveforms equally well as wavenets with other types of conditionals.--------b) analytically computing kl divergence stabilizes distillation.--------c) a text-to-speech system trained end-to-end from text to waveforms outperforms one that has separately trained text-to-spectrogram and spectrogram-to-waveform subsystems.----------------claims (a) and (c) are clearly demonstrated in the experiments. however, there is nothing in the paper that substantiates claim (b). i think the paper would be strengthened if the performance of sample-based kl distillation was added into table 2, and if learning curves were reported that evaluate the amount of stabilization that an analytical kl may offer vs a sample-based kl.----------------further points about the experiments:--------- it wasn't clear to me whether distillation happens at the same time as the autoregressive wavenet is trained on data, or after it has been fully trained. i think the paper should make this clear.--------- the paper says that distillation makes generation three orders of magnitude faster. i think it would be good if actual generation times (e.g. in seconds) were reported.----------------clarity:----------------the paper is generally well-written. sections 1 and 2 in particular are excellent.----------------however, section 3 contains several notational errors and technical inaccuracies, that makes it rather confusing to read. in particular:--------- q(x_t | z_{<=t}) is used in several places to mean the gaussian conditional q(x_t | z_{<t}) (e.g. in eqs (6) and (7), and elsewhere). this is confusing, as q(x_t | z_{<=t}) is actually a delta distribution.--------- q(x | z) is used in several places to mean q(x) (e.g. in eq. (7), in alg. 1 and elsewhere). this is confusing, as q(x | z) is also a delta distribution.--------i believe that section 3, especially subsections 3.2 and 3.3.1, should be reworked to be made clearer, and the notation should be carefully revised.----------------i don't think the paper needs to span 9 pages. section 3 is rather wordy, and should be compressed to the important points.----------------originality:----------------distilling a gaussian autoregressive model to another gaussian autoregressive model by matching their gaussian conditionals with an analytical kl is rather straightforward, and, methodologically speaking, i wouldn't consider it an original contribution on its own. however, i think its application and demonstration in text-to-speech constitutes an original contribution.----------------significance:----------------the paper contains a substantial amount of significant work that i think is important to be communicated to the iclr community, especially the text-to-speech community.----------------review summary:----------------pros:--------+ substantial amount of good work.--------+ significant improvement in text-to-speech end-to-end software.--------+ generally well-written (with the exception of section 3 which needs work).----------------cons:--------- some more experiments would be good to substantiate the claim that analytical kl is better.--------- notational errors and confusion in section 3.--------- too wordy, no need for 9 pages.----------------nitpicks:--------- as i said above, i wouldn't consider distillation of models with gaussian conditionals using analytical kls methodologically novel, so i think the phrase ""novel regularized kl divergence"" should be moderated.--------- eq. (1) should contain theta on the left hand side too.--------- page 3: ""at appendix b"" --> ""in appendix b"".--------- page 4: in flows we don't just ""suppose z has the same dimension as x""; rather, it's a necessary condition that must hold.--------- footnote 5: it's unclear to me what it means to ""make the loss less sensitive"".--------- references: real nvp, fourier, bayes, pixelcnn, wavenet, voiceloop should be properly capitalized.","the authors discuss an improved distillation scheme for parallel wavenet using a gaussian inverse autoregressive flow, which can be computed in closed-form, thus simplifying training. the work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably. the ac agrees with the reviewers that the work is a valuable contribution, particularly in the context of end-to-end neural text-to-speech systems.","if the authors argue this is one of their main contributions, i find that lack of a more comprehensive empirical or theoretical study disconcerting.----------------similarly, while i like that using gaussian kls makes the distillation objective in closed-form, there isn't evidence indicating the benefit.","however, i think its application and demonstration in text-to-speech constitutes an original contribution.----------------significance:----------------the paper contains a substantial amount of significant work that i think is important to be communicated to the iclr community, especially the text-to-speech community.----------------review summary:----------------pros:--------+ substantial amount of good work.--------+ significant improvement in text-to-speech end-to-end software.--------+ generally well-written (with the exception of section 3 which needs work).----------------cons:--------- some more experiments would be good to substantiate the claim that analytical kl is better.--------- notational errors and confusion in section 3.--------- too wordy, no need for 9 pages.----------------nitpicks:--------- as i said above, i wouldn't consider distillation of models with gaussian conditionals using analytical kls methodologically novel, so i think the phrase ""novel regularized kl divergence"" should be moderated.--------- eq.","however, i think its application and demonstration in text-to-speech constitutes an original contribution.----------------significance:----------------the paper contains a substantial amount of significant work that i think is important to be communicated to the iclr community, especially the text-to-speech community.----------------review summary:----------------pros:--------+ substantial amount of good work.--------+ significant improvement in text-to-speech end-to-end software.--------+ generally well-written (with the exception of section 3 which needs work).----------------cons:--------- some more experiments would be good to substantiate the claim that analytical kl is better.--------- notational errors and confusion in section 3.--------- too wordy, no need for 9 pages.----------------nitpicks:--------- as i said above, i wouldn't consider distillation of models with gaussian conditionals using analytical kls methodologically novel, so i think the phrase ""novel regularized kl divergence"" should be moderated.--------- eq.","after reading other reviews and author comments, i have raised my rating to a 6. my main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments).",the evaluation is based on mean opinion score and seems to follow good practices.----------------the paper makes three claims:--------a) a wavenet with gaussian conditionals can model speech waveforms equally well as wavenets with other types of conditionals.--------b) analytically computing kl divergence stabilizes distillation.--------c) a text-to-speech system trained end-to-end from text to waveforms outperforms one that has separately trained text-to-spectrogram and spectrogram-to-waveform subsystems.----------------claims (a) and (c) are clearly demonstrated in the experiments.,paper summary:----------------the paper presents two distinct contributions in text-to-speech systems:--------a) it describes a method for distilling a gaussian wavenet into a gaussian inverse autoregressive flow that uses an analytically computed kl between their conditionals.--------b) it presents a text-to-speech system that is trained end-to-end from text to waveforms.----------------technical quality:----------------the distillation method presented in the paper is technically correct.,"i highly recommend the authors either back up their claims in more detail, or center their work in terms of the result and less so about the ideas (which at the moment, are not convincing to use outside of this specific setup).----------------the authors propose two contributions:----------------1. they build on parallel wavenet which uses distillation by minimizing a kl divergence from a logistic iaf as a student to a mixture of logistic af as a teacher.","however, i think its application and demonstration in text-to-speech constitutes an original contribution.----------------significance:----------------the paper contains a substantial amount of significant work that i think is important to be communicated to the iclr community, especially the text-to-speech community.----------------review summary:----------------pros:--------+ substantial amount of good work.--------+ significant improvement in text-to-speech end-to-end software.--------+ generally well-written (with the exception of section 3 which needs work).----------------cons:--------- some more experiments would be good to substantiate the claim that analytical kl is better.--------- notational errors and confusion in section 3.--------- too wordy, no need for 9 pages.----------------nitpicks:--------- as i said above, i wouldn't consider distillation of models with gaussian conditionals using analytical kls methodologically novel, so i think the phrase ""novel regularized kl divergence"" should be moderated.--------- eq.",0.2666666666666666,0.0508474576271186,0.1333333333333333,0.1333333333333333,0.3142857142857143,0.048076923076923,0.1333333333333333,0.1333333333333333,0.3142857142857143,0.048076923076923,0.1333333333333333,0.1333333333333333,0.205607476635514,0.0,0.1121495327102803,0.1121495327102803,0.3312101910828025,0.0774193548387096,0.1528662420382165,0.1528662420382165,0.4225352112676056,0.1428571428571428,0.2394366197183098,0.2394366197183098,0.3178807947019867,0.0268456375838926,0.1456953642384106,0.1456953642384106,0.3142857142857143,0.048076923076923,0.1333333333333333,0.1333333333333333,5.309659004211426,7.2547502517700195,12.764212608337402,9.02487564086914,8.511270523071289,9.02487564086914,9.02487564086914,8.114309310913086,0.9247462040930032,0.9462048948339585,0.9353936010939338,0.17042502089608225,0.49159696135579467,0.4593363280939626,0.17042502089608225,0.49159696135579467,0.4593363280939626,0.9555166726669625,0.9556332160468707,0.9179381056045984,0.4240765179780394,0.4665612198891951,0.40256232766648553,0.9064145788318234,0.9187827804780456,0.9410255785835919,0.9691427956987037,0.9735986249439861,0.8818914777921468,0.17042502089608225,0.49159696135579467,0.4593363280939626
90,https://openreview.net/forum?id=Hklr204Fvr,"this work introduces fixed grouping layers to deep learning models. unlike convolutional layers, the fixed grouping layer only allows the output to be impacted by the specific inputs associated to it by its group. the paper stays firmly in the area of brain scans with the authors hoping to generalize to other applications later. their experiments compare to what they consider to be the state-of-the-art approaches to the problem. ----------i did find figure s2a helpful to quickly understand some of the simulated data approach, maybe it should be in the paper and not supplementary. ----------i argue for accepting this paper because the experiments, while focused solely in one discipline, are thorough. i would have liked clearer description of other areas that might benefit in the last sentence. ""towards a deep network architecture for structured smoothness"" proposes a new layer, dubbed fgl, specifically focused on structured smoothness.-----the paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fmri analysis. the writing is clear, the method is well formulated, and performance compared to relevant and recent benchmarks is strong. thus most of the questions remaining are about the finer details of the proposed layer and method.----------what is the sensitivity of fgl to non-optimal group proposals for a? given the authors reference aydore et. al., do randomized (or perhaps meta-optimized) groupings perform poorly? are there any potential ways to jointly-or-iteratively learn the groupings? is ward clustering the primary practical method for getting the groupings?----------can the authors discuss a bit the relationship and potential tradeoffs between ""early"" segmentation architectures (such as fgl), and those often favored in semantic segmentation (""late"" segmentation)? is there a particular reason why ""late"" segmentation is particularly a poor choice in brain imaging?----------my primary concerns come in relation to other methods for structured smoothness such as crf/mrf, and the trade-offs and drawbacks of imperfect or downright wrong a, and alternative methods for practically finding good a matrices (potentially with an eye to applications outside fmri). addressing some or all of these in a section or two of the text body would potentially raise my score.----------some small errors:-----optimg -> opting introduction-----guassian -> gaussian section 3---------potential useful references:-----crf as rnn https://arxiv.org/abs/1502----efficient piecewise training https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/lin_efficient_piecewise_training_cvpr_2016_paper.html-----deep learning markov random field for semantic segmentation https://arxiv.org/abs/1606----pixel adaptive convolutional neural networks http://openaccess.thecvf.com/content_cvpr_2019/html/su_pixel-adaptive_convolutional_neural_networks_cvpr_2019_paper.html","the ac has carefully looked at the paper/comments/discussion in order to arrive at this meta-review.----------looking over the paper, the fgl layer is an interesting idea, but its utility is only evaluated in a limited setting (fmri data), rather that other types of images/data. also, the approach seems to work on some of the fmri datasets, on others the performance is on par with the baselines. ----------overall, the paper is borderline but the ac believes the paper would be a good contribution to the conference.","is there a particular reason why ""late"" segmentation is particularly a poor choice in brain imaging?----------my primary concerns come in relation to other methods for structured smoothness such as crf/mrf, and the trade-offs and drawbacks of imperfect or downright wrong a, and alternative methods for practically finding good a matrices (potentially with an eye to applications outside fmri).","""towards a deep network architecture for structured smoothness"" proposes a new layer, dubbed fgl, specifically focused on structured smoothness.-----the paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fmri analysis.","""towards a deep network architecture for structured smoothness"" proposes a new layer, dubbed fgl, specifically focused on structured smoothness.-----the paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fmri analysis.",this work introduces fixed grouping layers to deep learning models.,"""towards a deep network architecture for structured smoothness"" proposes a new layer, dubbed fgl, specifically focused on structured smoothness.-----the paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fmri analysis.","""towards a deep network architecture for structured smoothness"" proposes a new layer, dubbed fgl, specifically focused on structured smoothness.-----the paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fmri analysis.","unlike convolutional layers, the fixed grouping layer only allows the output to be impacted by the specific inputs associated to it by its group.","""towards a deep network architecture for structured smoothness"" proposes a new layer, dubbed fgl, specifically focused on structured smoothness.-----the paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fmri analysis.",0.2133333333333333,0.0,0.1333333333333333,0.1333333333333333,0.2518518518518518,0.0300751879699248,0.1481481481481481,0.1481481481481481,0.2518518518518518,0.0300751879699248,0.1481481481481481,0.1481481481481481,0.0808080808080808,0.0,0.0606060606060606,0.0606060606060606,0.2518518518518518,0.0300751879699248,0.1481481481481481,0.1481481481481481,0.2518518518518518,0.0300751879699248,0.1481481481481481,0.1481481481481481,0.15929203539823,0.0,0.1238938053097345,0.1238938053097345,0.2518518518518518,0.0300751879699248,0.1481481481481481,0.1481481481481481,7.026252746582031,7.026252746582031,13.003365516662598,7.026252746582031,7.321403980255127,7.026252746582031,7.026252746582031,9.45962142944336,0.9679315478635209,0.9678092207753213,0.7330730841943011,0.9896830612072959,0.983224131172829,0.41163234398021364,0.9896830612072959,0.983224131172829,0.41163234398021364,0.9758689355501698,0.9759930356856199,0.9142996500639944,0.9896830612072959,0.983224131172829,0.41163216515542334,0.9896830612072959,0.983224131172829,0.41163333978493455,0.9472817935093937,0.9594379887489671,0.9426404729719527,0.9896830612072959,0.983224131172829,0.41163234398021364
91,https://openreview.net/forum?id=HyH9lbZAW,"the paper seems to be significant since it integrates pgm inference with deep models. specifically, the idea is to use the structure of the pgm to perform efficient inference. a variational message passing approach is developed which performs natural-gradient updates for the pgm part and stochastic gradient updates for the deep model part. performance comparison is performed with an existing approach that does not utilize the pgm structure for inference.--------the paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating pgms with deep model updates. as compared to the existing approach where the pgm parameters must converge before updating the dnn parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution.----------------the motivation of the paper, and the description of its contribution as compared to existing methods can be improved. one of the main aspects it seems is generality, but the encodings are specific to 2 types pgms. can this be generalized to arbitrary pgm structures? how about cases when computing z is intractable? could the proposed approach be adapted to such cases. i was not very sure as to why the proposed method is more general than existing approaches.----------------regarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets. the approach shows that the proposed methods converge faster than existing methods. however, i think there is value in the approach, and the connection between variational methods with dnns is interesting. this paper presents a variational inference algorithm for models that contain--------deep neural network components and probabilistic graphical model (pgm)--------components.--------the algorithm implements natural-gradient message-passing where the messages--------automatically reduce to stochastic gradients for the non-conjugate neural--------network components. the authors demonstrate the algorithm on a gaussian mixture--------model and linear dynamical system where they show that the proposed algorithm--------outperforms previous algorithms. overall, i think that the paper proposes some--------interesting ideas, however, in its current form i do not think that the novelty--------of the contributions are clearly presented and that they are not thoroughly--------evaluated in the experiments.----------------the authors propose a new variational inference algorithm that handles models--------with deep neural networks and pgm components. however, it appears that the--------authors rely heavily on the work of (khan & lin, 2017) that actually provides--------the algorithm. as far as i can tell this paper fits inference networks into--------the algorithm proposed in (khan & lin, 2017) which boils down to i) using an--------inference network to generate potentials for a conditionally-conjugate--------distribution and ii) introducing new pgm parameters to decouple the inference--------network from the model parameters. these ideas are a clever solution to work--------inference networks into the message-passing algorithm of (khan & lin, 2017),--------but i think the authors may be overselling these ideas as a brand new algorithm.--------i think if the authors sold the paper as an alternative to (johnson, et al., 2016)--------that doesn't suffer from the implicit gradient problem the paper would fit into--------the existing literature better.----------------another concern that i have is that there are a lot of conditiona-conjugacy--------assumptions baked into the algorithm that the authors only mention at the end--------of the presentation of their algorithm. additionally, the authors briefly state--------that they can handle non-conjugate distributions in the model by just using--------conjugate distributions in the variational approximation. though one could do--------this, the authors do not adequately show that one should, or that one can do this--------without suffering a lot of error in the posterior approximation. i think that--------without an experiment the small section on non-conjugacy should be removed.----------------finally, i found the experimental evaluation to not thoroughly demonstrate the--------advantages and disadvantages of the proposed algorithm. the algorithm was applied--------to the two models originally considered in (johnson, et al., 2016) and the--------proposed algorithm was shown to attain lower mean-square errors for the two--------models. the experiments do not however demonstrate why the algorithm is--------performing better. for instance, is the (johnson, et al., 2016) algorithm--------suffering from the implicit gradient? it also would have been great to have--------considered a model that the (johnson, et. al., 2016) algorithm would not work--------well on or could not be applied to show the added applicability of the proposed--------algorithm.----------------i also have some minor comments on the paper:--------- there are a lot of typos.--------- the first two sentences of the abstract do not really contribute anything-------- to the paper. what is a powerful model? what is a powerful algorithm?--------- dnn was used in section 2 without being defined.--------- using p() as an approximate distribution in section 3 is confusing notation-------- because p() was used for the distributions in the model.--------- how is the covariance matrix parameterized that the inference network produces?--------- the phrases ""first term of the inference network"" are not clear. just use the-------- dnn term and the pgm term of the inference networks, and better still throw-------- in a reference to eq. (4).--------- the term ""deterministic parameters"" was used and never introduced.--------- at the bottom of page 5 the extension to the non-conjugate case should be-------- presented somewhere (probably the appendix) since the fact that you can do-------- this is a part of your algorithm that's important. the authors adapts stochastic natural gradient methods for variational inference with structured inference networks. the variational approximation proposed is similar to svae by jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter. the authors then extends and adapts the natural gradient method by khan & lin (2017) to optimize all the variational parameters. in the experiments the authors generally show improved convergence over svae.----------------the idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation. the main motivation seems to be that it is easier to optimize.----------------- in the last two sentences of the updates for \theta_pgm you mention that you need to do svi/vmp to compute the function \eta_x\theta. might this also suffer from non-convergence issues like you argue svae does? or do you simply mean that computation of this is exact using regular message passing/kalman filter/forward-backward?--------- it was not clear to me why we should use a gaussian approximation for the \theta_nn parameters? the prior might be gaussian but the posterior is not? is this more of a simplifying assumption?--------- there has recently been interest in using inference networks as part of more flexible variational approximations for structured models. some examples of related work missing in this area is ""variational sequential monte carlo"" by naesseth et al. (2017) / ""filtering variational objectives"" by maddison et al. (2017) / ""auto-encoding sequential monte carlo"" le et al. (2017).--------- section 2.1, paragraph nr 5, ""algorihtm"" -> ""algorithm""","thank you for submitting you paper to iclr. the paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. the framework extends jonhson et al. (2016) and khan & lin (2017). the reviewers are all in agreement that the paper is suitable for publication. the paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. the experiments are not the strongest suit of the paper and, although the revision has improved this aspect, i would encourage a more comprehensive evaluation of the proposed methods. nevertheless, this is a strong paper.",in the experiments the authors generally show improved convergence over svae.----------------the idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation.,"these ideas are a clever solution to work--------inference networks into the message-passing algorithm of (khan & lin, 2017),--------but i think the authors may be overselling these ideas as a brand new algorithm.--------i think if the authors sold the paper as an alternative to (johnson, et al., 2016)--------that doesn't suffer from the implicit gradient problem the paper would fit into--------the existing literature better.----------------another concern that i have is that there are a lot of conditiona-conjugacy--------assumptions baked into the algorithm that the authors only mention at the end--------of the presentation of their algorithm.","what is a powerful algorithm?--------- dnn was used in section 2 without being defined.--------- using p() as an approximate distribution in section 3 is confusing notation-------- because p() was used for the distributions in the model.--------- how is the covariance matrix parameterized that the inference network produces?--------- the phrases ""first term of the inference network"" are not clear.",the paper seems to be significant since it integrates pgm inference with deep models.,this paper presents a variational inference algorithm for models that contain--------deep neural network components and probabilistic graphical model (pgm)--------components.--------the algorithm implements natural-gradient message-passing where the messages--------automatically reduce to stochastic gradients for the non-conjugate neural--------network components.,"these ideas are a clever solution to work--------inference networks into the message-passing algorithm of (khan & lin, 2017),--------but i think the authors may be overselling these ideas as a brand new algorithm.--------i think if the authors sold the paper as an alternative to (johnson, et al., 2016)--------that doesn't suffer from the implicit gradient problem the paper would fit into--------the existing literature better.----------------another concern that i have is that there are a lot of conditiona-conjugacy--------assumptions baked into the algorithm that the authors only mention at the end--------of the presentation of their algorithm.",is this more of a simplifying assumption?--------- there has recently been interest in using inference networks as part of more flexible variational approximations for structured models.,"these ideas are a clever solution to work--------inference networks into the message-passing algorithm of (khan & lin, 2017),--------but i think the authors may be overselling these ideas as a brand new algorithm.--------i think if the authors sold the paper as an alternative to (johnson, et al., 2016)--------that doesn't suffer from the implicit gradient problem the paper would fit into--------the existing literature better.----------------another concern that i have is that there are a lot of conditiona-conjugacy--------assumptions baked into the algorithm that the authors only mention at the end--------of the presentation of their algorithm.",0.2281879194630872,0.0136054421768707,0.1208053691275167,0.1208053691275167,0.3689320388349514,0.0784313725490196,0.203883495145631,0.203883495145631,0.2576687116564417,0.0496894409937888,0.1595092024539877,0.1595092024539877,0.1008403361344537,0.017094017094017,0.0672268907563025,0.0672268907563025,0.2567567567567567,0.095890410958904,0.1891891891891892,0.1891891891891892,0.3689320388349514,0.0784313725490196,0.203883495145631,0.203883495145631,0.1984732824427481,0.0,0.0763358778625954,0.0763358778625954,0.3689320388349514,0.0784313725490196,0.203883495145631,0.203883495145631,9.563847541809082,11.474836349487305,16.91800880432129,9.563847541809082,8.949844360351562,7.421168327331543,9.563847541809082,10.101520538330078,0.39878139539790924,0.4872109152803774,0.7786490711968838,0.957096580236915,0.9554767765537641,0.9357140144575468,0.17825592405225243,0.39596862193547416,0.7981106638868498,0.9793279632603188,0.9803381035951712,0.9289756912760905,0.991828805831527,0.9891437866310181,0.9455123376527196,0.957096580236915,0.9554767765537641,0.9357139579922706,0.3207121801204841,0.5076192797732153,0.8457908631102113,0.957096580236915,0.9554767765537641,0.9357140144575468
92,https://openreview.net/forum?id=HyHmGyZCZ,"this paper proposes a ranking-based similarity metric for distributional semantic models. the main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""ranking-based exponential similarity measure"" (resm), which is based on the recently proposed apsyn measure.----------------i think the work has several important issues:----------------1. the work is very light on references. there is a lot of previous work on evaluating similarity in word embeddings (e.g. hill et al, a lot of the papers in repeval workshops, etc.); specialization for similarity of word embeddings (e.g. kiela et al., mrksic et al., and many others); multi-sense embeddings (e.g. from navigli's group); and the hubness problem (e.g. dinu et al.). for the localized centering approach, hara et al.'s introduced that method. none of this work is cited, which i find inexcusable. ----------------2. the evaluation is limited, in that the standard evaluations (e.g. simlex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work. the results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper. it is unclear what exactly helps, in which case, and why. ----------------3. there are technical issues with what is presented, with some seemingly factual errors. for example, ""in this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance. number 1 in the equation stands for the normalizing, hence the similarity is defined as follows"" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in euclidean space due to the properties of the dot product). another example, ""are obtained using the glove vector, not using ppmi"" - there are close relationships between what glove learns and ppmi, which the authors seem unaware of (see e.g. the glove paper and omer levy's work). ----------------4. then there is the additional question, why should we care? the paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings. in other words, what is supposed to be the take-away, and why should we care?----------------as such, i do not recommend it for acceptance - it needs significant work before it can be accepted at a conference.----------------minor points:--------- typo in eq 10--------- typo on page 6 (/cite instead of \cite) the paper suggests taking glove word vectors, adjust them, and then use a non-euclidean similarity function between them. the idea is tested on very small data sets (80 and 50 examples, respectively). the proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets.----------------it isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove. is the point that glove is a bad algorithm? that these steps are general? if the latter, then the experimental results are far weaker than what i would find convincing. why not try on multiple different word embeddings? what happens if you start with random vectors? what happens when you try a bigger data set or a more complex problem?","this paper proposes a method for refining distributional semantic representation at the lexical level. the reviews are fairly unanimous in that they found both the initial version of the paper, which was deemed quite rushed, and the substantial revision unworthy of publication in their current state. the weakness of both the motivation and the experimental results, as well as the lack of a clear hypothesis being tested, or of an explanation as to why the proposed method should work, indicates that this work needs revision and further evaluation beyond what is possible for this conference. i unfortunately must recommend rejection.","the main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""ranking-based exponential similarity measure"" (resm), which is based on the recently proposed apsyn measure.----------------i think the work has several important issues:----------------1. the work is very light on references.","the main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""ranking-based exponential similarity measure"" (resm), which is based on the recently proposed apsyn measure.----------------i think the work has several important issues:----------------1. the work is very light on references.","the paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings.",this paper proposes a ranking-based similarity metric for distributional semantic models.,"the main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""ranking-based exponential similarity measure"" (resm), which is based on the recently proposed apsyn measure.----------------i think the work has several important issues:----------------1. the work is very light on references.","the main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""ranking-based exponential similarity measure"" (resm), which is based on the recently proposed apsyn measure.----------------i think the work has several important issues:----------------1. the work is very light on references.",is the point that glove is a bad algorithm?,"the main idea is to learn ""baseline"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called ""ranking-based exponential similarity measure"" (resm), which is based on the recently proposed apsyn measure.----------------i think the work has several important issues:----------------1. the work is very light on references.",0.1688311688311688,0.0,0.1038961038961038,0.1038961038961038,0.1688311688311688,0.0,0.1038961038961038,0.1038961038961038,0.2533333333333333,0.027027027027027,0.1466666666666666,0.1466666666666666,0.125,0.0727272727272727,0.125,0.125,0.1688311688311688,0.0,0.1038961038961038,0.1038961038961038,0.1688311688311688,0.0,0.1038961038961038,0.1038961038961038,0.073394495412844,0.0,0.055045871559633,0.055045871559633,0.1688311688311688,0.0,0.1038961038961038,0.1038961038961038,13.054436683654783,13.054436683654783,13.12257194519043,13.054436683654783,13.054436683654783,8.286304473876953,13.054436683654783,5.886528491973877,0.9838695363845206,0.9789104691377221,0.9397457197860748,0.9838695363845206,0.9789104691377221,0.9397457934852165,0.9733831522094705,0.9723322243535982,0.9433938339196724,0.97931823993253,0.9760342170190125,0.941360101017563,0.9838695363845206,0.9789104691377221,0.9397455970749801,0.9838695363845206,0.9789104691377221,0.9397457092835142,0.9503612664511758,0.9486212452673969,0.03181200042774013,0.9838695363845206,0.9789104691377221,0.9397457934852165
93,https://openreview.net/forum?id=HyIFzx-0b,"the paper proposes a neural net architecture that uses a predefined orthogonal binary basis to construct the filter weights of the different convolutional layers. since only the basis weights need to be stored this leads to an exponential reduction in memory. the authors propose to compute the filter weights on the fly in order to tradeoff memory for computation time. experiments are performed on imagenet, mnist, cifar datasets with comparisons to binaryconnect, binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.----------------positives--------- the idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.----------------negatives--------- the proposed method seems significantly worse than other binary techniques on imagenet, cifar and svhn. on imagenet in particular binary-weight-network is 21% better at only 2x the model size. would a binary-weight-network of the same model size be better than the proposed approach? it would help to provide results using the proposed method with the same model size as binary-weight-networks on the different datasets. --------- the citation to binary-weight-networks is missing.--------- the descriptions in section 3.3, 3.4 need to be more rigorous. for instance, how many basis weights are needed for a filter of size n. does n need to be a power of 2 or are extra dimentions from the basis just ignored? the paper presents a binary neural network architecture that operated on predefined orthogonal binary basis. the binary filters that are used as basis are generated using orthogonal variable spreading factor. --------because the filters are weighted combinations of predefined basis, only the weights need to be trained and saved. the network is tested on imagenet and able to achieve top-5 accuracy of 65.9%.----------------the paper is clearly written. few mistakes and questions: --------is equation 2 used to measure the quality of the kernel approximation?----------------in figure 2, what is sparse layer? is it flexmodule?----------------in 4.1 results section, the paper states that on imagenet, binaryflex is compared to binaryconnect and binaryneuralnet; otherwise, binaryflex is compared to binaryconnect and bnn. it should be binary weight network instead of binaryneuralnet. ----------------based on results in table 1, binaryflex is able to reduce the model size and provide better accuracy than binaryconnect (2015). however, the accuracy results are significantly worse than binary-weight-network (2016). could you comment on that? the imagenet results are worrying, while bnn (7.8mb) achieves 79.4%, this binaryflex (3.4mb) achieves 65.7%. the accuracy difference is huge.",the paper proposes using a set of orthogonal bases that combine to form convolution kernels for cnns leading to a significant reduction of memory usage. the main concerns raised by the reviewers were 1) clarity; 2) issues with writing and presentation of results; 3) some missing experiments. the authors released a revised version of the paper and a short summary of the enhancements. none of the reviewers changed scores following the author response. the reviews were detailed and came from those familiar with cnns. i have decided to go with reviewer consensus.,"experiments are performed on imagenet, mnist, cifar datasets with comparisons to binaryconnect, binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.----------------positives--------- the idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.----------------negatives--------- the proposed method seems significantly worse than other binary techniques on imagenet, cifar and svhn.","experiments are performed on imagenet, mnist, cifar datasets with comparisons to binaryconnect, binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.----------------positives--------- the idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.----------------negatives--------- the proposed method seems significantly worse than other binary techniques on imagenet, cifar and svhn.",on imagenet in particular binary-weight-network is 21% better at only 2x the model size.,the paper proposes a neural net architecture that uses a predefined orthogonal binary basis to construct the filter weights of the different convolutional layers.,"experiments are performed on imagenet, mnist, cifar datasets with comparisons to binaryconnect, binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.----------------positives--------- the idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.----------------negatives--------- the proposed method seems significantly worse than other binary techniques on imagenet, cifar and svhn.",the paper presents a binary neural network architecture that operated on predefined orthogonal binary basis.,"few mistakes and questions: --------is equation 2 used to measure the quality of the kernel approximation?----------------in figure 2, what is sparse layer?","experiments are performed on imagenet, mnist, cifar datasets with comparisons to binaryconnect, binary-weight-networks and studies showing the memory vs time vs accuracy tradeoff.----------------positives--------- the idea of using a predefined basis to estimate filter weights in a neural network is novel and leads to significant reduction in memory usage.----------------negatives--------- the proposed method seems significantly worse than other binary techniques on imagenet, cifar and svhn.",0.2641509433962264,0.0509554140127388,0.1509433962264151,0.1509433962264151,0.2641509433962264,0.0509554140127388,0.1509433962264151,0.1509433962264151,0.0185185185185185,0.0,0.0185185185185185,0.0185185185185185,0.2241379310344827,0.0701754385964912,0.1724137931034483,0.1724137931034483,0.2641509433962264,0.0509554140127388,0.1509433962264151,0.1509433962264151,0.1121495327102803,0.019047619047619,0.0747663551401869,0.0747663551401869,0.1391304347826087,0.0176991150442477,0.0869565217391304,0.0869565217391304,0.2641509433962264,0.0509554140127388,0.1509433962264151,0.1509433962264151,7.201512336730957,12.10696029663086,10.069761276245115,12.10696029663086,12.10696029663086,7.0615410804748535,12.10696029663086,7.716671466827393,0.9846128491165245,0.9819893236589478,0.8427844377471708,0.9846128491165245,0.9819893236589478,0.8427843938867499,0.967026916137462,0.9656462973809269,0.010438011112671295,0.9834597039195787,0.9824080839788305,0.9200911651327797,0.9846128491165245,0.9819893236589478,0.8427839263998081,0.9771115662102408,0.9779775910968437,0.8832149325342135,0.9688150012506973,0.9672618002649072,0.8804209934749642,0.9846128491165245,0.9819893236589478,0.8427843938867499
94,https://openreview.net/forum?id=HyMS8iRcK7,"summary:----------------this paper introduces a new rnn architecture with external memory for sequence modeling. the proposed architecture (marnn) is a simplification of tardis (gulcehre et al., 2017). it uses the similar reader-writer tying mechanism, gates to control information flow from previous hidden state and memory. however, it has a simpler addressing mechanism. authors show results in character level ptb and a temporal action detection/proposal task.----------------major comments:----------------marnn looks like a simplification of tardis architecture with gumbel softmax. the major difference between the two architectures is the addressing mechanism.----------------1. can the authors clearly differentiate marnn vs tardis?----------------2. authors compare against tardis only in character level ptb which is actually a task which does not require very long term dependencies. it would be better if authors consider more tasks and directly compare against the tardis addressing mechanism to prove that the proposed addressing mechanism is indeed better.----------------3. authors should consider more tasks, to show the efficiency of the proposed architecture.----------------4. the name of the model seems to be too generic. ntm, tardis, dnc with recurrent controller can be considered as memory augmented rnn. please change the name. this paper introduces a memory-augmented rnn (marnn) which aims at being lightweight and differentiable. in a nutshell, authors propose to augment a lstm-type architecture with several memory cells. at each time-step, marnn retrieves one memory cell, updates his state, and updates the memory cell content. to learn the retrieval operation that requires discrete addressing, authors rely on the gumbel-softmax. authors evaluate their approach on penntreebank character level modelling where they demonstrate competitive performances. they also report state-of-art performance on the thumos dataset. the paper is overall clear and pleasant to read. ----------------authors highlight that marnn is more lightweight compared to existing memory networks. marnn can indeed retrieves only memory cells at inference. however, since marnn uses a gumbel-softmax to train the discrete addressing scheme, it is it not clear if there is any advantage in term of memory and computation of marnn relatively to other network during training? it would be nice to compare the computation time/memory usage of marnn with other memory augmented network such as tardis, ntm or memory network during training and inference. ----------------another claim is that marnn can possibly boost training speed by reducing the lengths of tbtt. but marnn also haves a training time overhead as showed in figure 2. how does the overall training time/performances of marnn with tbptt of 50 compared to a lstm with tbptt of 100/150?----------------the writing can be sometime a bit imprecise. for instance authors say that marnn learns better representations that many hierarchical rnn structure. i agree that marnn outperforms in term of accuracy, however, it is not clear what the author are referring to by better representation of the marnn hidden state? performance gain of marnn could also be due to the external memory which allows to retain more information of the input? in addition, it would be nice to precise which type of hierarchical rnn structure marnn does (or doesnt) outperform. another claim is that marnn can easily learn long-term dependencies. while this is reasonable, i am unsure that the empirical evaluation support this. it would be nice to show how the gradients backpropagated through time behave in practice to support this claim? ------------------------memory-augmented network are a very important research directions and the marnn architecture is interesting. however, it is not entirely clear to me what is the main advantage of marnn relatively to other memory networks network such as tardis, ntm or memory network for training and/or inference. although authors do compare with tardis, further comparison with the other networks and in term of computation time and memory could help clarify those points. the paper proposed a rnn with skip-connection (external memory) to past hidden states, this is a slightly different version of the tardis network. the authors experimented on ptb and a temporal action detection method.----------------novelty:----------------i dont see a lot of novelty to the method. the authors proposed a method very similar to tardis, the difference seems to be that mmarnn does not use extra usage vectors for reading from previous memory, but this is not a fundamental difference between mmarnn and tardis.----------------shortcomings of the paper:----------------1. the experiments seem rather weak. the authors experimented on ptb and temporal action detection method. it is not clear why authors experimented with ptb, this is not a task with long-term dependencies, i do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used----------------2. the model uses a single past hidden state, it is not clear to me why this is better than using a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past. the authors should cite ""sparse attentive backtracking"" (https://arxiv.org/abs/1809.03702) at nips 2018. sab is very related in that it also propagate gradients to a few hidden states in the memory. the difference is that sab used a few hidden states from the past/ memory instead of one; another difference is that it propagates gradients locally to the selected hidden states/ memory slots.----------------3. the paper only demonstrated experimental results on ptb and temporal action prediction. i think it would make the paper a lot stronger if the authors experimented with a variety of different tasks. tasks that requires long term dependencies can really demonstrate the strength of the model (copy and adding tasks).----------------4. if the authors could run the model on copy and adding tasks, i would be curious to see if the model is picking the ""correct"" timestep in the memory / past.----------------post rebuttal: i feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. i have raised the score to reflect this changes.","there have been many variants of memory augmented neural nets since around 2014 when ntm, attention-based nmt and memnet were proposed. it is indeed still an interesting and important direction of research, but the bar for introducing yet another variant of memory-augmented neural nets has been significantly raised, which is a sentiment shared by the reviewers. the author's response had not swayed the reviewers' opinion, and i am sticking to the reviewers' decisions. i believe more streamlined and systematic comparison among different memory augmented networks across many different benchmarks (e.g., use the same set of latest variants of memory nets across all the benchmarks) in this submission would make it a better paper and increase the chance of acceptance.",authors show results in character level ptb and a temporal action detection/proposal task.----------------major comments:----------------marnn looks like a simplification of tardis architecture with gumbel softmax.,"it is not clear why authors experimented with ptb, this is not a task with long-term dependencies, i do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used----------------2. the model uses a single past hidden state, it is not clear to me why this is better than using a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past.","it would be nice to compare the computation time/memory usage of marnn with other memory augmented network such as tardis, ntm or memory network during training and inference.",summary:----------------this paper introduces a new rnn architecture with external memory for sequence modeling.,"it would be nice to compare the computation time/memory usage of marnn with other memory augmented network such as tardis, ntm or memory network during training and inference.","it would be nice to compare the computation time/memory usage of marnn with other memory augmented network such as tardis, ntm or memory network during training and inference.",authors show results in character level ptb and a temporal action detection/proposal task.----------------major comments:----------------marnn looks like a simplification of tardis architecture with gumbel softmax.,"it is not clear why authors experimented with ptb, this is not a task with long-term dependencies, i do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used----------------2. the model uses a single past hidden state, it is not clear to me why this is better than using a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past.",0.0933333333333333,0.0,0.0533333333333333,0.0533333333333333,0.1990521327014218,0.0095693779904306,0.1232227488151658,0.1232227488151658,0.1578947368421052,0.0266666666666666,0.1052631578947368,0.1052631578947368,0.0875912408759124,0.0,0.0437956204379562,0.0437956204379562,0.1578947368421052,0.0266666666666666,0.1052631578947368,0.1052631578947368,0.1578947368421052,0.0266666666666666,0.1052631578947368,0.1052631578947368,0.0933333333333333,0.0,0.0533333333333333,0.0533333333333333,0.1990521327014218,0.0095693779904306,0.1232227488151658,0.1232227488151658,11.65744400024414,11.65744400024414,17.48598289489746,5.884149074554443,12.35047721862793,11.65744686126709,5.884149074554443,12.350476264953612,0.974819144671485,0.9684348293560823,0.8537134032490383,0.5281654517798936,0.5837899692665951,0.6423224172317166,0.9625711575136353,0.9638969677058966,0.7314817651635891,0.9769456699580873,0.973599580569657,0.9346195859299725,0.9625711575136353,0.9638969677058966,0.7314820631953752,0.9625711575136353,0.9638969677058966,0.7314815485386353,0.974819144671485,0.9684348293560823,0.8537133054774437,0.5281654517798936,0.5837899692665951,0.6423224172317166
95,https://openreview.net/forum?id=Hye00pVtPS,"this paper describes a structure to approach the federated machine learning problem for hospitals. the approach does not seem very novel and it is hard to see what the representation learning challenges are. there is no open benchmark that the community can work on.----------i suggest that the paper focus on the method and not the private dataset used. if you cannot release a public dataset then maybe a synthetic dataset that presents known challenges you observe in private data. this can be used as a benchmark for the community to improve these methods. ----------typos:---------- ""step 1..claims""----------some of the citations seem to be listing every author of the paper which is very hard to read the paper. the authors propose a learning strategy to fit predictive models on data separated across nodes, and for which different set of features are available within each node. -----this concept is developed by introducing the concept of two degree separation across horizontal (nodes) and vertical (feature) axis. the proposed approach consists in an iterative scheme where i) models at independentently trained at each site, and ii) models' parameters are subsequently averaged and redistributed for the next optimisation round. ----------the problem tackled in this work is interesting, with an important application on medical records from > 100,000 individuals followed over time. unfortunately the paper is not clear in several aspects, and presents methodological issues. here my main comments on this work:----------- the authors should definitely refer to the concept of meta-learning [1], which addresses modelling problems very close to the one presented in this work: training a meta-model by aggregating information from different learning tasks. the paper should definitely compare the proposed methodology with respect to this paradigm. ----------- the fact that the parameters can be averaged across nodes implies that they must be of same dimension. this is counterintuitive, as the dimension of the data represented at each site may significantly differ depending on the kind of considered feature. this aspect points to some methodological inconsistency.----------- there is no comparison with any other federated method, neither with any classification method besides a nn, at least with the aggregated data. also it could have been possible to reduce the number of input features using simple dimensionality reduction previous to the nn, such as pca. ----------- vertical separation importance: at the end it looks like diagnosis is the main driver for the classification, showing results that are comparable to the ones obtained with the aggregated data. it is therefore not clear whether the proposed application allows to clearly illustrate the benefit of using this method with regard to vertical separation.----- ------ all in all, the paper appears in a draft form, and the text is often inconsistent. for example, there is often inconsistency in the number of branches, or types of data considered, figures are not self-explanatory and present notation and symbols not defined anywhere. the bibliography is given in a non-standard format. ----------[1] model-agnostic meta-learning for fast adaptation of deep networks. finn, c., abbeel, p., & levine, s. proceedings of the 34th international conference on machine learning-volume 70 (pp. 1126-1135).","this manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features.----------the reviewers and ac agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application). however, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. on the conceptual end, the ac also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings.","----------- vertical separation importance: at the end it looks like diagnosis is the main driver for the classification, showing results that are comparable to the ones obtained with the aggregated data.","here my main comments on this work:----------- the authors should definitely refer to the concept of meta-learning [1], which addresses modelling problems very close to the one presented in this work: training a meta-model by aggregating information from different learning tasks.","here my main comments on this work:----------- the authors should definitely refer to the concept of meta-learning [1], which addresses modelling problems very close to the one presented in this work: training a meta-model by aggregating information from different learning tasks.",this paper describes a structure to approach the federated machine learning problem for hospitals.,this paper describes a structure to approach the federated machine learning problem for hospitals.,"here my main comments on this work:----------- the authors should definitely refer to the concept of meta-learning [1], which addresses modelling problems very close to the one presented in this work: training a meta-model by aggregating information from different learning tasks.",the approach does not seem very novel and it is hard to see what the representation learning challenges are.,"the authors propose a learning strategy to fit predictive models on data separated across nodes, and for which different set of features are available within each node.",0.2343749999999999,0.0,0.15625,0.15625,0.2411347517730496,0.014388489208633,0.0992907801418439,0.0992907801418439,0.2411347517730496,0.014388489208633,0.0992907801418439,0.0992907801418439,0.125,0.0,0.1071428571428571,0.1071428571428571,0.125,0.0,0.1071428571428571,0.1071428571428571,0.2411347517730496,0.014388489208633,0.0992907801418439,0.0992907801418439,0.1025641025641025,0.0,0.0854700854700854,0.0854700854700854,0.3359999999999999,0.1463414634146341,0.24,0.24,6.856181144714356,13.606407165527344,13.606406211853027,6.856183052062988,6.642731666564941,6.856181144714356,6.971914768218994,9.03270149230957,0.9736756351683201,0.9705329784641132,0.39275544226987935,0.9807613692717506,0.9751738714317757,0.9514814293049543,0.9807613692717506,0.9751738714317757,0.9514814505723799,0.9748586118433884,0.9716798654306366,0.9271890699605981,0.9748586118433884,0.9716798654306366,0.9271889118688543,0.9807613692717506,0.9751738714317757,0.9514814363940961,0.9509611581881204,0.94880808973108,0.8349890389946187,0.9733797205030618,0.9720007200791345,0.9395054817891945
96,https://openreview.net/forum?id=Hygxb2CqKm,"this is an interesting paper that i expect will generate some interest within the iclr community and from deep learning researchers in general. the definition of stability is both intuitive and sound and the connection to exploding gradients is perhaps the most interesting and useful part of the paper. the sufficient conditions yield practical techniques for increasing the stability of, e.g., an lstm, by constraining the weight matrices. they also show that stable recurrent models can be approximated by models with finite historical windows, e.g., truncated rnns. experiments in sec 4 suggest that stable models produced by constraining standard rnn architectures can compete with their unconstrained unstable counterparts, and often without necessitating significant changes to architecture or hyperparameters. the perhaps most interesting observations are in sec 4.3, in which the authors claim that even fundamentally unstable models, e.g., unconstrained rnns, often operate in a stable regime, at least when being applied to in-sample data. i lean toward acceptance at the moment, but i am eager to discuss with the authors and other reviewers as i am not 100% confident that i fully understood the theory.----------------summary----------------this paper proposes a simple, generic definition of stability for recurrent, non-linear dynamical systems such as rnns: that given two hidden states h, h, the difference between their updated states given input x is bounded by the product between the difference between the states themselves and a small multiplier. the paper then immediately draws a connection between stability, asserting that unstable models are prone to gradient explosions during gradient descent-based training. in sec 2.2, the paper presents sufficient conditions for basic rnns and lstms to be stable. secs 3.2 and 3.3 argue that stable recurrent models can be approximated by feedforward models during both inference and training with a finite history horizon, such as a rnn with a truncated history. experiments in language and music modeling substantiate this claim: constrained, stable models are competitive with standard unconstrained models. sec 4.3 sheds some light on this phenomenon, arguing that there is a weaker form of data-dependent stability and that even unstable models may operate in a stable regime for some problems, thus explaining the parity between stable and unstable models.----------------strengths----------------* this paper is surprisingly engaging and easy to read.--------* the theorems are clearly stated and the proofs appear sound to me, though i will admit that i am not confident that i would catch a significant bug.--------* this paper provides a new (to me, anyway) and thought-provoking analysis of rnns. in particular, i was especially interested in the observation that stable models can be approximated by truncated models and that there is a connection between stability and long-term dependencies. this seems consistent with the fact that for many problems, non-recurrent models (convnets, transformers, etc.) are often competitive with more complex architectures.----------------weaknesses----------------* in practice it seems as though stability may depend on not only choice of model architecture but also the data themselves. there is probably no good way to know a priori what the stability characteristics of a given data set are, making it tough to apply the ideas of this paper in practice--------* the literature review seems a bit limited and appears to ignore the growing body of work on constraining rnn weight matrices to address both exploding and vanishing gradients. for example, i am pretty confident that the singular thresholding trick for renormalizing neural net weights has been described in the literature previously.--------* although stable and unstable models appear to be competitive in experiments, the theoretical analysis provides no insights into stability and how it relates to accuracy. + an interesting problem to study on the stability of rnns--------+ investigation of spectral normalization to sequential predictions is worthwhile, especially figure 2--------+ some theoretical justification of sgd for learning dynamic systems following hardt et al. (2016b).----------------- the take-home message of the paper is not clear. first, it defines a notion of stability based on lipchitz-continuity and proves sgd can learn it. then the experiments show such a definition is actually not correct, but rather a data-dependent one. --------- the theory only looks at the instantaneous dynamics from time t to t+1, without unrolling the rnns over time. then it is not much different from analyzing feed-forward networks. the theorem on sgd is remotely related to the contribution of the paper. --------- the spectral normalization technique that is actually used in experiments is not new",the paper presents both theoretical analysis (based upon lambda-stability) and experimental evidence on stability of recurrent neural networks. the results are convincing but is concerns with a restricted definition of stability. even with this restriction acceptance is recommended.,"there is probably no good way to know a priori what the stability characteristics of a given data set are, making it tough to apply the ideas of this paper in practice--------* the literature review seems a bit limited and appears to ignore the growing body of work on constraining rnn weight matrices to address both exploding and vanishing gradients.","sec 4.3 sheds some light on this phenomenon, arguing that there is a weaker form of data-dependent stability and that even unstable models may operate in a stable regime for some problems, thus explaining the parity between stable and unstable models.----------------strengths----------------* this paper is surprisingly engaging and easy to read.--------* the theorems are clearly stated and the proofs appear sound to me, though i will admit that i am not confident that i would catch a significant bug.--------* this paper provides a new (to me, anyway) and thought-provoking analysis of rnns.","in particular, i was especially interested in the observation that stable models can be approximated by truncated models and that there is a connection between stability and long-term dependencies.",this is an interesting paper that i expect will generate some interest within the iclr community and from deep learning researchers in general.,"the perhaps most interesting observations are in sec 4.3, in which the authors claim that even fundamentally unstable models, e.g., unconstrained rnns, often operate in a stable regime, at least when being applied to in-sample data.","the perhaps most interesting observations are in sec 4.3, in which the authors claim that even fundamentally unstable models, e.g., unconstrained rnns, often operate in a stable regime, at least when being applied to in-sample data.","+ an interesting problem to study on the stability of rnns--------+ investigation of spectral normalization to sequential predictions is worthwhile, especially figure 2--------+ some theoretical justification of sgd for learning dynamic systems following hardt et al.","sec 4.3 sheds some light on this phenomenon, arguing that there is a weaker form of data-dependent stability and that even unstable models may operate in a stable regime for some problems, thus explaining the parity between stable and unstable models.----------------strengths----------------* this paper is surprisingly engaging and easy to read.--------* the theorems are clearly stated and the proofs appear sound to me, though i will admit that i am not confident that i would catch a significant bug.--------* this paper provides a new (to me, anyway) and thought-provoking analysis of rnns.",0.2626262626262626,0.0,0.1212121212121212,0.1212121212121212,0.2238805970149253,0.0151515151515151,0.1194029850746268,0.1194029850746268,0.144927536231884,0.0298507462686567,0.144927536231884,0.144927536231884,0.1612903225806451,0.0,0.064516129032258,0.064516129032258,0.1282051282051282,0.0,0.0769230769230769,0.0769230769230769,0.1282051282051282,0.0,0.0769230769230769,0.0769230769230769,0.1891891891891892,0.0277777777777777,0.1351351351351351,0.1351351351351351,0.2238805970149253,0.0151515151515151,0.1194029850746268,0.1194029850746268,10.706620216369627,10.706620216369627,7.908916473388672,13.134580612182615,9.932195663452148,9.858438491821287,13.134580612182615,9.748221397399902,0.9551407386042292,0.9578421223962541,0.9333814198401693,0.9558706344151747,0.9670674589352096,0.9013954986072761,0.9433442708938699,0.9407102535922061,0.8988084200319238,0.9707305563410893,0.9652017996878132,0.9540599863352258,0.9250595067572124,0.94119146303715,0.9324996692839214,0.9250595067572124,0.94119146303715,0.9324996345452424,0.6081408937404538,0.5952842725446953,0.7693453347558368,0.9558706344151747,0.9670674589352096,0.9013954524335451
97,https://openreview.net/forum?id=IOqr2ZyXHz1,"the work adopts a continual lifelong learning paradigm for causal effect estimation. in five synthetic datasets, the proposed method achieves similar results to cfr-c, a strong baseline which requires to store all data and needs to be trained from scratch.-----i have some concerns about the motivation of this work. a simple question about the claim on the original data: why does the performance of the model trained continually on the original data matter? it is not difficult to save a checkpoint of the model trained on original data and use it for the original data whenever you want, given the fact the neural network based causal inference models are small. or you can let the model do the inference and save the results periodically. similarly, i could not get why catastrophic forgetting is an issue in this problem. if storing the original data would cause a copyright or privacy issue, i wonder why storing a model or feature representations trained by such data can avoid these issues. the memory issue of storing data is also not likely to happen in real-world since collecting data is often more complicated and expensive than storing them.-----in section 2, there is an issue in the strong ignorability assumption. it would be great if the authors can clarify. the notation \mathcal{x}_d assumes that the feature space can change over domains, but the ignorability needs a unified feature space. so, the ignorability assumption actually relies on varying observed covariates space when-----d-----changes.-----regarding table 1, could the authors report the exact setting for the cerl model which achieves the reported numbers? for example, the number of stored feature representations. this is because in figure 4, it shows m=10000 has higher pehe and \epsilon_ate than the one reported in table 1.-----since training from scratch with all data is expensive, it would be interesting to show a comparison of storage space, ram memory and runtime between cerl and cfr-a, b and c. i think the problem proposed in the paper is interesting and the method is moderately novel. i did not find the paper's writing very helpful. the method seems reasonably motivated but the discussion around it does not provide enough intuition about the influence of the different pieces on the final prediction, and no ablation experiment are provided.-----my main issue with the paper is the limited experimental evidence.-----without any real data (at least semi-synthetic) experiments, i do not think there is a good way to evaluate the significance of the problem and the proposed method.-----the single synthetic experiment in the paper is not sufficient to evaluate the effectiveness of the method. i believe at a minimum, different synthetic experiments with different structural models, dataset shifts and rate of shift over time should be considered.-----no ablation studies are done and no real baseline is considered. for example, a non-trivial baseline would be to do use a traditional continual learning objective with distillation with a new treatment-control population balancing objective with each new dataset. the value of the proposed method's representation-herding would then become clear.-----if the authors address these issues, i'm happy to change my score. this paper considers adopting continual learning on the problem of causal effect estimation. the paper combines methods and algorithms for storing feature representation and representative samples (herding algorithm), avoiding drifting feature representation when new data is learned (feature representation distillation), balanced representation by regularization, etc. consequently, the paper presents a system that makes use of existing methods as a loss function (the sum of losses and regularization terms).-----it is difficult to observe the novelty of the method---there is no apparent challenge arose in combining these methods as they can just be simply added. further, the development of this framework for causal inference is mostly orthogonal to the problem of causal effect estimation (and the use of regularization does not add any novelty.) in addition to the lack of novelty, the paper has many non-negligible mistakes in describing causal inference. for example, consider a phrase ""selection bias"" between ""treatment and control groups"" in the first sentence of the second paragraph. ""treatment and control groups"" usually are used to describe two groups under the context of rct. further, the bias we are talking in the paper is a ""confounding bias"" not a ""selection bias"". in section 2, ""each unit ... received ... one of ... treatments"" is also relevant to rct not observational data. there are many other places the authors mentioning ""selection bias"".-----question: what is ""real world evidence""?, which is used in the title, abstract, conclusion, and at the beginning of section 3? is it just observational data? the authors propose to estimate individualized treatment effects in a setting they describe as continual learning, batches of data becoming available over time. the technical proposal is to store a feature representation of the presently available data to be updated with new data in the future -- a technique that, together with other regularization methods for selection bias and variable selection, is shown to perform well on synthetic experiments.-----causal inference is increasingly relevant as machine learning assists decision-making, and an important aspect of this process is to handle information streams over time and update accordingly, just as humans would be expected to. in the context of causality though, i believe this requires a more detailed motivation. causal effects by their very definition exhibit invariance to interventions on observed variables, and if properly estimated, these should not vary between well-behaved environments. with enough data a priori there should not be large changes in estimation in new environments. this may certainly change if the underlying causal mechanisms change and is a problem that has been studied as data fusion, yet this line of research is not mentioned in the paper.-----writing needs to improve substantially. the use of the words extensibility, adaptability, and accessibility is grammatically incorrect, and the context in which they are used does not clarify the meaning of these ideas either. as i understand it, adaptability refers to domain adaptation yet how domains differ, how to merge them depending on their differences, or even an investigation of these problems is not discussed. accessibility refers to the amounts of data one needs to deal with. this does not strike me as a problem in causal inference where datasets are typically small. perhaps examples of applications where each of these aspects is a concern would be helpful.-----more importantly, the notation is counter-intuitive and inconsistent. for instance, \mathcal x is defined as the set of all observed variables, but then a specific realization x is written to be an element of \mathcal x (as if \mathcal x was a measurable space of possible realizations of a random variable). both interpretations cannot be correct. the notation \mathcal x_d is similarly ambiguous, does this mean each domain has different sets of variables or that the spaces in which they are defined differ?-----the experiments are underwhelming and certainly do not back up the claims made in the introduction. most benchmarks for individualized treatment effects are semi-synthetic (it would be advisable to stick to these data generating processes while modifying them to highlight specific features of your problem). many more competing algorithms could be evaluated.","the authors consider the problem of causal inference from multiple conditionally ignorable models that yield different observed data distributions. this problem is distinct from transportability (which assumes some types of causal invariance across domains, and aims to move causal conclusioned learned in one context to another using this invariance). the authors adapt a machine learning approach from (shalit et al, 2017).-----because the authors describe an algorithm rather than a model, it was a bit difficult to understand what assumptions tie the different observed data distributions together (i am guessing there is a way to formulate a 'global model' tying all datasets together in terms of the algorithm hyperparameters but the authors do not discuss this).-----the authors evaluate their method via a simulation study. moreover, in response to reviewer criticism, the authors uploaded additional results from semi-synthetic data.-----some of the concerns of reviewers were about novelty and scope of evaluation (in addition, some complained about writing and notation).","in five synthetic datasets, the proposed method achieves similar results to cfr-c, a strong baseline which requires to store all data and needs to be trained from scratch.-----i have some concerns about the motivation of this work.","the technical proposal is to store a feature representation of the presently available data to be updated with new data in the future -- a technique that, together with other regularization methods for selection bias and variable selection, is shown to perform well on synthetic experiments.-----causal inference is increasingly relevant as machine learning assists decision-making, and an important aspect of this process is to handle information streams over time and update accordingly, just as humans would be expected to.","in five synthetic datasets, the proposed method achieves similar results to cfr-c, a strong baseline which requires to store all data and needs to be trained from scratch.-----i have some concerns about the motivation of this work.",the work adopts a continual lifelong learning paradigm for causal effect estimation.,"the paper combines methods and algorithms for storing feature representation and representative samples (herding algorithm), avoiding drifting feature representation when new data is learned (feature representation distillation), balanced representation by regularization, etc.","the technical proposal is to store a feature representation of the presently available data to be updated with new data in the future -- a technique that, together with other regularization methods for selection bias and variable selection, is shown to perform well on synthetic experiments.-----causal inference is increasingly relevant as machine learning assists decision-making, and an important aspect of this process is to handle information streams over time and update accordingly, just as humans would be expected to.","in the context of causality though, i believe this requires a more detailed motivation.","the technical proposal is to store a feature representation of the presently available data to be updated with new data in the future -- a technique that, together with other regularization methods for selection bias and variable selection, is shown to perform well on synthetic experiments.-----causal inference is increasingly relevant as machine learning assists decision-making, and an important aspect of this process is to handle information streams over time and update accordingly, just as humans would be expected to.",0.22,0.0,0.1199999999999999,0.1199999999999999,0.2489626556016597,0.0251046025104602,0.1244813278008298,0.1244813278008298,0.22,0.0,0.1199999999999999,0.1199999999999999,0.0462427745664739,0.0,0.0346820809248554,0.0346820809248554,0.0932642487046632,0.0,0.0518134715025906,0.0518134715025906,0.2489626556016597,0.0251046025104602,0.1244813278008298,0.1244813278008298,0.0914285714285714,0.0115606936416184,0.0685714285714285,0.0685714285714285,0.2489626556016597,0.0251046025104602,0.1244813278008298,0.1244813278008298,9.094623565673828,5.162943840026856,13.93625831604004,9.094623565673828,10.531784057617188,10.531784057617188,9.094623565673828,9.03433609008789,0.9642540577771314,0.9622800586388727,0.8859120333239786,0.7903435145905003,0.8219741396210596,0.9527543634935898,0.9642540577771314,0.9622800586388727,0.88591231135467,0.9552820995315123,0.9532935179461772,0.7688499323425096,0.9662104632031369,0.9666506960352635,0.9288746699634072,0.7903435145905003,0.8219741396210596,0.9527543741415327,0.4936566822550903,0.734544096823885,0.9291551791791013,0.7903435145905003,0.8219741396210596,0.9527543634935898
98,https://openreview.net/forum?id=JoCR4h9O3Ew,"summary-----this paper introduces a semi-supervised learning procedure that does not require labeled adversarial data to learn an ensemble model that is robust to adversarial attacks on classification tasks.-----quality-----this paper is very well written; the design decisions of the training procedures are all supported by ablation tests and comparisons to other modern adversarial baselines. however, i am slightly worried by the ablation study results -- from table 3, it seems like the dpp component of the loss does not provide a statistically significant lift.-----clarity-----this paper is very clearly written! the choice of parameterization, experiment settings, and overall proposed methodology were clearly explained and motivated.-----originality-----as the authors have mentioned, improving diversity in ensembles is not a novel approach to improving robustness to adversarial attacks; the originality of the method lies in the parameterization of diversity through a dpp component and multi-view complementarity.-----significance-----improving robustness to adversarial attacks is in of itself an important contribution to this field. this paper proposes a method that achieves a significant improvement over other competitors (tables 1, 2); the proposed method is additionally intuitive, and can incorporate prior beliefs over the the data distribution, making it adaptable to several different settings.-----questions-----methodology-----my main question lies in the uses of the dpp loss: did the authors use the determinant directly? depending on the choice of kernel, i would expect the determinant portion of the loss to eclipse the other terms, potentially hampering learning.-----on a related note, i am curious to know how sensitive the armoured performance is to the-----dpp-----and-----nem-----hyper-parameters.-----experiments-----do the authors have insight in the significant gap between armoured-b and its variants for a large----------budget with-----l-----pgd? i am surprised by the significant gap in table 3.-----again in table 3, comparing the -f, no dpp, and -b variants, it seems like the highest lift in performance comes from the diversity in the nem component. do you know if a similar lift is observed by removing the nem component and keeping the dpp?-----at inference time, you state that the augmentation step is skipped. naively, i would not have been surprised to see these augmentations improve resilience to adversarial attacks. am i incorrect? summary:-----this work introduces armoured, a new method for learning models that are robust against adversarial attacks. the method uses multi-view-learning (as in using multiple models with different parameters to cast their votes/views on a given sample), semi-supervised learning to pseudo-label new data based on a consensus, and a diversity regularizer. the approach is evaluated in cifar-10 and svhn against recent baselines in the presence of white-box attacks. the results yield by armoured in these scenarios are superior.-----reasons for score:-----my recommendation is to accept the paper. i like the proposal, it is simple yet is able to produce better results than existing methods. it also performs well with clean samples, which is something that many other methods struggle with. the only thing i miss is to see experiments in imagenet.-----strengths:-----the problem is relevant for the community and the proposed approach seems to do a better job than the existing state-of-the-art leveraging weak-supervision/semi-supervision (definitely a plus).-----the method is simple and seems easy to adapt to other architectures and tasks.-----weaknesses:-----the ablation experiments point to the unlabelled data as one of the core reasons for the method to work. i was expecting to see additional experiments with extra unlabelled data to get a better feeling of this component.-----questions to be discussed during the rebuttal period:-----please, address the questions/comments expressed in the weaknesses section.-----other considerations:-----none this paper considers training an adversarially robust model in a semi-supervised setting. the authors propose an ensemble-based algorithm for this goal. the algorithm uses a regularization term to induce diversity in the ensemble. the algorithm also leverages the idea of multi-view training in semi-supervised learning to make use of unlabeled data. the experimental results show that the proposed algorithm outperforms several baselines.-----overall i enjoyed reading this paper. it is relatively clearly written, and easy to follow. i think the basic idea of this paper is a combination of the adp algorithm by pang et al and the classic multi-view idea in semi-supervised learning. in fact, some of the figures in this paper are quite similar to those in pang et al 2019. it is not surprising that this algorithm outperforms adp since it makes use of many unlabeled data. this makes me feel that the proposed algorithm does not have enough novelty. however, i do think this paper is beneficial to the adversarial robustness (ar) community since it brings an important idea of multi-view semi-supervised learning to ar research. therefore, i decided to give a score of weakly accept.-----====== after author response: i have read other reviews and the revised version. i think the paper's overall quality has improved. i decided to change my score to 7.","this paper focuses on adversarial robustness with unlabeled data. the philosophy behind sounds quite interesting to me, namely, utilizing unlabeled data to enforce labeling consistency while reducing adversarial transferability among the networks via diversity regularizers. this philosophy leads to a novel algorithm design i have never seen, i.e., armoured, an adversarially robust training method based on semi-supervised learning.-----the clarity and novelty are clearly above the bar of iclr. while the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. thus, all of us have agreed to accept this paper for publication! please carefully address all comments in the final version.","depending on the choice of kernel, i would expect the determinant portion of the loss to eclipse the other terms, potentially hampering learning.-----on a related note, i am curious to know how sensitive the armoured performance is to the-----dpp-----and-----nem-----hyper-parameters.-----experiments-----do the authors have insight in the significant gap between armoured-b and its variants for a large----------budget with-----l-----pgd?","the choice of parameterization, experiment settings, and overall proposed methodology were clearly explained and motivated.-----originality-----as the authors have mentioned, improving diversity in ensembles is not a novel approach to improving robustness to adversarial attacks; the originality of the method lies in the parameterization of diversity through a dpp component and multi-view complementarity.-----significance-----improving robustness to adversarial attacks is in of itself an important contribution to this field.","this paper proposes a method that achieves a significant improvement over other competitors (tables 1, 2); the proposed method is additionally intuitive, and can incorporate prior beliefs over the the data distribution, making it adaptable to several different settings.-----questions-----methodology-----my main question lies in the uses of the dpp loss: did the authors use the determinant directly?",summary-----this paper introduces a semi-supervised learning procedure that does not require labeled adversarial data to learn an ensemble model that is robust to adversarial attacks on classification tasks.-----quality-----this paper is very well written; the design decisions of the training procedures are all supported by ablation tests and comparisons to other modern adversarial baselines.,"summary:-----this work introduces armoured, a new method for learning models that are robust against adversarial attacks.","the choice of parameterization, experiment settings, and overall proposed methodology were clearly explained and motivated.-----originality-----as the authors have mentioned, improving diversity in ensembles is not a novel approach to improving robustness to adversarial attacks; the originality of the method lies in the parameterization of diversity through a dpp component and multi-view complementarity.-----significance-----improving robustness to adversarial attacks is in of itself an important contribution to this field.",the algorithm also leverages the idea of multi-view training in semi-supervised learning to make use of unlabeled data.,"the choice of parameterization, experiment settings, and overall proposed methodology were clearly explained and motivated.-----originality-----as the authors have mentioned, improving diversity in ensembles is not a novel approach to improving robustness to adversarial attacks; the originality of the method lies in the parameterization of diversity through a dpp component and multi-view complementarity.-----significance-----improving robustness to adversarial attacks is in of itself an important contribution to this field.",0.3128491620111731,0.0451977401129943,0.1564245810055865,0.1564245810055865,0.3186813186813186,0.0333333333333333,0.1648351648351648,0.1648351648351648,0.2352941176470588,0.0357142857142857,0.1411764705882353,0.1411764705882353,0.3214285714285714,0.0602409638554216,0.1547619047619047,0.1547619047619047,0.140625,0.0,0.078125,0.078125,0.3186813186813186,0.0333333333333333,0.1648351648351648,0.1648351648351648,0.1984732824427481,0.0465116279069767,0.1068702290076335,0.1068702290076335,0.3186813186813186,0.0333333333333333,0.1648351648351648,0.1648351648351648,8.070481300354004,11.060490608215332,12.255521774291992,8.070481300354004,8.911301612854004,7.994431495666504,8.070481300354004,7.209850311279297,0.9590962375673293,0.9777655700365027,0.9249171664169535,0.9466700015237678,0.9602189407811366,0.933294633614866,0.9803769823968257,0.9762916185591581,0.9229938266281408,0.9869493260175228,0.9848018118300359,0.9506662059784886,0.9788519808308407,0.9774456097611612,0.9408599379167393,0.9466700015237678,0.9602189407811366,0.9332945918929051,0.5652652938250261,0.831195786806305,0.877478839963822,0.9466700015237678,0.9602189407811366,0.933294633614866
99,https://openreview.net/forum?id=LtgEkhLScK3,"in this paper, the authors propose to use mixture models of policies and present good experimental results. the approach is quite sound, and the experimental results looks solid.-----however, there are some concerns that make this paper cannot be fully appreciated:-----the first and second listed contributions are about ""the undifferentiability problem"", and the third is a verification of the propose algorithm. however, a more fundamental question would be the following ones: a). why the author would like to propose mixture models and hope the community to adopt this framework, to further enhance the drl performance, right? i agree the mixture models have great potential, but could you please justify your topic/target issue first, and then present your solution? i would say, if the authors naturally expect the reviewer or readers to naturally believe in what you did, this is not right. please show your intellectual contributions explicitly, and use them to convince readers. now, from a reader's perspective, mixture model is well-established, and this work extends it a little. i agree mixture models are promising, but i am not convinced by this work. how would the authors respond? since, it is some kind of extension, the novelty cannot be fully appreciated. b). in terms of algorithm, is there some innovative contribution? it is hard to tell, i think the authors would also agree that such steps are straightforward. therefore, will the experiments solely enough to justify an acceptance? c). regarding your experiments, besides the three questions raised by the authors (which are reasonable). i would ask some questions beyond, would be mixture model be more powerful or practically important to address the robustness, stability issues that faced by current drl algorithms? (q1 mentioned stability, while the figures do not fully address it, right?) also, as for mixture models, would an adaptive algorithms be more appropriate solution? since mixture model allows more freedom to balance exploration and exploitation, and an adaptive scheme would allocate exploration budget to more informative policies or state regions? therefore, the current experiments do not fully justify the value of this paper. i have concerns. summary-----the paper focuses on the policy architecture of deep reinforcement learning algorithms. specifically, the authors apply the probabilistic mixture-of-experts (pmoe) model in the policy of a reinforcement learning agent, where each primitive is a unimodal gaussian distribution and the gating model is a simple state-conditioned categorical distribution. the authors derive the corresponding policy gradient objective for the pmoe policy.-----the authors apply the pmoe policy on top of sac and ppo, and perform experiments on the continuous locomotion tasks in the mujoco environments. the results indicate that in some tasks, the pmoe policy outperforms the naive policy baseline. the paper also includes ablation studies and visualizations to demonstrate the diversity of the learned primitives of the pmoe policy.-----comments-----the paper is well written and the idea proposed in this paper is really easy to follow. the authors also include a wide suite of experiments with both on-policy and off-policy rl algorithms to demonstrate the performance of the proposed method, and various ablation studies and visualizations to demonstrate the behavior of pmoe policy. despite these advantages, i cannot recommend acceptance of this paper due to the lack of novelty and significant performance improvement.-----first of all, as the experiment results in this paper suggest, the performance gain of the pmoe policy is marginal and highly task-specific. only in one of the 6 tasks the pmoe policy exhibits significant benefit over baselines. therefore, from the scope of experiments in this paper, it is hard to conclude that pmoe policy really has meaningful advantages over a naive policy parameterization.-----moreover, as described in the paper, the pmoe model is a fairly well-studied model, and it seems like the only contribution in this paper is the application of it in the reinforcement learning setting. im not convinced that such a straightforward application has enough contribution, especially given the fact that the performance improvement is not significant.-----therefore, due to the lack of novelty and significant performance improvement, i cannot recommend acceptance of this paper. i would like to thank the authors of ""probabilistic mixture-of-experts for efficient deep reinforcement learning "" for their valuable submission.-----summary of the paper-----the paper proposes an end-to-end method to train probabilistic mixture-of-experts policies in rl agents. they show that the approach can be applied in the context of popular on-policy and off-policy algorithms, and that it compares favourably (performance and sampling efficiency) to using the same algorithms to train the corresponding unimodal policies. furthermore they perform an empirically analysis of the individual resulting components and of the impact of these on exploration, assessment------- the positives -------the proposed approach is sound and seems to work well in practice. the empirical evaluation is extensive - the fact that the paper evaluates the proposed approach in combination with different baseline algorithms makes the findings more robust. the analysis is overall quite insightful, especially in terms of understanding the diversity of individual components and the impact of backprop-max vs backprop-all.------- the concerns -------the paper notes that the mixture of experts seems especially beneficial in high dimensional problems (such as continuous control). this seems an important claim, but it is not clearly backed up. it would be useful to make this statement more quantitative by plotting the improvement in performance over the baseline as a function of the number of dimensions.-----the paper notes that the mixture of experts might help by improving exploration. it would be therefore interesting to include a parameter study showing the performance of the baseline algorithm for different amounts of entropy regularisation. this would help to compare the proposed approach to a simpler way of tuning exploration, assess whether a mixture of experts delivers further benefits on top of this (e.g. by providing deeper exploration), and allow the reader to compare the sensitivity to the parameter k of the proposed approach to the sensitivity of the baseline to the weight of the entropy regularisation. suggestions-----figure 6 could be made more readable by plotting a parameter study instead of a bunch of learning curves, e.g. plot auc as a function of k.-----it would be helpful for the authors to better discuss the relation, similarity and difference between the proposed approach and popular hrl approaches, in order to better assess the novelty of the method, and to ensure that it is placed in the appropriate context.-----finally, the paper could use one more pass general pass to ensure the writing is fully correct and make it as readable as possible. please also fix the following typos:-----or without explicit probabilistic representation  the sentence doesnt connect to the previous one-----is our method outperform?  does our method outperform?","the paper studies mixture of expert policies for reinforcement learning agents, focusing on the problem of policy gradient estimation. the paper proposes a new way to compute the gradient, apply it to two reinforcement learning algorithms, ppo and sac, and demonstrate it in continuous mujoco environments, showing results that are comparable to or slightly exceeds unimodal policies. the main issue raised by multiple reviewers is novelty. mixture of expert models have been widely studied in the context of reinforcement learning, and while the paper proposes a new method for the gradient computation, a more suitable format, as pointed out by reviewer 2, could be to ground the paper around the proposed gradient estimator, and compare, both analytically and empirically, it to existing alternatives. therefore, i recommend rejecting this submission.","the analysis is overall quite insightful, especially in terms of understanding the diversity of individual components and the impact of backprop-max vs backprop-all.------- the concerns -------the paper notes that the mixture of experts seems especially beneficial in high dimensional problems (such as continuous control).","therefore, from the scope of experiments in this paper, it is hard to conclude that pmoe policy really has meaningful advantages over a naive policy parameterization.-----moreover, as described in the paper, the pmoe model is a fairly well-studied model, and it seems like the only contribution in this paper is the application of it in the reinforcement learning setting.","i would ask some questions beyond, would be mixture model be more powerful or practically important to address the robustness, stability issues that faced by current drl algorithms?","in this paper, the authors propose to use mixture models of policies and present good experimental results.","the authors also include a wide suite of experiments with both on-policy and off-policy rl algorithms to demonstrate the performance of the proposed method, and various ablation studies and visualizations to demonstrate the behavior of pmoe policy.","therefore, from the scope of experiments in this paper, it is hard to conclude that pmoe policy really has meaningful advantages over a naive policy parameterization.-----moreover, as described in the paper, the pmoe model is a fairly well-studied model, and it seems like the only contribution in this paper is the application of it in the reinforcement learning setting.","however, a more fundamental question would be the following ones: a).","therefore, from the scope of experiments in this paper, it is hard to conclude that pmoe policy really has meaningful advantages over a naive policy parameterization.-----moreover, as described in the paper, the pmoe model is a fairly well-studied model, and it seems like the only contribution in this paper is the application of it in the reinforcement learning setting.",0.24,0.0346820809248554,0.1485714285714285,0.1485714285714285,0.3263157894736842,0.0425531914893617,0.1684210526315789,0.1684210526315789,0.1528662420382165,0.0,0.0636942675159235,0.0636942675159235,0.1643835616438356,0.0138888888888888,0.1095890410958904,0.1095890410958904,0.2857142857142857,0.0120481927710843,0.1666666666666666,0.1666666666666666,0.3263157894736842,0.0425531914893617,0.1684210526315789,0.1684210526315789,0.0714285714285714,0.0144927536231884,0.0571428571428571,0.0571428571428571,0.3263157894736842,0.0425531914893617,0.1684210526315789,0.1684210526315789,8.268396377563477,7.555309295654297,14.281638145446776,8.268396377563477,7.29227352142334,11.685739517211914,8.268396377563477,5.771853446960449,0.056321351093684656,0.09142439746090644,0.08704777158887897,0.9727465767163239,0.9677450781957796,0.9395685082249564,0.9581390065480602,0.9590202984781273,0.06276931903059789,0.9806156620007326,0.9810407551944224,0.9177037813091506,0.9644210946154005,0.9635649033057788,0.8811308584385941,0.9727465767163239,0.9677450781957796,0.9395684240165393,0.9479334906497464,0.9416665241738106,0.20680651630814542,0.9727465767163239,0.9677450781957796,0.9395685082249564
100,https://openreview.net/forum?id=MaZFq7bJif7,"this paper proposes a multi-hop transformer method for the video-based object permanence task. the proposed method performs multi-hop reasoning via the encoder-decoder architecture of transformers over critical frames in the video. to mitigate the problem of lacking ground truth for the middle hops, the paper proposes some interesting training tricks. overall, the paper is well organized and easy to follow.-----reasons to accept the paper:-----the paper extends multi-hop reasoning techniques that are widely used in nlp domain to video domain, which may inspire other researchers working on other video-based tasks that require multi-hop reasoning.-----the paper proposes a new benchmark dataset, which requires longer reasoning chains.-----experiments on the cater dataset achieve state-of-the-art performance on the object permanence task.-----reasons to reject the paper:-----the paper claims to address the problem of biased video reasoning, however, all the experiments in this paper are done on a synthetic dataset and it's not clear why such dataset can rule out the possibility of having biases.-----the proposed method is somewhat dedicated to a proposed object permanence task, which may not be general enough to be extended to other video-based tasks.-----the paper motivates the task using real-world examples, such as asking ""which car was responsible for the accident"". however, such question seems much harder to answer even with the proposed technique for object permanence task. also, the paper only shows experimental results on synthetic dataset, and leaves experiments on real-world video datasets as future work. the authors proposed a multi-hop transformer, which takes information encoded in forms of object track and image track as input, to reason over the critical frame sequence to locate the final location of the object of interest. although i like the idea presented in the paper, i think there are several aspects that will strengthen the paper. pros: 1. the idea of using transformer in a recurrent manner for reasoning in videos is intuitive and interesting. 2. the authors show the effectiveness of the proposed model by superior quantitative results and the visualization of the most attended object in the inference process of one video sequence. 3. thorough ablation is provided for the proposed multi-hop transformer. 4. the paper is written and presented well.-----cons: 1. one baseline comparison is missing. the tracking baseline seems to be from prior works rather than from the tracking results produced in the proposed frameworks. it is important to know how well the tracking component itself performs. 2. more ablation on the framework should be provided. the authors used the detr for object detection. how does the final performance benefit from using this transformer-based object detection model? is the proposed framework only compatible with transformer-based object detection? 3. although i like the visualization results, it is still a question whether the most attended object is the most important one in the final results or not [1]. it will be interesting to see the gradient visualization rather than the attention visualization. [1] sofia serrano, and noah a. smith. 2019. is attention interpretable.arxiv preprint arxiv:1906.03731.-----==============-----i have read the authors' rebuttal information. the authors have addressed my concerns with additional ablation studies and experiments to verify the effectiveness of the proposed multi-hop transformer. and also some experiments are provided to illustrate whether the most attended object is the most important one.-----therefore, i am still standing on the previous justification for accepting the submitted manuscript.","this paper was reviewed by four experts in the field. based on the reviewers' feedback, the decision is to recommend the paper for acceptance to iclr 2021. the reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. the authors are encouraged to make the necessary changes. it is also very important to think about how to extend this framework to the more challenging cleverer dataset (http://clevrer.csail.mit.edu/).",2. the authors show the effectiveness of the proposed model by superior quantitative results and the visualization of the most attended object in the inference process of one video sequence.,"overall, the paper is well organized and easy to follow.-----reasons to accept the paper:-----the paper extends multi-hop reasoning techniques that are widely used in nlp domain to video domain, which may inspire other researchers working on other video-based tasks that require multi-hop reasoning.-----the paper proposes a new benchmark dataset, which requires longer reasoning chains.-----experiments on the cater dataset achieve state-of-the-art performance on the object permanence task.-----reasons to reject the paper:-----the paper claims to address the problem of biased video reasoning, however, all the experiments in this paper are done on a synthetic dataset and it's not clear why such dataset can rule out the possibility of having biases.-----the proposed method is somewhat dedicated to a proposed object permanence task, which may not be general enough to be extended to other video-based tasks.-----the paper motivates the task using real-world examples, such as asking ""which car was responsible for the accident"".","3. although i like the visualization results, it is still a question whether the most attended object is the most important one in the final results or not [1].",this paper proposes a multi-hop transformer method for the video-based object permanence task.,"overall, the paper is well organized and easy to follow.-----reasons to accept the paper:-----the paper extends multi-hop reasoning techniques that are widely used in nlp domain to video domain, which may inspire other researchers working on other video-based tasks that require multi-hop reasoning.-----the paper proposes a new benchmark dataset, which requires longer reasoning chains.-----experiments on the cater dataset achieve state-of-the-art performance on the object permanence task.-----reasons to reject the paper:-----the paper claims to address the problem of biased video reasoning, however, all the experiments in this paper are done on a synthetic dataset and it's not clear why such dataset can rule out the possibility of having biases.-----the proposed method is somewhat dedicated to a proposed object permanence task, which may not be general enough to be extended to other video-based tasks.-----the paper motivates the task using real-world examples, such as asking ""which car was responsible for the accident"".",this paper proposes a multi-hop transformer method for the video-based object permanence task.,pros: 1. the idea of using transformer in a recurrent manner for reasoning in videos is intuitive and interesting.,"overall, the paper is well organized and easy to follow.-----reasons to accept the paper:-----the paper extends multi-hop reasoning techniques that are widely used in nlp domain to video domain, which may inspire other researchers working on other video-based tasks that require multi-hop reasoning.-----the paper proposes a new benchmark dataset, which requires longer reasoning chains.-----experiments on the cater dataset achieve state-of-the-art performance on the object permanence task.-----reasons to reject the paper:-----the paper claims to address the problem of biased video reasoning, however, all the experiments in this paper are done on a synthetic dataset and it's not clear why such dataset can rule out the possibility of having biases.-----the proposed method is somewhat dedicated to a proposed object permanence task, which may not be general enough to be extended to other video-based tasks.-----the paper motivates the task using real-world examples, such as asking ""which car was responsible for the accident"".",0.1818181818181818,0.0555555555555555,0.1454545454545454,0.1454545454545454,0.3008130081300812,0.0491803278688524,0.1869918699186991,0.1869918699186991,0.1834862385321101,0.0560747663551401,0.128440366972477,0.128440366972477,0.1052631578947368,0.021505376344086,0.0842105263157894,0.0842105263157894,0.3008130081300812,0.0491803278688524,0.1869918699186991,0.1869918699186991,0.1052631578947368,0.021505376344086,0.0842105263157894,0.0842105263157894,0.1212121212121212,0.0,0.0808080808080808,0.0808080808080808,0.3008130081300812,0.0491803278688524,0.1869918699186991,0.1869918699186991,10.14389991760254,11.339727401733398,10.143898010253906,11.339727401733398,5.815944671630859,3.0969600677490234,11.339727401733398,7.851986408233643,0.9471556495766731,0.9492521336855217,0.9307924576610108,0.9910405113120222,0.9890839294849776,0.9108921360428607,0.9645107125066402,0.9546294754639977,0.887507659851183,0.9807058405734422,0.9783625272161349,0.9287627982988959,0.9910405113120222,0.9890839294849776,0.9108921564032122,0.9807058405734422,0.9783625272161349,0.9287627221798134,0.9559506390140201,0.9537255235280977,0.7994322266285646,0.9910405113120222,0.9890839294849776,0.9108921360428607
101,https://openreview.net/forum?id=MpStQoD73Mj,"summary:-----the authors introduce a library for differential weighted finite-state transducers. wfst are commonly used in speech or handwriting recognition systems but are generally not trained jointly with the deep neural networks components such as convnn. this is not due to theoretical limitation of wfst but rather to a lack of available implementation and the need of important computational power to train them. the authors show that this new library can be used to encode the asg criterion, by combining the emission graph (coming from a nn for example), the token graph (base recognition units) and the label graph (the sequence annotation) on one hand and the emission graph and a language model graph on the other hand. the authors show how word pieces decomposition can be learnt through marginalisation. finally, convolution wfst are rapidly presented. preliminary experiments are reported on wsj data base for speech recognition and iam database for handwriting recognition.-----##########################################################################-----reasons for score:-----i am very pleased to see an implementation of the gtn approach which has been proposed more than 20 years ago. wfst approaches have been shown to be more effective (and more elegant) than ad-hoc implementation for both speech and handwriting recognition. if efficient, this library will certainly have a major impact on future asr and htr systems. however, implementation details are not given or explained and experiments are still preliminary. despite its importance and impact, this work seems to be in a too early stage to be accepted to iclr this year.-----##########################################################################pros:-----pros:-----first implementation of a differentiable wfst library-----experiments both on asr and htr with interesting results for learning wfst parameters-----a new convolutional wfst is introduced-----##########################################################################-----cons:-----we dont' know to what extent the operations on wfst needed to build a real asr/htr application are available (determinisation, minimization, weight pushing, etc)-----asr/htr systems are not compared to state of the art, to measure the remaining progress to reach sota.-----as said by the authors, include wfst in a differentiable stack of layers needs a lot of computation. is it trackable for large scale systems ? table 2 gives epoch times for 1000 word pieces (which is small) and for bigrams only. is it on tpu or cpu ?-----the section 4 on learning algorithms is not very generic as only an implementation of asg is first presented then a comparison to ctc.-----section 4.3 on conv. wfst is too short to be really understand the proposed model. maybe this part should be dropped to leave more room to basic algorithms presentation. this paper presents how weighted finite-state transducers (wfst) and a few common operations performed on them can be integrated in a differentiable model, and therefore contribute to the training of complete systems. the authors propose a few case studies, mainly in language applications, where the wfsts are used to compute a sequence-level loss function, to keep ambiguity and let the model decide word-peices decomposition, or gracefully replace convolution layers. the code associated with the presented methods will be available.-----the benefits brought by the availability of wfst-based differentiable operation in deep learning libraries are clear and very relevant. as the authors mention, it would unlock a wealth of possibilities in the design of loss functions and other operations in language-related applications (but not only) and allow to more easily create end-to-end systems.-----however, the idea in itself is not entirely novel. as mentioned in the related work section, this idea has been extensively explored in the past, and a good example of this are the graph transformer networks (gtns). the corresponding paper(s) cover the same kind of operations described here if not more, and it is not clear how this paper brings more on that topic, other than putting them in the context of loss functions used today. the differentiability of the selected operations in the selected semiring is pretty straight-forward and already implemented in some frameworks for special cases. as rightfully mentioned by the authors, kaldi implements sequence-discriminative loss functions, either lattice-based or lattice-free, based on wfsts and on gpu. pychain implements it too, and i believe google mentioned at several occasions that their implementation of the ctc loss was based on wfsts.-----the novelty therefore does not lie so much in the differentiability of wfsts nor in their integration into the training procedure as in the implementation of an efficient way to integrate them in a generic fashion for gpu training. the paper should focus more on that aspect, which would indeed be very interesting for the community, reviving the very interesting and not-so-much exploited gtn idea.-----regarding the applications of the method to speech and handwriting recognition and word-piece selection, they are quite relevant for the scope of this paper. although the level of details on ctc and asg for example would be sufficient for application-specific conferences, i feel like a reminder of how and why these loss functions are computed would be nice for iclr since not all readers may be familiar with these losses.-----regarding the implementation, which to me is the main contribution of this paper, i would expect more details about runtime and efficiency compared to other implementation of ctc and asg for example, which by the way would not be too hard to modify to include the proposed variations (even though it would be reimplementing them for each new case, when the proposed method would be generic, which is indeed a very nice thing to have!). it would also be interesting to see more the impact of the graph size and structure, understand more how the epsilon transitions are handled and the derivative computed.-----regarding the experiments, a comparison with the state-of-the-art would be interesting. i understand that the page limit is tight and does not allow to present the models in details, but the reference to the paper presenting tds in the main text at least might help the reader understand better section 5.3, which is difficult to follow when one is not familiar with that architecture.-----overall, the paper is well-written, easy to read when one is familiar with the presented loss functions. the idea is attractive and the implementation would be very beneficial for the community. however, the novelty of the idea itself is very limited and the improvement over the gtn idea is not clearly stated. the contribution is in my opinion more related to an efficient implementation, but that part is not really described here and lacks an empirical study of its performance. although i'd be happy to use the implementation provided by the authors and understand the huge benefit of its availability, i do not see this paper above acceptance threshold for iclr.","this paper introduces a framework for automatic differentiation with weighted finite-state transducers (wfsts), which would allow user-specified graphs in structured output prediction tasks and easy plug-and-play of graphs through the composition operation (demonstrated with variants of ctc). the authors demonstrated their framework on the ocr and asr domains, which are important application scenarios. all reviewers agree the work is useful and can potentially be significant. however, the reviewers think the paper needs more discussions of similar/parallel work and the key differences from them, and clear description of the novelty in terms of either machine learning insights or algorithmic implementations. we understand that this may be an implementation-heavy work, but the level of details provided in the current version does not convince the reviewers that the proposed approach is already efficient and can scale up. this could be shown by fair comparison with existing approaches (e.g., hard-coded error back-propagation implementation with a fixed graph) in runtime and accuracy.","the authors propose a few case studies, mainly in language applications, where the wfsts are used to compute a sequence-level loss function, to keep ambiguity and let the model decide word-peices decomposition, or gracefully replace convolution layers.","despite its importance and impact, this work seems to be in a too early stage to be accepted to iclr this year.-----##########################################################################pros:-----pros:-----first implementation of a differentiable wfst library-----experiments both on asr and htr with interesting results for learning wfst parameters-----a new convolutional wfst is introduced-----##########################################################################-----cons:-----we dont' know to what extent the operations on wfst needed to build a real asr/htr application are available (determinisation, minimization, weight pushing, etc)-----asr/htr systems are not compared to state of the art, to measure the remaining progress to reach sota.-----as said by the authors, include wfst in a differentiable stack of layers needs a lot of computation.","the paper should focus more on that aspect, which would indeed be very interesting for the community, reviving the very interesting and not-so-much exploited gtn idea.-----regarding the applications of the method to speech and handwriting recognition and word-piece selection, they are quite relevant for the scope of this paper.",summary:-----the authors introduce a library for differential weighted finite-state transducers.,"despite its importance and impact, this work seems to be in a too early stage to be accepted to iclr this year.-----##########################################################################pros:-----pros:-----first implementation of a differentiable wfst library-----experiments both on asr and htr with interesting results for learning wfst parameters-----a new convolutional wfst is introduced-----##########################################################################-----cons:-----we dont' know to what extent the operations on wfst needed to build a real asr/htr application are available (determinisation, minimization, weight pushing, etc)-----asr/htr systems are not compared to state of the art, to measure the remaining progress to reach sota.-----as said by the authors, include wfst in a differentiable stack of layers needs a lot of computation.","despite its importance and impact, this work seems to be in a too early stage to be accepted to iclr this year.-----##########################################################################pros:-----pros:-----first implementation of a differentiable wfst library-----experiments both on asr and htr with interesting results for learning wfst parameters-----a new convolutional wfst is introduced-----##########################################################################-----cons:-----we dont' know to what extent the operations on wfst needed to build a real asr/htr application are available (determinisation, minimization, weight pushing, etc)-----asr/htr systems are not compared to state of the art, to measure the remaining progress to reach sota.-----as said by the authors, include wfst in a differentiable stack of layers needs a lot of computation.","if efficient, this library will certainly have a major impact on future asr and htr systems.","despite its importance and impact, this work seems to be in a too early stage to be accepted to iclr this year.-----##########################################################################pros:-----pros:-----first implementation of a differentiable wfst library-----experiments both on asr and htr with interesting results for learning wfst parameters-----a new convolutional wfst is introduced-----##########################################################################-----cons:-----we dont' know to what extent the operations on wfst needed to build a real asr/htr application are available (determinisation, minimization, weight pushing, etc)-----asr/htr systems are not compared to state of the art, to measure the remaining progress to reach sota.-----as said by the authors, include wfst in a differentiable stack of layers needs a lot of computation.",0.1463414634146341,0.0098522167487684,0.0780487804878048,0.0780487804878048,0.2867383512544803,0.0144404332129963,0.1146953405017921,0.1146953405017921,0.2191780821917808,0.0368663594470046,0.1187214611872146,0.1187214611872146,0.1123595505617977,0.0568181818181818,0.0898876404494382,0.0898876404494382,0.2867383512544803,0.0144404332129963,0.1146953405017921,0.1146953405017921,0.2867383512544803,0.0144404332129963,0.1146953405017921,0.1146953405017921,0.0659340659340659,0.0,0.0549450549450549,0.0549450549450549,0.2867383512544803,0.0144404332129963,0.1146953405017921,0.1146953405017921,8.053133964538574,8.053133964538574,9.08208465576172,8.053133964538574,8.057451248168945,5.380315780639648,8.053133964538574,8.133072853088379,0.9335994318410419,0.9399870123478063,0.912929902309638,0.9733159301507924,0.975401530706916,0.9031377259282483,0.12306524690383996,0.25254911794664214,0.842428096231474,0.9763760597142565,0.9723382636903041,0.9453608468484979,0.9733159301507924,0.975401530706916,0.9031376807362431,0.9733159301507924,0.975401530706916,0.9031377259282483,0.9425610886581927,0.9524760337802053,0.8698573797651585,0.9733159301507924,0.975401530706916,0.9031377259282483
102,https://openreview.net/forum?id=NZj7TnMr01,"this paper proposes a data augmentation scheme (named pad) to improve accuracy and calibration of nns. the idea is to generate ood data, close to the training data, where the model is overconfident, and force a higher entropy for their corresponding predictions.-----this topic is very relevant to the iclr community, the paper is clear, and i was excited with the goal in a first place. however, the paper as it is has major drawbacks.-----the biggest drawback is that the proposed approach is ad-hoc, a heuristic with no guarantees that it will work as desired. in fact, recent work has shown that data augmentation on top of ensembles can be harmful, the authors should discuss this in the paper (see [wen et.al, 2020: combining ensembles and data augmentation can harm your calibration). for this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work.-----using pad on top of other probabilistic approaches destroys the probabilistic interpretation.-----experimental results are extensive, but not convincing: figure 1 lacks the gp reference, and shows bad performance on the left extreme; the ablation study suggest that equation (5) could be simplified; finally results in table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (energy and kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of pad and other baselines, information which is currently missing.-----the authors do not compare nor mention recent advances on calibrating dnns, for example:-----(antoran et.al, 2020) depth uncertainty in neural networks-----(liu et.al, 2019) simple and principled uncertainty estimation with deterministic deep learning via distance awareness-----more comments:-----the proposed model does not seem to scale to high-dimensions, as ""filling the gaps"" with the ood data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim kin8nm dataset). up to how many dimensions would this approach be useful?-----the ood dataset produce an ""equally sized pseudo dataset"". yet, one might think that the amount of data needed to robustify uncertainty would depend on the manifold geometry.-----location of ood samples is chosen as an interpolation of latent representations for the observed data. that means that many generated datapoints will not bee out-of-sample.-----from the ablation study (tables 4 and 5), ""without ab"" gives similar results to regular (always within the reported error bars of ""regular"". that seems to indicate that terms a and b are not that relevant. am i missing something?-----figure 1: the authors should include one column for the gp behavior, since the authors claim that the observed behavior is similar to that. otherwise, it is unclear by eye what is best. in particular, pad-----could the proposed approach suffer from the opposite issue, i.e., deliver too high uncertainty in the augmented ood data? how do you avoid this issue?-----how does the proposed approach compare to a dnn whose last layer is gp or bayesian rbf network? (see http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/udl2020-paper-009.pdf)-----the proposed method encourages a reversion to a specific prior (0 mean functions)-----minor:-----the authors mention limited expressiveness of gps, but this is subject to a simple kernel. if the kernel is complicated enough, then gps are as expressive as we would like to (see equivalences between dnn and gps in [neil, 1997] and [lee et.al, 2017]). please clarify this statement.-----figure 3 is hard to read, i suggest to highlight the pad curves by changing the color scheme). overview:-----the authors propose a data augmentation scheme that generates samples out of distribution and helps with uncertainty estimates. comparisons are to various bayesian methods in uci regression and mnist/cfar for classification. pad seems to give some improvements in out of distribution uncertainty quantification.-----the major concern is that the gains seem relatively small, and the objective is ad-hoc. it would be nice to see either more substantial, uniform gains (so that the authors can justify the procedure on the results alone) or more solid conceptual motivation of the method, especially from the bayesian side. it seems like the motivation and intro is clear, and section 3 onwards becomes very ad-hoc and loses much of this. it would be nice to be convinced that there are a set of assumptions and conditions under which this is the right way to do uncertainty quantification.-----positives:-----the evaluations are extensive, and it's commendable that they include both positive and negative results in their regression evaluations.-----uncertainty estimation out of distribution is an important and timely problem.-----negatives:-----minor: i'm not sure why there is a claim that the problems with uncertainty estimation comes from p(theta|d) and not p_theta(y|x). the fact that non-bayesian methods have similar issues with uncertainty quantification would suggest that the latter is certainly an issue.-----figure 1 doesn't seem like a compelling argument for the narrative in the paper. mc dropout and deep ensembles both have decent behavior outside the support (x<0, x>1) but suffer in the gap between 0.25 to 0.75 which is arguably due to overaggressive interpolation.-----term a in equation 5 is justified as ""generating data where f is overconfident"" but i don't see how this is true. it's just generating data where there's low prediction entropy... this includes areas where it is confident for the right reasons.-----somewhat minor, but the sum of term a and term b seems a bit problematic, since a will be in terms of discrete entropy in classification, and term b is going to be differential entropy in general. rescaling x also seems like it would arbitrarily shift the weights between ab and c? the weird thresholding on the c penalty for regression problems does not inspire confidence.-----overall, equation 5 gives a sense of a fairly ad-hoc criterion. i'd like to be convinced that this is actually the right way of doing things, especially from a bayesian perspective.-----looking at equation (7) it seems like learning the distribution of tilde x is alot of work to regularize the kl towards the marginal with a squared-exponential penalty away from the training data. is it really not possible to post-process the model distribution to achieve the same thing?-----the experiments are extensive, but a bit mixed. the dataset construction for regression seems like it would naturally favor pad-type methods, because the clustering occurs on the basis of feature distances, and pad enforces uncertainty based on feature distances (via the squared-exp term in equation 7). in terms of results, i think overall pad gives gains, but it's not uniform and in cases like swag on table 2 seems to hurt more than it helps.-----the corruptions in mnist / cifar must also be pretty aggressive, as the accuracy numbers are quite low for both. does pad do similarly well on milder or no distribution shift settings? i am slightly concerned that the evaluations here focus so much on the large distribution setting and that pad is tuned to that case.-----minor:-----inline equation involving sines is missing a closing parenthesis.-----notation. g_phi is a 'generative model' in section 2 but it seems to be the output of an autoencoder in section 3.1 q_phi seems to be the actual generative model? i think the paper is interesting and well-written. i agree with the mis-calibration can be caused by out-of-distribution data, even though it is still commonly observed without such discrepancy. addressing ood data is an important direction, and i think the author proposed a reasonable approach to prevent models from overfitting on data points that are rarely observed during training. however, i believe there are some limitation of the method at the current stage, and the experiments did not fully convince me.-----strength:-----the paper addresses an important question of models being over-confident on out-of-distribution data. the method is practical for applications where uncertainty estimation is needed.-----the adversarial generation of ood data is intersting, and the rationale is well explained.-----the authors included a good selection of datasets and experiments. the pad-based methods are also compared to a good variety of baselines.-----weakness:-----to determine whether a data point is out-of-distribution, both the adversarial model and main model rely on the l2 distance and the length scale parameter \ell. my concern here is about 1) the heterogeneity in different dimensions, and 2) \ell seems particularly important and difficult parameter to tune. i would like the authors to give more details on how it is chosen.-----i believe the approach here is to revert to prior when evaluated data points are far away from the observed ones. thus the difference in accuracy depends on how good the starting prior is, and how much bias the baseline model learned from ood data. table 3 shows some of the trade-off, but i think it also be good to show the difference when there's no ood data, because it's not necessarily known in advance.-----looking at figure 3, i don't think the pad method shows significant improvement in most of the datasets. housing seems to be the only one here. in figure 5, it looks like the ood data are mostly in the convex hull of observed data (at least in this low dimensional embedding). it is unclear how to differentiate those from the region where models should be interpolating. moreover, all the ood data are artificially constructed. i think it would be more convincing to test the methods on some ood data that arises naturally. one such source could be temporal data where distribution could shift over time.-----minor comments:-----in section 5 ""excessive computation for large models an datasets"", an->and.-----thank the authors for lot of these responses. i'm still around neutral for this paper, but i will raise my score to marginally above acceptance.","this paper studies the problem of uncertainty estimation under distribution shift. the proposed approach (pad) addresses this under-estimation issue, by augmenting the training data with inputs that the network has unjustified low uncertainty estimates, and asking the model to correct this under-estimation at those augmented datapoints. results show promising improvement over a set of common benchmark tasks in uncertainty estimation, with comparisons to a number of existing approaches.-----all the reviewer agreed that the experiments are well conducted and the empirical results are very promising. however, they also had a shared concern on the justification of the approach. reviewers are less willing to accept a paper merely for commending its empirical performance.-----i share the above concern as the reviewers, and i personally found the presentation of the approach a bit rush and disconnected from the motivation. for example, the current presentation feels like the method is motivated by bnns but it is not clear to me how the proposed objective connects to the motivation. also no derivation of the objective is included in either main text or appendix.-----in revision, i would suggest a focus on improving the clarity and theoretical justification of the proposed objective function.","mc dropout and deep ensembles both have decent behavior outside the support (x<0, x>1) but suffer in the gap between 0.25 to 0.75 which is arguably due to overaggressive interpolation.-----term a in equation 5 is justified as ""generating data where f is overconfident"" but i don't see how this is true.","for this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work.-----using pad on top of other probabilistic approaches destroys the probabilistic interpretation.-----experimental results are extensive, but not convincing: figure 1 lacks the gp reference, and shows bad performance on the left extreme; the ablation study suggest that equation (5) could be simplified; finally results in table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (energy and kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of pad and other baselines, information which is currently missing.-----the authors do not compare nor mention recent advances on calibrating dnns, for example:-----(antoran et.al, 2020) depth uncertainty in neural networks-----(liu et.al, 2019) simple and principled uncertainty estimation with deterministic deep learning via distance awareness-----more comments:-----the proposed model does not seem to scale to high-dimensions, as ""filling the gaps"" with the ood data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim kin8nm dataset).","for this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work.-----using pad on top of other probabilistic approaches destroys the probabilistic interpretation.-----experimental results are extensive, but not convincing: figure 1 lacks the gp reference, and shows bad performance on the left extreme; the ablation study suggest that equation (5) could be simplified; finally results in table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (energy and kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of pad and other baselines, information which is currently missing.-----the authors do not compare nor mention recent advances on calibrating dnns, for example:-----(antoran et.al, 2020) depth uncertainty in neural networks-----(liu et.al, 2019) simple and principled uncertainty estimation with deterministic deep learning via distance awareness-----more comments:-----the proposed model does not seem to scale to high-dimensions, as ""filling the gaps"" with the ood data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim kin8nm dataset).",this paper proposes a data augmentation scheme (named pad) to improve accuracy and calibration of nns.,"for this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work.-----using pad on top of other probabilistic approaches destroys the probabilistic interpretation.-----experimental results are extensive, but not convincing: figure 1 lacks the gp reference, and shows bad performance on the left extreme; the ablation study suggest that equation (5) could be simplified; finally results in table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (energy and kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of pad and other baselines, information which is currently missing.-----the authors do not compare nor mention recent advances on calibrating dnns, for example:-----(antoran et.al, 2020) depth uncertainty in neural networks-----(liu et.al, 2019) simple and principled uncertainty estimation with deterministic deep learning via distance awareness-----more comments:-----the proposed model does not seem to scale to high-dimensions, as ""filling the gaps"" with the ood data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim kin8nm dataset).","for this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work.-----using pad on top of other probabilistic approaches destroys the probabilistic interpretation.-----experimental results are extensive, but not convincing: figure 1 lacks the gp reference, and shows bad performance on the left extreme; the ablation study suggest that equation (5) could be simplified; finally results in table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (energy and kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of pad and other baselines, information which is currently missing.-----the authors do not compare nor mention recent advances on calibrating dnns, for example:-----(antoran et.al, 2020) depth uncertainty in neural networks-----(liu et.al, 2019) simple and principled uncertainty estimation with deterministic deep learning via distance awareness-----more comments:-----the proposed model does not seem to scale to high-dimensions, as ""filling the gaps"" with the ood data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim kin8nm dataset).","pad seems to give some improvements in out of distribution uncertainty quantification.-----the major concern is that the gains seem relatively small, and the objective is ad-hoc.","for this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work.-----using pad on top of other probabilistic approaches destroys the probabilistic interpretation.-----experimental results are extensive, but not convincing: figure 1 lacks the gp reference, and shows bad performance on the left extreme; the ablation study suggest that equation (5) could be simplified; finally results in table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (energy and kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of pad and other baselines, information which is currently missing.-----the authors do not compare nor mention recent advances on calibrating dnns, for example:-----(antoran et.al, 2020) depth uncertainty in neural networks-----(liu et.al, 2019) simple and principled uncertainty estimation with deterministic deep learning via distance awareness-----more comments:-----the proposed model does not seem to scale to high-dimensions, as ""filling the gaps"" with the ood data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim kin8nm dataset).",0.1322957198443579,0.0,0.0856031128404669,0.0856031128404669,0.373134328358209,0.065,0.1890547263681592,0.1890547263681592,0.373134328358209,0.065,0.1890547263681592,0.1890547263681592,0.1018518518518518,0.0093457943925233,0.0833333333333333,0.0833333333333333,0.373134328358209,0.065,0.1890547263681592,0.1890547263681592,0.373134328358209,0.065,0.1890547263681592,0.1890547263681592,0.1403508771929824,0.0353982300884955,0.1052631578947368,0.1052631578947368,0.373134328358209,0.065,0.1890547263681592,0.1890547263681592,8.640839576721191,8.640839576721191,15.235179901123049,8.640839576721191,5.413527488708496,8.640839576721191,8.640839576721191,10.23928451538086,0.023695989528832305,0.03456060857043278,0.9201106766573466,0.979675942179709,0.9759146689540615,0.9369197352472688,0.979675942179709,0.9759146689540615,0.9369197352472688,0.9699772630966208,0.9687370776066571,0.9046994925013387,0.979675942179709,0.9759146689540615,0.9369197352472688,0.979675942179709,0.9759146689540615,0.9369197352472688,0.7242163701395982,0.7848015161611589,0.26510675908820835,0.979675942179709,0.9759146689540615,0.9369197352472688
103,https://openreview.net/forum?id=OOsR8BzCnl5,"this paper has proposed a novel trust-based multi-view classifier. the idea of transferring classification output to the parameter of dirichlet distribution to give uncertainty in output is novel and interesting. the model has interestingly used dirichlets strength to define the weight of a view.-----paper is well written and mathematically sound. to my knowledge using dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification is new.-----the proposed method has been also compared with recent existing works in the task of a single view and multiple view classification.-----the proposed model has been motivated as classification with uncertainty in multi-view case. but in the experiment section, it shows that even for single view setup the method is outperforming the existing models. it will be good to discuss the reason behind that. why it is better than other methods in a single view case too?-----details of the experimental setup are important for the reproducibility of experimental results. it would have been better to have in the main paper. this paper proposes a reliable multi-view classification mechanism equipped with uncertainty, called trusted multi-view classification. the goal is to dynamically assess the quality of different views for different samples to provide reliable uncertainty estimation. the idea is clear and well-motivated. the authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective, robust and reliable.-----strengths:-----it is interesting to conduct multi-view classification by dynamically integrating different views at an evidence level, which provides a novel and flexible way in multi-view classification.-----the way of using dempster-shafer theory for integrating evidences in a unified and learnable framework is quite neat.-----the paper is well-written and clearly presented.-----strong and sufficient empirical results are provided.-----minor comments:-----it is reasonable to use the subjective logic theory to directly model uncertainty, however, beyond the advantages mentioned, it will be better if intuitive comparison or discussion between using u (overall uncertainty) and softmax scores in multi-view learning could be provided.-----the authors could present failure cases which may be associated with high uncertainties (ideally).-----for the results (table 2) of the end-to-end experiments, are the data used original or being corrupted manually?-----overall, the paper is very well motivated and easy to follow. the assumptions and decisions are well supported. the stepwise experiments are helpful and provide good insights to evaluate the proposed algorithm. the method seems to be of great potential in real-world (cost-sensitive) applications.","the paper introduces a new idea for multi-view classification: using a dirichlet distribution over the views to model uncertainty.-----the paper appears to be clear, well written and sound. also, the experimental comparison is thorough.-----the authors have given pertinent responses to the reviewers' questions, including w.r.t comparing against bayesian/deep cca in terms of accuracy.-----overall, this is a good paper.","the authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective, robust and reliable.-----strengths:-----it is interesting to conduct multi-view classification by dynamically integrating different views at an evidence level, which provides a novel and flexible way in multi-view classification.-----the way of using dempster-shafer theory for integrating evidences in a unified and learnable framework is quite neat.-----the paper is well-written and clearly presented.-----strong and sufficient empirical results are provided.-----minor comments:-----it is reasonable to use the subjective logic theory to directly model uncertainty, however, beyond the advantages mentioned, it will be better if intuitive comparison or discussion between using u (overall uncertainty) and softmax scores in multi-view learning could be provided.-----the authors could present failure cases which may be associated with high uncertainties (ideally).-----for the results (table 2) of the end-to-end experiments, are the data used original or being corrupted manually?-----overall, the paper is very well motivated and easy to follow.","the authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective, robust and reliable.-----strengths:-----it is interesting to conduct multi-view classification by dynamically integrating different views at an evidence level, which provides a novel and flexible way in multi-view classification.-----the way of using dempster-shafer theory for integrating evidences in a unified and learnable framework is quite neat.-----the paper is well-written and clearly presented.-----strong and sufficient empirical results are provided.-----minor comments:-----it is reasonable to use the subjective logic theory to directly model uncertainty, however, beyond the advantages mentioned, it will be better if intuitive comparison or discussion between using u (overall uncertainty) and softmax scores in multi-view learning could be provided.-----the authors could present failure cases which may be associated with high uncertainties (ideally).-----for the results (table 2) of the end-to-end experiments, are the data used original or being corrupted manually?-----overall, the paper is very well motivated and easy to follow.",to my knowledge using dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification is new.-----the proposed method has been also compared with recent existing works in the task of a single view and multiple view classification.-----the proposed model has been motivated as classification with uncertainty in multi-view case.,this paper has proposed a novel trust-based multi-view classifier.,to my knowledge using dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification is new.-----the proposed method has been also compared with recent existing works in the task of a single view and multiple view classification.-----the proposed model has been motivated as classification with uncertainty in multi-view case.,to my knowledge using dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification is new.-----the proposed method has been also compared with recent existing works in the task of a single view and multiple view classification.-----the proposed model has been motivated as classification with uncertainty in multi-view case.,the idea of transferring classification output to the parameter of dirichlet distribution to give uncertainty in output is novel and interesting.,to my knowledge using dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification is new.-----the proposed method has been also compared with recent existing works in the task of a single view and multiple view classification.-----the proposed model has been motivated as classification with uncertainty in multi-view case.,0.2711864406779661,0.0683760683760683,0.1694915254237288,0.1694915254237288,0.2711864406779661,0.0683760683760683,0.1694915254237288,0.1694915254237288,0.3697478991596639,0.0512820512820512,0.1680672268907563,0.1680672268907563,0.131578947368421,0.027027027027027,0.1052631578947368,0.1052631578947368,0.3697478991596639,0.0512820512820512,0.1680672268907563,0.1680672268907563,0.3697478991596639,0.0512820512820512,0.1680672268907563,0.1680672268907563,0.3023255813953489,0.0476190476190476,0.2093023255813953,0.2093023255813953,0.3697478991596639,0.0512820512820512,0.1680672268907563,0.1680672268907563,12.398183822631836,12.398183822631836,11.402260780334473,10.011910438537598,10.011910438537598,12.398183822631836,12.398183822631836,11.470176696777344,0.9866354874579013,0.9864080784614884,0.8888494232376796,0.9866354874579013,0.9864080784614884,0.8888492377977613,0.9906882587910779,0.9904677823774438,0.8921881518794114,0.9843323141211889,0.9814925626899383,0.9093742900620833,0.9906882587910779,0.9904677823774438,0.8921881518794114,0.9906882587910779,0.9904677823774438,0.8921881518794114,0.9675249668262466,0.9734150877331106,0.9217047080387554,0.9906882587910779,0.9904677823774438,0.8921880986998927
104,https://openreview.net/forum?id=Pz_dcqfcKW8,"this paper proposes an unified framework for both streaming and non-streaming asr and the knowledge transfer between them. the results show that both latency and performance are improved. the benefit of training full-context and streaming together are two folds: 1) current full-context and streaming asr are trained separately. since usually the performance of streaming asr is inferior to the full-context version, the unified training scheme could enforce the model to fit both tasks well, thus could serve as some kind of regularization. 2) the weight sharing proposed in this paper could make it more efficient for deploying both streaming and non-streaming asr at the same time.-----cons:-----though conformer and context achieved sota performance as full-context models, they are pretty new and not widely acknowledged and studied compared to transformer. adding results of transformer could better support the claims and make the contribution in this paper more accessible to the community.-----the source of improvement on latency is not well explained.-----it might indicate that weight sharing itself encourages learning better deep representation for streaming asr., this claim should be further validated.-----question and comments:-----adding some related work on knowledge distillation could make it a more complete story.-----seems the statistics of multidomain dataset are not consistent in the paper (413,000 hours vs 163,000 hours)-----""for fair comparisons, on each dataset we train and report our models and baselines with the same settings (number of training iterations, hyper-parameters, optimizer, regularization, etc.)."" could it be possible different models perform the best with different hyperparameter settings?-----it could be interesting if the visualization of knowledge transferred from full-context model to streaming model is investigated. this submission proposes a framework for training online and offline asr models. experimental results suggest that at least on librispeech this approach provides tangible benefits for online asr models.-----quality: the quality of this submission suffers from (a) mostly verbal presentation (e.g. figure 1 is a very inefficient way to show that prediction at time t is either dependent on past only input or all of the input) and (b) limited benefits observed on the challenging multidomain data set. regarding (a), you work would have been significantly stronger if you would have provided more technical descriptions of changes that you are proposing to ensure that the elements you are using are online ""friendly"". this might have helped you to link what you are proposing to other work done in the past and further strengthen your submission. regarding (b), it seems that multidomain data set is very challenging or not enough tuning was performed to illustrate the benefit of your approach.-----clarity: the clarity of this submission suffers from a mostly verbal presentation of very technical operations.-----originality: this submission offers limited originality.-----significance: given experimental results, the significance of this particular submission is minor.-----pros: this submission presents what i believe are generally useful techniques but the presentation is verbal rather than mathematical which makes establishing connections significantly harder than it needs to be. at least on one of the data sets the results appear to be good.-----cons: technical elements are described in a very verbal fashion which may lead to misinterpretation. the results on more challenging multidomain data set do not make a convincing case that the proposed solution or its tuning is more generally useful.-----post author response stage: given the response from the authors and the input from other reviewers i increased the score from 4 to 6.","this paper proposes an approach to unifying both full-context and streaming asr in a single end-to-end model. techniques such as weight sharing, joint training and teacher-student knowledge distillation are used to improve the training. the so-called dual-mode asr is evaluated under the contextnet and conformer networks on librispeech and multidomain datasets. the performance is good. while the technical novelty is not overwhelmingly significant, all reviewers agree that it may have impact to the speech machine learning community as high-performance streaming asr is of great importance in real-world deployment of asr systems. the authors have meticulously addressed the reviewers' comments and, in particular, changed the title from ""universal asr"" to ""dual-mode asr"" as suggested by some of the reviewers. after the rebuttal, all reviewers are supportive on accepting the paper.","2) the weight sharing proposed in this paper could make it more efficient for deploying both streaming and non-streaming asr at the same time.-----cons:-----though conformer and context achieved sota performance as full-context models, they are pretty new and not widely acknowledged and studied compared to transformer.","regarding (b), it seems that multidomain data set is very challenging or not enough tuning was performed to illustrate the benefit of your approach.-----clarity: the clarity of this submission suffers from a mostly verbal presentation of very technical operations.-----originality: this submission offers limited originality.-----significance: given experimental results, the significance of this particular submission is minor.-----pros: this submission presents what i believe are generally useful techniques but the presentation is verbal rather than mathematical which makes establishing connections significantly harder than it needs to be.",this submission proposes a framework for training online and offline asr models.,this paper proposes an unified framework for both streaming and non-streaming asr and the knowledge transfer between them.,"regarding (b), it seems that multidomain data set is very challenging or not enough tuning was performed to illustrate the benefit of your approach.-----clarity: the clarity of this submission suffers from a mostly verbal presentation of very technical operations.-----originality: this submission offers limited originality.-----significance: given experimental results, the significance of this particular submission is minor.-----pros: this submission presents what i believe are generally useful techniques but the presentation is verbal rather than mathematical which makes establishing connections significantly harder than it needs to be.","regarding (b), it seems that multidomain data set is very challenging or not enough tuning was performed to illustrate the benefit of your approach.-----clarity: the clarity of this submission suffers from a mostly verbal presentation of very technical operations.-----originality: this submission offers limited originality.-----significance: given experimental results, the significance of this particular submission is minor.-----pros: this submission presents what i believe are generally useful techniques but the presentation is verbal rather than mathematical which makes establishing connections significantly harder than it needs to be.",this might have helped you to link what you are proposing to other work done in the past and further strengthen your submission.,"regarding (b), it seems that multidomain data set is very challenging or not enough tuning was performed to illustrate the benefit of your approach.-----clarity: the clarity of this submission suffers from a mostly verbal presentation of very technical operations.-----originality: this submission offers limited originality.-----significance: given experimental results, the significance of this particular submission is minor.-----pros: this submission presents what i believe are generally useful techniques but the presentation is verbal rather than mathematical which makes establishing connections significantly harder than it needs to be.",0.2872340425531915,0.043010752688172,0.1382978723404255,0.1382978723404255,0.2389380530973451,0.0,0.1061946902654867,0.1061946902654867,0.0933333333333333,0.0,0.08,0.08,0.1656050955414012,0.0516129032258064,0.1528662420382165,0.1528662420382165,0.2389380530973451,0.0,0.1061946902654867,0.1061946902654867,0.2389380530973451,0.0,0.1061946902654867,0.1061946902654867,0.1118012422360248,0.0,0.0869565217391304,0.0869565217391304,0.2389380530973451,0.0,0.1061946902654867,0.1061946902654867,8.453658103942871,8.453658103942871,15.551776885986328,8.453658103942871,12.82931900024414,12.58104133605957,8.453658103942871,4.935387134552002,0.9641801128131655,0.9660669586580427,0.8574860243922167,0.9639775745379835,0.9729176349306887,0.9153002818074708,0.9713587472903463,0.9730647845779783,0.8685800572490348,0.9768866196114445,0.974896767171248,0.9153571912922425,0.9639775745379835,0.9729176349306887,0.9153002818074708,0.9639775745379835,0.9729176349306887,0.9153002818074708,0.9350795452735289,0.9550229676779103,0.946205463639248,0.9639775745379835,0.9729176349306887,0.9153003823301838
105,https://openreview.net/forum?id=QSMvGB5j5-,"this paper provide a method for high-order structure prediction problem. specifically, the paper first defines a high-order structure on graphs named graph simplicial complex (gsc). then the paper introduces a feature generation method used for the high-order structures. the features are also used in the proposed method for high-order structure prediction. the proposed method is based on a nonparametric kernel which carries the feature similarities of high-order structures. with this kernel the method uses a bernoulli distribution for the prediction of the existence of the high-order structure in unseen times.-----in my opinion, the paper has the following strengths and weaknesses.-----for strengths, first, the kernel estimator based method does not require learning process and shows good efficiency in the prediction tasks. the kernel estimator also plays an important role in capturing the high-order interactions in the evolving graphs. second, the paper provides with theoretical analysis on the consistency and asymptotic normality of the proposed kernel estimator. the theories show possibility of inferring the estimation error and the confidence intervals for predictions.-----for weaknesses, first, the paper ought to make further clarification on the essential differences (and maybe even better, the advantages) between the defined gsc with the traditional high-order structures, such as simplex, hyperedges, or just small graphs. also experimentally the paper should show more of the advantages of using gscs. for example, the experiment only use d=1 and 2. perhaps the experiment should show its advantage when dealing with much higher-order structure predictions rather than these simple cases. second, the presentation of the paper needs further polish. the paper defines notations on-the-fly, and it makes the readers not easy to follow. for example, if we only look at the notations related to g, there are g_t, g_{-}^{(d)}, g_{t,p}, g't(), g_t(d), g{t-}^{(d)}, and they are defined here and there in section 3. the readers will need to make their own notation table just to follow the paper. i personally failed at looking for the definition of g_{t-}^{(d)}. in definition 3, it looks like it is a vertex set, but in the time complexity analysis it becomes a number. also, in the time complexity analysis, does the complexity of counting number of simplices of each d need to be considered? third, for the experiments, the paper should consider using more dynamic methods of link prediction especially when you are only comparing the results of prediction lower-order structures. in table 1, what is the runtime for baselines, training or making predictions? overall the experiments need to be more concrete. last but not least, this paper might not be a good fit for iclr community since it does not focus on the representation learning methods. this paper presents an estimator that predict higher-order structure in time-varying graphs. the authors present an kernel-based estimator, prove that it is consistent when the indicator variable for whether a particular (d+1)-dimensional simplex is bernoulli distributed with a function g. the authors prove that their estimator is asymptotically normal. the authors also present some experiments on real-world data-----some comments and questions:-----a concrete example that motivates this problem would be useful in the introduction. as someone who hadn't encountered this problem previously i found it hard to keep a motivational example in my mind while reading this paper.-----in equation 7, should it be |\tilde{g}_t -g|, that is, is the absolute value missing? otherwise, i dont see why \tilde{g}_t - g represents the error here. relatedly it would be nice if the authors provide some intuition behind why the two terms in the equation correspond to the variance and bias respectively.-----it would be nice if the authors could give an example of a scenario where the process is alpha-mixing.-----proposition 1 makes a reference to theorem 3 (which is deep within the appendix), mentioning this in the proposition statement would be nice.-----the statement of theorem 2 conditions of s_c which is undefined.","this paper proposes a method for predicting higher-order structure in time-varying graphs. the paper was reviewed by three expert reviewers, and while they expressed appreciation for the sensible solution, they have remaining concerns about the novel contributions and comparisons (analytical and empirical) with previous approaches. also, the paper would be clearer if examples are used to illustrate the important points of the paper. the authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.","the theories show possibility of inferring the estimation error and the confidence intervals for predictions.-----for weaknesses, first, the paper ought to make further clarification on the essential differences (and maybe even better, the advantages) between the defined gsc with the traditional high-order structures, such as simplex, hyperedges, or just small graphs.","with this kernel the method uses a bernoulli distribution for the prediction of the existence of the high-order structure in unseen times.-----in my opinion, the paper has the following strengths and weaknesses.-----for strengths, first, the kernel estimator based method does not require learning process and shows good efficiency in the prediction tasks.","with this kernel the method uses a bernoulli distribution for the prediction of the existence of the high-order structure in unseen times.-----in my opinion, the paper has the following strengths and weaknesses.-----for strengths, first, the kernel estimator based method does not require learning process and shows good efficiency in the prediction tasks.",this paper provide a method for high-order structure prediction problem.,"with this kernel the method uses a bernoulli distribution for the prediction of the existence of the high-order structure in unseen times.-----in my opinion, the paper has the following strengths and weaknesses.-----for strengths, first, the kernel estimator based method does not require learning process and shows good efficiency in the prediction tasks.",this paper provide a method for high-order structure prediction problem.,"for example, if we only look at the notations related to g, there are g_t, g_{-}^{(d)}, g_{t,p}, g't(), g_t(d), g{t-}^{(d)}, and they are defined here and there in section 3. the readers will need to make their own notation table just to follow the paper.","with this kernel the method uses a bernoulli distribution for the prediction of the existence of the high-order structure in unseen times.-----in my opinion, the paper has the following strengths and weaknesses.-----for strengths, first, the kernel estimator based method does not require learning process and shows good efficiency in the prediction tasks.",0.2962962962962963,0.0451127819548872,0.1481481481481481,0.1481481481481481,0.3503649635036496,0.074074074074074,0.218978102189781,0.218978102189781,0.3503649635036496,0.074074074074074,0.218978102189781,0.218978102189781,0.1935483870967742,0.0879120879120879,0.1505376344086021,0.1505376344086021,0.3503649635036496,0.074074074074074,0.218978102189781,0.218978102189781,0.1935483870967742,0.0879120879120879,0.1505376344086021,0.1505376344086021,0.2205882352941176,0.0149253731343283,0.1323529411764706,0.1323529411764706,0.3503649635036496,0.074074074074074,0.218978102189781,0.218978102189781,13.872477531433104,9.686484336853027,13.872477531433104,9.686484336853027,8.619101524353027,9.686484336853027,9.686484336853027,6.73060941696167,0.9386307107836945,0.9622363406412184,0.8187044880637067,0.9746538622563905,0.9735186463618233,0.8851357296850357,0.9746538622563905,0.9735186463618233,0.8851355310822023,0.9788756900952537,0.9759084037572093,0.7515879214965946,0.9746538622563905,0.9735186463618233,0.8851355310822023,0.9788756900952537,0.9759084037572093,0.7515874298362479,0.9690067047013144,0.9637169343614151,0.8759456895054136,0.9746538622563905,0.9735186463618233,0.8851355310822023
106,https://openreview.net/forum?id=R7aFOrR0b2,"this work is an empirical survey of the calibration problem with convnets. the authors use several existing benchmark datasets and create synthetic class-imbalance for datasets that are initially balanced. they then extend the well-known results on higher prediction error of minority class, to its calibration error. the work investigates several existing methods that alleviate prediction error in imbalanced datasets and examine their effect on calibration error. at last, the effect of dataset size and data augmentation on calibration error is reported. later on, the effect of random label noise is also examined. the observations, although not surprising, have not been reported before the work is interesting, the writing is clear, and the experiments are comprehensive. although the observations are very informative, the overall contribution of the paper is not sufficient for the iclr venue. the work is mostly focused on reporting an existing issue with no major theoretical analysis of the problem and guidelines for alleviating the mentioned problems. the paper is in an interesting direction but needs to become more mature.-----questions and suggestoins: 1- the label noise experiments are interesting. in reality, label noise is rarely random and is structured. it would be more helpful if the authors could extend the experiment to incorporate such scenarios. 2- there seems to be an interesting difference among various reweighting methods in table 1. it would be interesting if authors compared their calibration error performance to their prediction error performance to find out if there is a trade-off or the two phenomena are in the same direction. 3- in a lot of experiments, for instance the dataset size, it's expected to have higher calibration error for smaller data. it would be more informative if the general trend of calibration error is compared with the trend in prediction error side by side. in this work, authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation. in the field of applied ai to real-life problem, we face all the time decision-makings on what is the most effective strategy in the pipeline (eg. sampling, noise, labeling) and this paper present some evidence for those decisions.-----this type of work is important to systematically highlight areas or processes to follow in model development. the study is not very novel, but important. since the conclusions are very important and have key implications, i would suggest to apply this to more datasets, and also some of the existing synthetic datasets. personally, i would like to see if these observations remain solid with more datasets and more variation of datasets.-----i did not found any inconsistencies. this paper discussed how data properties (e.g., label noise, label imbalance, data size) affects calibration error. the author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and inaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) calibration error under different combinations of data augmentations. the experimental results show that poor calibration performance accompanies with large noisy label rate, large imbalance ratio and small dataset size. for the reason of small dataset size causing poor calibration error, this paper provided the theoretical proof. ----------advantages:-----  the idea of considering a softmax-cross entropy logit loss to help explain how data size affect the calibration error is interesting.----------major concerns:-----  organization should be improved. in particular, the factors that affect the calibration error should be listed and well described in a separate section (e.g., intro -> background -> (affected data properties) -> experiments), and the theoretical motivation could also be integrated in such section rather than put it after experiments. -----  the novelty and practicability of this paper is limited, since this paper only tells people that low label quality and small data size would arise calibration error, the paper analyzed the factors qualitatively but not quantitatively. in the future research, the researcher still hard to justify how much calibration error the current dataset whould bring or can't tell whether the current the current classifier whould be robust enough to defense the calibration bring by the current set. an example is: ----- [1] ""robustness of classifiers: from adversarial to random noise."" fawz et al. nips2016. this paper analyzed the robustness of classifiers quantitatively with considering adversarial and random noise. -----minor comments: -----  table 1, ""exp-inbalance"" -> ""exp-imbalance""-----  should the captions of figure 2 and figure 3 be changed?-----  assumption 1, ""x_i != x_i"" -> ""x_i != x_j""-----  equation 2, ""-sum_i(a+b)"" -> ""sum_i(a-b)"".-----  there are many typos in this paper, should go over the paper again and correct these small mistakes.","the authors empirically analyse the properties of datasets which lead to poor calibration. in particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per-class calibration. while there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for iclr. to improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers. for the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship -- is there a tradeoff? for the latter, the reviewers pointed to a concrete extension with structured label noise. finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice. therefore, i will recommend rejection.",----------advantages:-----  the idea of considering a softmax-cross entropy logit loss to help explain how data size affect the calibration error is interesting.----------major concerns:-----  organization should be improved.,"the author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and inaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) calibration error under different combinations of data augmentations.","this paper discussed how data properties (e.g., label noise, label imbalance, data size) affects calibration error.",this work is an empirical survey of the calibration problem with convnets.,"the author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and inaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) calibration error under different combinations of data augmentations.","the author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and inaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) calibration error under different combinations of data augmentations.","in the field of applied ai to real-life problem, we face all the time decision-makings on what is the most effective strategy in the pipeline (eg.","the author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and inaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) calibration error under different combinations of data augmentations.",0.1243523316062176,0.0,0.0725388601036269,0.0725388601036269,0.2123893805309734,0.0535714285714285,0.1238938053097345,0.1238938053097345,0.0883977900552486,0.0111731843575418,0.0552486187845303,0.0552486187845303,0.0909090909090909,0.0229885057471264,0.0681818181818181,0.0681818181818181,0.2123893805309734,0.0535714285714285,0.1238938053097345,0.1238938053097345,0.2123893805309734,0.0535714285714285,0.1238938053097345,0.1238938053097345,0.1145833333333333,0.0105263157894736,0.09375,0.09375,0.2123893805309734,0.0535714285714285,0.1238938053097345,0.1238938053097345,10.485690116882324,10.485690116882324,14.066210746765137,10.485690116882324,10.499067306518556,12.985523223876951,10.485690116882324,4.040067195892334,0.9783203163802794,0.9718209200669111,0.8437222648930186,0.9707565583866528,0.969316598170831,0.8301095893297382,0.977279161000626,0.9803235210480266,0.9466660380318501,0.9646047063460708,0.9590730947232675,0.9213759281675735,0.9707565583866528,0.969316598170831,0.8301095544632172,0.9707565583866528,0.969316598170831,0.8301095544632172,0.934174623891784,0.9449093134804137,0.8035274221805562,0.9707565583866528,0.969316598170831,0.830109350903851
107,https://openreview.net/forum?id=S191YzbRZ,"this work proposes an approach for transcription factor binding site prediction using a multi-label classification formulation. it is a very interesting problem and application and the approach is interesting. ----------------novelty:--------the method is quite similar to matching networks (vinyals, 2016) with a few changes in the matching approach. as such, in order to establish its broader applicability there should be additional evaluation on other benchmark datasets. the mnist performance comparison is inadequate and there are other papers that do better on it. --------they should clearly list what the contributions are w.r.t to the work by vinyals et al 2016.--------they should also cite works that learn embeddings in a multi-label setting such as starspace.----------------impact:--------in its current form the paper seems to be most relevant to the computational biology / tfbs community. however, there is no comparison to the exact networks used in the prior works deepbind/deepsea/danq/basset/deeplift or bidirectional lstms. further there is no comparison to existing one-shot learning techniques either. this greatly limits the impact of the work.----------------for biological impact, a comparison to any of the motif learning approaches that are popular in the biology/comp-bio community will help (for instance, homer, fimo).----------------cons:--------the authors claim they can learn tf-tf interactions and it is one of the main biological contributions, but there is no evidence of why (beyond very preliminary evaluation using the trrust database). their examples are 200-bp long which does not mean that all tfs binding in that window are involved in cooperative binding. the prototype loss is too simplistic to capture co-binding tendencies and the combinationlstm is not well motivated. one interesting source of information they could tap into for tf-tf interactions is cap-selex (jolma et al, nature 2015).----------------one of the main drawbacks is the lack of interpretability of their model where approaches like danq/deeplift etc benefit. the pwm-like filters in some of the prior works help understand what type of sequence properties contribute to binding events. can their model lead to an understanding of this sort?----------------evaluation:--------the empirical evaluation itself is not very strong as there are only modest improvements over simple baselines. further there are no error-bars etc to indicate the variance in their performance numbers.--------it will be useful to have a tf-level performance split-up to get an idea of which tfs benefit most.----------------clarity:--------the paper can benefit from more clarity in the technical aspects. it is hard to follow for anyone not already familiar with matching networks. the objective function, parameters need to be clearly introduced in one place. for instance, what is y_i in their multi-label framework?--------various choices are not well motivated; for instance cosine similarity, the value of hyperparameter epsilon.--------the prototype vectors are not motif-like at all -- can the authors motivate this aspect better?----------------update: i have updated my rating based on the author rebuttal the authors of this manuscript proposed a model called pmn based on previous works for the classification of transcription factor binding. overall, this manuscript is not well written. clarification is needed in the method and data sections. the model itself is an incremental work, but the application is novel. my specific concerns are given below.----------------1. it is unclear how the prototype of a tf is learned. detailed explanation is necessary. ----------------2. why did the authors only allow a tf to have only one prototype? a tf can have multiple distinct motifs.----------------3. why peaks with p-value>=1 were defined as positive? were negative classes considered in the computational experiments?----------------4. what's the relationship between the lstm component in the proposed method and sparse coding?----------------5. the manuscript contains lots of low-end issues, such as:--------5.1. inconsistency in the format when referring to equations (eq. equation, equation, attention lstm, attentionlstm, t and t etc);--------5.2. some ""0""s are missing in table 3;--------5.3. l2 should be l_2 norm; --------5.4. euclidean -> euclidean; pvalue-> p-value;--------5.5. some author name and year citations in the manuscript should be put in brackets;--------5.6. the encode paper should be cited properly, (""consortium et al., 2012"" is weird!) ;--------5.7. the references should be carefully reformatted, for example, some words in the references should be in uppercase (e.g. dna, jasper, cnn etc.), some items are duplicated, ...----------------comments for the revised manuscript: i decide to keep my decision as it is. my major and minor concerns are not fully well addressed in the revised paper.","this paper proposes an approach for predicting transcription factor (tf) binding sites and tf-tf interaction. the approach is interesting and may ultimately be valuable for the intended application. but in its current state, the paper has insufficient technical novelty (e.g. relative to matching networks of vinyals 2016), insufficient comparisons with prior work, and unclear benefit of the approach. the reviewers also had some concerns about clarity.","one interesting source of information they could tap into for tf-tf interactions is cap-selex (jolma et al, nature 2015).----------------one of the main drawbacks is the lack of interpretability of their model where approaches like danq/deeplift etc benefit.","for instance, what is y_i in their multi-label framework?--------various choices are not well motivated; for instance cosine similarity, the value of hyperparameter epsilon.--------the prototype vectors are not motif-like at all -- can the authors motivate this aspect better?----------------update: i have updated my rating based on the author rebuttal the authors of this manuscript proposed a model called pmn based on previous works for the classification of transcription factor binding.","the manuscript contains lots of low-end issues, such as:--------5.1. inconsistency in the format when referring to equations (eq.",this work proposes an approach for transcription factor binding site prediction using a multi-label classification formulation.,"for instance, what is y_i in their multi-label framework?--------various choices are not well motivated; for instance cosine similarity, the value of hyperparameter epsilon.--------the prototype vectors are not motif-like at all -- can the authors motivate this aspect better?----------------update: i have updated my rating based on the author rebuttal the authors of this manuscript proposed a model called pmn based on previous works for the classification of transcription factor binding.","for instance, what is y_i in their multi-label framework?--------various choices are not well motivated; for instance cosine similarity, the value of hyperparameter epsilon.--------the prototype vectors are not motif-like at all -- can the authors motivate this aspect better?----------------update: i have updated my rating based on the author rebuttal the authors of this manuscript proposed a model called pmn based on previous works for the classification of transcription factor binding.",it is hard to follow for anyone not already familiar with matching networks.,"for instance, what is y_i in their multi-label framework?--------various choices are not well motivated; for instance cosine similarity, the value of hyperparameter epsilon.--------the prototype vectors are not motif-like at all -- can the authors motivate this aspect better?----------------update: i have updated my rating based on the author rebuttal the authors of this manuscript proposed a model called pmn based on previous works for the classification of transcription factor binding.",0.2201834862385321,0.0560747663551401,0.18348623853211,0.18348623853211,0.2394366197183098,0.0285714285714285,0.1126760563380281,0.1126760563380281,0.1123595505617977,0.0,0.0898876404494381,0.0898876404494381,0.2588235294117647,0.1204819277108433,0.2117647058823529,0.2117647058823529,0.2394366197183098,0.0285714285714285,0.1126760563380281,0.1126760563380281,0.2394366197183098,0.0285714285714285,0.1126760563380281,0.1126760563380281,0.1481481481481481,0.0253164556962025,0.0987654320987654,0.0987654320987654,0.2394366197183098,0.0285714285714285,0.1126760563380281,0.1126760563380281,9.920150756835938,9.920150756835938,13.837570190429688,9.920150756835938,7.790183544158935,5.073915004730225,9.920150756835938,9.449912071228027,0.9638027236231461,0.8054078084860611,0.9009950732942459,0.9730077488038182,0.9715578727552412,0.5720023586561025,0.5603880298164985,0.6615227650370697,0.3577329252449043,0.9753645217452477,0.9709628773566018,0.9290786878131329,0.9730077488038182,0.9715578727552412,0.5720024353110083,0.9730077488038182,0.9715578727552412,0.5720024353110083,0.9183191110684338,0.9359478620930295,0.8099992872655545,0.9730077488038182,0.9715578727552412,0.5720020776679191
108,https://openreview.net/forum?id=S1cZsf-RW,"the authors propose a hybrid bayesian inference approach for deep topic models that integrates stochastic gradient mcmc for global parameters and weibull-based multilayer variational autoencoders (vaes) for local parameters. the decoding arm of the vae consists of deep latent dirichlet allocation, and an upward-downward structure for the encoder. gamma distributions are approximated as weibull distributions since the kullback-leibler divergence is known and samples can be efficiently drawn from a transformation of samples from a uniform distribution. ----------------the results in table 1 are concerning for several reasons, i) the proposed approach underperfroms dlda-gibbs and dlda-tlasgr. ii) the authors point to the scalability of the mini-batch-based algorithms, however, although more expensive, dlda-gibbs, is not prohibitive given results for wikipedia are provided. iii) the proposed approach is certainly faster at test time, however, it is not clear to me in which settings such speed (compared to gibbs) would be needed, provided the unsupervised nature of the task at hand. iv) it is not clear to me why there is no test-time difference between wai and whai, considering that in the latter, global parameters are sampled via stochastic-gradient mcmc. one possible explanation being that during test time, the approach does not use samples from w but rather a summary of them, say posterior means, in which case, it defeats the purpose of sampling from global parameters, which may explain why wai and whai perform about the same in the 3 datasets considered.----------------- \phi is in a subset of r_+, in fact, columns of \phi are in the p_0-dimensional simplex.--------- \phi should have k_1 columns not k.--------- the first paragraph in page 5 is very confusing because h is introduced before explicitly connecting it to k and \lambda. also, if k = \lambda, why introduce different notations? the paper presents a deep poisson model where the last layer is the vector of word counts generated by a vector poisson. this is parameterized by a matrix vector product, and the vector in this parameterizeation is itself generated by a vector gamma with a matrix-vector parameterization. from there the vectors are all gammas with matrix-vector parameterizations in a typical deep setup.----------------while the model is reasonable, the purpose was not clear to me. if only the last layer generates a document, then what use is the deep structure? for example, learning hierarchical topics as in figure 4 doesn't seem so useful here since only the last layer matters. also, since no input is being mapped to an output, what does going deeper mean? it doesn't look like any linear mapping is being learned from the input to output spaces, so ultimately the document itself is coming from a simple linear poisson model just like lda and other non-deep methods.----------------the experiments are otherwise thorough and convincing that quantitative performance is improved over previous attempts at the problem. the authors develop a hybrid amortized variational inference mcmc inference --------framework for deep latent dirichlet allocation. their model consists of a stack of-------- gamma factorization layers with a poisson layer at the bottom. they amortize --------inference at the observation level using a weibull approximation. the structure --------of the inference network mimics the mcmc sampler for this model. finally they --------use mcmc to infer the parameters shared across data. a couple of questions:----------------1) how effective are the mcmc steps at mixing? it looks like this approach helps a --------bit with local optima?----------------2) the gamma distribution can be reparameterized via its rejection sampler ----------------@inproceedings{pmlr-v54-naesseth17a,-------- title = {{reparameterization gradients through acceptance-rejection sampling algorithms}},-------- author = {christian naesseth and francisco ruiz and scott linderman and david blei},-------- booktitle = {proceedings of the 20th international conference on artificial intelligence and statistics},-------- pages = {489--498},-------- year = {2017}--------}----------------i think some of the motivation for the weibull is weakened by this work. maybe a --------comparison is in order?----------------3) analytic kl divergence can be good or bad. it depends on the correlation between --------the gradients of the stochastic kl divergence and the stochastic log-likelihood----------------4) one of the original motivations for dlda was that the augmentation scheme --------removed the need for most non-conjugate inference. however, this approach doesn't --------use that directly. thus, it seems more similar to inference procedure in deep exponential --------families. was the structure of the inference network proposed here crucial?----------------5) how much like a weibull do you expect the posterior to be? this seems unclear.","the paper proposes a new approach for scalable training of deep topic models based on amortized inference for the local parameters and stochastic-gradient mcmc for the global ones. the key aspect of the method involves using weibull distributions (instead of gammas) to model the variational posteriors over the local parameters, enabling the use of the reparameterization trick. the resulting methods perform slightly worse that the gibbs-sampling-based approaches but are much faster at test time. amortized inference has already been applied to topic models, but the use of weibull posteriors proposed here appears novel. however, there seems to be no clear advantage to using stochastic-gradient mcmc instead of vanilla sgd to infer the global parameters, so the value of this aspect of whai unclear.","iii) the proposed approach is certainly faster at test time, however, it is not clear to me in which settings such speed (compared to gibbs) would be needed, provided the unsupervised nature of the task at hand.",the authors propose a hybrid bayesian inference approach for deep topic models that integrates stochastic gradient mcmc for global parameters and weibull-based multilayer variational autoencoders (vaes) for local parameters.,the authors develop a hybrid amortized variational inference mcmc inference --------framework for deep latent dirichlet allocation.,the authors propose a hybrid bayesian inference approach for deep topic models that integrates stochastic gradient mcmc for global parameters and weibull-based multilayer variational autoencoders (vaes) for local parameters.,the authors propose a hybrid bayesian inference approach for deep topic models that integrates stochastic gradient mcmc for global parameters and weibull-based multilayer variational autoencoders (vaes) for local parameters.,"it looks like this approach helps a --------bit with local optima?----------------2) the gamma distribution can be reparameterized via its rejection sampler ----------------@inproceedings{pmlr-v54-naesseth17a,-------- title = {{reparameterization gradients through acceptance-rejection sampling algorithms}},-------- author = {christian naesseth and francisco ruiz and scott linderman and david blei},-------- booktitle = {proceedings of the 20th international conference on artificial intelligence and statistics},-------- pages = {489--498},-------- year = {2017}--------}----------------i think some of the motivation for the weibull is weakened by this work.",they amortize --------inference at the observation level using a weibull approximation.,the authors propose a hybrid bayesian inference approach for deep topic models that integrates stochastic gradient mcmc for global parameters and weibull-based multilayer variational autoencoders (vaes) for local parameters.,0.1951219512195122,0.0493827160493827,0.1585365853658537,0.1585365853658537,0.2929936305732484,0.1290322580645161,0.2165605095541401,0.2165605095541401,0.1258741258741259,0.0,0.0979020979020978,0.0979020979020978,0.2929936305732484,0.1290322580645161,0.2165605095541401,0.2165605095541401,0.2929936305732484,0.1290322580645161,0.2165605095541401,0.2165605095541401,0.196078431372549,0.0297029702970297,0.1078431372549019,0.1078431372549019,0.1014492753623188,0.0147058823529411,0.0869565217391304,0.0869565217391304,0.2929936305732484,0.1290322580645161,0.2165605095541401,0.2165605095541401,9.906224250793455,10.924600601196287,10.924599647521973,10.924599647521973,3.2561912536621094,11.887680053710938,10.924599647521973,8.34708309173584,0.9613498284527642,0.9613993473317851,0.9336840472242744,0.9778200488949245,0.9738249542460123,0.8865496356213043,0.9787867320087166,0.9772759276024665,0.04782036107098006,0.9778200488949245,0.9738249542460123,0.8865498210465426,0.9778200488949245,0.9738249542460123,0.8865496025936904,0.3610582016952597,0.542202021663869,0.8146160192769109,0.9640915828143822,0.972806116787569,0.727997842017424,0.9778200488949245,0.9738249542460123,0.8865496356213043
109,https://openreview.net/forum?id=S1eQuCVFvB,"update after author response:-----i would like to thank the authors for the thoughtful response, and for addressing several of the concerns raised by the reviewers. the updated draft look cleaner and conveys the value of the proposal better. i am changing my assessment to ""6: weak accept"" (the smallest jump from 3 is to 6 in the portal. i would choose a 5 if that was possible). my concerns for the significance of results and lack of a baseline that takes worker quality into account still stand, but the results look better and the authors make a more convincing case prompting me to change my rating.------------------------------------------in this paper, the authors propose a way to measure a notion of surprise (disparity between prior and posterior) and using that as a classification rule. so, if the posterior for a class is larger than its prior, the method outputs that class label. in order to estimate the average prior for a pool of models, the method recommends building additional models to predict these peer-priors, whereas the posterior is simply estimated by a maximum likelihood model (p(y=1) = indicator(y=1)/k). the authors also propose a variant that directly tries to predict when the majority answer might be wrong. the authors show that such methods can do better than majority voting and some ensemble methods on a few datasets.----------the paper looks at an interesting problem of when a majority vote response might be wrong by extending the notion of surprise previously defined for human labelers to machine learning models. the strong points of the paper:-----1. simple and interpretable extension of a previously studied method. -----2. the method can be plugged into existing frameworks to replace majority voting.-----3. the results on the datasets considered seem good.----------here are some of my concerns:-----1. the baseline of majority voting is fairly weak, since there are several models that take worker quality into account when aggregating responses. for example, the classic dawid-skene (1979) model. its not very promising to see a model just beat majority voting.-----2. the presentation of results in table 1 and 2 can be improved. even though its good to know how many predictions were corrected, it will also be good to see the overall accuracy numbers (like the ones table 3).-----3. dmts doesnt really have the same underlying machinery as hmts, since it doesnt operate on the surprise measure, and putting them together in the same paper dilutes the focus. -----4. what are the weights in the weighted majority?-----5. the results in table 3 show that hmts is better than other ensemble methods but only marginally. since hmts uses more complex intermediate models (mlps), i am not convinced whether the small improvement is from the proposed method or just more expressive models. for example, what would happen if the base classifiers in adaboost were mlps?-----6. how is minority defined in multi-class scenario?-----7. minor formatting and grammatical issues: likely to the correct, majority is tending to be correct, seemingly irrelevant topics, whether adopting the minority, aim to provide instruction to cases, linear regression (should be logistic regression since its a classifier), and so on.----------in summary, i think the paper has an interesting approach to an important problem, but with results that are only marginally convincing. i would have liked to see a more thorough empirical investigation to clearly establish the value of the proposed method. based on these observations, i think the paper misses the mark and is slightly below the acceptance threshold for me. inspired by work in ensembling human decisions, the authors propose an ensembling technique called ""machine truth serum"" (based off ""bayesian truth serum""). instead of using majority vote to ensemble the decisions of several classifiers, this paper follows the ""surprisingly popular"" algorithm; the ensembled decision is the decision whose posterior probability (based on several classifiers) most exceeds a prior probability (given by classifier(s) trained to predict the posterior predictions). it's quite a nice idea to bring this finding from human decision-making to machine learning. if it worked in machine learning, it would be quite surprising, as the surprisingly popular algorithm risks that the ensemble makes a decision against the majority vote, which is usually consider the safe/default option for ensembling.----------overall, i did not find the experiments (in the current state) to provide compelling enough support for the claim that mts is a useful approach to ensembling in machine learning. -----* unless i am mistaken, the authors use a more powerful model (an mlp) as the regressor compared to some of the models they ensemble over. in practice, people ensemble the most powerful models they have available, so it's unclear if using a regressor with the same capacity as the ensembled classifiers will provide any additional benefit. on a related note, it would be nice to know what is the classification performance of each individual classifier? as well as how often the regressor correctly choose to go with several weaker models rather than the strongest model. in particular, i am concerned that the performance of the ensemble might be less than or equal to the performance of a single mlp classifier (or whatever other model does best).-----* ""in this paper, each of the datasets we used has a small size - we chose to focus on the small data regime where the classifiers are likely to make mistakes."" why not try large data tasks that are challenging for state-of-the-art models? the paper makes a general claim that mts is a good way to aggregate predictions, so only evaluating on small datasets seems to be a limitation-----* as i understand (correct me if i am wrong), the reported results are only on examples with ""high disagreement"" between classifiers. however, for practical use cases, it is useful to know how the overall accuracy compares. one major risk of using the ""surprisingly popular"" algorithm is that the algorithm may cause the ensemble to make many incorrect predictions when the majority is right (but the minority prediction is selected). if you have those numbers, i would be interested to see them added to the paper.----------i am also unsure about if applying the ""surprisingly popular"" algorithm in machine learning makes sense. the algorithm is motivated by the fact that difference agents have different information. however, in the ml setting, various classifiers usually have the same information. it's possible to restrict the information given to each classifier, but that would limit the performance of each individual classifier (and hurt the ensemble). i would be curious if the authors have any thoughts on this point.----------i also have a few questions/concerns about how the approach is implemented:-----* why not train a single model to predict the average prediction of all models and use that model's prediction as the prior? this approach seems simpler but equivalent to the approach currently taken.-----* why not use model distillation (predicting all output logits/probabilities, or an average thereof) rather than just predicting an average of 0/1 predictions?-----* for hmts, why do the regressors for each of l labels need to be separate? it seems more efficient to use a multi-class model (as many model distillation approaches do)-----* if dmts can learn to predict when most classifiers are wrong, why wouldn't the original classifiers themselves learn to predict the answer correctly? it seems to me that the reason the experiments show that dmts/hmts work is that some/many of the underlying classifiers are weaker than the model that is used to ensemble the predictions (an assumption that doesn't hold in practice).---------------overall, i really like the high-level idea, and a better ensembling approach promises to bring empirical gains across many ml tasks. however, i have several concerns about the experiments, motivation, and algorithmic decisions which make me hesitant to recommend the paper for acceptance. summary: this paper proposes two machine learning adaptations of the bayesian truth serum approach to aggregating predictions from human experts. the first method proposed involves training two regression models for each classifier in the ensemble that predicts the proportion of other classifiers that assign the same label to a novel instance. the second approach is to train a binary classifier that, based on the features associated with an instance, determines whether the most common or second most common prediction made by individual ensemble members should be the prediction made by the ensemble.----------pros:-----+ the core idea of adapting bayesian truth serum to ensemble prediction in machine learning seems sensible-----+ there is some evidence that the methods have an advantage over other common ensemble approaches in practice-----+ although there are quite a few small english mistakes, the paper is well structured and generally quite easy to follow----------cons & questions: -----1. the theorem statements and proofs are underwhelming. first they seem quite vague. second, its not clearly spelled out how the theorems relate to the presented method, and whether they really says anything useful about its correctness or efficacy. at face value they do not obviously analyse the correctness of the algorithm as claimed on pg6. -----2. the paper contains some simple experiments, but i do not believe they are an adequate enough evaluation of the proposed approaches. the most useful comparison are the results given in table 3, but only three datasets are used, the margins are small, error bars are not provided, and no significance testing is performed. -----2.1 standard practice when comparing multiple classifiers on multiple datasets would be to employ friedman/nemenyi post-hoc tests to determine relative performance of methods---see ""statistical comparisons of classifiers over multiple data sets"" by janez demar (jmlr, 2006).-----2.2 at minimum we expect tab 3 to report results for all datasets used in the earlier experiments.-----2.3 the experiments compare with adaboost, random forest, and weighted majority. i feel that stacking is probably the most interesting baseline to compare with, as this is a method for learning how to aggregate predictions from ensemble members.----------other: -----a. this paper does not directly deal with representation learning, so is only loosely relevant to iclr.-----b. not clearly unpacked why regression models need to be trained to predict \hat{y}_i^j (equation 4) when this quantity can be computed at test time without knowing the ground truth label?-----c. i do not understand the ""x out of y"" description given in the caption to table 1, which makes the results in this table hard to interpret. what is meant by ""classifiers' disagreement is high enough""?","this paper proposes a family of new methods, based on bayesian truth serum, that are meant to build better ensembles -----from a fixed set of constituent models.----------reviewers found the problem and the general research direction interesting, but none of the three of them were convinced that the proposed methods are effective in the ways that the paper claims, even after some discussion. it seems as though this paper is dealing with a problem that doesn't generally lend itself to large improvements in results, but reviewers weren't satisfied that the small observed improvements were real, and urged the authors to explore additional settings and baselines, and to offer a full significance test.","the paper makes a general claim that mts is a good way to aggregate predictions, so only evaluating on small datasets seems to be a limitation-----* as i understand (correct me if i am wrong), the reported results are only on examples with ""high disagreement"" between classifiers.","the second approach is to train a binary classifier that, based on the features associated with an instance, determines whether the most common or second most common prediction made by individual ensemble members should be the prediction made by the ensemble.----------pros:-----+ the core idea of adapting bayesian truth serum to ensemble prediction in machine learning seems sensible-----+ there is some evidence that the methods have an advantage over other common ensemble approaches in practice-----+ although there are quite a few small english mistakes, the paper is well structured and generally quite easy to follow----------cons & questions: -----1. the theorem statements and proofs are underwhelming.",the authors show that such methods can do better than majority voting and some ensemble methods on a few datasets.----------the paper looks at an interesting problem of when a majority vote response might be wrong by extending the notion of surprise previously defined for human labelers to machine learning models.,"update after author response:-----i would like to thank the authors for the thoughtful response, and for addressing several of the concerns raised by the reviewers.","instead of using majority vote to ensemble the decisions of several classifiers, this paper follows the ""surprisingly popular"" algorithm; the ensembled decision is the decision whose posterior probability (based on several classifiers) most exceeds a prior probability (given by classifier(s) trained to predict the posterior predictions).","instead of using majority vote to ensemble the decisions of several classifiers, this paper follows the ""surprisingly popular"" algorithm; the ensembled decision is the decision whose posterior probability (based on several classifiers) most exceeds a prior probability (given by classifier(s) trained to predict the posterior predictions).",summary: this paper proposes two machine learning adaptations of the bayesian truth serum approach to aggregating predictions from human experts.,"instead of using majority vote to ensemble the decisions of several classifiers, this paper follows the ""surprisingly popular"" algorithm; the ensembled decision is the decision whose posterior probability (based on several classifiers) most exceeds a prior probability (given by classifier(s) trained to predict the posterior predictions).",0.248447204968944,0.0125786163522012,0.124223602484472,0.124223602484472,0.3287671232876712,0.0552995391705069,0.1735159817351598,0.1735159817351598,0.2545454545454545,0.0245398773006134,0.1212121212121212,0.1212121212121212,0.1428571428571428,0.0289855072463768,0.0999999999999999,0.0999999999999999,0.1987577639751552,0.0251572327044025,0.1366459627329192,0.1366459627329192,0.1987577639751552,0.0251572327044025,0.1366459627329192,0.1366459627329192,0.1492537313432835,0.0757575757575757,0.1343283582089552,0.1343283582089552,0.1987577639751552,0.0251572327044025,0.1366459627329192,0.1366459627329192,5.440751075744629,5.440751075744629,12.071669578552246,6.610869407653809,5.5491156578063965,7.70455265045166,5.440751075744629,4.63819694519043,0.6545176305508177,0.721234129918398,0.9218062556765738,0.31611749686196344,0.46605727690721055,0.935567390426096,0.971845048682071,0.9705549645305063,0.6386424966910847,0.9705750250399056,0.9666324716169378,0.9353277077306663,0.9587803589320347,0.9606198771993566,0.9389233006039523,0.9587803589320347,0.9606198771993566,0.9389233110972973,0.6787464348134176,0.7285096701406663,0.9426023530220995,0.9587803589320347,0.9606198771993566,0.9389233918316598
110,https://openreview.net/forum?id=S1enmaVFvS,"the authors describe a method to encode and decode the position of atoms in 3-d molecules. an encoder-decoder architecture is used to create a representation of a molecule and to reconstruct the molecule from its representation. then a second neural network segments the output and assigns an atomic number. prior work on this task has used 1d (smiles) and 2d (graph) representations. the authors argues that exploiting 3d structure can create better representations.----------as the paper's related work section shows, this is not the first attempt to use 3d structure to create molecular representations. unfortunately, the paper does not compare their work to prior work on 3d structure representations (e.g gebaur et al 2019). also, it is not clear whether the 3d representation is better than 1d or 2d representations especially since there have been many new 1d models that perform very well for tasks like molecular property prediction (for example all smiles vae https://arxiv.org/abs/1905.13343 ). i think the community will benefit if the authors perform a comparison with state of the art 1d and 2d models. i think this is a main drawback of this work.----------another concern is that the authors claim that the errors in atomic numbers differ only by 1 or 2. but doesn't this show that the network has not learnt a good representation? because atoms that differ in atomic number by 1 or 2 will have different valencies and hence exhibit different properties? on the other hand, if the authors can show that the errors in atomic numbers suggest they correspond to similar atoms (may be along the same column in the periodic table), then one can have better confidence that the network has learned a meaningful representation. the paper deals with accurately encoding and decoding 3d atomic positions and the crystals species using 2 sets of neural networks a) a vae that builds a compressed latent space representation of a crystal and b) a unet for segmenting the latent space into atoms and assigns each atom to its atomic number. experiments were conducted on over 120k 3d samples of crystals and the results seem to be promising.----------the paper is neatly written and well organized.----------comments:----------a) figure 2: for completion, consider marking m and s as outputs of the vae and u-net respectively. ----------b) in section 3.1, why do the authors use a cube with side of 10 angstrom? and why divide the cube into 30 bins?----------c) the paper revolves on vanilla 3d convolutions of the crystal structures. have the authors considered how the results would change if so(3) rotation invariant convolutions were used instead. the so(3) convolutions would empower capturing all possible rotations of the crystal other than only its canonical form.","this paper presents an encoder-decoder based approach to construct a compressed latent space representation of each molecule. then a second neural network segments the output and assigns an atomic number. unlike previous works using 1d or 2d representations, the proposed method focuses on the 3d representations.----------the reviewers have several major concerns. firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques. secondly, there is no clear baseline to compare with. finally, there is no clear quantitative results to measure the proposed method. the rebuttal did not well address these problems.----------overall, this paper did not meet the standard of iclr and i choose to reject the paper.","experiments were conducted on over 120k 3d samples of crystals and the results seem to be promising.----------the paper is neatly written and well organized.----------comments:----------a) figure 2: for completion, consider marking m and s as outputs of the vae and u-net respectively.",the paper deals with accurately encoding and decoding 3d atomic positions and the crystals species using 2 sets of neural networks a) a vae that builds a compressed latent space representation of a crystal and b) a unet for segmenting the latent space into atoms and assigns each atom to its atomic number.,then a second neural network segments the output and assigns an atomic number.,the authors describe a method to encode and decode the position of atoms in 3-d molecules.,then a second neural network segments the output and assigns an atomic number.,the paper deals with accurately encoding and decoding 3d atomic positions and the crystals species using 2 sets of neural networks a) a vae that builds a compressed latent space representation of a crystal and b) a unet for segmenting the latent space into atoms and assigns each atom to its atomic number.,"----------b) in section 3.1, why do the authors use a cube with side of 10 angstrom?",the paper deals with accurately encoding and decoding 3d atomic positions and the crystals species using 2 sets of neural networks a) a vae that builds a compressed latent space representation of a crystal and b) a unet for segmenting the latent space into atoms and assigns each atom to its atomic number.,0.2331288343558282,0.0496894409937888,0.147239263803681,0.147239263803681,0.3157894736842105,0.1183431952662721,0.1871345029239765,0.1871345029239765,0.1984732824427481,0.1860465116279069,0.1984732824427481,0.1984732824427481,0.1629629629629629,0.0,0.074074074074074,0.074074074074074,0.1984732824427481,0.1860465116279069,0.1984732824427481,0.1984732824427481,0.3157894736842105,0.1183431952662721,0.1871345029239765,0.1871345029239765,0.074074074074074,0.0,0.0592592592592592,0.0592592592592592,0.3157894736842105,0.1183431952662721,0.1871345029239765,0.1871345029239765,9.216968536376951,8.961782455444336,13.687762260437012,9.216968536376951,6.62703275680542,8.961782455444336,9.216968536376951,8.176568984985352,0.98299779607959,0.9846375860325153,0.9261371385056134,0.9818457675010411,0.979962838373935,0.8861572908314218,0.9508899042840231,0.9573957656142263,0.6959792170919731,0.9738326208242108,0.9753187182561363,0.8990091143378561,0.9508899042840231,0.9573957656142263,0.6959790590146913,0.9818457675010411,0.979962838373935,0.8861574034409168,0.9719550768120858,0.9697866893893126,0.8466638268711395,0.9818457675010411,0.979962838373935,0.8861573770307611
111,https://openreview.net/forum?id=S1xtAjR5tX,"this submission deals with text generation. it proposes a sequence level objective, motivated by optimal transport, which is used as a regularizer to the (more standard) mle. the goal is to complement mle by allowing `soft matches` between different tokens with similar meanings. empirical evaluation shows that the proposed technique improves over baselines on many sequence prediction tasks. i found this paper well motivated and a nice read. i would vote for acceptance.----------------pros:--------- a well-motivated and interesting method.--------- solid empirical results.--------- writing is clear.----------------cons:--------- the split of `syntax--mle` and `semantics--ot` seems a bit awkward to me. matching the exact tokens does not appear syntax to me. --------- some technical details need to be clarified.----------------details:--------- could the authors comment on the efficiency by using the ipot approximate algorithm, e.g., how much speed-up can one get? i'm not familiar with this algorithm, but is there convergence guarantee or one has to set some kind of maximum iterations when applying it in this model?----------------- bottom of page 4, the `complementary mle loss paragraph`. i thought the ot loss is used as a regularizer, from the introduction. if the paper claims mle is actually used as the complements, evidence showing that the ot loss works reasonably well on its own without including log loss, which i think is not included in the experiments.----------------- i really like the analysis presented in section 3. but it's a bit hard for me to follow, and additional clarification might be needed.----------------- it would be interesting to see whether the `soft-copying` version of ot-loss can be combined with the copy mechanisms based on attention weights.----------------================================----------------thanks for the clarification and revision! it addressed some of my concerns. i would stick to the current rating, and vote for an acceptance. this paper propose to add an ot-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences. indeed, it allows the computation of a distance between embeddings of a set of words, and this distance is then used to define a penalized objective function.--------the main issue with this computation is that it provides a distance between a set of words but not a sequence of words. the ordering is then not taken into account. authors should discuss this point in the paper.--------experiments show an improvement of the method w.r.t. not penalized loss.----------------minor comments:--------- in figure 1, the ot matching as described in the text is not the solution of eq (2) but rather the solution of eq. (3) or the entropic regularization (the set of ""edges"" is higher than the admissible highest number of edges).--------- introduction ""ot [...] providing a natural measure of distance for sequences comparisons"": it is not clear why this statement is true. ot allows comparing distributions, with no notion of ordering (see above). --------- table 1: what is nmt?--------- first paragraph, p7: how do you define a ""substantial"" improvement of the scores?--------- how do you set parameter in the experiments? why did you choose \beta=0.5 for the ipot algorithm?","the paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. strong empirical results are presented which support the use of optimal transport in conjunction with log-likelihood for training sequence models. i appreciate the improvements to the manuscript during the review process, and i encourage the authors to address the rest of the comments in the final version.","this paper propose to add an ot-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences.","if the paper claims mle is actually used as the complements, evidence showing that the ot loss works reasonably well on its own without including log loss, which i think is not included in the experiments.----------------- i really like the analysis presented in section 3. but it's a bit hard for me to follow, and additional clarification might be needed.----------------- it would be interesting to see whether the `soft-copying` version of ot-loss can be combined with the copy mechanisms based on attention weights.----------------================================----------------thanks for the clarification and revision!","this paper propose to add an ot-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences.",this submission deals with text generation.,i would vote for acceptance.----------------pros:--------- a well-motivated and interesting method.--------- solid empirical results.--------- writing is clear.----------------cons:--------- the split of `syntax--mle` and `semantics--ot` seems a bit awkward to me.,"if the paper claims mle is actually used as the complements, evidence showing that the ot loss works reasonably well on its own without including log loss, which i think is not included in the experiments.----------------- i really like the analysis presented in section 3. but it's a bit hard for me to follow, and additional clarification might be needed.----------------- it would be interesting to see whether the `soft-copying` version of ot-loss can be combined with the copy mechanisms based on attention weights.----------------================================----------------thanks for the clarification and revision!","indeed, it allows the computation of a distance between embeddings of a set of words, and this distance is then used to define a penalized objective function.--------the main issue with this computation is that it provides a distance between a set of words but not a sequence of words.","this paper propose to add an ot-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences.",0.3025210084033613,0.0512820512820512,0.2352941176470588,0.2352941176470588,0.2994011976047903,0.0242424242424242,0.1916167664670658,0.1916167664670658,0.3025210084033613,0.0512820512820512,0.2352941176470588,0.2352941176470588,0.0246913580246913,0.0,0.0246913580246913,0.0246913580246913,0.1851851851851851,0.0188679245283018,0.1296296296296296,0.1296296296296296,0.2994011976047903,0.0242424242424242,0.1916167664670658,0.1916167664670658,0.224,0.032520325203252,0.176,0.176,0.3025210084033613,0.0512820512820512,0.2352941176470588,0.2352941176470588,10.312822341918944,13.40873908996582,13.009748458862305,10.312822341918944,8.06970500946045,8.06970500946045,8.069705963134766,4.828449249267578,0.97926468749688,0.9763453144255033,0.9280146328691157,0.9782086970885193,0.980516697565244,0.38107269357832824,0.97926468749688,0.9763453144255033,0.9280145982974827,0.9608819391790601,0.966600408105885,0.9177762457398333,0.98576637195331,0.9791905938305168,0.9358092644506169,0.9782086970885193,0.980516697565244,0.3810732300356872,0.9666096826648966,0.9690281350819734,0.9068427064030735,0.97926468749688,0.9763453144255033,0.9280146121261356
112,https://openreview.net/forum?id=SJOl4DlCZ,"the paper proposes the use of a gan to learn the distribution of image classes from an existing classifier, that is a nice and straightforward idea. from the point of view of forensic analysis of a classifier, it supposes a more principled strategy than a brute force attack based on the classification of a database and some conditional density estimation of some intermediate image features. unfortunately, the experiments are inconclusive. ----------------quality: the key question of the proposed scheme is the role of the auxiliary dataset. in the emnist experiment, the results for the exact same and partly same situations are good, but it seems that for the mutually exclusive situation the generated samples look like letters, not numbers, and raises the question on the interpolation ability of the generator. in the facescrub experiment is even more difficult to interpret the results, basically because we do not even know the full list of person identities. it seems that generated images contain only parts of the auxiliary images related to the most discriminative features of the given classifier. does this imply that the gan models a biased probability distribution of the image class? what is the result when the auxiliary dataset comes from a different kind of images? due to the difficulty of evaluating gan results, more experiments are needed to determine the quality and significance of this work.----------------clarity: the paper is well structured and written, but sections 1-4 could be significantly shorter to leave more space to additional and more conclusive experiments. some typos on appendix a should be corrected.----------------originality: the paper is based on a very smart and interesting idea and a straightforward use of gans. ----------------significance: if additional simulations confirm the authors claims, this work can represent a significant contribution to the forensic analysis of discriminative classifiers. this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the black-box mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set. this approach can be regarded as a transfer learning version of acgan that generates data conditioned on its label.----------------overall i feel it unclear to judge whether this paper has made substantial contributions. the performance critically relies on the structure of aux dataset and how the supervised model interacts with it. it would be great if the author could show how the aux dataset is partitioned according to the function $f, and what is the representative sample from aux dataset that maximizes a given class label. in fig. 4, the face of leonardo dicaprio was reconstructed successfully, but is that because in the aux dataset there are other identities who look very similar to him and is classified as leonardo, or it is because gan has the magic to stitch characteristics of different face identities together? given the current version of the paper, it is not clear at all. from the results on emnist when the aux set and the training set are disjoint, the proposed model simply picks the most similar shapes as gan generation, and is not that interesting. in summary, a lot of ablation experiments are needed for readers to understand the proposed method better.----------------the writing is ok but a bit redundant. for example, eqn. 1 (and eqn. 2) which shows the overall distribution of the training samples (and aux samples) as a linear combinations of the samples at each class, are not involved in the method. do we really need eqn. 1 and 2? this paper considers a new problem : given a classifier f trained from d_tr and a set of auxillary samples from d_aux, find d_tr conditioned on label t*. its solution is based on a new gan: preimagegan. three settings of the similarity between auxillary distribution and training distribution is considered: exact same, partly same, mutually exclusive. experiments show promising results in generating examples from the original training distribution, even in the ""mutually exclusive"" setting.----------------quality: --------1. it is unclear to me if the generated distribution in the experiments is similar to the original distribution d_tr given y = t^*, either from inception accuracy or from pictorial illustration. since we have hold out the training data, perhaps we can measure the distance between the generated distribution and d_tr given y = t^* directly.----------------2. it would be great if we can provide experiments quantifying the utility of the auxillary examples. for example, when they are completely noise, can we still get sensible generation of images? ----------------3. how does the experimental result of this approach compare with model attack? for example, we can imagine generating labels by e_t^* + epsilon, where epsilon is random noise. if we invert these random labels, do we get a distribution of examples from class t^*?----------------clarity:--------1. i think the key here is to first generate auxillary labels (as in figure 2), then solve optimization problem (3) - this causes my confusion at first sight. (my first impression is that all labels, training or auxillary, are one-hot encoding - but this makes no sense since the dimension of f and y_aux does not match.)----------------originality: i am not familiar with relevant literature - and i think the gan formulation here is original.----------------significance: i see this as a nice step towards inferring training data from trained classifiers.","this paper addresses the very important problem of ensuring that sensitive training data remain private. it proposes an attack whereby the attacker can reconstruct information about the training data given only the trained classifier and an auxiliary dataset. if done well, such an attack would be a useful contribution that helps make discussion of differential privacy more complete. but as the reviewers pointed out, it's not clear from the paper whether the attack has succeeded. it works only when the auxiliary data is very similar to the training data, and it's not clear if it leaks information about the training set itself, or is just summarizing the auxiliary data. this work doesn't seem quite ready for publication, but could be a strong paper if it's convincingly demonstrated that information about the training set has been leaked.","from the point of view of forensic analysis of a classifier, it supposes a more principled strategy than a brute force attack based on the classification of a database and some conditional density estimation of some intermediate image features.","this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the black-box mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set.","this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the black-box mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set.","the paper proposes the use of a gan to learn the distribution of image classes from an existing classifier, that is a nice and straightforward idea.","experiments show promising results in generating examples from the original training distribution, even in the ""mutually exclusive"" setting.----------------quality: --------1. it is unclear to me if the generated distribution in the experiments is similar to the original distribution d_tr given y = t^*, either from inception accuracy or from pictorial illustration.","this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the black-box mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set.","it would be great if the author could show how the aux dataset is partitioned according to the function $f, and what is the representative sample from aux dataset that maximizes a given class label.","this paper proposed to learn a generative gan model that generates the training data from the labels, given that only the black-box mapping from data to label is available, as well as an aux dataset that might and might not overlap with the training set.",0.1452513966480447,0.0112994350282485,0.0893854748603351,0.0893854748603351,0.2903225806451613,0.0760869565217391,0.1827956989247312,0.1827956989247312,0.2903225806451613,0.0760869565217391,0.1827956989247312,0.1827956989247312,0.2048192771084337,0.0121951219512195,0.108433734939759,0.108433734939759,0.1780104712041884,0.0317460317460317,0.1151832460732984,0.1151832460732984,0.2903225806451613,0.0760869565217391,0.1827956989247312,0.1827956989247312,0.2057142857142857,0.0231213872832369,0.1371428571428571,0.1371428571428571,0.2903225806451613,0.0760869565217391,0.1827956989247312,0.1827956989247312,9.66646957397461,7.134453296661377,12.37473487854004,9.666467666625977,8.571239471435547,9.66646957397461,9.666468620300291,5.0722832679748535,0.9224162014711216,0.9352729144328709,0.8735939303793055,0.9725482372642176,0.9725760934747697,0.9154032433504042,0.9725482372642176,0.9725760934747697,0.9154031787025164,0.9707636076926152,0.9681525002608083,0.817596666772616,0.782752679253007,0.7937855466722894,0.8377684145065941,0.9725482372642176,0.9725760934747697,0.9154032128044095,0.9473151160818792,0.9523261243204176,0.9285038865078754,0.9725482372642176,0.9725760934747697,0.9154032638115399
113,https://openreview.net/forum?id=SJeS16EKPr,"the paper proposes an approach to find a map between two feature spaces to maximize correlation between them and to use the resulting map for inference. a theoretical exposition is given and some empirical results are provided showing that the approach speeds up convergence on supervised mnist and can be used for image completion (again on mnist).----------the paper should be rejected for the following reasons. first, the approach looks very similar to deep cca, but the connection is never mentioned. this connection needs to be clarified. the objective function needs to be clearly stated and related to the loss function in eq. (7). in particular, i would suggest to give a clear definition of the problem before delving into the theory in section 2. in its current version, it is difficult to assess how the parts of section 2 relate to the overall objective. the paper severely lacks in relation to relevant related work. half(!) of the 14 referenced papers are by the author himself. this can be verified since the double blind review process is compromised as the paper links to code in the authors public github account. finally, the empirical results are quite incomplete. it is not clear how the results compare to generative methods like vaes, which are referenced as a motivation for this work in this work.----------the improvement in convergence from rfa for supervised learning is interesting and this aspect deserves more analysis. it would be useful to look at the total amount of computation required to reach a given loss. i also wonder how this differs from simply mapping the output of the first network to a low-rank space via pca. is the dual-view really necessary in this case since the information content in the label space must be very limited, beyond simple class balance statistics? the paper considers how to learn correlations between two spaces, e.g., input/output, in order to generate data in one space conditioned on values from the other. this is performed by modeling features with neural networks and optimizing an objective function that maximizes a measure of correlation between the features versus learning a generative model such as a cvae. some illustrative examples using mnist are provided.----------my decision is to reject. i think there is value in the approach, but it is hard to see clearly at the moment given that the exposition is difficult to follow and the experiments aren't very compelling. if these issues could be addressed (concrete suggestions below), and some of the follow-on work in the last section could be performed, i think there could be a pretty interesting contribution here.----------***----------decision-related suggestions/questions:----------* include more datasets in the experimental section. the second sentence of the introduction lists possibilities such as time series and multi-modalities that i would have been very interesting.----------* the first claim that there is no tunable variable in the objective function is a little hard to parse. clearly, the rank of the low-rank approximation must be set, and the features of the two spaces need to be learned. some clarification here would be helpful. ----------* there are a number of unfortunate typos/grammar issues/presentation choices that really impact clarity. some examples:----- * in the third paragraph under theory, ""...linear spaces spanned by the probability distributions..."" should probably be ""...linear spaces spanned by all probability distributions..."" (?)----- * the following sentence is a run-on. ----- * in (8), occurrences of g_*(x_n) should be replaced with g_*(y_n).----- * the replacement of the (low) rank symbol, k_0, with the sample size symbol, n, in the second paragraph of section 4.1.----- * introducing a ""bayesian estimator for an l^2 distance"" w/o explanation. what does this mean?----------* how should the low-rank parameter k_0 be selected generally given that the singular value distribution may not always be useful in selecting it?----------* can anything be said quantitatively or qualitatively about the sample complexity required to estimate the matrices of (8) well enough to estimate the features?----------* is there an interpretation for why both spaces require the same feature dimension, k_0?----------***----------comments not related to decision:----------* it is generally good to avoid sweeping statements such as the first sentence of the introduction. perhaps consider replacing with a simple statement on the intended goal of the paper: ""...produce a useful model of correlations... for the task of data generation...""----------* consider placing a concrete, motivating example prior to the theory section as it is hard to digest (from an ml perspective) without a clear context. the analytical example with the gaussian from the supplementary material is one option.----------* the last statement of the paragraph under (3) needs a reference.----------* it seems strange to have the supervised learning experiment of 4.1 as the first experimental result of the paper since it is an unintended and unexplained side-effect of the approach. also, the claim of ""faster convergence"" should be demonstrated in wall-clock time. the paper proposes how the correlation between two different types of data can be extracted from learned representations. the proposed metric can also be used as an alternative to cross entropy loss. the paper provides analytical calculations as well as real data sets simulations/experiments. however there are significant draw-becks:----------1) similarity and metric learning is a booming area in machine learning with several different directions focusing on different problems. the paper fails to locates itself in the literature, how it compares itself into other techniques (both analytically and experimentally). ----------2) the proposed technique seems to be very similar to svd of learned representations. connection to quantum field theory is well established but more simpler comparisons to svd and other spectral techniques are not provided in metric learning.----------3) novelty is not clear. there are interesting experiments in disentangled feature feature extraction and data generation. however, they are mostly proof of concept and lack of baselines. it is not clear what problem this technique solves better compared to other existing solutions. ----------paper is mostly written clearly. i do suggest putting appendix a1 to the main paper though.","this manuscript proposes an approach for estimating cross-correlations between model outputs, related to deep cca. authors note that the procedure improves results when applied to supervised learning problems.----------the reviewers have pointed out the close connection to previous work on deep cca, and the author(s) have agreed. the reviewers agree that the paper has promise if properly expanded both theoretically and empirically.",the analytical example with the gaussian from the supplementary material is one option.----------* the last statement of the paragraph under (3) needs a reference.----------* it seems strange to have the supervised learning experiment of 4.1 as the first experimental result of the paper since it is an unintended and unexplained side-effect of the approach.,"what does this mean?----------* how should the low-rank parameter k_0 be selected generally given that the singular value distribution may not always be useful in selecting it?----------* can anything be said quantitatively or qualitatively about the sample complexity required to estimate the matrices of (8) well enough to estimate the features?----------* is there an interpretation for why both spaces require the same feature dimension, k_0?----------***----------comments not related to decision:----------* it is generally good to avoid sweeping statements such as the first sentence of the introduction.",----------2) the proposed technique seems to be very similar to svd of learned representations.,the paper proposes an approach to find a map between two feature spaces to maximize correlation between them and to use the resulting map for inference.,"some examples:----- * in the third paragraph under theory, ""...linear spaces spanned by the probability distributions..."" should probably be ""...linear spaces spanned by all probability distributions..."" (?)","what does this mean?----------* how should the low-rank parameter k_0 be selected generally given that the singular value distribution may not always be useful in selecting it?----------* can anything be said quantitatively or qualitatively about the sample complexity required to estimate the matrices of (8) well enough to estimate the features?----------* is there an interpretation for why both spaces require the same feature dimension, k_0?----------***----------comments not related to decision:----------* it is generally good to avoid sweeping statements such as the first sentence of the introduction.","----- * in (8), occurrences of g_*(x_n) should be replaced with g_*(y_n).----- * the replacement of the (low) rank symbol, k_0, with the sample size symbol, n, in the second paragraph of section 4.1.----- * introducing a ""bayesian estimator for an l^2 distance"" w/o explanation.","the paper considers how to learn correlations between two spaces, e.g., input/output, in order to generate data in one space conditioned on values from the other.",0.25,0.0338983050847457,0.15,0.15,0.2091503267973856,0.0264900662251655,0.1176470588235294,0.1176470588235294,0.1282051282051282,0.0,0.1025641025641025,0.1025641025641025,0.3111111111111111,0.0909090909090909,0.1777777777777777,0.1777777777777777,0.0449438202247191,0.0,0.0449438202247191,0.0449438202247191,0.2091503267973856,0.0264900662251655,0.1176470588235294,0.1176470588235294,0.1238938053097345,0.0,0.0707964601769911,0.0707964601769911,0.217391304347826,0.0444444444444444,0.1304347826086956,0.1304347826086956,7.220298767089844,6.322207927703857,10.505655288696287,7.220298767089844,7.098501682281494,8.087970733642578,7.419784545898437,5.622416496276856,0.07058448767344458,0.15647063265931924,0.9324798259866762,0.21120993531498528,0.5809798062015007,0.842757249086968,0.6005948902525119,0.6635097895833845,0.8244949818566243,0.9713894565573669,0.9711890592115946,0.9377989458111174,0.9704911569069612,0.96719930657468,0.9136688168396194,0.21120993531498528,0.5809798062015007,0.8427572271562258,0.9816604847109599,0.981151078432904,0.938946059072079,0.9686582558714827,0.9685612173560306,0.9463549663810188
114,https://openreview.net/forum?id=SJeW-A4tDS,"this paper presents a model that detects malicious pdf files. the model applies a convolutional neural network (cnn) to analyze the bytes of the input files. the generated features of cnn achieves good clustering results that are consistent with the ground-truth labels.----------this paper applies an existing cnn architecture. i do not observe any novelty in terms of modeling. i suspect this paper is not interesting to most members of the iclr community. therefore, i do not suggest the acceptance of this paper. the paper proposes to use cnn to detect malicious pdfs. the paper uses two simple cnn models and train them on a collected dataset, compare with other antivirus software, and conclude that cnn performs better. but i have serious concerns about the experiments.----------1. the major concern is with the dataset. the dataset used by this work are collected by authors themselves, and i see a serious problem in this process. the malicious and benign ones are not collected from the same distribution: ""the malicious samples are taken from virustotal (3). they were uploaded on the website between the 5/5/2018 and the 11/14/2018""; ""the benign files were obtained using collaboration with a private company"". thus, the model may not be actually learning what's malicious and what's benign, but only learning whether the pdf comes from that private company or virustotal. it's enough since the test set is also collected this way. it can be fairly easy to distinguish between two different datasets, and the same reasoning applies here. also, the authors are not making the dataset available due to privacy reasons, which further make the dataset's validity a question. ----------2. also, antivirus softwares are applicable on any pdf files, but the model trained with the dataset collected may not be useful in other circumstances. the comparison between them are not fair. it should compare the same model/software but on multiple different datasets to demonstrate the model's general applicability.----------3. the experiments done in this work is also not of a satisfactory level. for example, there are some missing blanks in table 1. why is that? the figures (e.g., fig 1/2) can be improved a lot. it did not include baseline and provide little information. the fonts are too small.----------4. no major novelty is introduced. the work is an application paper using very simple cnns on the malicious pdf detection problem. this itself does not make the paper bad but combined with the unconvincing experiments it's a serious weakness.----------in summary, the paper lacks solid experimental results to make its conclusion convincing and its model generalizable. i vote for rejection of the paper.","this submission addresses the problem of detecting malicious pdf files. the proposed solution trains existing cnn architectures on a collected dataset and verifies improved performance over available antivirus software. ----------there were a number of concerns raised about this work. the main concern the reviewers had with this submission is lack of novelty. the issue is that the paper tackles a standard supervised classification problem which has been extensively explored in the literature and applies an off-the-shelf classification model. though the particular application has seen less attention in the iclr community, the problem setting and solution are well known. thus, the contribution of the work is not sufficient for acceptance.",the model applies a convolutional neural network (cnn) to analyze the bytes of the input files.,"the paper uses two simple cnn models and train them on a collected dataset, compare with other antivirus software, and conclude that cnn performs better.","the paper uses two simple cnn models and train them on a collected dataset, compare with other antivirus software, and conclude that cnn performs better.",this paper presents a model that detects malicious pdf files.,"the paper uses two simple cnn models and train them on a collected dataset, compare with other antivirus software, and conclude that cnn performs better.",this paper presents a model that detects malicious pdf files.,this paper presents a model that detects malicious pdf files.,"the paper uses two simple cnn models and train them on a collected dataset, compare with other antivirus software, and conclude that cnn performs better.",0.1417322834645669,0.016,0.0787401574803149,0.0787401574803149,0.2352941176470588,0.0746268656716418,0.1323529411764706,0.1323529411764706,0.2352941176470588,0.0746268656716418,0.1323529411764706,0.1323529411764706,0.1487603305785124,0.0504201680672268,0.0826446280991735,0.0826446280991735,0.2352941176470588,0.0746268656716418,0.1323529411764706,0.1323529411764706,0.1487603305785124,0.0504201680672268,0.0826446280991735,0.0826446280991735,0.1487603305785124,0.0504201680672268,0.0826446280991735,0.0826446280991735,0.2352941176470588,0.0746268656716418,0.1323529411764706,0.1323529411764706,21.27231216430664,11.542802810668944,21.27231216430664,11.542803764343262,11.423177719116213,11.542803764343262,11.542802810668944,21.27231216430664,0.9632795549549568,0.9761056213976272,0.9137132574418539,0.9774048492950735,0.9711443550257606,0.910406175624408,0.9774048492950735,0.9711443550257606,0.9104061363680983,0.9811350655539393,0.9792518108680395,0.943308134846078,0.9774048492950735,0.9711443550257606,0.9104062095403973,0.9811350655539393,0.9792518108680395,0.9433081102471328,0.9811350655539393,0.9792518108680395,0.9433081102471328,0.9774048492950735,0.9711443550257606,0.910406175624408
115,https://openreview.net/forum?id=SJeoE0VKDS,"the paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space. the abstract space is learned utilizing both model-free and model-based losses, and the behaviour policy is based on planning combining the model-free and model-based components with an epsilon-greedy exploration strategy. learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space.----------as it stands, i am leaning towards rejecting the paper, for the following reasons.-----(1) while the idea proposed is interesting, the current work rather explores it in a limited manner which is unsatisfactory.-----(2) i think the presentation of the bonus itself -- novelty search (section 4), which is the core of the paper, is rather unclear. (3) the assumption of deterministic transition dynamics may be ignored in favour of games which seem to be our benchmarks, but the results presented for the control tasks, table 1, are not statistically significant, and the paper is missing details about the architecture/sweep for the baselines experimented with. -----(4) parts of the paper is rather unclear/feels disconnected -- for instance, the interpretable abstract representation bit; this was a loss in the original work, and seems to be just mentioned arbitrarily here while the loss isn't really used (unless it is used, and not mentioned in the paper).-----(5) overall, the proposed reward bonus is a heuristic whose specific design choice isn't statistically shown to be useful (ablation in appendix), and the empirical results comparing to other methods are underwhelming.----------here are my main points of concern which i hope the authors address in the rebuttal:-----(1) designing reward bonuses to induce exploratory behaviour in the agent has seen a surge of publications in the deep rl literature in recent years. the key property all these methods aim for is a bonus that pushes the agent to the boundaries of its current ""known region"", and then rely on the stochasticity due to epsilon-greedy to cross that boundary -- pushing this boundary further. while this is different from exploration to reduce uncertainty, it is nonetheless a reasonable approach leading to competitive policies when evaluated in deep rl. but a characteristic all these bonuses aim for is that they fade away with time -- for instance count-based bonus are inversely proportional to visit counts, or prediction error bonuses go to 0 as the prediction becomes more accurate. but what do these novelty bonuses converge to? is it just a stationary value based on consecutive loss parameter (in which case the hope is they don't affect the external reward scale, they just shift it uniformly)?-----(2) what exactly are the nearest neighbours? is it a search based on the data in the buffer or is it a notion of temporal neighbours?-----(3) if it's temporal, why would there ever be biased for some states -- ""we do this in order..novel states"".-----(4) i was completely unable to understand the section in the appendix which is making a case for the ranked weighting. if you have a succinct explanation for the heuristic it'd be great.-----(5) further, as a heuristic it is mentioned that l2 norm may not be effective if the dimensionality of the representation space is increased. so why the heuristic? i think it either needs more empirical validation, or a theoretical justification.-----(6) while the evaluation scheme used in the paper to quantify the exploration of the behaviour policy is interesting -- y-axis of plots in figure 4 for the labyrinth task -- why/what exactly is the role of figure 2? is the interpretability loss used here? is it to reason for utilizing e-greedy instead of a purely-greedy behaviour? i think this is a little unclear, and can be better clarified. further, the distinction of primary and secondary features is interesting, but their clear demarcation is rather questionable in more complicated domains -- in the abstract space.-----(7) do you have a hypothesis for why the 1-step value functions are not sufficient for decision making in this simple domain -- labyrinth - with the abstract representations?-----(8) if model-based algorithms get more steps to learn shouldn't model-free too? i'm not sure i understand the reasoning for the experiment design choice.-----(9) whats the architecture used for bootstrap dqn? it needs to have multiple heads -- but based on the current architecture that doesn't seem likely.-----(10) are the extrinsic rewards ignored in learning -- ""only focus on intrinsic rewards"" (section 6.2.2)? if they are for the proposed method, are they for the competitors too? if so why, and what is the reward for bootstrap dqn?-----(11) i think the discussion section raises interesting points about interpretability and metric learning, but i do think the conclusions drawn are a little inflated.-----(12) the ablation study in section d of the appendix is not statistically significant -- so why is wighted reward useful? please comment.-----(13) how would stochasticity in transition dynamics affect the abstract representation space? discussing this would be very interesting.-----(14) learning curves for the control tasks?----------comments about typos/possible points of confusion:-----(1) the last para in section 6.1 -- discusses ""open"" labyrinth heat map, then what do we mean by learning the dynamics of the wall? there is no wall in open, right?-----(2) in section 4 -- i think x_{t+1} is an estimate from the unrolled model -- \hat{x}_{t+1}? further, it would be helpful to mention that it is an estimate based on the learned model.-----(3) n_freq is used in the pseudocode in the main paper -- but no mention of it to explain it is made in the main.-----(4) contrasting the work to existing literature would be useful (in the related work section; as opposed to summarizing existing work).-----(5) buffered q network --> target networks? this paper proposes a method of sample-efficient exploration for rl agent. the main problem at hand is the presence of irrelevant information in raw observations. to solve this problem, the authors leverage novelty heuristics in a lower-dimensional representation of a state, for which they propose a novelty measure. then they describe a combination of model-based and model-free approaches with the novelty metric used as an intrinsic reward for planning that they use to compare with baselines solutions. they conduct experiments to show that their algorithm outperforms random and count-based baselines. they show that their approach has better results then random policy, prediction error incentivized exploration, hash count-based exploration, bootstrap dqn while playing acrobot and multi-step maze.----------authors propose a novel approach to the problem of exploration. they test their method by experiments conducted in two environments, where they use the same model architectures and model-free methods for all types of novelty metrics, which shows the contribution of the proposed method in the results of learning.----------to sum up, the decision is to accept the paper as the problem is important, ideas are rather new, and results are better compared to other approaches.----------1. the dependence of the quality of the dimensionality representational state is unclear. for different environments, different abstract representation dimensions are chosen, but the reason is not explained.-----2. word ""we"" is overused in the article","the two most experienced reviewers recommended the paper be rejected. the submission lacks technical depth, which calls the significance of the contribution into question. this work would be greatly strengthened by a theoretical justification of the proposed approach. the reviewers also criticized the quality of the exposition, noting that key parts of the presentation was unclear. the experimental evaluation was not considered to be sufficiently convincing. the review comments should be able to help the authors strengthen this work.","-----(4) parts of the paper is rather unclear/feels disconnected -- for instance, the interpretable abstract representation bit; this was a loss in the original work, and seems to be just mentioned arbitrarily here while the loss isn't really used (unless it is used, and not mentioned in the paper).-----(5) overall, the proposed reward bonus is a heuristic whose specific design choice isn't statistically shown to be useful (ablation in appendix), and the empirical results comparing to other methods are underwhelming.----------here are my main points of concern which i hope the authors address in the rebuttal:-----(1) designing reward bonuses to induce exploratory behaviour in the agent has seen a surge of publications in the deep rl literature in recent years.","learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space.----------as it stands, i am leaning towards rejecting the paper, for the following reasons.-----(1) while the idea proposed is interesting, the current work rather explores it in a limited manner which is unsatisfactory.-----(2) i think the presentation of the bonus itself -- novelty search (section 4), which is the core of the paper, is rather unclear.","-----(4) parts of the paper is rather unclear/feels disconnected -- for instance, the interpretable abstract representation bit; this was a loss in the original work, and seems to be just mentioned arbitrarily here while the loss isn't really used (unless it is used, and not mentioned in the paper).-----(5) overall, the proposed reward bonus is a heuristic whose specific design choice isn't statistically shown to be useful (ablation in appendix), and the empirical results comparing to other methods are underwhelming.----------here are my main points of concern which i hope the authors address in the rebuttal:-----(1) designing reward bonuses to induce exploratory behaviour in the agent has seen a surge of publications in the deep rl literature in recent years.","the paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space.","the paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space.","-----(4) parts of the paper is rather unclear/feels disconnected -- for instance, the interpretable abstract representation bit; this was a loss in the original work, and seems to be just mentioned arbitrarily here while the loss isn't really used (unless it is used, and not mentioned in the paper).-----(5) overall, the proposed reward bonus is a heuristic whose specific design choice isn't statistically shown to be useful (ablation in appendix), and the empirical results comparing to other methods are underwhelming.----------here are my main points of concern which i hope the authors address in the rebuttal:-----(1) designing reward bonuses to induce exploratory behaviour in the agent has seen a surge of publications in the deep rl literature in recent years.","further, the distinction of primary and secondary features is interesting, but their clear demarcation is rather questionable in more complicated domains -- in the abstract space.-----(7) do you have a hypothesis for why the 1-step value functions are not sufficient for decision making in this simple domain -- labyrinth - with the abstract representations?-----(8) if model-based algorithms get more steps to learn shouldn't model-free too?","learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space.----------as it stands, i am leaning towards rejecting the paper, for the following reasons.-----(1) while the idea proposed is interesting, the current work rather explores it in a limited manner which is unsatisfactory.-----(2) i think the presentation of the bonus itself -- novelty search (section 4), which is the core of the paper, is rather unclear.",0.2857142857142857,0.0597014925373134,0.1576354679802955,0.1576354679802955,0.3448275862068965,0.0697674418604651,0.1839080459770115,0.1839080459770115,0.2857142857142857,0.0597014925373134,0.1576354679802955,0.1576354679802955,0.1386138613861386,0.0202020202020202,0.099009900990099,0.099009900990099,0.1386138613861386,0.0202020202020202,0.099009900990099,0.099009900990099,0.2857142857142857,0.0597014925373134,0.1576354679802955,0.1576354679802955,0.1496598639455782,0.0,0.1224489795918367,0.1224489795918367,0.3448275862068965,0.0697674418604651,0.1839080459770115,0.1839080459770115,9.0151948928833,13.999711036682127,13.999715805053713,12.582809448242188,9.0151948928833,9.0151948928833,12.582809448242188,5.480734825134277,0.9779577676059217,0.9774454298252203,0.9277501608818013,0.9848923762750849,0.9826642802108495,0.9123413557352041,0.9779577676059217,0.9774454298252203,0.9277501608818013,0.9738187653435514,0.9756716077556767,0.9332919928775483,0.9738187653435514,0.9756716077556767,0.9332920425519563,0.9779577676059217,0.9774454298252203,0.9277502593551223,0.5001194069796084,0.6674708489390143,0.5761056313346256,0.9848923762750849,0.9826642802108495,0.9123413557352041
116,https://openreview.net/forum?id=SJeqs6EFvB,"javascript is the standard programming language of the web; according to the stats it is used on 95% of the (at least) 1.6 billion websites in the world. compared to other programming languages, it posses certain unique characteristics which are also responsible for making it so popular for the web. in this interesting work the authors aim to provide a novel data-driven system for detecting and automatically fixing bugs in javascript. the authors provide motivating examples behind their research. indeed, javascript poses unique challenges as described eloquently in section 2, and there is a lot of space for improvement. static analyzers have non-trivial limitations, while there is space for improving data-driven approaches. this is what hoppity aims to achieve by translating the program into a graph, embedding the nodes and the graph as points in a d dimensional space using graph neural networks, and then using a controller that uses lstms decide the action to be taken among a predefined set of possible actions or a single step graph edit. these actions are reasonable, and are able to fix many bugs assuming the right sequence of actions is performed. for the purposes of learning, the author(s) use a corpus crawled and preprocessed from github that contains more than half a million programs. overall, i found this paper very interesting to read, and with large potential impact in practice. the paper contains a solid engineering effort, starting from the dataset collection and its preprocessing, to using state-of-the-art machinery to develop hoppity. therefore, i support its acceptance. however, some things were not clear from the writeup, and i hope the author(s) of the paper can give some insights. ----------- what is the effect of parameter t, i.e., the number of iterations? how is it set in the experiments? clearly, the authors have a knowledge of what t should be since they have preferred programs with fewer commits.------ following on my previous point, given the sequence of changes/graph transformations you perform, what is the distribution of the 'edit distance' (i.e., number of hops to fix a bug) in the dataset that you have? while the author(s) have -----already provided a lot of stats in the appendix, it would be interesting to see such a plot. this distribution could be insightful and serve as a rule of thumb for understanding the effect of t. ------ what is the effect of the beam size? can you plot the accuracy as a function of the beam size? ------ have you tried points with more than 500 nodes to see how the size of these graphs affect the performance of hoppity? ------ can you provide further details on the running times and the gpu specs? ------ the evaluation does not put enough emphasis on false positives/false negative analysis. is it the case that bug-free programs are treated as such? ------ have you tried hoppity on other programming language(s)? do you expect such an improved performance over baselines for other languages (e.g., c++) as well? the paper proposes to learn from bugfixing commits to fix errors in other code. there are several good contributions of the paper, but the main one is to design a language of instructions that fix a program and to formulate the prediction to be a sequence of such instructions. the model, however, is insightful in other ways as well. the lack of naming features or generation of names makes it to point to other identifiers in a program - a major problem for most models for code. instead, the proposed model only builds embeddings from the structure around the variables.----------the work is also well evaluated, on real dataset and also it attempts to compare to the best available static analysis tools. the kind of bugs addressed by the work was also not considered in previous papers. one thing that comes to the examples of the discovered bugs is that even when the fix is shown, the user would still need detailed explanations on why was it a bug.----------so, i also have questions for the authors:-----1. do you think it is possible to observe a similar reasoning to the reasoning in your text for why the buggy examples from figure 1 are wrong, if the activations of the neural network are exposed or with other nn debugging technique.-----2. it seems that a specific sequence of actions is provided in the training data and that sequence is left-to-right edits to apply the fix. in this case, doesnt it make sense to apply restrictions on the location primitive similar in spirit to the attention masking (see here: http://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention )----------minor:------ no_op was used in some places (pages 4 and 5) and stop in others (figure 2).------ n_k(v) is not defined.","this paper presents a learning-based approach to detect and fix bugs in javascript programs. by modeling the bug detection and fix as a sequence of graph transformations, the proposed method achieved promising experimental results on a large javascript dataset crawled from github.----------all the reviews agree to accept the paper for its reasonable and interesting approach to solve the bug problems. the main concerns are about the experimental design, which has been addressed by the authors in the revision. ----------based on the novelty and solid experiments of the proposed method, i agreed to accept the paper as other revises.","in this case, doesnt it make sense to apply restrictions on the location primitive similar in spirit to the attention masking (see here: http://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention )----------minor:------ no_op was used in some places (pages 4 and 5) and stop in others (figure 2).------ n_k(v) is not defined.","this is what hoppity aims to achieve by translating the program into a graph, embedding the nodes and the graph as points in a d dimensional space using graph neural networks, and then using a controller that uses lstms decide the action to be taken among a predefined set of possible actions or a single step graph edit.",------ have you tried hoppity on other programming language(s)?,javascript is the standard programming language of the web; according to the stats it is used on 95% of the (at least) 1.6 billion websites in the world.,in this interesting work the authors aim to provide a novel data-driven system for detecting and automatically fixing bugs in javascript.,"this is what hoppity aims to achieve by translating the program into a graph, embedding the nodes and the graph as points in a d dimensional space using graph neural networks, and then using a controller that uses lstms decide the action to be taken among a predefined set of possible actions or a single step graph edit.","compared to other programming languages, it posses certain unique characteristics which are also responsible for making it so popular for the web.","clearly, the authors have a knowledge of what t should be since they have preferred programs with fewer commits.------ following on my previous point, given the sequence of changes/graph transformations you perform, what is the distribution of the 'edit distance' (i.e., number of hops to fix a bug) in the dataset that you have?",0.1518987341772152,0.0128205128205128,0.1139240506329113,0.1139240506329113,0.240506329113924,0.0,0.1645569620253164,0.1645569620253164,0.055045871559633,0.0,0.036697247706422,0.036697247706422,0.1860465116279069,0.0314960629921259,0.1550387596899225,0.1550387596899225,0.2295081967213114,0.0833333333333333,0.1311475409836065,0.1311475409836065,0.240506329113924,0.0,0.1645569620253164,0.1645569620253164,0.1147540983606557,0.0,0.0655737704918032,0.0655737704918032,0.2692307692307693,0.0779220779220779,0.1666666666666666,0.1666666666666666,1.4824869632720947,11.689891815185549,15.890128135681152,1.482488036155701,1.439389944076538,4.3709869384765625,2.098991870880127,11.539148330688477,0.17684283675482218,0.4216287461377239,0.6277809896047081,0.9626323557216826,0.9644206051762052,0.8810566608530099,0.9596350239915677,0.9658573478917901,0.8823521667473013,0.9673886072259145,0.9656672796118149,0.9583938972290893,0.9649187666123836,0.9601783486957045,0.9476259478845825,0.9626323557216826,0.9644206051762052,0.8810566608530099,0.8864517251923321,0.9489688980336446,0.5493680055067794,0.9484833205544634,0.9600275290973289,0.7071826807120476
117,https://openreview.net/forum?id=SJgn3lBtwH,"this is a well-written paper and i enjoyed reading it. in summary the paper tries to address the following shortcomings of rembo:----------(1) rembo uses a random embedding to project a point in high dimensional space to a lower dimensional embedding space basing on the relation f(x) = f(ay) with high probability, where x in d dimensions and y in d dimensions and d<<d. the problem of rembo is that the relation f(x) = f(ay) is only guaranteed with high probability and so, the embedding space may not contain an optimum. second, when a point y where ay is outside the search space x, rembo uses a projection to map ay to its nearest point in x. this projection is not enough good. these observation are identified in paper of binois et al (2018)as well.-----(2)hesbo is an extension of rembo that avoid above restrictions by proposing a new random projection. however, as the paper mentioned, hesbo have a limitation is that the probability that the embedding will contain an optimum can be quite low! -----(3)besides, the paper also identify a new observation that linear projections do not preserve product kernels. ----------then, the paper proposes a new solution of bo to overcome these restrictions by using a mahalanobis kernel to avoid (3). this kernel is a replace of ard euclidean distance to a mahalanobis distance. to avoid (1), the paper use equation 1 (please find in the paper). to avoid (2), they use the projection p_opt. in all , i think the theoretical contribution is good enough.----------having said that, i am a bit disappointed that this paper does not talk about linebo (icml 2019). linebo is a good solution for high dimensions without any assumption on structure like low effective dimensionality. it uses even one-dimensional subspaces to solve high dimensional problem with the strong theoretical guarantee. it do not need to learn subspace, and so it avoids disvantages (1), (2) and (3) that cause due to the fact that the embedding may not contain an optimum as mentioned above. thus, linebo is a stronger contender to the proposed algorithm. i will lift my rating if the author provide their response to this point.----------additionally, the author should compare their method to the algorithm of binois et al (2018) that solved very well disadvantages of rembo by setting bounds to avoid (1). moreover, because the problem of the paper is high-dimensional bayesian optimization under the assumption of low effective dimensionality, they should compare to other strong algorithms under the same assumption such as si-bo algorithm( nips 2013) that used active learning to learn the low-dimensional subspace instead of using random embedding like rembo. this paper criticizes existing high dimensional bo (hdbo) via linear embedding literature for the following reasons:----------- points in the embedded space projected mostly to the facet of the bounding box in the original space.------ the projection induces a distorted space which is not fit to be modeled by a gp.------ linear projections do not preserve product kernels.------ linear embeddings have low probability of containing an optimum.----------the paper then proposes alebo which supposedly improves these aspects over other linear embedding bo techniques.----------i have the following concerns regarding the authors' criticisms above:----------1. what is wrong with the points being projected mostly to the facet of the original bounding box?-----if i understand correctly, theorem 3 of the rembo paper proved that there exists y* \in r^{d_e} such that f(ay*) = f(x*)-----(i.e., the projected space contains the optimum) with high probability so to me, it does not really matter if the projection does not include the interior of the bounding box. fig. 1 seems to make a point that most projections seem to indeed land on the facet of the bounding box (to be thorough, how many points did the authors sample to make this plot) but given what i said above, i do not think there is much of a point in fig. 1 here. ----------2. the authors claimed that it is not appropriate to model the distorted space with gp, but ended up using gp -----for alebo anyway (although with a different kernel). i understand that the authors did not project ay back to b-----like rembo did, but the authors also gave me no reason to believe that this will improve things either. in fact, i think -----the authors should show the space induced by alebo embedding as a comparison. i suspect that with the imposed -----constraint -1 <= (b^t)y <= 1 the space will have discontinuous regions and is also not fit to be modeled -----with any gp. ----------3. the authors stated that ""a product kernel in the true subspace will not produce a product kernel in the embedding; -----we will see this more explicitly in sec. 5.1"" but i did not see it explicitly in sec. 5.1. at the very-----least, i do not see how rembo fails to do the same thing. also, the authors claimed that ""inside the embedding, -----moving along a single dimension will move across all dimensions of the true subspace, at rates depending -----on the angles between the embedding and the true subspace"". this seems like a very qualitative claim.-----can the authors formally define what this statement means, and prove it or at least provide some backup citations? ----------other comments:----------4. why do the authors use conjugate transpose (if b^t means what i think it means) instead of normal transpose -----when b is drawn from r^{d_e \times d}? shouldn't they be the same?----------5. please explain the choice of \epsilon(b) = {x: b^t b x = x} ----------6. the experiments provided are very limited. there is only one set of experiments showing performance of-----alebo against other methods and it was done on a very small extrinsic dimension too (d = 100). i would like to see how -----alebo scales with truly large dimension (rembo also claimed that it could scale up to much higher extrinsic dimension). what about other important properties like does it guarantee that an optimum lies in the constraint space? what about rotational invariance? there are so many elements missing from the analysis.----------overall conclusion:----------this paper is largely empirical and lacks technical depth. it is not at all convincing that the problem it-----addresses is real, much less important. it also does not offer strong empirical evidence (too few experiments). -----given these reasons, i do not think the paper is not ready to be published as it is. the authors investigate pitfalls common to random embedding-based approaches to high-dimensional bayesian optimization (hdbo). each of several practical shortcomings is separately analyzed and, subsequently, addressed in straightforward fashion:---------- a. large-scale distortions caused by clipping are handled by generalizing box constraints----- (in the embedded space) to the polytope corresponding to the set of points that project----- to the interior of the original search space.---------- b. local distortions caused by defining squared euclidean distances in embedded spaces----- are handled by substitution for mahalanobis distances.---------- c. embedded spaces potentially failing to contain optima are handled by constructing an----- estimator for the probability of this happening, which is then be used to pick better----- embeddings.---------------feedback:----- to the extent that i enjoyed reading this piece, i am not sure that it warrants publication at this time. specifically, the degree of novelty on offer seems minimal and the empirical results are underwhelming.---------- 1) constraints on embedded candidate are naively defined in terms of a polytope, but this formulation has previously been deemed impractical. if nothing else, it would be good to clarify this matter: why did preceding works chose not to explore this direction and/or how did you make it work here?---------- 2) regarding use of mahalanobis distances, evidence here (as provided in a.2) seems thin. firstly, predictive mse (figure 6) seems like an odd choice of metric; log marginal probabilities would seemingly be more natural for gps. reporting of mse is particularly suspect when results shown in figure 7 indicate that the mahalanobis distance based gps are (markedly) overconfident. this issue is allegedly improved by marginalizing mahalanobis parameters ; however, the appropriate baseline here would be ard with marginalized lengthscales (which, to my knowledge, is not shown).---------- 3) regarding alebo itself, algorithm 1 states that acquisition functions were expressed in terms of an approximate posterior formed via moment matching against the gaussian mixture formed by different samples of hyperparameters ? one usually pushes this uncertainty through the acquisition function as, e.g., . what motivated this design choice?---------------questions:----- - does need to be sampled or can it chosen to maximize ?----- - marginalization of hyperparameters----- - did you jointly marginalize over all hyperparamerters or just mahalanobis parameters ?----- - why was a laplace approximation used lieu of, e.g., slice sampling?----------nitpicks, spelling, & grammar:----- - various figures: 'newmethod' -> 'alebo'----- - please report log immediate regret along with error bars----- - to the extent that testing on e.g. ""high-dimensional"" variants of branin and hartmann-6 is standard, it isn't particularly convincing.","this paper explores the practice of using lower-dimensional embeddings to perform bayesian optimization on high dimensional problems. the authors identify several issues with performing such an optimization on a lower-dimensional projection and propose solutions leading to better empirical performance of the optimization routine. overall the reviewers found the work well written and enjoyable. however, the reviewers were concerned primarily about the connection to existing literature (r2) and the empirical analysis (r1, r3). the authors claim that their method outperforms state-of-the-art on a range of problems but the reviewers did not feel there was sufficient empirical evidence to back up this claim. ----- unfortunately, as such the paper is not quite ready for publication. the authors claim to have significantly expanded the experiments in the response period, however, which will likely make it much stronger for a future submission.","specifically, the degree of novelty on offer seems minimal and the empirical results are underwhelming.---------- 1) constraints on embedded candidate are naively defined in terms of a polytope, but this formulation has previously been deemed impractical.","this paper criticizes existing high dimensional bo (hdbo) via linear embedding literature for the following reasons:----------- points in the embedded space projected mostly to the facet of the bounding box in the original space.------ the projection induces a distorted space which is not fit to be modeled by a gp.------ linear projections do not preserve product kernels.------ linear embeddings have low probability of containing an optimum.----------the paper then proposes alebo which supposedly improves these aspects over other linear embedding bo techniques.----------i have the following concerns regarding the authors' criticisms above:----------1. what is wrong with the points being projected mostly to the facet of the original bounding box?-----if i understand correctly, theorem 3 of the rembo paper proved that there exists y* \in r^{d_e} such that f(ay*) = f(x*)-----(i.e., the projected space contains the optimum) with high probability so to me, it does not really matter if the projection does not include the interior of the bounding box.",it also does not offer strong empirical evidence (too few experiments).,this is a well-written paper and i enjoyed reading it.,"each of several practical shortcomings is separately analyzed and, subsequently, addressed in straightforward fashion:---------- a. large-scale distortions caused by clipping are handled by generalizing box constraints----- (in the embedded space) to the polytope corresponding to the set of points that project----- to the interior of the original search space.---------- b. local distortions caused by defining squared euclidean distances in embedded spaces----- are handled by substitution for mahalanobis distances.---------- c. embedded spaces potentially failing to contain optima are handled by constructing an----- estimator for the probability of this happening, which is then be used to pick better----- embeddings.---------------feedback:----- to the extent that i enjoyed reading this piece, i am not sure that it warrants publication at this time.","this paper criticizes existing high dimensional bo (hdbo) via linear embedding literature for the following reasons:----------- points in the embedded space projected mostly to the facet of the bounding box in the original space.------ the projection induces a distorted space which is not fit to be modeled by a gp.------ linear projections do not preserve product kernels.------ linear embeddings have low probability of containing an optimum.----------the paper then proposes alebo which supposedly improves these aspects over other linear embedding bo techniques.----------i have the following concerns regarding the authors' criticisms above:----------1. what is wrong with the points being projected mostly to the facet of the original bounding box?-----if i understand correctly, theorem 3 of the rembo paper proved that there exists y* \in r^{d_e} such that f(ay*) = f(x*)-----(i.e., the projected space contains the optimum) with high probability so to me, it does not really matter if the projection does not include the interior of the bounding box.","what motivated this design choice?---------------questions:----- - does need to be sampled or can it chosen to maximize ?----- - marginalization of hyperparameters----- - did you jointly marginalize over all hyperparamerters or just mahalanobis parameters ?----- - why was a laplace approximation used lieu of, e.g., slice sampling?----------nitpicks, spelling, & grammar:----- - various figures: 'newmethod' -> 'alebo'----- - please report log immediate regret along with error bars----- - to the extent that testing on e.g. ""high-dimensional"" variants of branin and hartmann-6 is standard, it isn't particularly convincing.","this paper criticizes existing high dimensional bo (hdbo) via linear embedding literature for the following reasons:----------- points in the embedded space projected mostly to the facet of the bounding box in the original space.------ the projection induces a distorted space which is not fit to be modeled by a gp.------ linear projections do not preserve product kernels.------ linear embeddings have low probability of containing an optimum.----------the paper then proposes alebo which supposedly improves these aspects over other linear embedding bo techniques.----------i have the following concerns regarding the authors' criticisms above:----------1. what is wrong with the points being projected mostly to the facet of the original bounding box?-----if i understand correctly, theorem 3 of the rembo paper proved that there exists y* \in r^{d_e} such that f(ay*) = f(x*)-----(i.e., the projected space contains the optimum) with high probability so to me, it does not really matter if the projection does not include the interior of the bounding box.",0.146067415730337,0.0227272727272727,0.1123595505617977,0.1123595505617977,0.3246753246753247,0.0522875816993464,0.1753246753246753,0.1753246753246753,0.065359477124183,0.0132450331125827,0.0522875816993464,0.0522875816993464,0.1176470588235294,0.0132450331125827,0.0915032679738562,0.0915032679738562,0.2835249042145594,0.0154440154440154,0.1302681992337164,0.1302681992337164,0.3246753246753247,0.0522875816993464,0.1753246753246753,0.1753246753246753,0.1785714285714285,0.009009009009009,0.0982142857142857,0.0982142857142857,0.3246753246753247,0.0522875816993464,0.1753246753246753,0.1753246753246753,10.13609218597412,8.057482719421387,1.0558617115020752,10.13609218597412,5.852885723114014,5.849482536315918,10.13609218597412,7.639579772949219,0.26086755803528944,0.3489897688350334,0.7270101304813436,0.9119152750836867,0.851329208526031,0.9478922923090431,0.09053359864276468,0.15789441998927034,0.6184873806325387,0.9677009417554197,0.9698932573380067,0.9583247595438552,0.5320057202551101,0.6762719977459202,0.8994975908858945,0.9119152750836867,0.851329208526031,0.9478922923090431,0.31428082995346723,0.3165248781844665,0.3743632439837713,0.9119152750836867,0.851329208526031,0.9478922923090431
118,https://openreview.net/forum?id=SJl2ps0qKQ,"this paper proposes a knowledge-based qa system that learns to decompose compound questions into simple ones. the decomposition is modeled by assigning each token in the input question to one of the partitions and receiving reward signal based on the final gold answer. the model achieves the state-of-the-art performance on the metaqa dataset. ----------------my main complaint about the paper is its lack of technical details and analysis of empirical results. parts of the paper seem quite unclear, for example:----------------in the last paragraph of section 3.1, it says we do not assume that nay question should be divided into exactly three parts.  see section 4 for case study. does this mean that the model can have <=3 partitions, but not more? how is this number decided?----------------section 3.2 describes the simple-question answer. from eq (4), it seems that the answerer only uses the current partition, is that the case? moreover, how is the gold relation r obtained?----------------it would be nice to add more explanation to the caption of figure 4 to make it self-contained.----------------the case study section (4.3) only contains a single example. it would be very helpful to include more examples of question partitions (there is enough space). error analysis would also be helpful to understand, for example, why the proposed model is worse than vrn (zhang et al. 2017) on 1- and 2-hop questions. this paper proposes a new approach for answering questions requiring multi-hop reasoning. the key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. the labeler is trained with the whole kb-qa pipeline with reinforce in an end-to-end way.----------------the proposed approach was applied to a synthetic dataset and a new kb-qa dataset metaqa, and achieves good results.----------------i like the proposed idea, which sounds a straightforward solution to compound question answering. i also like the clarification between ""compound questions"" instead of ""multi-hop questions"". in my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.----------------my only concern is about the evaluation on metaqa, which seems a not widely used dataset in our community. therefore i am wondering whether the authors could address the following related questions in the rebuttal or revision:----------------(1) i was surprised that webquestions is not used in the experiments. could you explain the reason? my guess is that webquestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. if this is not true, could you provide results on webquestions (or webqsp).----------------(2) there were several previous methods proposed for decomposition of compound questions, although they are not proposed for kb-qa. examples include ""search-based neural structured learning for sequential question answering"" and ""complexwebquestions"". i think the authors should compare their approach with previous work. one choice is to reimplement their methods. an easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to kb-qa, as long as the simple question answerer is replaced to other components like a reader in the complexwebquestions work.","+ an interesting task -- learning to decompose questions without supervision - reviewers are not convinced by evaluation. initially evaluated on metaqa only, later relation classification on webquestions has been added. it is not really clear that the approach is indeed beneficial on webquestion relation classification (no analysis / ablations) and metaqa is not a very standard dataset. - reviewers have concerns about comparison to previous work / the lack of state-of-the-art baselines. some of these issues have been addressed though (e.g., discussion of iyyer et al. 2016)",the decomposition is modeled by assigning each token in the input question to one of the partitions and receiving reward signal based on the final gold answer.,"in my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.----------------my only concern is about the evaluation on metaqa, which seems a not widely used dataset in our community.",this paper proposes a new approach for answering questions requiring multi-hop reasoning.,this paper proposes a knowledge-based qa system that learns to decompose compound questions into simple ones.,this paper proposes a new approach for answering questions requiring multi-hop reasoning.,"the labeler is trained with the whole kb-qa pipeline with reinforce in an end-to-end way.----------------the proposed approach was applied to a synthetic dataset and a new kb-qa dataset metaqa, and achieves good results.----------------i like the proposed idea, which sounds a straightforward solution to compound question answering.","examples include ""search-based neural structured learning for sequential question answering"" and ""complexwebquestions"".","in my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.----------------my only concern is about the evaluation on metaqa, which seems a not widely used dataset in our community.",0.1769911504424778,0.018018018018018,0.0884955752212389,0.0884955752212389,0.2442748091603054,0.0310077519379844,0.1374045801526717,0.1374045801526717,0.0606060606060606,0.0,0.0202020202020202,0.0202020202020202,0.116504854368932,0.0396039603960396,0.0776699029126213,0.0776699029126213,0.0606060606060606,0.0,0.0202020202020202,0.0202020202020202,0.1884057971014492,0.0,0.1159420289855072,0.1159420289855072,0.0606060606060606,0.0,0.0606060606060606,0.0606060606060606,0.2442748091603054,0.0310077519379844,0.1374045801526717,0.1374045801526717,7.965882301330566,8.751344680786133,12.517762184143066,7.380828380584717,6.687521934509277,8.751346588134766,7.380827903747559,7.456185340881348,0.9545809180381076,0.9620493304344696,0.9212788262090185,0.9788754970865964,0.9751725032773793,0.6991076686422043,0.9649842168300893,0.96268496533914,0.9534981197700906,0.9736194411740111,0.9717425756315462,0.9353241300811923,0.9649842168300893,0.96268496533914,0.9534980522805556,0.9663183122842576,0.9780519656418948,0.028291918314300647,0.6884148848549411,0.8676822834845429,0.7923163385024544,0.9788754970865964,0.9751725032773793,0.6991076686422043
119,https://openreview.net/forum?id=SJlpYJBKvH,"this paper provides a unified way to provide robust statistics in evaluating rl algorithms in experimental research. though i don't believe the metrics are particularly novel, i believe this work would be useful to the broader community and was evaluated on a number of environments. i do have a few concerns, however, about experimental performance per environment being omitted from both the main paper and the appendix.----------comments: ----------+ i think this is a valuable work and the ideas/metrics are useful, though i'm not sure i would call them novel (cvar and the like have been seen before). i think the value comes in the unification of the metrics to give more robust pictures of algorithmic performance. -----+ the details of all of these evaluations and individual performance should be provided in the appendix, however, it seems only mujoco curves were included. moreover, it says that a blackbox optimizer was used to find hyperparameters, but these hyperparameters were not provided in the appendix or anywhere else as far as i can tell. i think it's important for a paper which recommends evaluation methodology in particular to be more explicit regarding all details within the appendix. i hope to see additional details in future revisions -- including per-environment performance.-----+ i believe clustering results across environments can be potentially misleading. say that we have an environment where the algorithm always fails but is very consistent and an environment where it excels. these are blended together in the current figures. while it requires more space, i believe it is important to separate these two. i am concerned that a recommendation paper like this one will set a precedent for only including the combined metrics of algorithmic performance across environments, masking effects. i would suggest splitting out results per environment as well and pointing out particular cross-environment phenomena. ----------there is a missing discussion of prior work on statistical testing of rl evaluation:-----+ colas, cdric, olivier sigaud, and pierre-yves oudeyer. ""a hitchhiker's guide to statistical comparisons of reinforcement learning algorithms."" arxiv preprint arxiv:1904.06979 (2019).-----+ colas, cdric, olivier sigaud, and pierre-yves oudeyer. ""how many random seeds? statistical power analysis in deep reinforcement learning experiments."" arxiv preprint arxiv:1806.08295 (2018).----------edit: score boosted after significant updates to the paper. *summary*----------authors proposed a variety of metrics to measure the reliability of an rl algorithm. mainly looking at dispersion and risk across time and runs while learning, and also in the evaluation phase. -----authors have further proposed ranking and also confidence intervals based on bootstrapped samples. they also compared the famous continuous control and discrete actions algorithms on atari and openai gym on the metrics they defined.----------*decision*----------i believe the paper is discussing a very important issue, and some possible solutions to it, even if not perfect it's an important step toward paying more attention to maybe similar metrics. i am in favor of the paper in general, but i have some concerns.----------1. my main concern is that why authors think that community will adopt these metrics and report them? i like how authors have proposed different metrics, but having one or two easy to compute metric is much more likely to be adopted, than 6 different metrics, which im not sure how easy it is to use the python package? its of main importance, because if community dont use these metrics in the future, the contribution of the paper is minimal. ----------2. there is no question of the importance of reliability of rl algorithms, but we need to be careful that rl algorithms are not optimizing for metics like cvar, so maybe a better learning algorithm (in the sense of expectation learning) might not have better reliability metrics because it is not the main objective. -----so following this, how would authors think their metrics can be used to design a more reliable algorithms? for example there is good literature on cvar learning for safe policies. do you think there exists a proxy for metrics you introduced that can be used to for the objective of the optimization?----------3. another main concern is the effect of exploration strategy: all these metrics can be highly affected by different exploration strategy in different environments. for example if an environment has a chain like structure, then given the exploration strategy you may have an extremely high cvar or iqr. how do authors think they can strip off this effect? (running all algorithm with the same exploration strategy is not sufficient, since the interplay of learning algorithm and exploration may be important)----------4. generalizability: how do authors think these metrics are generalizable. for example if algorithm a has better metrics than algorithm b on open ai gym task for continuous control, how much we expect the same ranking applies while learning on a new environment. i am asking this, because to me, some of these metrics are very environment dependent, and being reliable in some environments may not imply reliability in other environments.---------------*note*:-----code and modularity of it: the main contribution of this paper will be shown when other researchers start using it and report the metric, if the code is hard to use, the contribution of the paper is hard to be significant. ----------==== post rebuttal ====-----thanks for the responses authors posted, i think there is a good chance that the community will benefit from this experimental metrics in the future, so i increase my rating to accept.","main content:----------this paper provides a unified way to provide robust statistics in evaluating the reliability of rl algorithms, especially deep rl algorithms. though the metrics are not particularly novel, the investigation should be useful to the broader community as it compares seven specific evaluation metrics, including 'dispersion across time (dt): iqr across time', 'short-term risk across time (srt): cvar on differences', 'long-term risk across time (lrt): cvar on drawdown', 'dispersion across runs (dr): iqr across runs', 'risk across runs (rr): cvar across runs', 'dispersion across fixed-policy rollouts (df): iqr across rollouts' and 'risk across fixed-policy rollouts (rf): cvar across rollouts'. the paper further proposed ranking and also confidence intervals based on bootstrapped samples, and compared against continuous control and discrete actions algorithms on atari and openai gym.----------------------discussion:----------the reviews clearly agree on accepting the paper, with a weak accept coming from a reviewer who does not know much about this subarea. comments are mostly just directed at clarifications and completeness of description, which the authors have addressed.----------------------recommendation and justification:----------this paper should be accepted due to its useful contributions toward doing a better job of measuring performance of rl.","----------there is a missing discussion of prior work on statistical testing of rl evaluation:-----+ colas, cdric, olivier sigaud, and pierre-yves oudeyer.","----------2. there is no question of the importance of reliability of rl algorithms, but we need to be careful that rl algorithms are not optimizing for metics like cvar, so maybe a better learning algorithm (in the sense of expectation learning) might not have better reliability metrics because it is not the main objective.","----------2. there is no question of the importance of reliability of rl algorithms, but we need to be careful that rl algorithms are not optimizing for metics like cvar, so maybe a better learning algorithm (in the sense of expectation learning) might not have better reliability metrics because it is not the main objective.",this paper provides a unified way to provide robust statistics in evaluating rl algorithms in experimental research.,"----------there is a missing discussion of prior work on statistical testing of rl evaluation:-----+ colas, cdric, olivier sigaud, and pierre-yves oudeyer.","----------2. there is no question of the importance of reliability of rl algorithms, but we need to be careful that rl algorithms are not optimizing for metics like cvar, so maybe a better learning algorithm (in the sense of expectation learning) might not have better reliability metrics because it is not the main objective.","""a hitchhiker's guide to statistical comparisons of reinforcement learning algorithms.""","----------2. there is no question of the importance of reliability of rl algorithms, but we need to be careful that rl algorithms are not optimizing for metics like cvar, so maybe a better learning algorithm (in the sense of expectation learning) might not have better reliability metrics because it is not the main objective.",0.0825688073394495,0.0092592592592592,0.055045871559633,0.055045871559633,0.208,0.0483870967741935,0.12,0.12,0.208,0.0483870967741935,0.12,0.12,0.1314553990610328,0.1137440758293839,0.1314553990610328,0.1314553990610328,0.0825688073394495,0.0092592592592592,0.055045871559633,0.055045871559633,0.208,0.0483870967741935,0.12,0.12,0.0483091787439613,0.0,0.0483091787439613,0.0483091787439613,0.208,0.0483870967741935,0.12,0.12,11.327593803405762,12.308701515197754,13.297392845153809,11.327592849731444,12.308701515197754,11.327593803405762,11.327593803405762,6.34766960144043,0.9749113260445712,0.9752243116629518,0.6538448172814487,0.9710664114850496,0.9642366665422043,0.9311331319075756,0.9710664114850496,0.9642366665422043,0.9311331180325215,0.9690860375281991,0.9656458182714693,0.925496910265179,0.9749113260445712,0.9752243116629518,0.6538455119282469,0.9710664114850496,0.9642366665422043,0.9311331319075756,0.9505798649725359,0.9457866965464863,0.945559998472922,0.9710664114850496,0.9642366665422043,0.9311332968476056
120,https://openreview.net/forum?id=SJx0q1rtvS,"this paper leverages differential privacys stability properties to investigate its use for improved anomaly and backdoor attack detection. under an assumption (called uniformly asymptotic empirical risk minimization), the authors show that difference between the expected loss of a differentially private learning algorithm on an outlier (where the expectation is taken over the randomness of the learning algorithm) and the expected loss of the same algorithm on data from the underlying distribution (expectation taken over data & randomness of the algorithm) is lower bounded by a (possibly/hopefully) non-negative quantity with high probability. the authors then conduct a set of experiments to show that differential privacy improves the performance of outliers, novel examples, and backdoor attack detection. ----------overall, the paper is very well written and easy to read. the paper also tackles an important and timely problem that is relevant to the iclr community. while there has been some recent work on connecting differential privacy to robustness & attacks, this paper investigates the use of differential private model training as a means to improve novelty detection at inference time. ----------a few points that need attention from the authors: ----------1. the theory developed is insightful in general but has very little (to no) practical value. for starters, it assumes that differentially private model training is uniformly asymptotic to empirical risk minimization. this is not necessarily true for highly non-convex models trained with sgd. further, it cannot be verify via experimentations (despite the authors attempt to sanity check it using figure 1). more importantly, the theory developed in section 3 is not used in any meaningful way in the experiments section  the anomaly detection schemes are agnostic to it. -----2. the authors make no attempt to co-optimize the performance of the model with its ability to be used for better anomaly detection. for instance, the authors choose an l2-clipping-norm c of 1 and do not consider trading off c with the noise variance. ----------when the training set contains anomalies, this work can be viewed as what is the impact of differential privacy on a training sets with a majority group (training examples from a given distribution) and a minority group (training examples from a different distribution). under this view, this paper essentially says that differential privacy leads to disparate impact on model accuracy/loss. this has been recently investigated in the following neurips19 paper: https://arxiv.org/abs/1905.12101. thus the contributions of the paper are not substantial. this paper proposes the idea of using differential privacy (dp) to improve the performance of outlier and novelty detection. differential privacy was proposed as a privacy metric which limits the contribution of a single data point in the training set to the output. this property naturally controls how poisoned data would affect the output of the learned model. under the assumption that a well-trained model would incur a higher loss on the outliers, the paper gives a theoretic bound on how this loss will decrease if there are poisoned samples in the training set. ----------the paper also performs several experiments on synthetic and real-world datasets. the paper shows that add differential privacy during training can improve the performance of autoencoder-based outlier detection on mnist data. for real-world data, the paper improves the performance of anomaly detection on the hdfs dataset over the state-of-the-art algorithm. the paper also shows empirically how dp can help improve backdoor attack detection. ----------the paper is overall nicely written with some nice results. the paper could be improved if the following confusions can be resolved.----------1. novelty detection is generally referred to as detecting samples in the test set that are not in the distribution of the training set. in the theory part, the analysis is mostly based on data poisoning, which is not typical in the novelty detection setting. i hope this can be clarified.-----2. in the experiment part, the paper uses figure 1 to show how uaerm is satisfied. i find this a bit confusing. in definition 4, the h^* is referred to as the global minimizer while in the experiment, the empirical minimizer is used.-----3. theorem 2 presents some theoretical bound to show the power of dp on improving outlier detection, however, in the parameter setting used in the experiment, theorem 2 does not provide meaningful bounds. there is a bit disconnection between the two parts.----------based on the above comments, i think the paper can be accepted if there is room for it. but i won't push it for acceptance. interesting topic but lacks of novelty-----#summary:-----the paper proposes that by applying differential privacy, the performance on outlier and novelty detection can be improved. it first presents a theoretical analysis, which establishes a lower bound on the prediction performance difference between normal and outlier data. by adding noise into the training process, the outliers in the dataset will be hidden by the noise, which will result in a model that utilizes the normal data. in this way, when deploying the model, the model will find the outlier by observing low confidence.----------#strength-----it is good to see that the paper builds a connection between the privacy parameter and the noise level and the experiments make the theory valid.----------#weakness-----im not an expert in differential privacy. but as far as im concerned, a typical downside is that the false positive rate will increase and there is no theoretical guarantee that the increase of false-positive rate will be negligible compared with the increase of true positive rate.-----its effectiveness in detecting backdoor attacks seems elusive. as we know, the backdoor attacks exist when users want to outsource the task of training the network to a third-party, which may potentially be an attack. therefore, the training process is out-of-control to the detector. however, the paper proposes to use differential privacy to the model training process, which is not in the settings of a backdoor attack.","thanks for the submission. this paper leverages the stability of differential privacy for the problems of anomaly and backdoor attack detection. the reviewers agree that this application of differential privacy is novel. the theory of the paper appears to be a bit weak (with very strong assumptions on the private learner), although it reflects the basic underlying idea of the detection technique. the paper also provides some empirical evaluation of the technique.","the authors then conduct a set of experiments to show that differential privacy improves the performance of outliers, novel examples, and backdoor attack detection.","while there has been some recent work on connecting differential privacy to robustness & attacks, this paper investigates the use of differential private model training as a means to improve novelty detection at inference time.","however, the paper proposes to use differential privacy to the model training process, which is not in the settings of a backdoor attack.",this paper leverages differential privacys stability properties to investigate its use for improved anomaly and backdoor attack detection.,"under an assumption (called uniformly asymptotic empirical risk minimization), the authors show that difference between the expected loss of a differentially private learning algorithm on an outlier (where the expectation is taken over the randomness of the learning algorithm) and the expected loss of the same algorithm on data from the underlying distribution (expectation taken over data & randomness of the algorithm) is lower bounded by a (possibly/hopefully) non-negative quantity with high probability.","the authors then conduct a set of experiments to show that differential privacy improves the performance of outliers, novel examples, and backdoor attack detection.","under an assumption (called uniformly asymptotic empirical risk minimization), the authors show that difference between the expected loss of a differentially private learning algorithm on an outlier (where the expectation is taken over the randomness of the learning algorithm) and the expected loss of the same algorithm on data from the underlying distribution (expectation taken over data & randomness of the algorithm) is lower bounded by a (possibly/hopefully) non-negative quantity with high probability.","while there has been some recent work on connecting differential privacy to robustness & attacks, this paper investigates the use of differential private model training as a means to improve novelty detection at inference time.",0.2916666666666667,0.0851063829787234,0.2083333333333333,0.2083333333333333,0.2641509433962264,0.0576923076923076,0.1509433962264151,0.1509433962264151,0.2526315789473684,0.064516129032258,0.1894736842105263,0.1894736842105263,0.2888888888888888,0.1590909090909091,0.2444444444444444,0.2444444444444444,0.3287671232876712,0.0416666666666666,0.2054794520547945,0.2054794520547945,0.2916666666666667,0.0851063829787234,0.2083333333333333,0.2083333333333333,0.3287671232876712,0.0416666666666666,0.2054794520547945,0.2054794520547945,0.2641509433962264,0.0576923076923076,0.1509433962264151,0.1509433962264151,11.215131759643556,11.510141372680664,14.278389930725098,12.069534301757812,11.215131759643556,11.439767837524414,12.069537162780762,11.510141372680664,0.9679337048520872,0.9553783927330108,0.9161961510448466,0.9709421061905148,0.9637706965272406,0.9470771252051291,0.3973229255247827,0.7441404010129156,0.7046123504399124,0.9730716806258385,0.9685913382314071,0.07919597761861051,0.9685565640604733,0.9594773779422651,0.9224490756292955,0.9679337048520872,0.9553783927330108,0.9161960964341181,0.9685565640604733,0.9594773779422651,0.9224489885326258,0.9709421061905148,0.9637706965272406,0.9470772091338646
121,https://openreview.net/forum?id=SJxDDpEKvH,"the ideas presented in the paper are interesting and original. whereas the theory presented has a lot of potential, it seems that the clarity of the paper could be greatly improved, in particular i would have liked more of the formal theory to be included in the body of the paper instead of relying only on the appendix. this especially matters since the theoretical aspect is a key contribution of the paper and the experimental section remains on the light side (it presents mostly examples and lacks more extensive results). ----------i find the introduction of the proposed definition of disentanglement in sections 2.2/2.3 confusing. the authors first define extrinsic disentanglement of a transformation in the data space as corresponding to a transformation of one dimension only in the latent space. in section 2.3 a transformation is called intrinsically disentangled if it corresponds to a transformation of a subset of variables in the space of endogenous variables. it should be made clearer from the start that disentanglement is here only a property of the transformations and that the authors are not trying to define a disentangled internal representation. further, some important questions like how to choose the reference endogenous variables and how to choose the subset e are left entirely to the experiments section. the definition of disentanglement proposed is however tied to these choices and a quick discussion would be helpful. ----------whereas the theory from section 2 seems precise and formal (at least in the appendix, although i did not check all the proofs), the procedure to identify modules comes with no guarantees and relies on several choices: local averaging, thresholding, nbr of clusters (the choice of this one is in my opinion well justified). taking that into account a more extensive experimental validation would be needed to demonstrate that modules can be reliably identified. the results presented on celeba and imagenet are interesting, in particular using different models is a good idea, however the evaluation relies mostly on a few cases or examples and i would have liked to see more quantitative results, e.g. like in figure 8 appendix f. ----------note on related work:-----it has been shown (isolating sources of disentanglement in vaes by duvenaud et al., disentangling by factorising by kim et al., challenging common assumptions in the unsupervised learning of disentangled representations by bachem et al.) that beta-vae is far from optimal for extrinsic disentanglement, the text in section 4.1 should take these results into account. it would also be interesting to contrast with the following paper: robustly disentangled causal mechanisms: validating deep representations for interventional robustness by bauer et al. which (whilst doing something pretty different) also treats of causality and disentanglement. the paper presents a means to uncover the modular structure of deep generative models of images using counterfactuals and presenting evidence for the fact that there are interpretable modules within current popular generative models. the paper is extremely well written with a good balance between mathematical notation and intuitive explanations. i think this paper should certainly be accepted as it provides an interesting and rigorous tool to understand the behavior and properties of deep generative.----------i have a few questions and comments:----------1) how early does this sort of modularity arise over the course of training? does it vary for gans versus beta-vae like models?----------2) i think it would be good to contrast this approach with ones that also have latent mixing strategies such as [1, 2] - these do not uncover latent modular structure, but can also produce hybridized images via linear latent space mixing unlike counterfactual assignment of latent vectors like in your work.----------3) from what i gather, this approach cannot produce hybridizations between two explicitly provided samples from a dataset, since the models you consider do not have an inference network like in bigan or ali to get the corresponding z vector for a specific image. if this were available, it would be interesting to study influence maps estimated by taking the expectation over pairs of images rather than z-space vectors. ----------4) can we quantify modularity or the extent of disentanglement of internal representations under such a framework?----------references----------[1] manifold mixup - https://arxiv.org/pdf/1806.05236.pdf-----[2] adversarial mixup resynthesis - https://arxiv.org/pdf/1903.02709v3.pdf----------minor:----------proposition 2 - typo - then and transformation applied to it","this paper provides a fresh application of tools from causality theory to investigate modularity and disentanglement in learned deep generative models. it also goes one step further towards making these models more transparent by studying their internal components. while there is still margin for improving the experiments, i believe this paper is a timely contribution to the iclr/ml community.-----this paper has high-variance in the reviewer scores. but i believe the authors did a good job with the revision and rebuttal. i recommend acceptance.",i think this paper should certainly be accepted as it provides an interesting and rigorous tool to understand the behavior and properties of deep generative.----------i have a few questions and comments:----------1) how early does this sort of modularity arise over the course of training?,"the results presented on celeba and imagenet are interesting, in particular using different models is a good idea, however the evaluation relies mostly on a few cases or examples and i would have liked to see more quantitative results, e.g. like in figure 8 appendix f. ----------note on related work:-----it has been shown (isolating sources of disentanglement in vaes by duvenaud et al., disentangling by factorising by kim et al., challenging common assumptions in the unsupervised learning of disentangled representations by bachem et al.) that beta-vae is far from optimal for extrinsic disentanglement, the text in section 4.1 should take these results into account.","does it vary for gans versus beta-vae like models?----------2) i think it would be good to contrast this approach with ones that also have latent mixing strategies such as [1, 2] - these do not uncover latent modular structure, but can also produce hybridized images via linear latent space mixing unlike counterfactual assignment of latent vectors like in your work.----------3) from what i gather, this approach cannot produce hybridizations between two explicitly provided samples from a dataset, since the models you consider do not have an inference network like in bigan or ali to get the corresponding z vector for a specific image.",the ideas presented in the paper are interesting and original.,"does it vary for gans versus beta-vae like models?----------2) i think it would be good to contrast this approach with ones that also have latent mixing strategies such as [1, 2] - these do not uncover latent modular structure, but can also produce hybridized images via linear latent space mixing unlike counterfactual assignment of latent vectors like in your work.----------3) from what i gather, this approach cannot produce hybridizations between two explicitly provided samples from a dataset, since the models you consider do not have an inference network like in bigan or ali to get the corresponding z vector for a specific image.","the results presented on celeba and imagenet are interesting, in particular using different models is a good idea, however the evaluation relies mostly on a few cases or examples and i would have liked to see more quantitative results, e.g. like in figure 8 appendix f. ----------note on related work:-----it has been shown (isolating sources of disentanglement in vaes by duvenaud et al., disentangling by factorising by kim et al., challenging common assumptions in the unsupervised learning of disentangled representations by bachem et al.) that beta-vae is far from optimal for extrinsic disentanglement, the text in section 4.1 should take these results into account.","if this were available, it would be interesting to study influence maps estimated by taking the expectation over pairs of images rather than z-space vectors.","the results presented on celeba and imagenet are interesting, in particular using different models is a good idea, however the evaluation relies mostly on a few cases or examples and i would have liked to see more quantitative results, e.g. like in figure 8 appendix f. ----------note on related work:-----it has been shown (isolating sources of disentanglement in vaes by duvenaud et al., disentangling by factorising by kim et al., challenging common assumptions in the unsupervised learning of disentangled representations by bachem et al.) that beta-vae is far from optimal for extrinsic disentanglement, the text in section 4.1 should take these results into account.",0.2878787878787879,0.0307692307692307,0.1818181818181818,0.1818181818181818,0.2680412371134021,0.0416666666666666,0.1134020618556701,0.1134020618556701,0.2722513089005235,0.0,0.1151832460732984,0.1151832460732984,0.1041666666666666,0.0212765957446808,0.0833333333333333,0.0833333333333333,0.2722513089005235,0.0,0.1151832460732984,0.1151832460732984,0.2680412371134021,0.0416666666666666,0.1134020618556701,0.1134020618556701,0.125,0.0,0.0714285714285714,0.0714285714285714,0.2680412371134021,0.0416666666666666,0.1134020618556701,0.1134020618556701,5.667054653167725,4.100822448730469,10.14067268371582,5.667054653167725,6.167632579803467,4.100822448730469,5.667054653167725,4.741772651672363,0.9736785067390914,0.9737547112097934,0.13438562314490204,0.9657261951011697,0.9545767991894611,0.49548668665575696,0.8928179549940493,0.9073877377967371,0.3477589026489847,0.9669072561155663,0.9670949213296858,0.9499966664019582,0.8928179549940493,0.9073877377967371,0.3477588236035693,0.9657261951011697,0.9545767991894611,0.4954869778529094,0.8962268534894489,0.9474823340793448,0.9107547160926781,0.9657261951011697,0.9545767991894611,0.49548707764559463
122,https://openreview.net/forum?id=SJxHMaEtwB,"summarize what the paper claims to do/contribute.-----* the paper proposes to perform domain adaptation via separating domain-specific and cross-domain features, by what is referred to as ""domain-adaptive filter decomposition"". each domain contributes its own share of features to be combined by a subsequent common layer. the method was benchmarked against competing methods on simple classification tasks and a hard semantic segmentation task.----------clearly state your decision (accept or reject) with one or two key reasons for this choice.-----weak accept.-----* i think the paper was very well written, the explanations were clear and the technical contributions seem sound.-----* the experiments were satisfying for the most part. i would have wanted to see mnist->svhn for the unsupervised case as well, as this is a particularly hard one.----------provide supporting arguments for the reasons for the decision.-----* please do not use the office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it's hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets.-----* for gta->cityscapes you are missing a few works eg, the cycada work. also please use citations in the tables if you did not yourselves run experiments (as to make it clear that experimental protocols also might be slightly different etc). ----------provide additional feedback with the aim to improve the paper. make it clear that these points are here to help, and not necessarily part of your decision assessment.-----* tables 1 & 2: it'd be better to not refer to methods as a1,2,3 but rahter with some specific names or explicitly describe what these abbreviations mean in the caption of the table. this paper introduces a way to decompose features for better domain adaptation via learning domain-invariant representations. the main advantage of the proposed approach is that by only introducing a few model parameters, the proposed approach could quickly adapt to new domains. similar idea has been proposed in [1], where the authors propose to use domain-specific encoders to extract domain-invariant features. compared with [1], the main novelty of this paper lies in a new feature decomposition of the traditional convolution layer.----------my main concern is that this paper seems to miss a significant line of recent work on learning domain-invariant representations [2-3]. basically, it has been shown that invariant representations provably hurt generalization on the target domain when the marginal label distributions are different between the source and target domains. note that such result also holds when different feature extractors are used in source and target domains, so the model proposed in this manuscript is also subject to such inherent tradeoffs. ---------------[1]. domain separation networks, nips 2016.-----[2]. on learning invariant representations for domain adaptation, icml 2019.-----[3]. domain adaptation with asymmetrically-relaxed distribution alignment, icml 2019. the paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. the approach is similar to methods that use multi-stream networks (a stream for each domain), but using the filer decomposition scheme, the authors avoid the issue of excessive increase in the number of parameters typical in fully multi-stream architectures. ----------this is achieved by learning a separate basis for convolutional filters for each domain while sharing the basis coefficients across domains. this encourages shared semantics across domains while maintaining a balance between the network expressiveness and the computational complexity.----------the authors argue that the basis learned for each domain can be understood as correction/alignment mappings to bring together the representations of each domain. a toy example presented is convincing and shows the correction basis learned in a simple synthetic case. theoretical arguments are provided to show that the proposed correction scheme covers a large range of possible domain shifts.----------the authors also show that plugging the decomposition scheme into existing cnn based unsupervised domain adaptation algorithms results in consistent improvements across methods and datasets. ----------how does the method compare to approaches that learn to adapt the representations using conditional/adaptive batch norm [1,2]? ----------overall, the paper was well motivated and easy to read. the methods appear to be a useful addition to tools available for domain invariant learning.----------[1] chang, woong-gi, tackgeun you, seonguk seo, suha kwak, and bohyung han. ""domain-specific batch normalization for unsupervised domain adaptation."" in proceedings of the ieee conference on computer vision and pattern recognition, pp. 7354-7362. 2019.-----[2] kumar, abhishek, prasanna sattigeri, kahini wadhawan, leonid karlinsky, rogerio feris, bill freeman, and gregory wornell. ""co-regularized alignment for unsupervised domain adaptation."" in advances in neural information processing systems, pp. 9345-9356. 2018.","the paper proposes a domain-adaptive filter decomposition method via separating domain-specific and cross-domain features, towards learning invariant representations for unsupervised domain adaptation.----------overall, this well-written paper is well motivated with a better technique for learning invariant representations using convolutional filters. nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain-invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method. the paper can be strengthened by further discussions on how to mitigate the aforementioned negative results. ----------hence i recommend rejection.","note that such result also holds when different feature extractors are used in source and target domains, so the model proposed in this manuscript is also subject to such inherent tradeoffs.","domain adaptation with asymmetrically-relaxed distribution alignment, icml 2019. the paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters.",----------this is achieved by learning a separate basis for convolutional filters for each domain while sharing the basis coefficients across domains.,"summarize what the paper claims to do/contribute.-----* the paper proposes to perform domain adaptation via separating domain-specific and cross-domain features, by what is referred to as ""domain-adaptive filter decomposition"".","domain adaptation with asymmetrically-relaxed distribution alignment, icml 2019. the paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters.","domain adaptation with asymmetrically-relaxed distribution alignment, icml 2019. the paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters.","the method was benchmarked against competing methods on simple classification tasks and a hard semantic segmentation task.----------clearly state your decision (accept or reject) with one or two key reasons for this choice.-----weak accept.-----* i think the paper was very well written, the explanations were clear and the technical contributions seem sound.-----* the experiments were satisfying for the most part.","domain adaptation with asymmetrically-relaxed distribution alignment, icml 2019. the paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters.",0.1710526315789473,0.0,0.0657894736842105,0.0657894736842105,0.2837837837837837,0.1232876712328767,0.1621621621621621,0.1621621621621621,0.1830985915492957,0.0142857142857142,0.1126760563380281,0.1126760563380281,0.3116883116883117,0.1842105263157894,0.2207792207792207,0.2207792207792207,0.2837837837837837,0.1232876712328767,0.1621621621621621,0.1621621621621621,0.2837837837837837,0.1232876712328767,0.1621621621621621,0.1621621621621621,0.2197802197802197,0.0222222222222222,0.1208791208791208,0.1208791208791208,0.2837837837837837,0.1232876712328767,0.1621621621621621,0.1621621621621621,9.108372688293455,9.108373641967772,15.371821403503418,9.108372688293455,8.306329727172852,10.27654266357422,9.108373641967772,9.996742248535156,0.9376619923110907,0.9435064803759892,0.887908087638085,0.8601255175495596,0.9450671404861438,0.0031887706476247603,0.9749495674415836,0.9771456673905933,0.9503441002774219,0.9724553768426386,0.9645274363510872,0.7377685807453864,0.8601255175495596,0.9450671404861438,0.0031887706476247603,0.8601255175495596,0.9450671404861438,0.003188779931176164,0.9641292410599558,0.9654786847401576,0.9139836531297936,0.8601255175495596,0.9450671404861438,0.003188785733395703
123,https://openreview.net/forum?id=SJxSOJStPr,"the paper proposes an elegant method for task-free continual learning problems. it has nicely pointed out that the conventional continual learning algorithms had the limitation of knowing the task boundaries. followings are my summary. ----------summary: -----by applying dpm, the authors proposed a method of automatically determining whether to add a new expert for a new task or train the existing experts. while the dirichlet process mixture (dpm) is not new, applying such nonparametric method to continual learning is new. the experimental results are impressive given the single-epoch setting. ----------pros:-----1. good experimental results for the task-free setting, in which no information about task boundaries is given. particularly, even with smaller memory-usage than the experience replay (er) methods, the proposed method achieves better results. -----2. many past work should suffer from increased number of tasks due to the model capacity limit, but the proposed method efficiently expands the model capacity. -----3. writing flow is good and is easy to follow. ----------cons & questions: -----1. i am not sure whether the proposed methods should work well for ""all"" cases. is there any cases in which the proposed dpm would fail?-----2. what happens when you actually know the task boundaries? would following the framework with known number of experts also excel other methods?-----3. can you apply this to the rl setting? summary: the paper proposes to use a bayesian nonparametric mixture model for task-free (without explicit task labels) continual learning. the main idea is to use an expansion-based model where the number of mixture components (experts) adapts to the training data/tasks. specifically, a dirichlet process mixture model (dpmm) consisting of a set of neural network experts is used. empirical results demonstrate improved performance on three different datasets over some of the baselines. ----------i find the methodological contribution in the paper to be somewhat limited since the main idea of the model was initially proposed in the prior work (cited in the main paper): dahual lin - online learning of nonparametric mixture models via sva. in fact, the paper claims its contribution is expansion-based task-free continual learning. however, this task-free characteristic is the contribution of sva based inference. nevertheless, i do like how the existing sva based inference has been adapted from an online learning setting to a more general continual learning setting by using various approximations/tricks (like short-term memory with wake-sleep training, point estimates). ----------pros:------ the idea of using a nonparametric model for cl is interesting and can lead to follow-up work.------ results show that the approach works well.------ the code has been released.----------overall i am inclining towards voting for acceptance if the authors could address my following questions:----------- could you comment on the creation of test data? it is not clear to me how the model is evaluated if the task-boundaries are not known a priori. shouldnt the evaluation be based on tasks? how are you evaluating catastrophic forgetting? i am interested to know what was the test accuracy for the task for which the training data was seen early on during the training.----------- it seems to me that the method works on the assumption that the number of data points for each task is at least m (size of stm) and moreover, that these data points appear together sequentially. the method should be sensitive to the size of the stm. how are you choosing m? would the framework work if data points for each task do not appear together?----------- assuming you have clear task boundaries, how would you adapt this framework? was the model compared to other methods that assume known task-boundaries like (vcl, ewc, memory replay)? ----------other comments:----------- the method is inspired by a bayesian framework but calling it bayesian wouldnt be fair since only a point estimate is being learned for parameters. this is important to distinguish since there are other methods that are fully bayesian like nguyen et. al. variational continual learning (although such methods may have other pros and cons)----------- the samples from the base distribution of the posterior (v) are not iid anymore due to lateral connections b/w the representations. do you think the theoretical result in appendix b that the number of clusters is upper bounded by o(alpha*logn) is still valid? this paper proposes continual neural dirichlet process mixture model (cn-dpm) to solve task-free continual learning. the core idea is to employ dirichlet process mixture model to create novel experts in online fashion when task distributions change. the proposed method is validated on various tasks and demonstrated to perform well compared to the other baselines.----------overall i find this paper to be well-written and the experiments are conducted thoroughly. the method is compared to proper baselines in various settings, and the paper describes detailed experimental settings and architectural choices to help readers willing to reproduce. ive gone through the appendix and they provide enough additional experiments to support the authors claim.----------the main algorithm itself cannot be considered to be novel. dpm or other bayesian nonparametric models have been extensively used for the problems requiring to adapt the model size according to the change of data. nevertheless, the application of dpm in task-free continual learning context seems to be considered as a contribution.----------i have much experience in implementing bayesian nonparametric models with parametric distributions and compared various methods to conduct the posterior inference of them. in my experience, even for the low-dimensional parametric models, the posterior inference algorithms for dpm usually suffer from local optima, and the sequential methods such as sva depends heavily on the data processing order. according to the experimental setting presented in the paper, the algorithm goes through a single pass over the data stream, yet still able to reasonably train (deep) neural networks and identify mixture components jointly. do you have any intuition about how this becomes feasible?----------i dont fully understand why generative modeling is required. in page 4 the authors stated that the generative model prevents catastrophic forgetting. but in my understanding, using expert-specific parameters is the part that prevents catastrophic forgetting, not the generative model itself. learning generative model in online fashion may work well in simple structured data such as mnist, but i highly doubt that the generative model could be trained properly for cifar10 or cifar100, especially in online setting. my concern is that learning generative model part may even impede the discriminative learning. could you elaborate more on this?----------another minor concern is the way the concentration parameter alpha is selected. the authors stated that they chose proper value of alpha according to the number of tasks known in advance. i think this does not make sense. alpha should also be inferred along with other parameters, or fixed to non-informative value if the performance of the algorithm is not very sensitive to the choice of alpha.----------i think it would be more helpful to show how the task-assignment p(z=k|x) is learned. for instance, the clustering accuracy according to p(z=k|x) against the ground-truth task label can be measured, or at least qualitatively show what examples were assigned to each task.","this paper proposes an expansion-based approach for task-free continual learning, using a bayesian nonparametric framework (a dirichlet process mixture model).----------it was well-reviewed, with reviewers agreeing that the paper is well-written, the experiments are thorough, and the results are impressive. another positive is that the code has been released, meaning its likely to be reproducible.----------the main concern shared among reviewers is the limited novelty of the approach, which i also share. reviewers all mentioned that the approach itself isnt novel, but they like the contribution of applying it to task-free continual learning. this wasnt mentioned, but im concerned about the overlap between this approach and curl (rao et al 2019) published in neurips 2019, which also deals with task-free continual learning using a generative, nonparametric approach. could the authors comment on this in their final version?----------in sum, it seems that this paper is well-done, with reproducible experiments and impressive results, but limited novelty. given that reviewers are all satisfied with this, im willing to recommend acceptance.",----------pros:------ the idea of using a nonparametric model for cl is interesting and can lead to follow-up work.------ results show that the approach works well.------ the code has been released.----------overall i am inclining towards voting for acceptance if the authors could address my following questions:----------- could you comment on the creation of test data?,----------i find the methodological contribution in the paper to be somewhat limited since the main idea of the model was initially proposed in the prior work (cited in the main paper): dahual lin - online learning of nonparametric mixture models via sva.,"i am interested to know what was the test accuracy for the task for which the training data was seen early on during the training.----------- it seems to me that the method works on the assumption that the number of data points for each task is at least m (size of stm) and moreover, that these data points appear together sequentially.",the paper proposes an elegant method for task-free continual learning problems.,this paper proposes continual neural dirichlet process mixture model (cn-dpm) to solve task-free continual learning.,summary: the paper proposes to use a bayesian nonparametric mixture model for task-free (without explicit task labels) continual learning.,"however, this task-free characteristic is the contribution of sva based inference.",summary: the paper proposes to use a bayesian nonparametric mixture model for task-free (without explicit task labels) continual learning.,0.2510822510822511,0.0786026200873362,0.1558441558441558,0.1558441558441558,0.2314814814814814,0.0467289719626168,0.1111111111111111,0.1111111111111111,0.211864406779661,0.0256410256410256,0.1271186440677966,0.1271186440677966,0.0962566844919786,0.0756756756756756,0.0855614973262032,0.0855614973262032,0.1354166666666666,0.0842105263157894,0.1354166666666666,0.1354166666666666,0.1641025641025641,0.0932642487046632,0.1333333333333333,0.1333333333333333,0.0855614973262032,0.0432432432432432,0.0748663101604278,0.0748663101604278,0.1641025641025641,0.0932642487046632,0.1333333333333333,0.1333333333333333,10.22339153289795,14.165781021118164,14.769376754760742,7.640166282653809,5.569618225097656,6.23710823059082,10.22339153289795,7.311118125915527,0.9913003511190215,0.9871485018824371,0.9153517965725949,0.9744151556081555,0.9686555109994653,0.8864881248369226,0.9686937107159658,0.9686426438353651,0.9377658241595015,0.9603449103780861,0.9629237997759228,0.814357642231403,0.8564547033463782,0.8388510651480889,0.8472612209698912,0.9679087032177128,0.965175379794865,0.9442448803531015,0.9628121586471948,0.9534827035702389,0.8276346215309331,0.9679087032177128,0.965175379794865,0.9442450032757612
124,https://openreview.net/forum?id=SVP44gujOBL,"summary: this paper studies curriculum learning and proposes two methods to order the examples by (1) gradient information and (2) statistical measures like standard derivation and entropy. the experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works.-----strengths:-----the proposed ""dynamic curriculum algorithm"" can speed up the convergence by ~45% and the proposed task-specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%.-----the code and data are shared and helpful for reproducing the experiments conducted in the paper.-----weaknesses:-----the paper is poorly written. there are even no sections to discuss related works and experimental settings in the main paper. although some related works are discussed scatteredly in the paper, it might be helpful to have a specific section to compare related works with the proposed methods, which makes it much easier to identify the contributions and novelty of this work. besides, although i found the experimental settings in the supplementary, the main paper at least should have discussed the basic experimental setup to understand how the experiments are conducted.-----the proposed dcl algorithm requires an optimal weight or a local optimal weight to calculate the difficulty scores. this requirement is unreasonable and renders the proposed methods useless.-----the proposed scoring function (the equation at the end of page 3) requires to compute the gradients on each sample. perform back-propagation and computing gradient are highly prohibitive. the learning curves against the time cost should also be reported, in complement to the learning curves against the training steps in fig.1.-----the pace function is just a constant, dependent on a tunable hyperparameter k. from fig.2, it seems that the value of k has a large impact on the testing accuracy. it is not mentioned in the paper how the value of k is selected.-----suggestions for improvement:-----it might be better to have a specific section to discuss related works and compare them with the proposed method.-----index all equations.-----questions: the questions to be addressed in the rebuttal are listed below:-----where does the optimal weight in dcl comes from? can the authors justify why a given optimal weight can be used during the training?-----what is the time cost of the scoring function?-----how is the value of k in the pace function tuned?------------------------------------post-rebuttal------------------------------------thank you for revising the submission and the clarification in the rebuttal. after reading the rebuttal and other reviews, my main concerns about the novelty and computation cost are still unsolved. therefore, i will keep my original score. the paper contains two curriculum learning algorithms of which one assume knowledge of the parameters found by the baseline, uniform-sampling, model to push updates in that direction, and the second orders images according to an increasing stddev/entropy of pixels. while the first approach is impractical because of the strong assumption, the second approach demonstrates small gains that lie within random variance (fig. 5, fig. 6) and would be not straight-forward to apply to non-image data e.g. text. these reasons make the paper hard to accept.-----the main problem is knowing the parameters of the baseline, sgd, optimization. it's not clear why would one even need optimization again, if (a good enough) result is already known and gains from this re-optimization do not significantly improve over this baseline. the speedups mentioned in the abstract (45% and 43%) could not be located in the results in main body of the paper. how were they measured? even if aligning updates with the sgd-trained parameters does speed up convergence, re-training from scratch will cost 143% of baseline time instead of 43%, as the standard training needs to be counted too.-----issues include:-----how to sample using \rho_{t,i}? it's not a distribution and can be negative.-----figure 1: judging from the plot, the vanilla curve converges faster than the curriculum. how can one see the >40% curriculum speed up?-----abstract's claim of removing noise is only supported in section 4 through citing related works. also, more evidence would be needed to call k a regularizer.-----lines 9-10 in algorithm 1 would interfere with bucketing in seq2seq applications and adversely affect performance.-----regarding related work in sec. 3: i couldn't confirm in (graves et al, 2017) that they also sort examples by difficulty.-----the last approach to define curriculum through statistical quantities makes sense, in principle, although the difference between curves in fig. 5 is very small and could be caused by random variance as the error bars on fig. 5 and fig. 6 show. another problem is that it's straight-forwardly applicable only to images and not categorical data, like text.-----one suggestion of possible paper improvement: consider swapping and reworking sections 5 and 3, so that the content of sec. 5 becomes the main proposal and a reworked sec. 3 - its analysis. there one could analyze if the example ordering according to stddev does bias updates towards some ""good"" point of convergence, with one possible definition of ""good"" according to (now, unknown during optimization) sgd results.-----other minor remarks:-----""greedy approach"" is mentioned multiple times before being explained on page 4. consider deferring the use of term to that place.-----contributions: useful is a vacuous word, consider dropping it.-----notation: square brackets used to denote several objects - sequences [b1, b2, ..] , ranges [t] and vectors [x1, x2, .. ]. using different brackets could be better.-----well-known concepts:-----no need to define stddev and mean in (2)-----(arora, 1981): if entropy requires a citation at all then citing shannon directly would be more appropriate.-----sec. 2: curriculum is defined by two functions -> we define curriculum by two functions-----conclusion: display -> show-----while cl indicate that -> while cl indicates that-----judicial ordering -> judicious ordering-----=== after rebuttal ===-----thank you for your answers. i'm keeping the rating at 3.-----1)+2). i'm still not convinced that it's fair to claim an improvement of x% for a curriculum that, relying on final weights of a ""vanilla"" sgd-trainedmodel, converges in additional x% to the 100% of ""vanilla"" time.-----3). fair enough, but the revised draft still reads like examples are sampled from it.-----i double checked the context of citing (graves et al, 2017) and believe it's still imprecise as in the original draft.","this paper proposed two algorithms for curriculum learning, one based on the the knowledge of a good solution (e.g. a local minima or a solution found by sgd) and another one proposed for natural image datasets based on entropy and standard deviation over pixels.-----reviewers seem to like the ideas behind the proposed algorithms and their simplicity. however, there are several major concerns that are shared among reviewers: 1- one of the algorithms needs knowledge of a good solution (e.g. a local minima or a solution found by sgd) which makes it impractical and the other one doesn't use any information about the mapping between input and the label. 2- discussing previous work on curriculum learning, explaining how proposed algorithms are different than previous work and empirical comparison to other curriculum learning methods are lacking or need a significant improvement. 3- the experiment section needs improvement both in terms of experimental methodology and having more tasks/datasets.-----reviewers have done a great job at pointing to specific areas that need improvement. i hope authors would use reviewers' comments to improve their work.-----given the above major concerns, i recommend rejecting this paper.","the paper contains two curriculum learning algorithms of which one assume knowledge of the parameters found by the baseline, uniform-sampling, model to push updates in that direction, and the second orders images according to an increasing stddev/entropy of pixels.","the experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works.-----strengths:-----the proposed ""dynamic curriculum algorithm"" can speed up the convergence by ~45% and the proposed task-specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%.-----the code and data are shared and helpful for reproducing the experiments conducted in the paper.-----weaknesses:-----the paper is poorly written.","the experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works.-----strengths:-----the proposed ""dynamic curriculum algorithm"" can speed up the convergence by ~45% and the proposed task-specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%.-----the code and data are shared and helpful for reproducing the experiments conducted in the paper.-----weaknesses:-----the paper is poorly written.",summary: this paper studies curriculum learning and proposes two methods to order the examples by (1) gradient information and (2) statistical measures like standard derivation and entropy.,summary: this paper studies curriculum learning and proposes two methods to order the examples by (1) gradient information and (2) statistical measures like standard derivation and entropy.,"the experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works.-----strengths:-----the proposed ""dynamic curriculum algorithm"" can speed up the convergence by ~45% and the proposed task-specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%.-----the code and data are shared and helpful for reproducing the experiments conducted in the paper.-----weaknesses:-----the paper is poorly written.","it's not a distribution and can be negative.-----figure 1: judging from the plot, the vanilla curve converges faster than the curriculum.","the experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works.-----strengths:-----the proposed ""dynamic curriculum algorithm"" can speed up the convergence by ~45% and the proposed task-specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%.-----the code and data are shared and helpful for reproducing the experiments conducted in the paper.-----weaknesses:-----the paper is poorly written.",0.2118644067796609,0.0427350427350427,0.1271186440677966,0.1271186440677966,0.3285714285714285,0.0647482014388489,0.1642857142857142,0.1642857142857142,0.3285714285714285,0.0647482014388489,0.1642857142857142,0.1642857142857142,0.1711711711711711,0.0272727272727272,0.1171171171171171,0.1171171171171171,0.1711711711711711,0.0272727272727272,0.1171171171171171,0.1171171171171171,0.3285714285714285,0.0647482014388489,0.1642857142857142,0.1642857142857142,0.0825688073394495,0.0,0.0642201834862385,0.0642201834862385,0.3285714285714285,0.0647482014388489,0.1642857142857142,0.1642857142857142,14.734071731567385,11.512094497680664,11.512094497680664,14.734071731567385,9.097650527954102,14.734071731567385,14.734071731567385,9.174449920654297,0.9696824008042078,0.9652186827857627,0.8583962551596931,0.9868245476248223,0.9860843125278675,0.9071434772549587,0.9868245476248223,0.9860843125278675,0.9071434772549587,0.9682237919498947,0.9679352134767434,0.9219410187943216,0.9682237919498947,0.9679352134767434,0.9219410119252274,0.9868245476248223,0.9860843125278675,0.9071436437784091,0.9473683300765653,0.9574394161582976,0.7453688313844954,0.9868245476248223,0.9860843125278675,0.9071434772549587
125,https://openreview.net/forum?id=SkYMnLxRW,"this paper describes an extension to the recently introduced transformer networks which shows better convergence properties and also improves results on standard machine translation benchmarks. ----------------this is a great paper -- it introduces a relatively simple extension of transformer networks which only adds very few parameters and speeds up convergence and achieves better results. it would have been good to also add a motivation for doing this (for example, this idea can be interpreted as having a variable number of attention heads which can be blended in and out with a single learned parameter, hence making it easier to use the parameters where they are needed). also, it would be interesting to see how important the concatenation weight and the addition weight are relative to each other -- do you possibly get the same results even without the concatenation weight? ----------------a suggested improvement: please check the references in the introduction and see if you can find earlier ones -- for example, language modeling with rnns has been done for a very long time, not just since 2017 which are the ones you list; similar for speech recognition etc. (which probably has been done since 1993!).----------------addition to the original review: your added additional results table clarifies a lot, thank you. as for general references for rnns, i am not sure hochreiter & schmidhuber 1997 is a good reference as this only points to a particular type of rnn that is used today a lot. for speech recognition there are many better citations as well, check the conference proceedings from icassp for papers from microsoft, google, ibm, which are the leaders in speech recognition technology. however, i know citations can be difficult to get right for everybody, just try to do your best. tl;dr of paper: they modify the transformer architecture of vaswani et al. (2017) to used branched attention with learned weights instead of concatenated attention, and achieve improved results on machine translation.----------------using branches instead of a single path has become a hot architecture choice recently, and this paper applies the branching concept to multi-head attention. weirdly, they propose using two different sets of weights for each branch: (a) kappa, which premultiplies the head before fully connected layers, and (b) alpha, which are the weights of the sum of the heads after the fully connected layers. both weights have simplex constraints. a couple of questions about this:----------------* what is the performance of only using kappa? only alpha? neither? what happens if i train only of them?--------* what happens if you remove the simplex constraints (i.e., don't have to sum to one, or can be negative)?--------* why learn a global set of weights for the branch combiners? what happens if the weights are predicted for each input example? this is the moe experiment, but where k = m (i.e., no discrete choices made).--------* are the ffn layer parameters shared across the different heads?--------* at the top of page 4, it is said ""all bounds are respected during each training step by projection"". what does this mean? is projected gradient descent used, or is a softmax used? if the former, why not use a softmax?--------* in figure 3, it looks like the kappa and alpha values are still changing significantly before they are frozen. what happens if you let them train longer? on the same note, the claim is that transformer takes longer to train. what is the performance of transformer if using the same number of steps as the weighted transformer?--------* what are the transformer variants a, b, and c?----------------while the results are an improvement over the baseline transformer, my main concern with this paper is that the improved results are because of extensive hyperparameter tuning. design choices like having a separate learning rate schedule for the alpha and kappa parameters, and needing to freeze them at the end of training stoke this concern. i'm happy to change my score if the authors can provide empirical evidence for each design choice the paper presentes a small extension to the neural transformer model of vaswani et al 2017:--------the multi-head attention computation (eq. 2,3):--------head_i = attention_i(q,k,w)--------multihead = concat_i(head_i) * w = \sum_i head_i * w_i----------------is replaced with the so-called branchedattention (eq. 5,6,7,4):--------head_i = attention_i(q,k,w) // same as in the base model--------branchedattention = \sum_i \alpha_i max(0, head_i * w_i * kappa_i * w^1 + b^1) w^2 + b^2----------------the main difference is that the results of application of each attention head is post-processed with a 2-layer relu network before being summed into the aggregated attention vector.----------------my main problem with the paper is understanding what really is implemented: the paper states that with alpha_i=1 and kappa_i=1 the two attention mechanism are equivalent. the equations, however, tell a different story: the original multihead attention quickly aggregates all attention heads, while the proposed branchedattention adds another processing step, effectively adding depth to the model.----------------since the branchedattention is the key novelty of the paper, i am confused by this contradiction and treat it as a fatal flaw of this paper (i am willing to revise my score if the authors explain the equations) - the proposed attention either adds a small amount of parameters (the alphas and kappas) that can be absorbed by the other weights of the network, and the added alphas and kappas are easier/faster to optimize, as the authors state in the text, or the branchedattention works as shown in the equations, and effectively adds depth to the network by processing each attention's result with a small mlp before combining multiple attention heads. this has to be clarified before the paper is published.----------------the experiment show that the proposed change speeds convergence and improves the results by about 1 bleu point. however, this requires a different learning rate schedule for the introduced parameters and some non-standard tricks, such as freezing the alphas and kappas during the end of the training.----------------i also have a questions about the presented results:--------1) the numbers for the original transformer match the ones in vaswani et al 2017, am i correct to assume that the authors did not rerun the tensor2tensor code and simply copied them from the paper?--------2) is all of the experimental setup the same as in vaswani et al 2017? are the results obtained using their tensor2tensor implementation, or are some hyperparameters different?----------------detailed review:--------quality:--------the equations and text in the paper contradict each other.----------------clarity:--------the language is clear, but the main contribution could be better explained.----------------originality:--------the proposed change is a small extension to the neural transformer model.----------------significance:--------rather small, the proposed addition adds little modeling power to the network and its advantage may vanish with more data/different learning rate schedule.----------------pros and cons:--------+ the proposed approach is a simple way to improve the performance of multihead attentional models.--------- it is not clear from the paper how the proposed extension works: does it regularize the model or dies it increase its capacity?","the paper proposes a modification to the transformer network, which mostly consists in changing how the attention heads are combined. the contribution is incremental, and its novelty is limited. the results demonstrate an improvement over the baseline at the cost of a more complicated training procedure with more hyper-parameters, and it is possible that with similar tuning the baseline performance could be improved in a similar way.",i'm happy to change my score if the authors can provide empirical evidence for each design choice the paper presentes a small extension to the neural transformer model of vaswani et al 2017:--------the multi-head attention computation (eq.,"the equations, however, tell a different story: the original multihead attention quickly aggregates all attention heads, while the proposed branchedattention adds another processing step, effectively adding depth to the model.----------------since the branchedattention is the key novelty of the paper, i am confused by this contradiction and treat it as a fatal flaw of this paper (i am willing to revise my score if the authors explain the equations) - the proposed attention either adds a small amount of parameters (the alphas and kappas) that can be absorbed by the other weights of the network, and the added alphas and kappas are easier/faster to optimize, as the authors state in the text, or the branchedattention works as shown in the equations, and effectively adds depth to the network by processing each attention's result with a small mlp before combining multiple attention heads.","the equations, however, tell a different story: the original multihead attention quickly aggregates all attention heads, while the proposed branchedattention adds another processing step, effectively adding depth to the model.----------------since the branchedattention is the key novelty of the paper, i am confused by this contradiction and treat it as a fatal flaw of this paper (i am willing to revise my score if the authors explain the equations) - the proposed attention either adds a small amount of parameters (the alphas and kappas) that can be absorbed by the other weights of the network, and the added alphas and kappas are easier/faster to optimize, as the authors state in the text, or the branchedattention works as shown in the equations, and effectively adds depth to the network by processing each attention's result with a small mlp before combining multiple attention heads.",this paper describes an extension to the recently introduced transformer networks which shows better convergence properties and also improves results on standard machine translation benchmarks.,"the equations, however, tell a different story: the original multihead attention quickly aggregates all attention heads, while the proposed branchedattention adds another processing step, effectively adding depth to the model.----------------since the branchedattention is the key novelty of the paper, i am confused by this contradiction and treat it as a fatal flaw of this paper (i am willing to revise my score if the authors explain the equations) - the proposed attention either adds a small amount of parameters (the alphas and kappas) that can be absorbed by the other weights of the network, and the added alphas and kappas are easier/faster to optimize, as the authors state in the text, or the branchedattention works as shown in the equations, and effectively adds depth to the network by processing each attention's result with a small mlp before combining multiple attention heads.","the equations, however, tell a different story: the original multihead attention quickly aggregates all attention heads, while the proposed branchedattention adds another processing step, effectively adding depth to the model.----------------since the branchedattention is the key novelty of the paper, i am confused by this contradiction and treat it as a fatal flaw of this paper (i am willing to revise my score if the authors explain the equations) - the proposed attention either adds a small amount of parameters (the alphas and kappas) that can be absorbed by the other weights of the network, and the added alphas and kappas are easier/faster to optimize, as the authors state in the text, or the branchedattention works as shown in the equations, and effectively adds depth to the network by processing each attention's result with a small mlp before combining multiple attention heads.","is projected gradient descent used, or is a softmax used?","the equations, however, tell a different story: the original multihead attention quickly aggregates all attention heads, while the proposed branchedattention adds another processing step, effectively adding depth to the model.----------------since the branchedattention is the key novelty of the paper, i am confused by this contradiction and treat it as a fatal flaw of this paper (i am willing to revise my score if the authors explain the equations) - the proposed attention either adds a small amount of parameters (the alphas and kappas) that can be absorbed by the other weights of the network, and the added alphas and kappas are easier/faster to optimize, as the authors state in the text, or the branchedattention works as shown in the equations, and effectively adds depth to the network by processing each attention's result with a small mlp before combining multiple attention heads.",0.2222222222222222,0.0377358490566037,0.1481481481481481,0.1481481481481481,0.3033175355450236,0.0287081339712918,0.1706161137440758,0.1706161137440758,0.3033175355450236,0.0287081339712918,0.1706161137440758,0.1706161137440758,0.2150537634408602,0.0659340659340659,0.1720430107526881,0.1720430107526881,0.3033175355450236,0.0287081339712918,0.1706161137440758,0.1706161137440758,0.3033175355450236,0.0287081339712918,0.1706161137440758,0.1706161137440758,0.0769230769230769,0.0,0.0769230769230769,0.0769230769230769,0.3033175355450236,0.0287081339712918,0.1706161137440758,0.1706161137440758,6.605179786682129,6.605179786682129,13.119144439697266,6.605179786682129,6.14279842376709,6.605179786682129,6.605179786682129,6.888465881347656,0.1689153408876866,0.1647705578119889,0.022082127995760443,0.8530260479085415,0.9128315277476948,0.9475255356064505,0.8530260479085415,0.9128315277476948,0.9475255356064505,0.9831868267012648,0.9766675675997613,0.9552936898744221,0.8530260479085415,0.9128315277476948,0.9475255356064505,0.8530260479085415,0.9128315277476948,0.947525514829554,0.9185758431813393,0.9256104954448705,0.871946821537707,0.8530260479085415,0.9128315277476948,0.947525514829554
126,https://openreview.net/forum?id=SkeGvaEtPr,"i think their strong point is at the same time their weak point. "", they do not rely on explicitly specified first-order logic rules."" my question would be, why not use that information if available as prior knowledge? perhaps i'd like to see a stronger motivation for the use of having to learn this part. i can see how its might be useful but would be very happy to see more motivation for this. i think i've seen kotler learn the rules of sudoku but sudoku is such a specified problem that im not convinced yet this is usueful to learn. thats a different paper but i missed the motivation for that here too.----------i like the honesty of the authors for saying it doesn't scale to larger problems. regardless, i think this paper is good to push the field in that direction. i particularly like the graph generation task. graph generation, afaik, is not easy. ----------what i invite the authors to do is to not be restricted by theoretically/principled motivated ways. i believe its better to find things that work well first and then to find a theory (the other way round). this is not enough to reject the paper for me because i do believe this is pushing the field forward in a good direction. if possible i'd suggest to relax the theory and then compare the two models if possible.----------in the contribution it says ""(i) we introduce a new statistical relational model, which-----overcomes actual limitations of both classical and recent related models such as "" i would have really liked it to have been spelt out which limitations, very specifically and concisely the paper overcomes in that section. this paper presents neural markov logic networks (nmln), which is a generalization of markov logic networks (mln). unlike mln which relies on pre-specified first-order logic (fol) rules, nmln learns potential functions parameterized by neural networks on fragments of the graph. the potential function can possibly take into account the constants present using embeddings to better solve transductive problemsotherwise the potential can only use relational structure). to make computation tractable, the size of local potential functions is constrained. training of this mrf is performed by solving a min-max entropy problem: conditioned on an informative potential, the uncertainties shall be decreased. experiments on a knowledge base completion task and a graph generation task show superior performance compared to baselines like neural theorem provers.----------pros:-----1. no need to specify fol rules and can potentially discover subtle relations not evident to us.-----2. can be used for generation since the learned rules might be more fine-grained than what we can specify.-----3. it's interesting that on nations knowledge base completion problem even without constant embeddings it works fine, which shows the power of just using relational structure.----------cons:-----1. the computation complexity of the global potential function grows combinatorally with the clique size k and polynomially with graph size n, which is unrealistic to any larger graphs than the small molecules, if any higher order statistics matters (e.g. in molecules there are rings).----------questions:-----1. for training can we use mle?----------overall this is an interesting work. i think it is a natural generalization of markov logic networks and works on two small problems. i am inclined to recommend this paper to the community.--------------------updates after reading rebuttal----------thanks for the clarification. i don't have further questions. the following paper provides an extension to markov logic networks(mlns), by removing their dependency on pre-defined first-order logic rules. this is handled via neural networks which are able to capture the statistical relations, so-called neural markov logic networks(nmlns). as this is an implicit representation from the neural network, the rules act as potential functions on the mln structure. as general mln techniques are reliant on domain experts or exhaustive structure learning approaches, nmlns are able to model more domains as provided in the work with knowledge-base completion tasks and generative modelling of molecules. ----------the primary contribution in this body of work is based on the observation that relational structure repeats regularities in the data, and where deriving the statistics of these regularities is what allows for improved accuracy in a model. the proposed nmln is architecturally identical to mlns with the difference being the addition of the potential function.----------as defined by the paper, fragments are connected subsets derived from relational data. the authors derived sets of fragments with constants defined by values in the data and anonymized fragment sets with integer assignments. with potential functions sampled from the anonymized and the true value fragments. the objective is a search for a maximum-entropy distribution to model the data derived fragments. the neural network aspect comes in the form of the minimum-maximum entropy modelling with weights for given fragments being learned by minimising the entropy of the fragment potential function. where the model also maximizes the log-likelihood related to the anonymized fragments. the intuition in this work is that by selecting the maximum entropy distribution while also minimizing it by selecting the most informative statistical information for it, we will derive an accurate probability distribution given the possible worlds.-----overall, the paper performs a well enough job explaining the technical aspects. it does a thorough job explaining the algorithmic detail in the main body, and the appendix provides clear and implementable pseudocode equations. it is not exactly clear to me why the anonymization of fragments is necessary, but the authors suggest this places a greater focus on the graph structure and minimizes the model acting differently with different constants. the min-max entropy modelling also appears to be a novel approach in terms of statistical relational modelling. the results also demonstrate the success of nmlns modelling on relational data and kb completion.----------regarding the technical aspects, a few concerns are the claims that the domains for experimentation seem rather trivial since the smoking and nations dataset are common relational datasets, and the strength of this model is the ability to learn on other domains. this is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper. it is also difficult to measure the success of their model with generative modelling as no baseline was present for the molecule experiments. i still view this paper as a positive contribution to mln research, with a technique that is successful among the few experiments tested.----------at the same time, however, given the emerging contributions in the area, such as relational neural networks (poole et al.), although the technical development is sensible, i dont find the contribution that technically novel, and seems somewhat straightforward. given the preliminary investigations, this is a reject from my side.","this paper on extending mlns using nns is borderline acceptable: one reviewer is strongly opposed, although i confess i don't really understand their response to the rebuttal or see what the issue with novelty is (a position shared by the other reviewers). i'm not sure how to weigh this review, but there is not a lot of signal in favour of rejection aside from the rating.----------the remaining two reviews are in favour of acceptance, with their enthusiasm only bounded by the lack of scalability of the method, something they appreciate the authors are upfront about. my view is this paper brings something new to the table which will interest the community, but doesn't oversell the result.----------given the distribution of papers in my area, this one is just a little too borderline to accept, but this is primarily a reflection of the number of high-quality papers reviewed and the limited space of the conference. i have no doubt this paper will be successful at another conference, and it's a bit of a shame we were not in a position to accept it to this one.","unlike mln which relies on pre-specified first-order logic (fol) rules, nmln learns potential functions parameterized by neural networks on fragments of the graph.","if possible i'd suggest to relax the theory and then compare the two models if possible.----------in the contribution it says ""(i) we introduce a new statistical relational model, which-----overcomes actual limitations of both classical and recent related models such as "" i would have really liked it to have been spelt out which limitations, very specifically and concisely the paper overcomes in that section.","unlike mln which relies on pre-specified first-order logic (fol) rules, nmln learns potential functions parameterized by neural networks on fragments of the graph.",i think their strong point is at the same time their weak point.,"this paper presents neural markov logic networks (nmln), which is a generalization of markov logic networks (mln).","unlike mln which relies on pre-specified first-order logic (fol) rules, nmln learns potential functions parameterized by neural networks on fragments of the graph.","to make computation tractable, the size of local potential functions is constrained.",the neural network aspect comes in the form of the minimum-maximum entropy modelling with weights for given fragments being learned by minimising the entropy of the fragment potential function.,0.0555555555555555,0.0093457943925233,0.037037037037037,0.037037037037037,0.1945525291828794,0.0078431372549019,0.1167315175097276,0.1167315175097276,0.0555555555555555,0.0093457943925233,0.037037037037037,0.037037037037037,0.0588235294117647,0.0,0.0490196078431372,0.0490196078431372,0.0673076923076923,0.0194174757281553,0.0576923076923076,0.0576923076923076,0.0555555555555555,0.0093457943925233,0.037037037037037,0.037037037037037,0.0394088669950738,0.0,0.0394088669950738,0.0394088669950738,0.1085972850678733,0.0182648401826484,0.0814479638009049,0.0814479638009049,7.679530143737793,5.912908554077148,5.726035118103027,4.256936073303223,7.679530143737793,7.679531574249268,5.371980667114258,5.585454940795898,0.9374739860885216,0.9533215987655346,0.8993200714148238,0.8667072123019992,0.9034975132640928,0.4881690912707399,0.9374739860885216,0.9533215987655346,0.8993202837349346,0.8687729880796893,0.874210044988208,0.9121776522134721,0.9435980233405497,0.9592794213080034,0.9596944766042486,0.9374739860885216,0.9533215987655346,0.8993200714148238,0.8861239374190977,0.932483786608338,0.917900851911681,0.7542879255288926,0.8699567283493254,0.9152971642143997
127,https://openreview.net/forum?id=SkfhIo0qtQ,"the work concerns convolution in the unit sphere. it differentiates itself from previously mentioned work by working in the volume space and not the surface space. while i can't say i understand all of the implications of the work, i was left with several questions. many of these questions are in regards to claims made by the authors whose answer or reference was not made clear.----------------- it was not made clear why there is a benefit to convolving the object in the unit sphere vs the unit cube, especially given that the work was not able to perform better than other work that was based on the unit sphere. this point was the stated problem of the paper. although it was mentioned that the unit sphere preserves all of the points of the object, it isn't clear if the transformation causes any deformations of the object. furthermore, the fixing of one axis seems to be a way to hack around problems of increased dimensionality, but there was no justification given.----------------- how does the number of trainable layers help to differentiate resource usage. wouldn't a better measure be number of parameters? the authors make that claim that shallowness is a virtue, but there is little discussion as to the size of each layer in comparable terms.----------------- why was no ""ablation"" or ""accuracy vs trained layers"" data shown for the modelnet40 dataset? i would think that would be stronger evidence than for the modelnet10 data.----------------- why wasn't the 1d conv net used for creating the viewing angles included in the size of the architecture? was there a verification as to what the filters from this network were actually giving? the authors mention how we should interpret them, but not enough information about the structure of the network is given to satisfy this question.----------------- i would have liked to see a description of the types of features that are found by these networks.----------------- the authors say they are only going to show experiments on one possible use case, but then make claims for other use cases. i am referencing that since the texture data in the datasets used is constant, there was no need to model the texture data. there is no experimental evidence to show this is the case, however.----------------overall, i think the paper would have been stronger if it had more experiments. volumetric convolution, automatic representation learning in unit ball----------------this work proposes to tackle the challenging problem of learning on unit balls. the method uses volumetric convolutions based on the zernike polynomial trick, which makes it convenient to use on convolution networks. invariance to 3d rotation enables a transformation to a volumetric space, where convolutions could be used in a conventional process. clarity of the methodology may benefit from a motivation discussed from a global perspective. the reader is currently facing heavy mathematical concepts fairly quickly with global rationale on the proposed choices, in particular, in the explanation of symmetry analysis. clarity on the use of 2d and 3d features could also benefit from more details on what is exactly proposed. results are shown on an object recognition task achieving performance comparable with the state-of-the-art. ----------------positive--------+ tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls--------+ the contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls----------------specific comments--------- how to handle mixed topologies, for instance, with random presence of holes in the meshes--------- extension beyond unit balls?--------- fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - does this pose a problem - how to handle these metric distorsion?--------- zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics? beyond spherical shapes?","using volumetric convolutions, this paper focuses on learning in (rather than on) the unit sphere. the novelty of the approach is debatable, and the mathematical analysis not strong enough to merit that. in combination with good but not outstanding results, interest of the research community is doubted. an extended experimental analysis of the method would greatly improve the paper.","----------------positive--------+ tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls--------+ the contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls----------------specific comments--------- how to handle mixed topologies, for instance, with random presence of holes in the meshes--------- extension beyond unit balls?--------- fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - does this pose a problem - how to handle these metric distorsion?--------- zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics?","----------------positive--------+ tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls--------+ the contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls----------------specific comments--------- how to handle mixed topologies, for instance, with random presence of holes in the meshes--------- extension beyond unit balls?--------- fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - does this pose a problem - how to handle these metric distorsion?--------- zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics?","the authors make that claim that shallowness is a virtue, but there is little discussion as to the size of each layer in comparable terms.----------------- why was no ""ablation"" or ""accuracy vs trained layers"" data shown for the modelnet40 dataset?",the work concerns convolution in the unit sphere.,"volumetric convolution, automatic representation learning in unit ball----------------this work proposes to tackle the challenging problem of learning on unit balls.","----------------positive--------+ tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls--------+ the contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls----------------specific comments--------- how to handle mixed topologies, for instance, with random presence of holes in the meshes--------- extension beyond unit balls?--------- fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - does this pose a problem - how to handle these metric distorsion?--------- zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics?","furthermore, the fixing of one axis seems to be a way to hack around problems of increased dimensionality, but there was no justification given.----------------- how does the number of trainable layers help to differentiate resource usage.","----------------positive--------+ tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls--------+ the contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls----------------specific comments--------- how to handle mixed topologies, for instance, with random presence of holes in the meshes--------- extension beyond unit balls?--------- fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - does this pose a problem - how to handle these metric distorsion?--------- zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics?",0.2164948453608247,0.0416666666666666,0.1237113402061855,0.1237113402061855,0.2164948453608247,0.0416666666666666,0.1237113402061855,0.1237113402061855,0.202020202020202,0.0,0.1212121212121212,0.1212121212121212,0.1791044776119403,0.0615384615384615,0.1492537313432835,0.1492537313432835,0.25,0.0512820512820512,0.2,0.2,0.2164948453608247,0.0416666666666666,0.1237113402061855,0.1237113402061855,0.1473684210526316,0.0,0.1263157894736842,0.1263157894736842,0.2164948453608247,0.0416666666666666,0.1237113402061855,0.1237113402061855,8.55827522277832,10.436657905578612,19.93236541748047,8.55827522277832,8.55827522277832,5.337862968444824,8.55827522277832,7.228836059570312,0.8710103761371704,0.9099500150841895,0.9029425419793085,0.8710103761371704,0.9099500150841895,0.9029425419793085,0.9642860591431734,0.9572409991440026,0.8994839157161332,0.9616445172143647,0.9672441009367697,0.8381864404117596,0.9799369729065261,0.9774366148140312,0.32829748129512004,0.8710103761371704,0.9099500150841895,0.9029425419793085,0.9616752409802627,0.9576069462966463,0.546160299082262,0.8710103761371704,0.9099500150841895,0.9029425419793085
128,https://openreview.net/forum?id=Skg8gJBFvr,"this paper presents a new method for adversarial certification using non-gaussian noise. a new framework for certification is proposed, which allows to use different distributions compared to previous work based on gaussian noise. from this framework, a trade-off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade-off than with gaussian noise. using these new distributions, they re-certify models obtained in previous work.----------i am hesitating between a weak reject and a weak accept. the theoretical results are interesting, showing a clear trade-off between robustness and accuracy with a new lower bound and deriving better smoothing distributions. however, the experimental results are lacking, and do not support much the proposed method. training with this new distribution would have been a natural experiment given the argument. moreover, the results for l_inf are partial and it would be expected to have some results for imagenet as claimed in the introduction. i would have given an accept if the previous points had been addressed and i feel that with some more work on it, it would become an excellent paper.----------main arguments:-----my main concern is about the experiments: why were cohen et al.s models used instead of salman et al.s? salman et al.s have achieved better certified accuracy under the l_2 norm so it would only seem natural to use their model.-----about the main results: there seems to be a discrepancy between the results reported for cohen et al. and the original paper for both cifar-10 and imagenet l2 certification. also, the reported certified accuracy for salman et al.s model for l_inf on cifar-10 reported in the original paper is 68.2 at 2/255, which is very far from the 58 in table 3. what is the reason for these differences?----------minor comments:-----in the third paragraph, it is claimed that l_inf attacks are a stronger and more relevant type of attacks than l_2 attacks. these two different objectives cannot be compared in those terms.-----defenses such as adversarial training have not been broken as claimed in section 2 in the sense that the claims made in the original paper still hold true. the term broken is used for defenses in which the claimed accuracy against stronger attacks were found to be much lower than what was claimed in the original paper.-----it is claimed that if ||z||_inf is too large to exceed the region of natural images, the accuracy will be obviously rather poor; however, the common practice is to clip to the input space bounds. how would that affect the method?----------things to improve the paper that did not impact the score:-----in the first paragraph, goodfellow et al., 2015 is cited, however, papers on adversarial attacks were published earlier than that such as szegedy et al., 2014 or biggio et al., 2013.-----vershynin, 2018 is cited about the distribution of a gaussian in high-dimensional spaces. however, this is a very well known result and does not need any citation (or if any, bellman, 1961).-----typo after equation 4: ||f||_{l_p}-----typo in black-box certification with randomness paragraph: by convovling-----typos in table 2.: the columns 2.0 to 3.5 are mislabeled the paper introduces an improvement to the randomized smoothing analysis in cohen et al. (2019), using lagrangian relaxation to achieve a more general lower bound. using this, it considers different adversarial smoothing distributions that yield some increase in certified adversarial accuracy.----------overall assessment: while the lagrangian relaxation idea is interesting and could yield interesting follow-up work, the paper is sloppy in several respects and needs to be tightened before it can be considered for publication.----------key issues:-----1. proof of main theorem (strong duality) is incorrect. likely the statement itself is also incorrect. fortunately the most important direction (lower bound) is still true, so this isn't a fatal flaw to the approach.-----2. the paper makes several references (in italics) to a ""fundamental trade-off between accuracy and robustness"". but a fundamental trade-off means that *any* method that attains good accuracy must sacrifice robustness and vice versa; this requires a ""for all"" statement, i.e. a lower bound. all the paper shows is that the *particular upper bound* exhibits a trade-off (and even then, the notions of ""accuracy"" and ""robustness"" are merely interpretations of quantities in the bound; it's not clear why the robustness term in particular is tied to more standard notions of robustness).-----3. the justification for why the particular smoothing distributions are good ideas is sketchy.----------i elaborate on 1 and 3 below. addressing 1-3 effectively will improve my score.----------#1 (main theorem is incorrect): claim 3 in the appendix is wrong. the fact that (delta', f') outperforms (delta-bar, f-bar) with respect to lambda* does not imply that (delta', f', lambda*) is a better solution to the primal problem, because we must take max over lambda and the maximizing lambda need not be lambda*. in particular if f' doesn't satisfy the constraint we would instead take lambda to infinity.----------#3 (sketchy justification): the paper justifies a smoothing distribution that concentrates more mass around the center as follows: ""this phenomenon makes it problematic to use standard gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate)."" i don't see why we should want more mass near the center---in the limit as we move all the mass towards the center and get the original classifier, our certified bound will be terrible, so it's not clear why moving in that direction should be expected to help. indeed, the experimental gains are minimal (1 to 3 percentage points) and on methods that were not carefully tuned, so one could imagine that the baseline method could be improved by that much just with careful tuning.----------i similarly didn't understand the justification for the mixed l-inf / l-2 distribution for l-infinity verification. the main justification was ""the motivation is that this allows us to allocate more probability mass along the pointy directions with larger`norm, and hence decrease the maximum distance term max b`,rdf(0)."" this is at the very least too brief for justifying the main experimental innovation in the paper (here at least the empirical improvements are bigger, although still not huge).----------minor but related: why is the x-axis in figure 4 so compressed? this is also in a regime where all 3 methods fail to certify so not clear it's meaningful.----------writing comment: change some of the theorems to propositions. theorems should be for key claims in paper (there shouldn't be 4 of them in one 8-page paper).","the authors extend the framework of randomized smoothing to handle non-gaussian smoothing distribution and use this to show that they can construct smoothed models that perform well against l2 and linf adversarial attacks. they show that the resulting framework can obtain state-of-the-art certified robustness results improving upon prior work.----------while the paper contains several interesting ideas, the reviewers were concerned about several technical flaws and omissions from the paper:----------1) a theorem on strong duality was incorrect in the initial version of the paper, though this was fixed in the rebuttal. however, the reasoning of the authors on the ""fundamental trade-off"" is specific to the particular framework they consider, and is not really a fundamental trade-off.----------2) the justification for the new family of distributions constructed by the author is not very clear and the experiments only show marginal improvements over prior work. thus, the significance of this contribution is not clear.----------some of the issues were clarified during the rebuttal, but the reviewers remained unconvinced about the above points.----------thus, the paper cannot be accepted in its current form.","the main justification was ""the motivation is that this allows us to allocate more probability mass along the pointy directions with larger`norm, and hence decrease the maximum distance term max b`,rdf(0).""","in particular if f' doesn't satisfy the constraint we would instead take lambda to infinity.----------#3 (sketchy justification): the paper justifies a smoothing distribution that concentrates more mass around the center as follows: ""this phenomenon makes it problematic to use standard gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate).""","the theoretical results are interesting, showing a clear trade-off between robustness and accuracy with a new lower bound and deriving better smoothing distributions.",this paper presents a new method for adversarial certification using non-gaussian noise.,"in particular if f' doesn't satisfy the constraint we would instead take lambda to infinity.----------#3 (sketchy justification): the paper justifies a smoothing distribution that concentrates more mass around the center as follows: ""this phenomenon makes it problematic to use standard gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate).""","in particular if f' doesn't satisfy the constraint we would instead take lambda to infinity.----------#3 (sketchy justification): the paper justifies a smoothing distribution that concentrates more mass around the center as follows: ""this phenomenon makes it problematic to use standard gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate).""","the term broken is used for defenses in which the claimed accuracy against stronger attacks were found to be much lower than what was claimed in the original paper.-----it is claimed that if ||z||_inf is too large to exceed the region of natural images, the accuracy will be obviously rather poor; however, the common practice is to clip to the input space bounds.","in particular if f' doesn't satisfy the constraint we would instead take lambda to infinity.----------#3 (sketchy justification): the paper justifies a smoothing distribution that concentrates more mass around the center as follows: ""this phenomenon makes it problematic to use standard gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate).""",0.1,0.0,0.0727272727272727,0.0727272727272727,0.2272727272727272,0.0305343511450381,0.1212121212121212,0.1212121212121212,0.1428571428571428,0.0192307692307692,0.0952380952380952,0.0952380952380952,0.0904522613065326,0.0101522842639593,0.0402010050251256,0.0402010050251256,0.2272727272727272,0.0305343511450381,0.1212121212121212,0.1212121212121212,0.2272727272727272,0.0305343511450381,0.1212121212121212,0.1212121212121212,0.2151394422310757,0.0240963855421686,0.1354581673306773,0.1354581673306773,0.2272727272727272,0.0305343511450381,0.1212121212121212,0.1212121212121212,8.361638069152832,8.361638069152832,16.303783416748047,8.361638069152832,4.272364616394043,9.958396911621094,8.361638069152832,5.281071662902832,0.11766353861648357,0.22986824132023748,0.7796262632328658,0.1347688324714202,0.25650880225473527,0.7694384324206353,0.9036432253619295,0.9331635841702215,0.7976011845657753,0.9620815225011193,0.9632143880777417,0.9386802855989984,0.1347688324714202,0.25650880225473527,0.7694389207355368,0.1347688324714202,0.25650880225473527,0.7694384324206353,0.9309741127599974,0.9350191506788716,0.9185248256414815,0.1347688324714202,0.25650880225473527,0.7694384324206353
129,https://openreview.net/forum?id=SklckhR5Ym,"this paper proposes an additional loss term to use when training an lstm lm. the authors argue that, intuitively, we want the output distribution to retain some information about the context, or ""past"". given this, they use the output distribution as input to a one layer network that must predict the current token. the loss for this network is incorporated as an additional term used when training the lm. the authors show that by adding this loss term they can achieve sota (for single softmax model) perplexity on a number of lm benchmarks.----------------the technical contribution is proposing a new loss term to use when training a language model. the idea is clear, simple, and well explained, and it seems to be effective in practice. one drawback is that it is highly specific to language models. other recent works which have demonstrated effective regularization of lstm lms have proposed methods that can be used in any lstm model, but that is not the case here. in addition, there is not much theoretical justification for it, it seems like a one-off trick. the loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?----------------although it is specific to language models, there are a few reasons it might be of broader significance:--------- it falls in the recent line of work in incorporating auxiliary losses for various tasks. this idea has touched many problems and seen success in practice.--------- perhaps it can be applied to other sequence models. for example in encoder-decoder models, the decoder can be thought of as a conditional lm.----------------experiments are comprehensive and rigorous. they might be more convincing if there were results on a very large corpus such as 1 billion word corpus.----------------pros:--------- new sota for single softmax model on lm benchmarks.--------- simple, clearly explained idea.--------- demonstrates effectiveness of auxiliary losses.--------- rigorous experiments.----------------cons--------- trick is specific to lm.--------- no large corpus results. the paper suggests a new regularization technique which can be added on top of those used in awd-lstm of merity et al. (2017) with little overhead.----------------this is a well-written paper with a clear structure. the experiments are presented in a clear and understandable fashion, and the evaluation seems thorough. the methodology seems sound, and the authors present the reader with all the information needed to replicate the experiments.----------------i would only suggest evaluating this technique on awd-lstm-mos of yang et al. (2017) to get a more complete picture.----------------references--------- merity, s., keskar, n.s. and socher, r., 2017. regularizing and optimizing lstm language models. arxiv preprint arxiv:1708.02182.--------- yang, z., dai, z., salakhutdinov, r. and cohen, w.w., 2017. breaking the softmax bottleneck: a high-rank rnn language model. arxiv preprint arxiv:1711.03953. in their abstract, the authors claim to provide state-of-the-art perplexity on penn treebank, which is not true. as the authors state, their notion of ""state-of-the-art"" excludes exactly that earlier work, which does provide state-of-the-art perplexity on penn treebank (yang et al. 2017), as stated in sec. 4.1. the question is, why one would exlude the mixture-of-softmax approach here? this is clearly misleading.----------------the authors introduce the idea of past decoding for the purpose of regularization. it remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.----------------the results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on penn treebank. considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets. the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. it remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.","the paper proposes an additional module to train language models, adding a new loss that tries to predict the previous token given the next one, thus enforcing the model to remember the past. two out of 3 reviewers recommend to accept the paper; the third one said it was misleading to claim sota since authors didn't try the mixture-of-softmax model that is actually currently sota. the authors acknowledged and modified the paper accordingly, and added a few more experiments. the reviewer still thinks the improvements are not important enough to claim significant novelty. overall, i think the idea is simple and adds some structure to language modeling, but i also concur with the reviewer about limited improvements, which makes it a borderline paper. when calibrating with other area chairs, i decided to recommend to reject the paper.","the methodology seems sound, and the authors present the reader with all the information needed to replicate the experiments.----------------i would only suggest evaluating this technique on awd-lstm-mos of yang et al. (2017) to get a more complete picture.----------------references--------- merity, s., keskar, n.s.",the authors show that by adding this loss term they can achieve sota (for single softmax model) perplexity on a number of lm benchmarks.----------------the technical contribution is proposing a new loss term to use when training a language model.,this paper proposes an additional loss term to use when training an lstm lm.,this paper proposes an additional loss term to use when training an lstm lm.,"they might be more convincing if there were results on a very large corpus such as 1 billion word corpus.----------------pros:--------- new sota for single softmax model on lm benchmarks.--------- simple, clearly explained idea.--------- demonstrates effectiveness of auxiliary losses.--------- rigorous experiments.----------------cons--------- trick is specific to lm.--------- no large corpus results.","the loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?----------------although it is specific to language models, there are a few reasons it might be of broader significance:--------- it falls in the recent line of work in incorporating auxiliary losses for various tasks.",its utility in actual applications.,the authors show that by adding this loss term they can achieve sota (for single softmax model) perplexity on a number of lm benchmarks.----------------the technical contribution is proposing a new loss term to use when training a language model.,0.160427807486631,0.0108108108108108,0.106951871657754,0.106951871657754,0.2333333333333333,0.0561797752808988,0.1333333333333333,0.1333333333333333,0.1038961038961038,0.0394736842105263,0.0909090909090909,0.0909090909090909,0.1038961038961038,0.0394736842105263,0.0909090909090909,0.0909090909090909,0.1361256544502617,0.0105820105820105,0.0837696335078534,0.0837696335078534,0.2686567164179104,0.0402010050251256,0.1492537313432835,0.1492537313432835,0.0137931034482758,0.0,0.0137931034482758,0.0137931034482758,0.2333333333333333,0.0561797752808988,0.1333333333333333,0.1333333333333333,12.355157852172852,10.882530212402344,19.04288482666016,14.47397232055664,6.044906139373779,19.04288482666016,14.473968505859377,5.55949592590332,0.9439804507755993,0.5379189901728576,0.4213726404137851,0.9801869087659026,0.980169365935448,0.8858365341234857,0.9780414452005025,0.9758567329108231,0.8759079317684636,0.9780414452005025,0.9758567329108231,0.8759078386608443,0.8639763899325686,0.9460873742749286,0.9203146249258511,0.9790269012124329,0.976583128422791,0.9328459232093167,0.08157303294604724,0.5504279198971814,0.01658769341202283,0.9801869087659026,0.980169365935448,0.8858365341234857
130,https://openreview.net/forum?id=Skld1aVtPB,"the authors apply linear time subset scanning to find groups of anomalous inputs and network activations. they further use this method to detect effects of the same adversarial perturbation algorithm over a set of images.-----major comments:-----overall this is quite interesting work, and seems like a promising method to detect groups of anomalies. however, this work is not complete without comparison to existing methods, the lack of which makes it impossible to evaluate its usefulness. see [1,2].----------minor questions/comments:-----the results seem to be applied to relu activation networks and anomalous signals are described as having higher activation that the background or the clean images. could you clear up whether this method works for all activations or just relu networks?-----im envisioning a case where the anomalous images are all anomalous because they are all very similar. if i put 100 of the same (or very similar images) would this be something that this method could detect?-----your results seem to suggest that adversarial perturbation methods produce adversarial images with activations that are more extreme than natural images. this doesnt seem immediately obvious to me and i would be interested in further exploration of this direction.-----a value of \epsilon=0.02 was used in experiments for bim. while i agree that smaller values of \epsilon are harder to detect it would be useful to have an evaluation of performance over smaller values of \epsilon even if these did not perform as well.-----[1] xiong, l., p oczos, b., schneider, j., connolly, a., vanderplas, j.: hierarchical probabilistic models for group anomaly detection. in: aistats 2011 (2011)-----[2] chalapathy r., toth e., chawla s. (2019) group anomaly detection using deep generative models. in: ecml pkdd 2018. lecture notes in computer science, vol 11051. springer, cham the paper proposed a scheme to detect the presence of anomalous inputs, such as samples designed adversarially for deep learning tasks, that is based on a ""subset scanning"" approach to detect anomalous activations in the deep learning network. the paper is considering a very interesting problem and provides the suitable application of an approach previously developed for pattern detection. the approach is motivated by p-value statistics of the activation patterns in the deep learning network under the ""null hypothesis"" of a non-anomalous input.----------my rating is ""weak reject"" because the explanation of the subset scanning approach is not clear. the paper describes two functionals f and g that are defined over subsets of the data and activations. section 2.2 mentions a method that maximizes f over data and activation subsets by maximizing f over the data subsets under a fixed activation subset and vice versa, and iterating over these two. the section also discusses the function g that measures the priority of an image, but does not describe how the priority is known to provide an optimal solution for the former optimization over f. another function with the same name is defined to measure the priority of an activation node, and again it is not clear why this will provide an optimal solution for the latter optimization. it would have been helpful to establish (or at least instantiate) the optimality results for this approach; only a citation is provided.----------the applicability of the approach is limited to the requirement that multiple adversarial samples be present for accurate detection. it would appear that other approaches to anomaly detection do not require this condition, particularly if they are designed to operate on individual samples. furthermore, the requirement for the ""same system"" to design the anomalous samples limits the applicability of the anomaly detection approach to cases like the single adversarial design detection setting studied here.----------some questions for the authors:-----is there a reason why the figures (fig. 2) show results only for a single class?-----is there a reason why no other anomaly detection algorithms for the activation patterns were used in comparisons? how about other adversarial noise detection algorithms?-----is there intuition behind the difference in performance when all target classes vs. a single target class is used? does this mean that a multi-class adversary generation would be too diverse for the proposed approach to detect it effectively?----------minor comments-----""weird together"" - is too informal, and it's not clear why this phrasing is needed. consider replacing or explaining.-----typos ""anomlaous"" multiple times.","the paper investigates the use of the subset scanning to the detection of anomalous patterns in the input to a neural network. the paper has received mixed reviews (one positive and two negatives). the reviewers agree that the idea is interesting, has novelty, and is worth investigating. at the same time they raise issues about the clarity and the lack of comparisons with baselines. despite a very detailed rebuttal, both of the negative reviewers still feel that addressing their concerns through paper revision would be needed for acceptance.","does this mean that a multi-class adversary generation would be too diverse for the proposed approach to detect it effectively?----------minor comments-----""weird together"" - is too informal, and it's not clear why this phrasing is needed.","in: ecml pkdd 2018. lecture notes in computer science, vol 11051. springer, cham the paper proposed a scheme to detect the presence of anomalous inputs, such as samples designed adversarially for deep learning tasks, that is based on a ""subset scanning"" approach to detect anomalous activations in the deep learning network.","they further use this method to detect effects of the same adversarial perturbation algorithm over a set of images.-----major comments:-----overall this is quite interesting work, and seems like a promising method to detect groups of anomalies.",the authors apply linear time subset scanning to find groups of anomalous inputs and network activations.,"furthermore, the requirement for the ""same system"" to design the anomalous samples limits the applicability of the anomaly detection approach to cases like the single adversarial design detection setting studied here.----------some questions for the authors:-----is there a reason why the figures (fig.","in: ecml pkdd 2018. lecture notes in computer science, vol 11051. springer, cham the paper proposed a scheme to detect the presence of anomalous inputs, such as samples designed adversarially for deep learning tasks, that is based on a ""subset scanning"" approach to detect anomalous activations in the deep learning network.",could you clear up whether this method works for all activations or just relu networks?-----im envisioning a case where the anomalous images are all anomalous because they are all very similar.,"in: ecml pkdd 2018. lecture notes in computer science, vol 11051. springer, cham the paper proposed a scheme to detect the presence of anomalous inputs, such as samples designed adversarially for deep learning tasks, that is based on a ""subset scanning"" approach to detect anomalous activations in the deep learning network.",0.1904761904761904,0.0161290322580645,0.111111111111111,0.111111111111111,0.2733812949640288,0.0583941605839416,0.1726618705035971,0.1726618705035971,0.2380952380952381,0.032258064516129,0.1746031746031746,0.1746031746031746,0.1923076923076923,0.0588235294117647,0.1538461538461538,0.1538461538461538,0.2575757575757575,0.0307692307692307,0.1818181818181818,0.1818181818181818,0.2733812949640288,0.0583941605839416,0.1726618705035971,0.1726618705035971,0.1166666666666666,0.0,0.0666666666666666,0.0666666666666666,0.2733812949640288,0.0583941605839416,0.1726618705035971,0.1726618705035971,7.301414489746094,8.344115257263184,12.517095565795898,7.301414489746094,6.115134716033936,11.380517959594728,7.301414489746094,9.18171501159668,0.27555750574583737,0.42498211662292457,0.9217857712111422,0.9737005143820279,0.9644166547624548,0.027885336969643647,0.9394377651512802,0.9559844275462039,0.9259731903167252,0.9479652791243521,0.9528370414282709,0.6527104523807693,0.9075138354757216,0.9457369867832928,0.5778164212829257,0.9737005143820279,0.9644166547624548,0.027885291998698068,0.9541945811545496,0.9488719051867733,0.8975336447509571,0.9737005143820279,0.9644166547624548,0.027885336969643647
131,https://openreview.net/forum?id=SklzIjActX,"this paper designs a system to automatically quantize the cnn pretrained models. this system contains three main components: 1) different scale factors for channel-wise network; 2) winograd 8bit quantization; 3) topology wise 8bit operation support. all these three techniques are standard ways to perform model quantization. the work is solid in the sense that 1) as far as i know there is no work actually using all of these quantization schemes, and designs a system to automatically do quantization with additional algorithm support(retrain strategy). 2) significantly amount of experiment on quantizing 18 existing widely used cnn models for different applications, e.g., image classification, image segmentation, etc. 3) it reports the actually inference speed up comparing int 32 to int 8, although most of the speed up is less than 2.0.----------------several questions:----------------1) what is the difference between retrain and calibration? as mentioned in the paper, the system does not require retrain, but it seems to me that calibration step, e.g., sampling to find the maximum values, etc, is a form of retrain. i think if that is the case, maybe some quantized models with retrain is worthy comparing with.----------------2) many quantization techniques are used in the system, are there any conclusions for what techniques are most important for a particular cnn network/application?----------------3) are there any new/novel quantization algorithms in the system? ----------------4) the inference speed up is mostly less than 2 times, with some are achieving 2.1 speed up while some are without any speedup. any reason for that? also what is the overhead of the inference time? this paper try to speedup cnn inference with 8-bit quantization. it is practically useful and may be a nice reference for other developers. but the ideas in this paper are trivial and the motivation is not so convincing. ----------------1) for depthwise convolution in figure 1(b), the dynamic range differences can be eliminated by batch normalization folding. i doubt the necessity of channel-wise scaling. besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation. they should explain more about this method.--------3)pooling and concatenation support is quite easy. batch normalization folding is a common practice.--------4) the author should explain in more detail on fusing convolution and element-wise operations. sorry i can't get their point. --------5) calibration results should be provided. the author should also tell readers what hardware is used to evaluate the throughput and latency.--------6) detail results of winograd should be given.----------------overall, this paper is in-complete and the authors should add more experimental results and improve their description. this paper reads more like a technical/hardware white paper than a research paper. no real theory is offered, and the results are not really experiments testing hypotheses, but simply reporting the results of their design choices on a various of models. thus, it is not clear that this paper is especially suitable for iclr research track per se. furthermore, the calibration method (to find suitable int8 weights from fp32 ones without further training) appears to be essentially identical to techniques already reported by nvidia as used by their tensorrt. the discussion of int8 for winograd is something that could have been new and interesting (i.e. this reader has not seen nvidia discuss this issue previously), but in the end this paper did not offer anything surprising or especially insightful in the brief section 3.2.2 explaining their approach. furthermore, the experiments such as table 2 do not include winograd results anyway because that does not give competitive results using int8, as the authors admit.","the paper proposes to combine three methods of quantization and apply them to neural network compression. the methods are known in the literature. there is a lack of theoretical contribution, and experimental results show variable speedups that may not be competitive with the current state-of-the-art in neural network compression. the majority of reviewers recommend that this paper be rejected. the authors have not provided a response.",this system contains three main components: 1) different scale factors for channel-wise network; 2) winograd 8bit quantization; 3) topology wise 8bit operation support.,"besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation.","besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation.",this paper designs a system to automatically quantize the cnn pretrained models.,"besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation.","besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation.","besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation.","besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.--------2) for winograd convolution, the authors proposed to use scale factors after transformation.",0.086021505376344,0.0,0.043010752688172,0.043010752688172,0.1869158878504672,0.038095238095238,0.1121495327102803,0.1121495327102803,0.1869158878504672,0.038095238095238,0.1121495327102803,0.1121495327102803,0.1481481481481481,0.0253164556962025,0.0987654320987654,0.0987654320987654,0.1869158878504672,0.038095238095238,0.1121495327102803,0.1121495327102803,0.1869158878504672,0.038095238095238,0.1121495327102803,0.1121495327102803,0.1869158878504672,0.038095238095238,0.1121495327102803,0.1121495327102803,0.1869158878504672,0.038095238095238,0.1121495327102803,0.1121495327102803,5.268637657165527,5.268637657165527,16.372215270996094,5.268636703491211,9.199407577514648,5.268637657165527,5.268635272979736,5.268637657165527,0.9841771086767193,0.9825750117560311,0.8974884520699846,0.9381847547956117,0.9464906882951981,0.33480873358187063,0.9381847547956117,0.9464906882951981,0.3348070447746248,0.9786378668510107,0.9771853647105179,0.28609105118024836,0.9381847547956117,0.9464906882951981,0.33480779970257546,0.9381847547956117,0.9464906882951981,0.33480785944092245,0.9381847547956117,0.9464906882951981,0.33480738245700536,0.9381847547956117,0.9464906882951981,0.33480873358187063
132,https://openreview.net/forum?id=SkxHRySFvr,"this paper uses a meta-learning approach to solve semi-supervised learning. the main idea is to simulate an sgd step on the loss of the meta-validation data and see how the model will perform if the pseudo-labels of unlabelled data are perturbed. experiments on classification and regression problems show that the proposed method can improve over existing methods. the idea itself is intriguing but the derivation and some design choice are not very well-explained.----------(1) the derivation from eq.(3) to (4) is confusing. note that in eq.(3), the prediction \phi_\theta also depends on \theta in addition to the pseudo-label z. when taking a step of sgd, the second term of eq.(3) (with unlabelled data) will always be zero if both arguments of the loss (\phi_\theta(x) and z_\theta(x)) change simultaneously. eq.(4) somehow only considers the gradient of unsupervised loss, then the gradient would be zero because there is no incentive to deviate from the pseudo-label z. the pseudo-code does not help much. the update from \hat{\theta}^{t} to \hat{\theta}^{t+1} has the same issue: there is no incentive for \hat{\theta}^{t} to deviate because z is exactly produced by it.----------(2) for classification problems, it is natural to use cross-entropy loss for the probability vector z. are there any specific reasons for using gumbel-softmax? in addition, using l2 loss for probability vectors (as mentioned in appendix a) is known to be problematic as it may create exponentially many local minima (auer et al, 1996).----------(3) the recent work of li et al. (2019) also considers iteratively improving pseudo-labels with meta-updates so it should be discussed and compared.----------(4) experiments------ what are the sizes of the meta-validation sets in the experiments?------ error bars in the tables and fig.2?------ the mm results in table 2 are noticeably worse than the original results. for example, with 250 labeled data, mm achieved 11.08% in cifar-10 as reported in the original paper. (and 4000 labeled data can achieve 4.95%)------ it is said that option 2 is consistently better than option 1, which is not true for the mm baseline.------ 22500 training steps for experiment 4 seems arbitrary. what are the candidates for the hyper-parameters?----------typos:------ in the first paragraph of sec.2, one of the x and one of the y should be bold.------ above eq.(4), x^{u\in u} should be x^i \in u------ the transpose in eq.(7) is not necessary------ it is said on page 6 that fig.2 reports classification loss but the task is a regression problem.----------ref------ auer, p., herbster, m. and warmuth, m.k., 1996. exponentially many local minima for single neurons. in advances in neural information processing systems (pp. 316-322).------ li, x., sun, q., liu, y., zheng, s., chua, t.s. and schiele, b., 2019. learning to self-train for semi-supervised few-shot classification. in advances in neural information processing systems. this paper looks into problem of semi-supervised learning and in order to be mindful of generalization on the unlabeled data, they add a term to the loss function which includes loss on imputed labels.-----i have 3 main concerns with the paper -----1. the authors mention that the meta-validation set is a random subset of train set. and they check the final performance on meta-validation set. this does not seem a right way to measure performance of the model as meta-validation set is already used in training. the set of labeled points should be partitioned into train and meta-validation set.----------2. the derivation of the updates given the added term to the loss. in option 1, the authors mention they use eqn. 8 to update z, while eqn 8. has the reverse information. ----------3.in option 2, z = \sigmoid(\phi_\theta) and reducing the loss on z, l( \sigmoid(\phi_\theta) , \phi_\theta), does not look very meaningful. trying to get \phi_\theta close to its sigmoid means getting it close to zero. but we do not know what is the label for unlabeled data, so why getting the label close to zero?-----also the authors mention that second order derivatives will come to play without any explanation. i suggest spending more effort on explaining the problem formulation as that's the core of the paper.----------more comments:-----* as mentioned above the problem formulation is not clean and there are unjustified choice there. moreover, the experiment results are mostly declared without any justification (for example, the proposed method does not always lead to improvement and not all cases are explained. the authors only note that the method works well in low data regime). ----------* in the first experiment pl is compared to two cases of the proposed algorithm whereas in other experiments pl is compared to combining pl with versions of the proposed method. is there a reason for this?----------* the models used as baseline are only explained briefly in the last page of the paper, while being used multiple time in the experiment section. this is not good writing practice. this paper proposes a semi-supervised approach to impute the labels of unlabeled samples such that a network achieves better generalization when it is trained on these labels. the proposed strategy can be easily used to improve the state-of-the-art semi-supervised methods. it mainly uses a validation data set to evaluate the updating rules of the unlabeled samples with pseudo-labels. the proposed method is applicable to both classification and regression problems including image classification and facial landmark detection tasks, which has shown in the experiments. but the following should be improved in the following aspects: -----[1] in the proposed method, the model parameters are updated both on the unlabeled samples and validation data set. the experimental results show that such a strategy is effective to improve the performance of the state-of-the-art method. but why the strategy is effective should be further analyzed.-----[2] how the validation data can improve the generalization ability of the model should be given with theoretical analysis. whether the size of the validation data has a great influence?-----[3] some experimental settings are not clear. in the experiments, how many unlabeled data is labeled with pseudo-labels. for different size of the unlabeled data, how many samples should be used in the validation data to evaluate the model with pseudo labeled samples.-----[4] how to divide the training data and the validation data? whether the validation data need much more that the training data? how about the results only with all the labeled samples, which can further improve the confidence of the proposed method.","there is insufficient support to recommend accepting this paper. the reviewers unanimously criticize the quality of the exposition, noting that many key elements in the main development and experimental set up are not clear. the significance of the contribution could be made stronger with some form of theoretical analysis. the current paper lacks depth and insufficient justification for the proposed approach. the submitted comments should be able to help the authors improve the paper.","in addition, using l2 loss for probability vectors (as mentioned in appendix a) is known to be problematic as it may create exponentially many local minima (auer et al, 1996).----------(3) the recent work of li et al. (2019) also considers iteratively improving pseudo-labels with meta-updates so it should be discussed and compared.----------(4) experiments------ what are the sizes of the meta-validation sets in the experiments?------ error bars in the tables and fig.2?------ the mm results in table 2 are noticeably worse than the original results.","for different size of the unlabeled data, how many samples should be used in the validation data to evaluate the model with pseudo labeled samples.-----[4] how to divide the training data and the validation data?","in the experiments, how many unlabeled data is labeled with pseudo-labels.",this paper uses a meta-learning approach to solve semi-supervised learning.,"in addition, using l2 loss for probability vectors (as mentioned in appendix a) is known to be problematic as it may create exponentially many local minima (auer et al, 1996).----------(3) the recent work of li et al. (2019) also considers iteratively improving pseudo-labels with meta-updates so it should be discussed and compared.----------(4) experiments------ what are the sizes of the meta-validation sets in the experiments?------ error bars in the tables and fig.2?------ the mm results in table 2 are noticeably worse than the original results.","in addition, using l2 loss for probability vectors (as mentioned in appendix a) is known to be problematic as it may create exponentially many local minima (auer et al, 1996).----------(3) the recent work of li et al. (2019) also considers iteratively improving pseudo-labels with meta-updates so it should be discussed and compared.----------(4) experiments------ what are the sizes of the meta-validation sets in the experiments?------ error bars in the tables and fig.2?------ the mm results in table 2 are noticeably worse than the original results.",the idea itself is intriguing but the derivation and some design choice are not very well-explained.----------(1) the derivation from eq.,"for different size of the unlabeled data, how many samples should be used in the validation data to evaluate the model with pseudo labeled samples.-----[4] how to divide the training data and the validation data?",0.2804878048780488,0.037037037037037,0.1585365853658536,0.1585365853658536,0.2727272727272727,0.0555555555555555,0.1818181818181818,0.1818181818181818,0.1162790697674418,0.0238095238095238,0.0697674418604651,0.0697674418604651,0.0930232558139534,0.0238095238095238,0.0930232558139534,0.0930232558139534,0.2804878048780488,0.037037037037037,0.1585365853658536,0.1585365853658536,0.2804878048780488,0.037037037037037,0.1585365853658536,0.1585365853658536,0.1666666666666666,0.0212765957446808,0.125,0.125,0.2727272727272727,0.0555555555555555,0.1818181818181818,0.1818181818181818,8.875792503356934,8.875792503356934,10.44754409790039,6.28474235534668,8.875792503356934,6.0416765213012695,6.284743309020996,11.290191650390623,0.9625839277991318,0.9598352584867476,0.2286526572475658,0.703820570619705,0.7503761221395021,0.4483874552770456,0.8439501063324647,0.9155396455842136,0.5357813896837395,0.9700101281681259,0.9712260350689413,0.9218513572738587,0.9625839277991318,0.9598352584867476,0.22865257117458204,0.9625839277991318,0.9598352584867476,0.22865265417697525,0.9546585013505057,0.9616577853803233,0.6167345791171555,0.703820570619705,0.7503761221395021,0.4483874552770456
133,https://openreview.net/forum?id=SkxW23NtPH,"summary: this work proposes to use a combination of graph neural networks (gnns) and proximal policy optimization (ppo) to train policies for generalized device placement in dataflow graphs. essentially, (1) a gnn is used to learn representations of a dataflow graph (in an inductive manner), (2) a transformer is used to output a device placement action for each node in the graph, and (3) the entire system is trained end-to-end via ppo. extensive experimental results show very impressive results compared to strong baselines.----------assessment: overall, this is a solid application paper. the authors gnns, ppo, and transformers in an effective, well-motivated, and sound manner. moreover, the task is interesting and relevant. there is not significant methodological novelty, as the authors are essentially combining standard components in a straightforward way. that said, the results are strong and the paper is well-written, so it certainly has merits as an application paper. ----------will code be released? this is essential for reproducibility, as the paper does not contain sufficient technical details to allow for reproduction.----------reasons to accept:------ strong empirical results on an interesting application ------ well-written paper------ thorough experiments----------reasons to reject:------ incremental methodological contribution------ likely difficult to reproduce in this paper the authors propose an end-to-end policy for graph placement and partitioning of computational graphs produced ""under-the-hood"" by platforms like tensorflow. as the sizes of the neural networks increase, using distributed deep learning is becoming more and more necessary. primitives like the one suggested by the authors are very important in many ways, including improving the ability of the nn to process more data, reduce energy consumption etc. the authors compared to prior work propose a method that can take as input more than one data flow graphs, and learns a policy for graph partitioning/placement of the operations in a set of machines that minimizes the makespan. this problem in principle is np-hard as it entails both graph partitioning and graph scheduling as its components. the authors propose a heuristic that composes of two existing methods: graph neural networks are used to produce an embedding of the computation/data flow graph, and then a seq-2-seq placement network. the method is able to generalize to unseen instances.----------i vote for weak reject since some issues that i would like to see addressed by the author(s) are not. these include the fact that since the goal is to minimize the makespan, scheduling within each machine the operations should be addressed in a better way. also, while the objective j(\theta) is reasonable, the distribution for the makespans could be very skewed (e.g., heavy tails over the dataflow graphs). doesn't this affect the results? finally, the novelty from a deep learning perspective is limited.----------- how do the authors address the issue of scheduling the operations within each machine? ------ how is \mathcal{d} formally defined (i.e., the range of the mapping function)? do you take into account the different number of machines, their memory footprints that can be significantly different, the different processing units they may have (gpus, cpus, tpus)? is the number of machines used for the partition automatically learned by the policy? that part was not very clear.------ since the authors compare with metis, it is worth also comparing with scotch https://www.labri.fr/perso/pelegrin/scotch/ that is also publicly available.------ can the authors comment on the scalability of their method as a function of n (number of nodes), and k (number of devices)? ---------------update: thanks to the author(s) for the detailed feedback. i have upgraded my score accordingly. gdp: generalized device placement for dataflow graphs----------this paper presents a method to assign the individual operations making up the dataflow graph of a deep neural network to a set of connected devices on which they should be executed, with the objective of maximizing runtime performance of inference. the papers proposed approach relies on graph neural networks to produce embeddings of the dataflow graph nodes that are claimed to be transferable to unseen graphs. these embeddings serve as input to a transformer-based sequence model that outputs the final assignment. the end-to-end model is trained with a reinforcement learning criterion minimizing the expected placement runtime. experimental comparisons against alternative placement methods (including human expert) are presented for several large neural networks of different architectures.----------overall, the paper is well written, easy to follow, and addresses an important concern is scaling up neural networks to large model sizes. the proposed approach combining graph neural networks with transformer-based placement network appears, at first glance, novel. ----------my main reservations with the paper is that it lacks many details in several key sections, preventing a full appreciation of the contributions, and making reproducibility of results impossible to contemplate. in particular:----------1. how the placement network is designed and used (section 3.2) is completely lacking.-----2. the details of how the training and testing datasets are obtained are also lacking. in particular, the specific model architectures for the likes of rnnlm, including hyperparameter choices, the runtime data fed to those models, etc. (this can be given in appendix). it is also not clear if the models listed in table 1 are singular models with fixed values of hyperparameters, or if they correspond to distributions over models (e.g. with differences in hyperparameters).-----3. how ppo is used is glossed over: for instance, is the proposed placement of a given model tried  live  in the inner loop of ppo to generate the reward corresponding to its runtime? what runtime data is fed to the model in order to do this? since this would be a somewhat unusual setup, an illustration of the overall training loop would certainly solidify understanding. moreover, a few sentences or equations explaining how ppo is used in this context would also help. is training with ppo imperative for getting good model performance? have other methods been tried?----------in section 3.3, the parameter superposition mechanism appears to perform a kind of meta-learning. explicit connections to such should be made, such as  tadam: task dependent adaptive metric for improved few-shot learning  (neurips 2018).----------moreover, at the end of section 4.4, the paper claims to  report superhuman results on 8-layer gnmt . unless this reviewer misunderstands, the results in table 1 (wherein gdp-one clocks in at 0.649 for 8-layer gnmt, versus 0.610 for human placement) would contradict this claim. this should be clarified.----------given these reservations, in spite of the potential of the proposed approach, the paper appears in its current form too immature to recommend acceptance at iclr.",this paper presents a new reinforcement learning based approach to device placement for operations in computational graphs and demonstrates improvements for large scale standard models.----------the paper is borderline with all reviewers appreciating the paper even the reviewer with the lowest score. the reviewer with the lowest score is basing the score on minor reservation regarding lack of detail in explaining the experiments.----------based upon the average score rejection is recommended. the reviewers' comments can help improve the paper and it is definitely recommended to submit it to the next conference.,"explicit connections to such should be made, such as  tadam: task dependent adaptive metric for improved few-shot learning  (neurips 2018).----------moreover, at the end of section 4.4, the paper claims to  report superhuman results on 8-layer gnmt .","this is essential for reproducibility, as the paper does not contain sufficient technical details to allow for reproduction.----------reasons to accept:------ strong empirical results on an interesting application ------ well-written paper------ thorough experiments----------reasons to reject:------ incremental methodological contribution------ likely difficult to reproduce in this paper the authors propose an end-to-end policy for graph placement and partitioning of computational graphs produced ""under-the-hood"" by platforms like tensorflow.","this is essential for reproducibility, as the paper does not contain sufficient technical details to allow for reproduction.----------reasons to accept:------ strong empirical results on an interesting application ------ well-written paper------ thorough experiments----------reasons to reject:------ incremental methodological contribution------ likely difficult to reproduce in this paper the authors propose an end-to-end policy for graph placement and partitioning of computational graphs produced ""under-the-hood"" by platforms like tensorflow.",summary: this work proposes to use a combination of graph neural networks (gnns) and proximal policy optimization (ppo) to train policies for generalized device placement in dataflow graphs.,"this is essential for reproducibility, as the paper does not contain sufficient technical details to allow for reproduction.----------reasons to accept:------ strong empirical results on an interesting application ------ well-written paper------ thorough experiments----------reasons to reject:------ incremental methodological contribution------ likely difficult to reproduce in this paper the authors propose an end-to-end policy for graph placement and partitioning of computational graphs produced ""under-the-hood"" by platforms like tensorflow.","gdp: generalized device placement for dataflow graphs----------this paper presents a method to assign the individual operations making up the dataflow graph of a deep neural network to a set of connected devices on which they should be executed, with the objective of maximizing runtime performance of inference.","this should be clarified.----------given these reservations, in spite of the potential of the proposed approach, the paper appears in its current form too immature to recommend acceptance at iclr.","this is essential for reproducibility, as the paper does not contain sufficient technical details to allow for reproduction.----------reasons to accept:------ strong empirical results on an interesting application ------ well-written paper------ thorough experiments----------reasons to reject:------ incremental methodological contribution------ likely difficult to reproduce in this paper the authors propose an end-to-end policy for graph placement and partitioning of computational graphs produced ""under-the-hood"" by platforms like tensorflow.",0.1526717557251908,0.0155038759689922,0.1221374045801526,0.1221374045801526,0.2857142857142857,0.0377358490566037,0.124223602484472,0.124223602484472,0.2857142857142857,0.0377358490566037,0.124223602484472,0.124223602484472,0.1848739495798319,0.017094017094017,0.1176470588235294,0.1176470588235294,0.2857142857142857,0.0377358490566037,0.124223602484472,0.124223602484472,0.2446043165467625,0.0875912408759123,0.1438848920863309,0.1438848920863309,0.1983471074380165,0.0168067226890756,0.1322314049586777,0.1322314049586777,0.2857142857142857,0.0377358490566037,0.124223602484472,0.124223602484472,9.127309799194336,6.670841217041016,12.803619384765623,6.670841217041016,7.554915428161621,6.670841217041016,6.670841217041016,4.475549221038818,0.3926341139432246,0.5746136932316466,0.823081330989242,0.9750025318086342,0.979526708161535,0.19224718645371855,0.9750025318086342,0.979526708161535,0.19224642303434716,0.9724167639700853,0.9686128074929177,0.9428634108119734,0.9750025318086342,0.979526708161535,0.19224642303434716,0.9620827241227383,0.95938038445255,0.9433043360501427,0.3167714987851189,0.6585680455244195,0.7875901583284053,0.9750025318086342,0.979526708161535,0.19224759223682739
134,https://openreview.net/forum?id=Skxuk1rFwB,"this work proposes crown-ibp - novel and efficient certified defense method against adversarial attacks, by combining linear relaxation methods which tend to have tighter bounds with the more efficient interval-based methods. with an attempt to augment the ibp method with its lower computation complexity with the tight crown bounds, to get the best of both worlds. one of the primary contributions here is that reduction of computation complexity by an order of \ln while maintaining similar or better bounds on error. the authors show compelling results with varied sized networks on both mnist and cifar dataset, providing significant improvements over past baselines.----------the paper itself is very well written, lucidly articulating the key contributions of the paper and highlighting the key results. the method and rationale behind it quite easy to follow.---------------pros:-----> show significant benefits over previous baseline with 7.02% verified test error on mnist at \epsilon = 0.3, and 66.94% on cifar-10 with \epsilon = 8/255-----> the proposed method is computationally viable, with up to 20x faster than linear relaxation methods with similar. better test errors and within 5-7x slower than the conventional ibp methods with worse errors----------cons:-----> extensive experiments with more advanced networks/datasets would have been more convincing, esp. given the computation efficiency that enables such experiments-----> more elaborate insights into the choice of the training config/hyper-params esp. with the choice of \k_start, \k_end across the different datasets---------------other comments:-----> for the computational efficiency studies, it would be helpful to provide a breakdown of the costs between the different layers and operations, to better asses/confirm that benefits of crown-ibp method-----> impact of other complementary techniques such a lower precision/quantization? one fo the references compared against is the gowal et al. 2018 for the as a baseline, however, it seems to be those results were obtained on a different hw platform (tpus - motioned in appendix-b), with potentially different computational accuracies (bfloat16 ?). so, this bears to question of the impact of precision on these methods and also the computation complexity.-----> since i'm not very well versed with the current baseline and state-of-art for variable robust training of dnn, it would be good to get an additional confirmation on the validity of the used baselines. this paper proposes a new variation on certified adversarial training method that builds on two prior works ibp and crown. they showed the method outperformed all previous linear relaxation and bound propagation based certified defenses. ----------pros:-----1. the empirical results are strong. the method achieved sota.----------cons:-----1. novelty seems small. it is a straightforward combination of prior works, by adding two bounds together.-----2. adds a new hyperparameter for tuning.-----3. lack of any theoretical insights/motivation for the proposed method. why would we want to combine the two lower bounds? the reason given in the paper is not very convincing:----------""ibp has better learning power at larger epsilon and can achieve much smaller verified error.-----however, it can be hard to tune due to its very imprecise bound at the beginning of training; on the-----other hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it-----over-regularizes the network and forbids us to achieve good accuracy.""----------my questions with regards to this:-----(i) why does loose bound result in unstable training? tighter bound stabilize training?-----(ii) if we're concerned with using a tighter bound could result in over-regularization, then why not just combine the natural loss with the tight bound, as natural loss can be seen as the loosest bound. is ibp crucial? and why? this paper proposes a new method for training certifiably robust models that achieves better results than the previous sota results by ibp, with a moderate increase in training time. it uses a crown-based bound in the warm up phase of ibp, which serves as a better initialization for the later phase of ibp and lead to improvements in both robust and standard accuracy. the crown-based bound uses ibp to compute bounds for intermediate pre-activations and applies crown only to computing the bounds of the margins, which has a complexity between ibp and crown. the experimental results are verify detailed to demonstrate the improvement.----------the improvement is significant enough to me and i tend to accept the paper. the results on cifar10 with epsilon=8/255 is so far the state-of-the-art. however, it is far from being scalable enough to large networks and datasets, which has already been achieved by randomized smoothing based approaches. on cifar10, it takes 32 tpu cores to train a 4-conv-layer network. still, such an approach has the advantage of making robust inferences much more efficiently than randomized smoothing, and thus still worth further explorations.","this paper presents a method that hybridizes the strategies of linear programming and interval bound propagation to improve adversarial robustness. while some reviewers have concerns about the novelty of the underlying ideas presented, the method is an improvement to the sota in certifiable robustness, and has become a benchmark method within this class of defenses.","with the choice of \k_start, \k_end across the different datasets---------------other comments:-----> for the computational efficiency studies, it would be helpful to provide a breakdown of the costs between the different layers and operations, to better asses/confirm that benefits of crown-ibp method-----> impact of other complementary techniques such a lower precision/quantization?","the reason given in the paper is not very convincing:----------""ibp has better learning power at larger epsilon and can achieve much smaller verified error.-----however, it can be hard to tune due to its very imprecise bound at the beginning of training; on the-----other hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it-----over-regularizes the network and forbids us to achieve good accuracy.""","the method and rationale behind it quite easy to follow.---------------pros:-----> show significant benefits over previous baseline with 7.02% verified test error on mnist at \epsilon = 0.3, and 66.94% on cifar-10 with \epsilon = 8/255-----> the proposed method is computationally viable, with up to 20x faster than linear relaxation methods with similar.","this work proposes crown-ibp - novel and efficient certified defense method against adversarial attacks, by combining linear relaxation methods which tend to have tighter bounds with the more efficient interval-based methods.","this work proposes crown-ibp - novel and efficient certified defense method against adversarial attacks, by combining linear relaxation methods which tend to have tighter bounds with the more efficient interval-based methods.","the reason given in the paper is not very convincing:----------""ibp has better learning power at larger epsilon and can achieve much smaller verified error.-----however, it can be hard to tune due to its very imprecise bound at the beginning of training; on the-----other hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it-----over-regularizes the network and forbids us to achieve good accuracy.""",----------pros:-----1. the empirical results are strong.,"the reason given in the paper is not very convincing:----------""ibp has better learning power at larger epsilon and can achieve much smaller verified error.-----however, it can be hard to tune due to its very imprecise bound at the beginning of training; on the-----other hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it-----over-regularizes the network and forbids us to achieve good accuracy.""",0.2702702702702703,0.018348623853211,0.1621621621621621,0.1621621621621621,0.2698412698412698,0.0,0.1428571428571428,0.1428571428571428,0.1981981981981981,0.036697247706422,0.1441441441441441,0.1441441441441441,0.3218390804597701,0.0,0.160919540229885,0.160919540229885,0.3218390804597701,0.0,0.160919540229885,0.160919540229885,0.2698412698412698,0.0,0.1428571428571428,0.1428571428571428,0.032258064516129,0.0,0.032258064516129,0.032258064516129,0.2698412698412698,0.0,0.1428571428571428,0.1428571428571428,9.839422225952148,13.213127136230469,13.213126182556152,9.839422225952148,9.19903564453125,7.593291282653809,9.839422225952148,9.458206176757812,0.9437662362036577,0.9427210531867497,0.6539237829140679,0.9323988038435793,0.9259187953848453,0.8760924829297534,0.8554373598868417,0.8713478870027024,0.3204856761791827,0.9557911769782177,0.9555784157386474,0.9261838934786653,0.9557911769782177,0.9555784157386474,0.9261839348832043,0.9323988038435793,0.9259187953848453,0.8760925368660561,0.9406812305463862,0.9450269876408545,0.09379992564399939,0.9323988038435793,0.9259187953848453,0.8760925368660561
135,https://openreview.net/forum?id=SyProzZAW,"summary and significance: the authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only o(n) neurons. --------the paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth.----------------+ves:--------explaining the power of depth in nns is fundamental to an understanding of deep learning. the paper is very easy to follow. and the proofs are clearly written. the theorems provide exponential gaps for very simple polynomial functions.-----------------ves:--------1. my main concern with the paper is the novelty of the contribution to the techniques. the results in the paper are more general than that of lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas. --------2. the second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of lin et al.).--------3. finally, in prop 3.3, reducing from uniform approximations to taylor approximations, the inequality |e(x)| <= ^(d+1) |n(x) - p(x)| does not follow from the definition of a taylor approximation.----------------despite these criticisms, i contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for iclr. the paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations. it shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial. ----------------by focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question. results such as proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question. here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation. ----------------theorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by lin et al that i did not verify). the idea being to construct a superposition of taylor approximations of the individual monomials. here it would be good to be more specific about the domain. also, in the discussion of taylor series, it would be good to mention the point around which the series is developed, e.g. the origin. ----------------the paper mentions that ``the theorem is false for rectified linear units (relus), which are piecewise linear and do not admit a taylor series''. however, a relu can also be approximated by a smooth function and a taylor series. ----------------theorem 4.1 seems to be implied by theorem 4.2. similarly, parts of section 4.2 seem to follow directly from the previous discussion. ----------------in page 1 ```existence proofs' without explicit constructions'' this is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions. experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks. --------this paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum j of monomials of degree at most c. --------in this setup, theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most ~ j*n, that is, linear in the number of terms and the number of variables. the paper also has bounds for neural networks of a specified depth k (theorem 5.1), and the authors conjecture this bound to be tight (conjecture 5.2). ----------------this is an interesting result, and is an improvement over lin 2017 (where a similar bound is presented for monomial approximation). --------overall, i like the paper.----------------pros: new and interesting result, theoretically sound. --------cons: nothing major.--------comments and clarifications:--------* what about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary? could you comment on this problem?--------* is the assumption that \sigma has taylor expansion to order d tight? (that is, are there counter examples for relaxations of this assumption?) --------* as noted, the assumptions of your theorems 44.3 do not apply to relus, but relus network perform well in practice. could you provide some further comments on this?","all the reviewers are agree on the significance of the topic of understanding expressivity of deep networks. this paper makes good progress in analyzing the ability of deep networks to fit multivariate polynomials. they show exponential depth advantage for general sparse polynomials. i am very surprised that the paper misses the original contribution of andrew barron. he analyzes the size of the shallow neural networks needed to fit a wide class of functions including polynomials. the deep learning community likes to think that everything has been invented in the current decade. @article{barron1994approximation, title={approximation and estimation bounds for artificial neural networks}, author={barron, andrew r}, journal={machine learning}, volume={14}, number={1}, pages={115--133}, year={1994}, publisher={springer} }","----------------theorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by lin et al that i did not verify).","--------this paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum j of monomials of degree at most c. --------in this setup, theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most ~ j*n, that is, linear in the number of terms and the number of variables.",the paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations.,"summary and significance: the authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only o(n) neurons.",experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks.,"--------this paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum j of monomials of degree at most c. --------in this setup, theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most ~ j*n, that is, linear in the number of terms and the number of variables.","----------------theorem 4.1 seems to be implied by theorem 4.2. similarly, parts of section 4.2 seem to follow directly from the previous discussion.","--------this paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum j of monomials of degree at most c. --------in this setup, theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most ~ j*n, that is, linear in the number of terms and the number of variables.",0.175,0.0126582278481012,0.1125,0.1125,0.3009708737864078,0.0490196078431372,0.174757281553398,0.174757281553398,0.158273381294964,0.0291970802919708,0.1438848920863309,0.1438848920863309,0.1772151898734177,0.0,0.1012658227848101,0.1012658227848101,0.1103448275862069,0.0139860139860139,0.0689655172413793,0.0689655172413793,0.3009708737864078,0.0490196078431372,0.174757281553398,0.174757281553398,0.0689655172413793,0.0,0.0551724137931034,0.0551724137931034,0.3009708737864078,0.0490196078431372,0.174757281553398,0.174757281553398,10.979132652282717,11.877592086791992,13.167816162109377,10.979132652282717,9.26236629486084,7.173928260803223,10.979132652282717,3.959096908569336,0.9743202806414678,0.9134786467488758,0.3432285923776464,0.9405431233641353,0.9409128103858074,0.9533082508168844,0.970126706566012,0.9694837091249552,0.7092691274663729,0.9763904499940411,0.9718609417601686,0.9460169300763593,0.9536435540998106,0.9593961039912443,0.9091274202294547,0.9405431233641353,0.9409128103858074,0.9533082508168844,0.9772779321477775,0.9721428602375152,0.9617085311628666,0.9405431233641353,0.9409128103858074,0.9533082508168844
136,https://openreview.net/forum?id=SyVuRiC5K7,"this paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner. semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. this is an interesting work. ----------------the merits of this paper lie in the following aspects: (1) it is the first to learn label propagation for transductive few-shot learning. (2) the proposed approach produced effective empirical results.----------------the drawbacks of the work include the following: (1) there is not much technical contribution. it merely just puts the cnn representation learning and the label propagation together to perform end-to-end learning. considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective. (2) empirically, it seems tpn achieved very small improvements over the very baseline label propagation. moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. for example, on miniimagenet, tadam(oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. this is a major concern. summary--------this paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner. the proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. ----------------pros. ---------this paper is well-motivated. studying label propagation in the meta-learning setting is interesting and novel. intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. ---------the empirical results show improvement over the baselines, which are expected. ----------------cons.---------some technical details are missing. in section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing. constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. can you give more explanations?---------does episode training help label propagation? how about the results of label propagation without the episode training? the paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. ----------------there is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. however, the paper does cover a setup that i am not aware that was studied before. the paper is written clearly, and the experiments seem solid. ----------------comments: ---------- what can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. ---------- it is not clear how the per-example scalar sigma-i is learned. (for eq 2)---------- solving eq 3 by matrix inversion does not scale. would be best to also show results using iterative optimization","as far as i know, this is the first paper to combine transductive learning with few-shot classification. the proposed algorithm, tpn, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. the reviewers liked the idea, however there were concerns of novelty and clarity. i think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since september. it is therefore recommended that the authors improve the clarity based on the reviewer feedback. in particular, clarifying the details around learning \sigma_i and graph construction. it would also be useful to include the discussion of timing complexity in the final draft.",summary--------this paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.,the paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.,the paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.,this paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.,the paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.,this paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.,how about the results of label propagation without the episode training?,the paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.,0.1842105263157894,0.0,0.1184210526315789,0.1184210526315789,0.2119205298013245,0.0134228187919463,0.1059602649006622,0.1059602649006622,0.2119205298013245,0.0134228187919463,0.1059602649006622,0.1059602649006622,0.2119205298013245,0.0268456375838926,0.1456953642384106,0.1456953642384106,0.2119205298013245,0.0134228187919463,0.1059602649006622,0.1059602649006622,0.2119205298013245,0.0268456375838926,0.1456953642384106,0.1456953642384106,0.1159420289855072,0.0294117647058823,0.072463768115942,0.072463768115942,0.2119205298013245,0.0134228187919463,0.1059602649006622,0.1059602649006622,13.912609100341797,10.105342864990234,13.912610054016112,10.105340003967283,10.068832397460938,10.105340003967283,10.105342864990234,9.182830810546877,0.9788162605682174,0.9765525099964238,0.922570412011124,0.9747233037309525,0.9739323668327301,0.9152266617214077,0.9747233037309525,0.9739323668327301,0.9152268013893148,0.9781446643569426,0.9754134346136063,0.9309263084230277,0.9747233037309525,0.9739323668327301,0.9152268013893148,0.9781446643569426,0.9754134346136063,0.9309263431031889,0.9610109067918141,0.95631381805182,0.7257352674263552,0.9747233037309525,0.9739323668327301,0.9152266617214077
137,https://openreview.net/forum?id=SyYe6k-CW,"this paper presents the comparison of a list of algorithms for contextual bandit with thompson sampling subroutine. the authors compared different methods for posterior estimation for thompson sampling. experimental comparisons on contextual bandit settings have been performed on a simple simulation and quite a few real datasets.----------------the main paper + appendix are clearly written and easy to understand. the main paper itself is very incomplete. the experimental results should be summarized and presented in the main context. there is a lack of novelty of this study. simple comparisons of different posterior estimating methods do not provide insights or guidelines for contextual bandit problem. ----------------what's the new information provided by running such methods on different datasets? what are the newly observed advantages and disadvantages of them? what could be the fundamental reasons for the variety of behaviors on different datasets? no significant conclusions are made in this work.----------------experimental results are not very convincing. there are lots of plots show linear cumulative regrets within the whole time horizon. linear regrets represent either trivial methods or not long enough time horizon. the paper ""deep bayesian bandits showdown"" proposes a comparative study about bandit approaches using deep neural networks. ----------------while i find that such a study is a good idea, and that i was really interested by the listing of the different possibilities in the algorithms section, i regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches. the given discussion is not enough connected to the presented results from my point of view and it is difficult to figure out what is the basis of some conclusion.----------------also, the considered algorithms are not enough described to allow the reader to have enough insights to fully understand the proposed arguments. maybe authors should have focused on less algorithms but with more implementation details. also, what does not help is that it is very hard to conect the names in the result table with the corresponding approaches (some abbreviations are not defined at all - bbbn or rms for instances).----------------at last, the experimental protocol should be better described. for instance it is not clear on how the regret is computed : is it based on the best expectation (as done in most os classical studies) or on the best actual score of actions? the wheel bandit protocol is also rather hard to follow (and where is the results analysis?).----------------other remarks:-------- - it is a pitty that expectation propagation approaches have been left aside since they correspond to an important counterpart to variational ones. it would have been nice to get a comparaison of both; -------- - variational inference decsription in section algorithms is not enough developped w.r.t. the importance of this family of approaches-------- - neural linear is strange to me. uncertainty does not consider the neural representation of inputs ? how does it work then ?-------- - that is strange that \lambda_0 and \mu_0 do not belong to the stated asumptions in the linear methods part (ok they correspond to some prior but it should be clearly stated)-------- - figure 1 is referenced very late (after figure 2)","this paper is not aimed at introducing new methodologies (and does not claim to do so), but instead it aims at presenting a well-executed empirical study. the presentation and outcomes of this study are quite instructive, and with the ever-growing list of academic papers, this kind of studies are a useful regularizer.","----------------while i find that such a study is a good idea, and that i was really interested by the listing of the different possibilities in the algorithms section, i regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches.","----------------while i find that such a study is a good idea, and that i was really interested by the listing of the different possibilities in the algorithms section, i regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches.",the importance of this family of approaches-------- - neural linear is strange to me.,this paper presents the comparison of a list of algorithms for contextual bandit with thompson sampling subroutine.,"the paper ""deep bayesian bandits showdown"" proposes a comparative study about bandit approaches using deep neural networks.","----------------while i find that such a study is a good idea, and that i was really interested by the listing of the different possibilities in the algorithms section, i regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches.",the experimental results should be summarized and presented in the main context.,"----------------while i find that such a study is a good idea, and that i was really interested by the listing of the different possibilities in the algorithms section, i regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches.",0.2990654205607477,0.019047619047619,0.1682242990654205,0.1682242990654205,0.2990654205607477,0.019047619047619,0.1682242990654205,0.1682242990654205,0.1791044776119403,0.0307692307692307,0.1194029850746268,0.1194029850746268,0.2535211267605634,0.0579710144927536,0.1971830985915492,0.1971830985915492,0.1408450704225352,0.0,0.1126760563380281,0.1126760563380281,0.2990654205607477,0.019047619047619,0.1682242990654205,0.1682242990654205,0.1212121212121212,0.0,0.0909090909090909,0.0909090909090909,0.2990654205607477,0.019047619047619,0.1682242990654205,0.1682242990654205,7.165641784667969,11.349621772766112,14.453339576721191,7.165640830993652,7.165641784667969,8.949689865112305,7.165641784667969,6.691804885864258,0.9840798257636648,0.975589279306604,0.9528832067581572,0.9840798257636648,0.975589279306604,0.9528831755514617,0.9653414073951915,0.9670626757905193,0.0024062581927621856,0.9706566079500378,0.969980779163397,0.8923684235054885,0.9749224951364417,0.9737740248061584,0.9395193324374478,0.9840798257636648,0.975589279306604,0.9528831755514617,0.961672067847528,0.9644358811623361,0.8945582166967552,0.9840798257636648,0.975589279306604,0.9528831755514617
138,https://openreview.net/forum?id=SyfiiMZA-,"the paper presents a model-free strategy for jointly optimizing robot design and a neural network-based controller. while it is well-written and covers quite a lot of related work, i have a few comments with regards to the algorithm and experiments.----------------- the algorithm boils down to an alternating policy gradient optimization of design and policy parameters, with policy parameters shared between all designs. this requires the policy to have to generalize across the current design distribution. how well the policy generalizes is then in turn fed back into the design parameter distribution, favoring those designs it could improve on the quickest. however, these designs are not guaranteed to be optimal in the long run, with further specialization. the results for the walker2d might be hinting at this. a comparison between a completely shared policy vs. a specialized policy per design, possibly aided by a meta-learning technique to speed up the specialization, would greatly benefit the paper and motivate the use of a shared policy more quantitatively. if the condition of a common state/action space (morphology) is relaxed, then the assumption of smoothness in design space is definitely not guaranteed.--------- related to that, it would be interesting to see a visualization of the design space distribution. is the gmm actually multimodal within a single run (which implies the policy is able to generalize across significantly different designs)? --------- there are a separate number of optimization steps for the design and policy parameters within each iteration of the training loop, however the numbers used for the experiments are not listed. it would be interesting to see what the influence of the ratio of these steps is, as well as to know how many design iterations were taken in order to get to those in fig. 4. this is especially relevant if this technique is to be used with real physical systems. one could argue that, although not directly used for optimization or planning, the physics simulator acts a cheap dynamics model to test new designs.--------- i wonder how robust and/or general the optimized designs are with respect to the task settings. do small changes in the task or reward structure (i.e. friction or reward coefficients) result in wildly different designs? in practice, good robot designs are also robust and flexible and it would be great to get an idea how locally optimal the found designs are.----------------in summary, while the paper presents a simple but possibly effective and very general co-optimization procedure, the experiments and discussion don't definitively illustrate this.----------------minor remarks:--------- sec. 1, final paragraph: ""to do the best of our knowledge""--------- sec. 2, 3rd paragraph: ""contract constraints""--------- sec. 4.1: a convolutional neural network consisting only of fully-connected layers can hardly be called convolutional--------- fig. 3: 20% difference to the baseline for the walker2d is borderline of what i would call comparable, but it seems like some of these runs have not converged yet so that difference might still decrease.--------- fig. 3: please add x & y labels. this is a well written paper, very nice work.--------it makes progress on the problem of co-optimization of the physical parameters of a design--------and its control system. while it is not the first to explore this kind of direction,--------the method is efficient for what it does; it shows that at least for some systems, --------the physical parameters can be optimized without optimizing the controller for each --------individual configuration. instead, they require that the same controller works over an evolving--------distribution of the agents. this is a simple-but-solid insight that makes it possible--------to make real progress on a difficult problem.----------------pros: simple idea with impact; the problem being tackled is a difficult one--------cons: not many; real systems have constraints between physical dimensions and the forces/torques they can exert-------- some additional related work to consider citing. the resulting solutions are not necessarily natural configurations, -------- given the use of torques instead of musculotendon-modeling. but the current system is a great start.----------------the introduction could also promote that over an evolutionary time-frame, the body and--------control system (reflexes, muscle capabilities, etc.) presumably co-evolved.----------------the following papers all optimize over both the motion control and the physical configuration of the agents.--------they all use derivative free optimization, and thus do not require detailed supervision or precise models--------of the dynamics.----------------- geijtenbeek, t., van de panne, m., & van der stappen, a. f. (2013). flexible muscle-based locomotion-------- for bipedal creatures. acm transactions on graphics (tog), 32(6), 206.-------- (muscle routing parameters, including insertion and attachment points) are optimized along with the control).----------------- sims, k. (1994, july). evolving virtual creatures. in proceedings of the 21st annual conference on-------- computer graphics and interactive techniques (pp. 15-22). acm.-------- (a combination of morphology, and control are co-optimized)----------------- agrawal, s., shen, s., & van de panne, m. (2014). diverse motions and character shapes for simulated-------- skills. ieee transactions on visualization and computer graphics, 20(10), 1345-1355.-------- (diversity in control and diversity in body morphology are explored for fixed tasks)----------------re: heavier feet requiring stronger ankles--------this commment is worth revisiting. stronger ankles are more generally correlated with --------a heavier body rather than heavy feet, given that a key role of the ankle is to be able--------to provide a ""push"" to the body at the end of a stride, and perhaps less for ""lifting the foot"".----------------i am surprised that the optimization does not converge to more degenerate solutions--------given that the capability to generate forces and torques is independent of the actual--------link masses, whereas in nature, larger muscles (and therefore larger masses) would correlate--------with the ability to generate larger forces and torques. the work of sims takes these kinds of --------constraints loosely into account (see end of sec 3.3).----------------it would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.----------------i suspect that the reward function that penalizes torques in a uniform fashion across all joints would--------favor body configurations that more evenly distribute the motion effort across all joints, in an effort--------to avoid large torques. ----------------are the four mixture components over the robot parameters updated independently of each other--------when the parameter-exploring policy gradients updates are applied? it would be interesting--------to know a bit more about how the mean and variances of these modes behave over time during--------the optimization, i.e., do multiple modes end up converging to the same mean? what does the--------evolution of the variances look like for the various modes?","the chief contribution of this paper is to show that a single set of policy parameters can be optimized in an alternating fashion while the design parameters of the body are also optimized with policy gradients and sampled. the fact that this simple approach seems to work is interesting and worthy of note. however, the paper is otherwise quite limited - other methods are not considered or compared, incomplete experimental results are given, and important limitations of the method are not addressed. as it is an interesting but preliminary work, the workshop track would be appropriate.","the work of sims takes these kinds of --------constraints loosely into account (see end of sec 3.3).----------------it would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.----------------i suspect that the reward function that penalizes torques in a uniform fashion across all joints would--------favor body configurations that more evenly distribute the motion effort across all joints, in an effort--------to avoid large torques.","while it is well-written and covers quite a lot of related work, i have a few comments with regards to the algorithm and experiments.----------------- the algorithm boils down to an alternating policy gradient optimization of design and policy parameters, with policy parameters shared between all designs.","while it is well-written and covers quite a lot of related work, i have a few comments with regards to the algorithm and experiments.----------------- the algorithm boils down to an alternating policy gradient optimization of design and policy parameters, with policy parameters shared between all designs.",the paper presents a model-free strategy for jointly optimizing robot design and a neural network-based controller.,"ieee transactions on visualization and computer graphics, 20(10), 1345-1355.-------- (diversity in control and diversity in body morphology are explored for fixed tasks)----------------re: heavier feet requiring stronger ankles--------this commment is worth revisiting.","the work of sims takes these kinds of --------constraints loosely into account (see end of sec 3.3).----------------it would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.----------------i suspect that the reward function that penalizes torques in a uniform fashion across all joints would--------favor body configurations that more evenly distribute the motion effort across all joints, in an effort--------to avoid large torques.","in practice, good robot designs are also robust and flexible and it would be great to get an idea how locally optimal the found designs are.----------------in summary, while the paper presents a simple but possibly effective and very general co-optimization procedure, the experiments and discussion don't definitively illustrate this.----------------minor remarks:--------- sec.","while it is well-written and covers quite a lot of related work, i have a few comments with regards to the algorithm and experiments.----------------- the algorithm boils down to an alternating policy gradient optimization of design and policy parameters, with policy parameters shared between all designs.",0.3023255813953488,0.0352941176470588,0.1395348837209302,0.1395348837209302,0.352112676056338,0.0714285714285714,0.1267605633802817,0.1267605633802817,0.352112676056338,0.0714285714285714,0.1267605633802817,0.1267605633802817,0.1061946902654867,0.018018018018018,0.1061946902654867,0.1061946902654867,0.1076923076923076,0.0,0.0769230769230769,0.0769230769230769,0.3023255813953488,0.0352941176470588,0.1395348837209302,0.1395348837209302,0.3199999999999999,0.054054054054054,0.1466666666666667,0.1466666666666667,0.352112676056338,0.0714285714285714,0.1267605633802817,0.1267605633802817,8.181589126586914,5.875777244567871,11.609619140625,11.728960990905762,8.181589126586914,11.728960990905762,11.728960990905762,12.899868965148926,0.1303769932432216,0.19249129638716805,0.9439853440743403,0.9734515374024867,0.9704661849128392,0.8881961656136502,0.9734515374024867,0.9704661849128392,0.8881959801791047,0.9695663031075582,0.9714662194264062,0.9017669303928882,0.06785929112803907,0.24668770329079207,0.8589269590401593,0.1303769932432216,0.19249129638716805,0.9439853196357271,0.970517933336053,0.9711914473902215,0.20639354757357034,0.9734515374024867,0.9704661849128392,0.8881961656136502
139,https://openreview.net/forum?id=Sygn20VtwH,"update: as no rebuttal has been posted i stand by my assessment.----------summary-----this papers proposes a recursive parameterization of gates in a recurrent model. instead of directly conditioning gates on the input and previous hidden representation, the proposed model recursively calls itself to parameterize the gate. the recursion depth is dynamically determined (up to a predefined maximum recursion depth). the approach shows slight improvements over baselines on a range of tasks.----------strengths-----slight improvements on a wide range of downstream tasks-----qualitative analysis of dynamic recursion highlighting the adaptability of the method to different task properties----------weaknesses-----from the equations in section 2, i understand that a gate is conditioned not only on the input and previous hidden representation, but also the input and hidden representations from repeated application of the rnn cell for the given time step. as far as i understand, this is very closely related to act by graves, alex. ""adaptive computation time for recurrent neural networks."" 2016. they also have a dynamic way of determining how many recursive rnn cell applications per step should be performed.-----i am missing a clear description of differences of the proposed approach to the baselines tested in section 3. at some point it is mentioned that a different optimizer was used (r-adam). are there any other confounding factors? i believe it is important to get clarity on these given that the difference between the model and baselines are very small.-----results in table 1 are highlighted in a misleading way. for example, stacked bilstm do as well for tree traversal (em, n=5 and em, n=10). for logical inference, there is a more recent paper investigating the limits of rnns and i believe comparisons on that dataset could strengthen the paper: evans, richard, et al. ""can neural networks understand logical entailment?."" iclr 2018.----------minor comments-----p1: ""the ability to reason deeply""  i understand you mean literally ""deep"", but also ""reason"" is a loaded term.-----p1: ""bears a totally different meaning"" should be made concrete-----p3: i am a bit confused by the fact that metagross is used to extend transformers here given that you called it a recurrent (and recursive) model on the previous slide.-----p4: what is the number of layers used in he stacked bilstm and is this the same as the maximum depth used in metagross? i believe for a fair comparison they should be the same. i am also missing stacked lstm in experiments on logical inference (table 2).-----questions to authors this paper proposes a neural sequence modelling unit called metagross. in principle, the aim of this unit is to introduce recursive parametrization of gating functions, building on the gated rnn paradigm. the authors motivate this work by arguing that while gated-rnns tackle vanishing gradient problems and facilitate learning long-range dependencies in sequences, improvements can be made with respect to learning on hierarchically-structured data. the authors propose a method to do so by also learning the depth of the parametrization, and claim that the inductive bias that emerges from this configuration is beneficial to learning such tasks.----------i think the idea behind this work is sensible: introducing a meta-controller with recursive parametrization of gating functions for hierarchical tasks is sensible. another also strong point of this paper is that several experiments under different settings are presented, along with ablation studies and some model exploration. the authors also show that integrating a non-autoregressive variant of the proposed meta-controller into the transformer architecture can also be beneficial. results in general do show improvement over compared architectures.----------on the negative side and besides empirical/experimental evidence, the paper would be much more convinving with some more insight into the model itself and some more qualitative evidence. figure 1 shows the architecture of the proposed method with a max depth of 3 indicating soft recursion with grayscale levels. however, this figure is not referenced in the text and not explained further. the non-autoregressive version (sec 2.3) simply does away with the dependence on hidden states and applies the proposed architecture directly on the input. i think it would be very beneficial to see a simple toy example where the benefits of utilizing this meta-architecture can be qualitatively explained. finally, one can argue that this work is incremental, in the sense that it is a (relatively straightfwd) combination of meta-controllers with recursive architectures. differences and variations with respect to other methods in literature should be more clearly explained.----------some more questions------ although most experiments show some resilience with respect to the varying max depth parameter, have the authors noticed any problems and limitations arising from setting the max-depth to be very high? for example, in table 1 it seems that accuracy may drop when increasing max length. i think that more discussions and ellaboration on this could be useful, as the authors also propose a way of learning the depth parameters and also state that this is task dependent. ------ figures 7-8-9 show variations of dynamic recursion on three databases. although these figures do show variation in learning (ranging from high activation fluctuations to static), it would be interesting to examine why this fluctuations occur in cifar ------ which brings me to the second question: although the results do not show this, could this task-dependent nature of the controller lead to more chances of overfitting on a given training set that may be noisy?------ although the authors perform ablation tests with multiple units and evaluate for max depth, we don't see many experiments with depth of more than 2 or 3 (besides table 1 - in many experiments the max depth is not mentioned). are the conclusions the same with all experiments wrt depth?------ it would be useful to have a comment on model complexity the authors propose a variant of gating functions for recurrent neural networks and feed-forward layers of transformer and apply it to variety of tasks including toy tasks such as sorting, tree traversal and more realistic tasks such as machine translation. the gating function is applied recursively for n number of steps and depth of recursion is learned softly in data-driven function. authors show similar or slightly better performance of their approach when applied to lstm and transformer compared to vanilla lstm and transformer. ----------i have several comments regarding this work:----------1) i believe there is a typo in equation in section 2 describing parametrization of o^{n}_{t} where h_{t-1} has un-necessary upper-script {n}----------2) i would like to see the equations showing differences/similarities between metagross gating function and gru/lstm in section 2 to better understand how metagross relates to previous work.----------3) calling parallel version of metagross that computes gating values given entire sequence *non-autoregressive* is really correct since decoding for machine translation is still done in autoregressive fashion one token at a time. i would suggest the authors to remove word non-autoregressive and just stick with word parallel.----------4) the obvious connection with this work and work of alex graves on adaptive computation for recurrent neural nets and many of its followups including universal transformers by dehghani et al is missing from experimental comparison and is not even mentioned in related section. i believe that not mentioning these papers and not comparing to them empirically this is a major drawback of this paper. ----------5) i don't think that task 3 (logical inference) contains a language vocabulary of six words because it is a natural english language unless i am misunderstanding something. ----------6) what does em stand for in table 1. would be great if you could include description of what em, p(perplexity), n(depth of recursion) stands for in caption of table 1----------overall it is a good and well written paper, although i believe that the variants of recursively gated functions have been proposed and applied before (see comment 4). would be open to discussions and raising scores if authors convince me otherwise.","this paper proposes a recurrent architecture based on a recursive gating mechanism. the reviewers leaned towards rejection on the basis of questions regarding novelty, analysis, and the experimental setting. surprisingly, the authors chose not to engage in discussion, as all reviewers seems pretty open to having their minds changed. if none of the reviewers will champion the paper, and the authors cannot be bothered to champion their own work, i see no reason to recommend acceptance.","the approach shows slight improvements over baselines on a range of tasks.----------strengths-----slight improvements on a wide range of downstream tasks-----qualitative analysis of dynamic recursion highlighting the adaptability of the method to different task properties----------weaknesses-----from the equations in section 2, i understand that a gate is conditioned not only on the input and previous hidden representation, but also the input and hidden representations from repeated application of the rnn cell for the given time step.","although these figures do show variation in learning (ranging from high activation fluctuations to static), it would be interesting to examine why this fluctuations occur in cifar ------ which brings me to the second question: although the results do not show this, could this task-dependent nature of the controller lead to more chances of overfitting on a given training set that may be noisy?------ although the authors perform ablation tests with multiple units and evaluate for max depth, we don't see many experiments with depth of more than 2 or 3 (besides table 1 - in many experiments the max depth is not mentioned).",i am also missing stacked lstm in experiments on logical inference (table 2).-----questions to authors this paper proposes a neural sequence modelling unit called metagross.,update: as no rebuttal has been posted i stand by my assessment.----------summary-----this papers proposes a recursive parameterization of gates in a recurrent model.,i am also missing stacked lstm in experiments on logical inference (table 2).-----questions to authors this paper proposes a neural sequence modelling unit called metagross.,"the approach shows slight improvements over baselines on a range of tasks.----------strengths-----slight improvements on a wide range of downstream tasks-----qualitative analysis of dynamic recursion highlighting the adaptability of the method to different task properties----------weaknesses-----from the equations in section 2, i understand that a gate is conditioned not only on the input and previous hidden representation, but also the input and hidden representations from repeated application of the rnn cell for the given time step.","instead of directly conditioning gates on the input and previous hidden representation, the proposed model recursively calls itself to parameterize the gate.","----------6) what does em stand for in table 1. would be great if you could include description of what em, p(perplexity), n(depth of recursion) stands for in caption of table 1----------overall it is a good and well written paper, although i believe that the variants of recursively gated functions have been proposed and applied before (see comment 4).",0.2838709677419355,0.0392156862745098,0.1677419354838709,0.1677419354838709,0.2444444444444444,0.0337078651685393,0.1111111111111111,0.1111111111111111,0.196078431372549,0.0599999999999999,0.0784313725490196,0.0784313725490196,0.2574257425742574,0.101010101010101,0.1386138613861386,0.1386138613861386,0.196078431372549,0.0599999999999999,0.0784313725490196,0.0784313725490196,0.2838709677419355,0.0392156862745098,0.1677419354838709,0.1677419354838709,0.2040816326530612,0.0208333333333333,0.1428571428571428,0.1428571428571428,0.2335766423357664,0.0148148148148148,0.1021897810218978,0.1021897810218978,9.714427947998049,5.099612712860107,12.13005256652832,4.438272476196289,9.714427947998049,5.099613189697266,5.047513484954834,12.16540241241455,0.9654466793275649,0.9639025634533478,0.9242154774777687,0.46832537224475757,0.5833509644438493,0.9210291051327031,0.9618874297706211,0.9596590717322213,0.05336967718388977,0.9728925438101336,0.9636551060031002,0.9258590864463042,0.9618874297706211,0.9596590717322213,0.05336963908304174,0.9654466793275649,0.9639025634533478,0.9242155622585577,0.9337829660549876,0.9391225455431063,0.9222516233703217,0.5200419030418449,0.6804341136652283,0.7741044127528469
140,https://openreview.net/forum?id=Syjha0gAZ,"this is an interesting paper, in the sense of looking at a problem such as multiset prediction in the context of sequential decision making (using a policy). ----------------in more detail, the authors construct an oracle policy, shown to be optimal (in terms of precision and recall). a parametrized policy instead of the oracle policy is utilized in the proposed multiset loss function, while furthermore, a termination policy facilitates the application on variable-sized multiset targets. the authors also study other loss functions, ordered sequence prediction as well as reinforcement learning.----------------results show that the proposed order-invariant loss outperforms other losses, along with a set if experiments evaluating choice of rank function for sequence prediction and selection strategies. the experiments seem rather comprehensive, as well as the theoretical analysis. the paper describes an interesting approach to the problem.----------------while the paper is comprehensive it could be improved in terms of clarity & flow (e.g., by better preparing the reader on what is to follow) this paper proposes a type of loss functions for the problem of multiset prediction. a detailed discussion on the intuition is provided and extensive experiments are conducted to show that this loss function indeed provides some performance gain in terms of exact matching and f1-score.----------------the idea of this paper is as follows: instead of viewing the multiset prediction task as a classification problem, this paper models it as a sequential decision problem (this idea is not new, see welleck et al., 2017, as pointed out by the authors). define pi* to be the optimal policy that outputs the labels of input x in a certain way. we learn a parameterized policy pi(theta) which takes x and all previous predictions as input, and outputs a label as a new prediction. at each time step t, pi* and pi(theta) can be viewed as distributions over all remaining labels; the kl divergence is then used to calculate the difference between those two distributions. finally, the loss function sums those kl divergences over all t. computing this loss function directly can be intractable, so the author suggested that one can compute the entire trajectory of predictions and then do aggregation.----------------admitted that such construction is quite intuitive, and can possibly be useful, the technical part of this paper seems to be rather straightforward. in particular, i think the solution to the issue of unknown t is very heuristic, making the proposed loss function less principled. ------------------------other detailed comments/issues:------------------ it seems to me that models that can utilize this loss function must support varying-length inputs, e.g. lstm. any idea how to apply it to models with fixed-length inputs?------------------ the proposed loss function assumes that t (i.e. the size of the output set) is known, which is unrealistic. to handle this, the authors simply trained an extra binary classifier that takes x and all previous predictions as the input at each time step, and decides whether or not to terminate. i think this solution is rather hacky and id like to see a more elegant solution, e.g. incorporate the loss on t into the proposed loss function.------------------ could you formally define exact match?------------------ in section 4.4, maybe it is better to run the stochastic sampling multiple times to reduce the variance? i would expect that the result can be quite different in different runs.------------------ any discussion on how will the classifier for t affect the experimental results?","the submission addresses the problem of multiset prediction, which combines predicting which labels are present, and counting the number of each object. experiments are shown on a somewhat artificial mnist setting, and a more realistic problem of the coco dataset. there were several concerns raised by the reviewers, both in terms of the clarity of presentation (reviewer 1), and that the proposed solution is somewhat heuristic (reviewer 3). on the balance, two of three reviewers did not recommend acceptance.","to handle this, the authors simply trained an extra binary classifier that takes x and all previous predictions as the input at each time step, and decides whether or not to terminate.","the authors also study other loss functions, ordered sequence prediction as well as reinforcement learning.----------------results show that the proposed order-invariant loss outperforms other losses, along with a set if experiments evaluating choice of rank function for sequence prediction and selection strategies.",define pi* to be the optimal policy that outputs the labels of input x in a certain way.,"this is an interesting paper, in the sense of looking at a problem such as multiset prediction in the context of sequential decision making (using a policy).","a parametrized policy instead of the oracle policy is utilized in the proposed multiset loss function, while furthermore, a termination policy facilitates the application on variable-sized multiset targets.","the authors also study other loss functions, ordered sequence prediction as well as reinforcement learning.----------------results show that the proposed order-invariant loss outperforms other losses, along with a set if experiments evaluating choice of rank function for sequence prediction and selection strategies.","a parametrized policy instead of the oracle policy is utilized in the proposed multiset loss function, while furthermore, a termination policy facilitates the application on variable-sized multiset targets.","the authors also study other loss functions, ordered sequence prediction as well as reinforcement learning.----------------results show that the proposed order-invariant loss outperforms other losses, along with a set if experiments evaluating choice of rank function for sequence prediction and selection strategies.",0.1441441441441441,0.0,0.1081081081081081,0.1081081081081081,0.180327868852459,0.0333333333333333,0.1147540983606557,0.1147540983606557,0.1443298969072164,0.0,0.1030927835051546,0.1030927835051546,0.2075471698113207,0.0192307692307692,0.1320754716981132,0.1320754716981132,0.2037037037037037,0.0377358490566037,0.1296296296296296,0.1296296296296296,0.180327868852459,0.0333333333333333,0.1147540983606557,0.1147540983606557,0.2037037037037037,0.0377358490566037,0.1296296296296296,0.1296296296296296,0.180327868852459,0.0333333333333333,0.1147540983606557,0.1147540983606557,9.91057586669922,13.937602996826172,11.797513961791992,9.910577774047852,7.440158843994141,7.525371074676514,9.91057586669922,13.93760108947754,0.949655220976486,0.9551291737082436,0.9171369481345872,0.9740915057088074,0.9736669006990397,0.8549163971809658,0.9534712920502356,0.9660721287556271,0.8781450694282579,0.979092683372648,0.977775737968938,0.9515602871736151,0.9622094776430776,0.9616144207598194,0.9385150947248639,0.9740915057088074,0.9736669006990397,0.8549165036547114,0.9622094776430776,0.9616144207598194,0.9385150352881317,0.9740915057088074,0.9736669006990397,0.8549163971809658
141,https://openreview.net/forum?id=SylpBgrKPH,"this contribution considers deep latent-factor models for causal inference -inferring the effect of a treatment- in the presence of missing values. the core challenge is that of confounders not directly observed, and accessible only via noisy proxys, in particular in the missingness. the contributed method relies on using the latent-factor model for multiple imputation in doubly-robust causal treatment effect estimators. as a consequence, it requires the missing at random assumption to control for the impact of imputation. given that the confounders are not directly assumed, a first approach estimates their effect via an estimate of p(z|x*) (probability of confounder given observed data), which is then plugged in the doubly robust estimator in a multiple imputation strategy. a second approach uses heuristically the estimated latent confounders as regressors of non interest in a linear-regression model. the approaches are empirically compared to other imputation strategies used as plugins in the doubly-robust estimator. the contributed approach show marked benefits when the problem is highly non-linear.----------the manuscript is clearly written. ----------i do not have many comments.----------one concern though is that the vae comes with a significant amount of hyper-parameters that do not seem obvious to set. this is to be contrasted with other approaches compared to. how was the specific architecture and learning strategy of the vae selected?----------the simulation settings are somewhat artificial. more simulations inspired from real-life causal scenario would improve the work.----------i hope that in the final version, the code will be available publicly, and not on request. this paper introduces missdeepcausal method to address the problem of treatment effect estimation with incomplete covariates matrix (missing values at random -- mar). it makes use of variational autoencoders (vae) to learn the latent confounders from incomplete covariates. this also helps encoding complex non-linear relationships in the data, a capability that is missing in the work of kallus et al. (2018) -- the work which this paper extends. they employ the missing data importance weight autoencoder (miwae) approach (mattei & frellsen, 2019) to approximate the posterior of their latent factors z given the observed incomplete covariates x*. the main contributions of this work are presented in sections 3.2 and 3.3, where they use the approximated posterior derived from miwae to sample z to be used for estimating outcomes and finally calculating the average treatment effect (ate). this is done according to the doubly robust estimator developed for data with incomplete covariates (mayer et al., 2019b). ----------in summary, i am not convinced that the contribution of this paper is enough, nor of its novelty. however, i will read the rebuttal carefully and am willing to increase the score if the authors address this concern.----------there are several points that need further clarification; e.g., ----- - figure 1 as well as figure 2 show a directed edge from x* to x_{miss}. does this mean that x* has all the proxies needed to identify x_{miss}? ----- - how does this method assure/evaluate that z embeds enough information to predict accurate effects?----- - how are \mu_0 and \mu_1 functions trained on z----------things to improve the paper that did not impact the score:----- - page 2, par. 2, last line: state-of-the-art methods----- - page 3, under unconfoundedness par., line -7: [...] for each observation comma treatment assignment [...]----- - page 3, figure 1: according to iclrs formatting guidelines, the figure number and caption must always appear after the figure.----- - page 3, missingness par., line 1: [...] is one of the most [...]----- - page 5, line after eq. (8): 8 should be in parentheses.----- - page 7, figure 3: box-plots are hardly legible.----- - page 7, figure 3 caption, line 2: keep (logistic-)linear together with \mbox{} or ~ in latex----------references:----- - kallus, n., mao, x., & udell, m. (2018). causal inference with noisy and missing covariates via matrix factorization. in advances in neural information processing systems (pp. 6921-6932).----- - mattei, p. a., & frellsen, j. (2019). miwae: deep generative modelling and imputation of incomplete data sets. in international conference on machine learning (pp. 4413-4423).----- - mayer, i., wager, s., gauss, t., moyer, j. d., & josse, j. (2019). doubly robust treatment effect estimation with missing attributes. preprint.---------------********update after reading the rebuttal********-----the authors have provided further clarifications in their rebuttal and therefore, i increased my score form weak reject to weak accept.","this paper addresses the problem of causal inference from incomplete data. the main idea is to use a latent confounders through a vae. a multiple imputation strategy is then used to account for missing values. reviewers have mixed responses to this paper. initially, the scores were 8,6,3. after discussion the reviewer who rated is 8 reduced their score to 6, but at the same time the score of 3 went up to 6. the reviewers agree that the problem tackled in the paper is difficult, and also acknowledge that the rebuttal of the paper was reasonable and honest. the authors added a simulation study which shows good results.----------the main argument towards rejection is that the paper does not beat the state of the art. i do think that this is still ok if the paper brings useful insights for the community even though it does not beat the state fo the art. for now, with the current score, the paper does not make the cut. for this reason, i recommend to reject the paper, but i encourage the authors to resubmit this to another venue after improving the paper.","the main contributions of this work are presented in sections 3.2 and 3.3, where they use the approximated posterior derived from miwae to sample z to be used for estimating outcomes and finally calculating the average treatment effect (ate).","given that the confounders are not directly assumed, a first approach estimates their effect via an estimate of p(z|x*) (probability of confounder given observed data), which is then plugged in the doubly robust estimator in a multiple imputation strategy.",this paper introduces missdeepcausal method to address the problem of treatment effect estimation with incomplete covariates matrix (missing values at random -- mar).,this contribution considers deep latent-factor models for causal inference -inferring the effect of a treatment- in the presence of missing values.,"they employ the missing data importance weight autoencoder (miwae) approach (mattei & frellsen, 2019) to approximate the posterior of their latent factors z given the observed incomplete covariates x*.",this paper introduces missdeepcausal method to address the problem of treatment effect estimation with incomplete covariates matrix (missing values at random -- mar).,"4413-4423).----- - mayer, i., wager, s., gauss, t., moyer, j. d., & josse, j.","given that the confounders are not directly assumed, a first approach estimates their effect via an estimate of p(z|x*) (probability of confounder given observed data), which is then plugged in the doubly robust estimator in a multiple imputation strategy.",0.1459227467811159,0.0086580086580086,0.0858369098712446,0.0858369098712446,0.1545064377682403,0.0519480519480519,0.0772532188841201,0.0772532188841201,0.1121495327102803,0.0471698113207547,0.0934579439252336,0.0934579439252336,0.1214953271028037,0.0283018867924528,0.0747663551401869,0.0747663551401869,0.0909090909090909,0.0,0.0545454545454545,0.0545454545454545,0.1121495327102803,0.0471698113207547,0.0934579439252336,0.0934579439252336,0.0097560975609756,0.0,0.0097560975609756,0.0097560975609756,0.1545064377682403,0.0519480519480519,0.0772532188841201,0.0772532188841201,8.575971603393555,6.190203666687012,12.00193977355957,10.307948112487791,6.919802665710449,8.575970649719238,10.307945251464844,2.6668295860290527,0.9464442682461953,0.9484273715565946,0.8356270669366961,0.9116873465079264,0.9205013345169314,0.9266348353802335,0.9528963831727258,0.9565659229328846,0.3439332976055277,0.9505933470938062,0.9540754479692791,0.9485484262577993,0.9443057246822703,0.9488485023687395,0.6639854347496431,0.9528963831727258,0.9565659229328846,0.34393288036388714,0.01034194925042097,0.15274762035447592,0.289459753639682,0.9116873465079264,0.9205013345169314,0.9266348837086453
142,https://openreview.net/forum?id=SyxBxCNFwr,"this paper presents a method based on convolutional lstm that achieves compression of very small resolution images (e.g., 32x32). in this setting it achieves what may be seen as sota performance for its class (i.e., recurrent, progressive). the authors also look into applying this method in the setting of distributed source coding.----------i think we may want to accept this paper because i have not seen any such work before (though, i have to say i have never encountered distributed source coding problems before; but i have extensively studied neural image approaches).----------the paper uses a framework very similar to that of toderici et al., and it applies the modified architecture to cifar and mnist (the original method was never tested on those datasets by the authors). in this paper that method is outperformed by the proposed architecture, and results are presented in the case in which multiple signals need to be encoded together. since i am not familiar with the field of dsc, i cannot comment how important this is for that community.----------with respect to the evaluation on cifar and mnist, i have soem concerns. in particular, most containers (jpeg, jpeg 2k, bpg) have a considderably sized header. when dealing with such small images, this header may take a significant part of the size of the file. i don't see the authors discussing this, and describing how they adjusted the bitrates for those codecs. moreover, for datasets like cifar, jpeg and bpg are not able to operate properly unless they're set to run in 4:4:4 mode (it's by default using 4:2:0, which puts them at a huge disadvantage). therefore, i don't know whether i can trust parts of the evaluation, since the authors didn't call out any of these issues.----------moreover, it's not clear what psnr refers to. i can guess that it may be psnr in the rgb color space, but this is not called out.----------the figures are also poorly labeled. for example, in figure 4, it's not explicitly said which datasets are used (e.g., i am *guessing* it's cifar and mnist, but it's not clear). the caption for figure 5 should also be updated, and honestly, i don't think anyone can read the various labels in the charts presented in that figure if printed on a piece of paper, unless they have a looking glass...----------i also find it disturbing to be reading an image compression paper with no compressed images shown! i have no idea qualitatively whether this method is any good. in the final version i expect to see images compressed both in the multiple channel setup, and in the single channel setup. i expect to see images compressed with the codecs to be compared against at comparable bitrates, while taking into account to remove the headers and make sure that 4:4:4 mode is enabled, and that a hyperparameter sweep is attempted.----------if this paper is to be accepted, the evaluation section needs to be absolutely clear and all points i mentioned need to be addressed.----------on the novelty side of things, since it's a tweak on an existing method, i am not very excited, and since i know nothing on dsc, i can't comment on the novelty in that area. i'll defer that judgement to the other reviwers, who hopefully have more insight into this than i do.----------overall, i think it's an interesting twist on an existing type of method, and may be worth publishing assuming that the authors address the problems i mentioned earlier. the authors propose a method to train image compression models on multiple sources, with a separate encoder on each source, and a shared decoder. the model is based on a recurrent image compression network. the main difference from a standard image compression network is that it has multiple copies of encoders and at training time, it optimizes on all sources jointly. the experiments are conducted on the cifar10, mnist, and kodak dataset. it shows that the method works slightly worse than jointly training all sources, and clearly outperforms training separate models.----------the paper is clearly written and easy to follow. the proposed method is simple. ----------my main concerns are regarding the experiment results: -----1. the experiments are conducted on a not very realistic setting of splitting cifar10 and mnist by class labels. first of all these datasets are small, simple, and low-resolution. second, by splitting by the labels, the sources are still highly correlated as they are collected from the same distribution. jointly training multiple larger datasets that show more diverse properties would make the results more convincing. -----2. i don't find the motivations of training multiple encoders clear. given the fact that jointly training on a larger dataset typically gives a better performance, it's still not clear when we would want to train separate encoders. an extended discussion would help.-----3. i found the results not very surprising. the observations that a jointly trained model sees more examples, so performs -----better seems expected (particularly when the dataset is small). -----4. the technical contributions are light-weight. using multiple copies of parameters for different data distributions is used e.g., in multi-task learning and domain adaptation literatures. ----------overall i think this paper would benefit from improving the experiments and providing better supporting evidence for demonstrating practical value of the system. the paper proposed a distributed recurrent auto-encoder for image compression that uses a convlstm to learn binary codes that are constructed progressively from residuals of previously encoded information. experiments show that the proposed model achieved better psnr than other compression auto-encoders and algorithms. the paper claims distributed representation has advantage for image compression.----------the idea in the paper is a good one, except for the part about ""distributed"". 2 main concerns here:----------1. the paper uses the word ""distributed"" to mean multi-source or clustered data inputs. this is different from what the deep learning community normally means, and the paper has the risk of misleading readers without clarification or using a different terminology. why not multi-source / multi-modal?----------2. by analyzing figure 4 and figure 5 together, it seems that the psnr results are similar between multi-source / clustered data and normal auto-encoder. the paper mentioned that the multi-source model may have advantage over information corruption and balance between encoder/decoder model sizes, but no experiments are shown to demonstrate that. this rendered the motivation of the whole ""distributed"" part unclear.----------since the ""distributed"" seems to occupy a significant portion of the paper, i vote for rejection based on the current manuscript.","main content:----------blind review #3 summarizes it well:----------this paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. the central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious.----------the proposed problem seems sensible, and the method is a reasonable approach. the evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation).----------------------discussion:----------the reviewers were concerned about the relatively low level of novelty, simplicity of the proposed approach (which the authors argue could be seen as a feature rather than a flaw, given its good performance), and inadequate motivation.----------------------recommendation and justification:----------after the authors' revision in response to the reviews, this paper could be a weak accept if not for the large number of stronger submissions.",the paper proposed a distributed recurrent auto-encoder for image compression that uses a convlstm to learn binary codes that are constructed progressively from residuals of previously encoded information.,"the authors also look into applying this method in the setting of distributed source coding.----------i think we may want to accept this paper because i have not seen any such work before (though, i have to say i have never encountered distributed source coding problems before; but i have extensively studied neural image approaches).----------the paper uses a framework very similar to that of toderici et al., and it applies the modified architecture to cifar and mnist (the original method was never tested on those datasets by the authors).","for example, in figure 4, it's not explicitly said which datasets are used (e.g., i am *guessing* it's cifar and mnist, but it's not clear).","this paper presents a method based on convolutional lstm that achieves compression of very small resolution images (e.g., 32x32).","the authors propose a method to train image compression models on multiple sources, with a separate encoder on each source, and a shared decoder.","the authors also look into applying this method in the setting of distributed source coding.----------i think we may want to accept this paper because i have not seen any such work before (though, i have to say i have never encountered distributed source coding problems before; but i have extensively studied neural image approaches).----------the paper uses a framework very similar to that of toderici et al., and it applies the modified architecture to cifar and mnist (the original method was never tested on those datasets by the authors).","since i am not familiar with the field of dsc, i cannot comment how important this is for that community.----------with respect to the evaluation on cifar and mnist, i have soem concerns.","the authors also look into applying this method in the setting of distributed source coding.----------i think we may want to accept this paper because i have not seen any such work before (though, i have to say i have never encountered distributed source coding problems before; but i have extensively studied neural image approaches).----------the paper uses a framework very similar to that of toderici et al., and it applies the modified architecture to cifar and mnist (the original method was never tested on those datasets by the authors).",0.1063829787234042,0.021505376344086,0.0957446808510638,0.0957446808510638,0.2489959839357429,0.0323886639676113,0.1204819277108433,0.1204819277108433,0.0744680851063829,0.0,0.0531914893617021,0.0531914893617021,0.0782122905027933,0.0112994350282485,0.0670391061452514,0.0670391061452514,0.1311475409836065,0.0220994475138121,0.087431693989071,0.087431693989071,0.2489959839357429,0.0323886639676113,0.1204819277108433,0.1204819277108433,0.1354166666666666,0.0210526315789473,0.0833333333333333,0.0833333333333333,0.2489959839357429,0.0323886639676113,0.1204819277108433,0.1204819277108433,10.013195037841797,10.308874130249023,11.53167724609375,10.013195037841797,10.148009300231934,4.29506254196167,10.013195037841797,3.999480247497559,0.7357106179915474,0.8171511974408968,0.9180144723552525,0.97954173462005,0.9810437153323741,0.9488335932105326,0.8725523622077751,0.9125978251375649,0.8909107429316455,0.9744159518028258,0.9748855895698658,0.8862762819202964,0.9745298341549723,0.9749202173164752,0.9224474487613888,0.97954173462005,0.9810437153323741,0.9488336179535058,0.9403945284834695,0.9577974998590343,0.8719723109141441,0.97954173462005,0.9810437153323741,0.9488335234861801
143,https://openreview.net/forum?id=TYXs_y84xRj,"summary-----in this paper, the authors propose a simple but effective keypoint-based anchor-free object detection system. the main idea is to replace the cartesian coordinate with the polar coordinate, compared to the closest related work, fcos. according to the extensive empirical results, the proposed system achieves a better trade-off between speed and accuracy.-----the overall writing looks good to me. the storyline is consistent and well-motivated. the authors provide enough detail to shed light on the design choices for the state-of-the-art anchor-free detector. the figures and tables are also quite informative. for example, i particularly love the figure 1 because it helps the readers catch up with the most recent progress on keypoint-based object detection frontier. it could be better if the authors could describe more details in the caption. by the way, the black color of rho and theta should be changed into a lighter one.-----questions-----scale-sensitive vs. scale-invariant in this paper, the author mentioned the scale-related terminology many times. i wonder if the authors could explain more detail about why polarnet is scale-invariant? from my perspective, the offset regression is scale-sensitive because the target numbers are strongly related to the actual object size. even though the proposed method utilizes the corner points to localize objects, the bounding box offset part is still scale-sensitive, right?-----center-based method + polar coordinate i wonder if the authors could try to put the polar coordinate offset regression into a center-based anchor-free detector. for example, centernet[1] regresses the bounding box offset in the cartesian coordinate system. what if we change the distance encoding to polar coordinate? in section 4.4, the authors claim, ""specically, compared with other center-based methods such as centernet or fsfa, our method not only extracts features from the central region, but also encodes features from the whole bounding boxes."" i wonder how much performance gain could be seen if we only change the coordinate system. it could be a helpful ablation study to support this claim.-----reference-----[1] zhou, xingyi, dequan wang, and philipp krhenbhl. ""objects as points."" arxiv preprint arxiv:1904.07850 (2019).--------- post-rebuttal comments---------the rebuttal and the paper revision address my concerns. i keep my original rating. this paper proposes a new key-point based object detector, polarnet, which predicts the distances between key-points and corner pairs (such as top-left and bottom-right pair or top-right and bottom-left pair) on polar coordinates. this is different from other key-point based object detectors such as fcos which predicts distances between key-points and bounding box boundaries on cartesian coordinates. the authors claim that the advantage of representing the offsets in the form of polar coordinates is this representation reduces the variance in the offsets, which makes learning easier.-----pros: this is an interesting approach and new, to the best of my knowledge, in the context of object detection. the authors show that polarnet outperforms other approaches such as fcos and foveabox under the same backbone network, resnet-101. the use of polar coordinates improves the performance of fcos by more than 4% which shows the effectiveness of the polar coordinates. with a larger backbone and deformable convolution, polarnet demonstrates state-of-the-art performance among all anchor-free detectors on the challenging coco dataset.-----cons: i am confused about the corner supervision in section 3.4.2. it seems to me that the corner supervision is to train polarnet to predict the offsets which are in polar coordinates. but the authors also apply this to fcos (fcos + centerness + corner) and compare it with fcos with polar coordinates (fcos + polar). i dont understand the difference between them. how do the authors apply corner supervision if fcos is predicting on cartesian coordinates (i.e. fcos + centerness + corner)? how is that different from fcos + polar exactly? or do i misunderstand the meaning of corner supervision? the authors also mention in the section where they introduce corner supervision that they train a regressor based on corner features. what does corner supervision mean exactly? does it mean the authors extract features from the corners and use the features together with the key-point features when they predict the offsets? or do they simply refer to the regression loss function?-----the use of iou loss seems to a bit redundant. it seems that the-----lcorner-----already trains the network to predict the offsets. why do the authors still need the iou loss? the authors should provide an ablation study to demonstrate how the iou loss is affecting the performance of polarnet. ##########################################################################-----summary:-----this paper proposes an anchor-free object detector that does bounding box regression in the polar coordinate instead of in the cartesian coordinate. the motivation of doing this is because there are larger variance in offset vectors in the cartesian coordinate (the extreme case when a point is on one of the four corners of the bounding box, the offset vector becomes [0, w, 0, h]). the authors propose a solution to regress to the pair of corners (either tl+br or tr+bl) in the polar coordinate, and select the corner pair that gives the smallest variance during training.-----##########################################################################-----pros:-----experiment results show using polar coordinate is effective.-----##########################################################################-----cons:-----the authors did some analysis on the variance of the offset vector in section 3.1, however, i think the analysis is not enough. first, the analysis only contains the worst case analysis, that is, the range of offset targets. and the author directly concluded from this: polarnet ""significantly reduces the variance"" (page 4 last line). what is the ratio of variance under cartesian and polar coordinate to make it ""significant""? i do not see any number either theoretically proves it or empirical analysis of the variance during training.-----the term ""keypoint"" used in this paper is confusing. sometimes the ""keypoint"" refers to corner points of the box (""keypoint position"") and other times the ""keypoint"" simply refers to any point within the bounding box (""keypoint offsets"").-----the introduction of polarnet is not clearly presented, specifically there are several confusing points:-----section 3.4.1, what is the usage of t_{x,y}? i don't see how t is used during inference.-----section 3.5, ""we select the optimal box from b_{x,y} as the final output of the predicted box"", what is ""optimal box""?-----figure 3, why the ""corner supervision"" comes from the feature map? i don't see how corner supervision uses any feature.-----the proposed corner supervision is simply the l1 loss, and there are methods that already use it with iou loss. i don't think it is a contribution and section 3.4.2 and 3.4.3 should be combined.-----experiments are not solid:-----there are ways to reduce variance of offsets under cartesian coordinate, e.g. only use points within the center region of the bounding box to learn offset. such experiment should be compared.-----the importance of extra loss function is also not studied, what are the benefit of using more losses?-----from reading section 3, i feel the method is exactly as applying fcos + polar coordinate, but table 2 shows there is still some gap. where does the extra gain come from?-----i checked the fcos paper and found the r101 results in the paper is 43.2 but the number in this paper is 41.5.-----##########################################################################-----reasons for score:-----overall, i vote for rejecting. this paper proposed an interesting idea, but i think way it is presented is not good enough to be accepted. specifically, i think the paper still misses analysis on the variance of offset prediction, and also misses some important ablation studies. furthermore, the paper is not well-written and requires some revision. summary:-----in this paper, the author proposes a new anchor-free keypoint based detector, which learns keypoints based on polar coordinates. it can avoid the large variance of learned offsets compared to the existing anchor-free detectors and make bounding box prediction scale-invariant.-----reasons for score:-----overall, i vote for accepting. my major concern is about the clarity of the paper and some additional analysis (see cons below). hopefully the authors can address my concern in the rebuttal period.-----pros:-----1.this paper proposes a new keypoint based object detector, which represents keypoints based on polar coordinates. it can avoid poor quality keypoints suffered by the existing keypoint based detectors and make regression scale-invariant.-----2.experiments are well thought out and highlight the key advantages of the method over other keypoint based detectors.-----cons:-----1.in the section 3.4.2, why use tangent function for angle regression? for tan, its derivative is-----sec2-----. when-----|brbr|2-----, the gradient is very large. will this condition happen? in addition, are there some constraints or operations on----------to make-----(0,2)-----?-----2.for the training, iou threshold is adaptively change. why use this trick? it may lead to unfair comparison with other methods. could you provide some experiments with fixed iou threshold?-----3.iou loss is kept in the training. why keep it? could you provide some experiments without it to demonstrate its benefits?-----questions during rebuttal period:-----please address and clarify the cons above-----some typos:-----(1)3.1:-----{0,2}(0,2)-----(2)4.1 5th line: four types -> three types","this paper received overall positive scores. one reviewer (r3) recommended clear reject.-----all reviewers agree that the paper introduces a novel idea and its effectiveness is supported by the experimental results. there are concerns about clarity of presentation and certain missing analyses, which have been addressed by the authors in the rebuttal. thus the acs recommend acceptance.","it can avoid the large variance of learned offsets compared to the existing anchor-free detectors and make bounding box prediction scale-invariant.-----reasons for score:-----overall, i vote for accepting.","the authors propose a solution to regress to the pair of corners (either tl+br or tr+bl) in the polar coordinate, and select the corner pair that gives the smallest variance during training.-----##########################################################################-----pros:-----experiment results show using polar coordinate is effective.-----##########################################################################-----cons:-----the authors did some analysis on the variance of the offset vector in section 3.1, however, i think the analysis is not enough.",##########################################################################-----summary:-----this paper proposes an anchor-free object detector that does bounding box regression in the polar coordinate instead of in the cartesian coordinate.,"summary-----in this paper, the authors propose a simple but effective keypoint-based anchor-free object detection system.","it can avoid the large variance of learned offsets compared to the existing anchor-free detectors and make bounding box prediction scale-invariant.-----reasons for score:-----overall, i vote for accepting.",##########################################################################-----summary:-----this paper proposes an anchor-free object detector that does bounding box regression in the polar coordinate instead of in the cartesian coordinate.,"according to the extensive empirical results, the proposed system achieves a better trade-off between speed and accuracy.-----the overall writing looks good to me.","the authors propose a solution to regress to the pair of corners (either tl+br or tr+bl) in the polar coordinate, and select the corner pair that gives the smallest variance during training.-----##########################################################################-----pros:-----experiment results show using polar coordinate is effective.-----##########################################################################-----cons:-----the authors did some analysis on the variance of the offset vector in section 3.1, however, i think the analysis is not enough.",0.1590909090909091,0.0,0.0909090909090909,0.0909090909090909,0.224,0.032520325203252,0.144,0.144,0.1728395061728395,0.050632911392405,0.1728395061728395,0.1728395061728395,0.1866666666666666,0.0547945205479452,0.1333333333333333,0.1333333333333333,0.1590909090909091,0.0,0.0909090909090909,0.0909090909090909,0.1728395061728395,0.050632911392405,0.1728395061728395,0.1728395061728395,0.1707317073170731,0.0,0.097560975609756,0.097560975609756,0.224,0.032520325203252,0.144,0.144,14.561232566833496,7.4394145011901855,11.534784317016602,10.307896614074709,7.4394145011901855,14.561232566833496,10.307896614074709,10.428876876831056,0.2887077497711927,0.6337436226369592,0.42373149795379583,0.8017828512476928,0.8290953912009571,0.9258066690759107,0.8848558711681569,0.8937202278852291,0.9418023462936554,0.9662104788867135,0.9681784965455058,0.9498469821071698,0.2887077497711927,0.6337436226369592,0.42373149795379583,0.8848558711681569,0.8937202278852291,0.9418023462936554,0.9686614894823387,0.9683209775169911,0.8669024325418659,0.8017828512476928,0.8290953912009571,0.9258066690759107
144,https://openreview.net/forum?id=TuK6agbdt27,"this looks like an interesting paper with an original proposal. the empirical results on synthetic tasks are also good. the main problem that i am having is with the proposed network, specifically equation 3. i do not see why it makes sense to consider an outer product of-----n-----and-----e-----as an argument to fwm. as is mentioned in the paper (in appendix a) it would make more sense (both conceptually and from the perspective of complexity) to concatenate those two inputs, or even consider two separate inputs for the associative memory module. the authors argue that in that case the memories would interfere with each other. this is true if a weak associative memory, like the one considered in this work is used. however, if the authors used a modern hopfield network such an interference would not be a problem. specifically, consider the situation when after applying the fwm weights to-----n-----and-----e-----the results are passed through a steep non-linear activation function, like in ref [1] (see for instance formula 10). this would suppress the interference between the memories and provide a nice memory recovery. additionally, with these stronger models of associative memory the key vectors do not have to be orthogonal.-----experimental results look fine, however, i think the work would benefit from some comparisons with other proposals for fast changing weights models, for example ref [2].-----i am not sure i understand the last paragraph on page 2. it is very easy to convert modern hopfield networks from the autoassociative to heteroassociative type. one just needs to introduce additional matrices for queries, keys and values, like it is done in ref [3] when comparing modern hopfield networks with attention. also, when referring to modern hopfield networks, the reference for the original work, ref [1], is missing.-----a couple of presentational suggestions:-----figure 1 seem to be inaccurate. in order to generate x_{t+1} one needs to take into account both the output of fwm and current state h_t. only the first arrow is shown in figure 1.-----after equation 4, what is w_n? looks like a misprint - should it be w_q? also in the second line after equation 4 there are some misprints in the formulas.-----i also have some questions:-----typically most associative memory models converge to a fixed point if one runs them for a long time. it is not obvious to me if dynamical rules described by equations 1-3 converge to a fixed point after a sufficiently large number nr of iterations. do they converge to a fixed point or not?-----it looks to me that the results reported in table 2 indicate that lstm without fwm have lower perplexity than lstm with fwm on that task. at the same time, the authors seem to say in the text (second paragraph on page 8) the opposite. could the authors please clarify this?-----i am willing to increase the scores for this submission if the questions/comments above are addressed.-----references:-----[1] krotov and hopfield, neurips 2016. dense associative memory for pattern recognition, arxiv:1606.01164.-----[2] ba, et al, neurips 2016, using fast weights to attend to the recent past, arxiv:1610.06258.-----[3] ramsauer, et al., 2020. hopfield networks is all you need, arxiv:2008.02217. the solution proposed is the combination of an rnn (lstm) and fast weighted memory (fwm). the lstm produces a query to the memory used to retrieve information from the memory and be presented at the model output. it also controls the memory through fast weights that are updated through a hebbian mechanism. the fwm is based on tensor product representations (tpr). the fwm is differentiable and builds upon the work of tpr-rnn from schlag and schmidhuber and metalearned neural memory (mnm) by munkhdalai et al. in the experimental section, the authors propose a concatenated version of the babi dataset to test their model with language modeling and question answering. further the model is trained on a meta-learning task over pomdps on graphs, and on language modeling on the penntree bank dataset. they show that the lstm-fwm model generalizes better than without memory and similar models and with smaller capacity.-----======================================-----indeed, the fwm model is relevant to this community and involves current scientific discussion and challenges. the paper is clear and is enjoyable to read. math derivations and experimental results seem sound. nevertheless, there are some clarity issues with the ptb language modeling task.-----======================================-----would appreciate if the authors can answer to the following questions:-----how is the fwm (tensor-----ft-----) initialized? how does the initialization influence training and performance?-----how is nr selected?-----what is the vocabulary size in catbabi? is the embedding layer learned or pre-trained?-----the experimental results in table 2 demonstrate a relative improvement over the awd- lstm baselines, which suggest the benefit of our fwm. it is unclear what is the benefit in the ptb dataset. the results show that the lstm model has slightly better perplexity (60.0 / 57.3) than the lstm-fwm (61.39 / 59.37). please, could you clarify the above note versus the numbers?-----does figure 2 have missing details? the caption doesnt seem to match the figure or it is unclear what authors are referring to.-----figure 3 can benefit from using a bigger font for the node and edge values.-----======================================-----i'm inclined to accepting this paper. i found the idea simple but yet effective, and tested correctly in the experimental sections. would appreciate it if the authors can improve the clarity surrounding figure 2, and explain the misleading comment regarding the ptb task.-----======================================-----minor issues:------page 2: an biologically -> a biologically------page 2: pattern is is different -> pattern is different------page 5: please correct with the missing number suffered a todo% drop------page 5: figure 4.1.1 -> figure 2------page 6: noteable -> notable-----================================== update-----thank you for replying to my questions and clarifying in the document. this paper presents a new method called fast weights memory (fwm) to add an associative memory to an lstm.-----model:-----fwm updates its fast weights through a differentiable perceptron like update at every step of an input sequence. the slow weights of the lstm are instead updated only during training using gradient descent.-----fwm is based on previous work: tpr (tensor product representation). tpr is a mechanism that uses tensor products to generate unique representations of combination of components.-----for long sequences fwm also has specialized components that allow it to update deprecated associations.-----fwm is related to:-----tpr-rnn: a sentence-level model for reasoning on text, achieving excellent results on babi.-----mnm (metalearned neural memory): a word-level model which augments an lstm with a ffnn as its memory, trained with a meta-learning objective.-----the authors propose the new task ""catbabi"", a variation of the existing task ""babi"". catbabi seems to be mostly just a concatenation of the stories, questions and answers in babi into a single textual sequence. it's unclear how much harder catbabi is compare to babi in principle.-----tpr-rnn and mnm are only trained for short sequences and so will have a hard time on catbabi. the authors show that mnm in particular does poorly on the long sequences in catbabi.-----results:-----good performance on catbabi (language reasoning) -- but this is a new task, so no real baselines in other papers.-----good results meta-reinforcement-learning for pomdps compared to lstms.-----good results on ptb language models, better than other published models, but not state of the art.-----limitations:-----fwm requires an order 3 tensor, which scales poorly in both time and space computational complexity. this limits this work to relatively small models.-----questions:-----catbabi simply converts babi into a single sequence of tokens. does this really increase the true difficulty of the task, or is it rather a way of artificially limiting the class of models used to solve the task to simple lm-like models? is it possible to reconstruct babi from catbabi with simple heuristics?-----could you report results for fwm on babi? its pretty unclear at the moment how to compare the results on babi of fmw to the ones e.g. in cited metalearned neural memory paper. or at least results on a version of babi where predictions are run for each story separately, so that mnm is not as penalized for not being able to deal with long sequences of text.-----in figure 2, what does the color represent?-----typos:-----page 2:-----*a biologically more plausible-----*stateful weights that can adapt-----most memory-augmented nns are based *on content-based-----page 3:-----becomes a part of the *model's output.-----figure 1: a simplified illustration of *our proposed method-----third-order tensor operations using *matrix multiplications-----page 4:-----wq  wn in equation (1)-----page 5:-----there is todo left summary:-----the authors present a working memory model composed of a recurrent neural network trained via gradient descent and an associative memory based on the approach taken by ba et al. (2016) in ""using fast weights to attend to the recent past"". the model consists of an lstm to which takes the input and its own state from the previous step to produce an output (or new state) which is then passed to a fast weight memory (fwm) module.-----the application of the fast weights are decomposed into two steps: read and write. the write step composes the fast weight matrix update where new information is written into f given the lstm hidden state and the fast weight matrix from the last step. the read step consists of potentially several recurrent ""inference steps"" over the fwm producing an output (e.g. a next step prediction or encoding).-----the authors evaluate the model over two separate datasets. the first is a modified version of the babi dataset which concatenates separate babi stories together and can be trained and evaluated in either a language modelling (lm) mode or question-answering (qa) mode where knowledge about past facts must be utilized.-----strengths & weaknesses:-----the problem itself is well motivated since associative inference is useful in solving problems that require an accurate working memory. fast weight approaches allow us to learn to produce good state representations of the input sequence via slow weights (h_t) and where fast weights provide the associative mechanism to make important links across time.-----the authors propose a new model that combines a novel read-write mechanism that relies on a number of inference steps over the fast weights allowing a nice disentanglement of read/write operations taking advantage of the associative inference to both add new relevant associative information (v) while also filtering stale data (v_old).-----that said the overall form of the model doesn't seem fundamentally different from what is proposed by ba et al. (2016) who also used fast weights as a way to attend over past hidden states in combination with a ""slow"" weighted rnn trained via gradient descent optimization albeit some of the details differ.-----further it would be helpful if the authors could clarify more around the rationale around why particular architectural choices were made. for instance, why are two keys are generated in the write operation?-----results for both catbabi don't seem to exceed the performance of the transformerxl when comparing perplexities in both qa and lm mode and don't exceed trxl accuracy in lm mode. however it is noted that the fwm model is in fact much smaller. it may have been useful to investigate the gated transformer xl which is known to exhibit stronger stability for rl. figure 2 is nice though, is there any intuition why the reads vary among strong negative or positive activations as it seems to indicate?-----as for the meta-rl problem it would have been nice to see comparisons to baselines other than an lstm. for instance, ritter et al. (2020) in ""rapid task solving in novel environments"" introduce a model that combines an episodic memory with self attention to meta-learn how to explore and exploit navigation to goals in connected graphs.-----other points:-----the labelling for the edges figure 3 isn't really clear. there's a missing reference in second to last paragraph on page 5: ""... qa-mode suffered a todo% drop in accuracy ...""-----recommendation:-----i don't think there's enough here to recommend acceptance. for starters, i don't think there's quite enough in justification around the architectural choices of the model and exactly what distinguishes this from the model proposed by ba et al. which also used fast weights in combination with a ""slow"" weighted rnn. next, the results are not strong enough and additional or stronger baselines would have helped paint a better picture of the potential benefits of this approach. for the results in general, while i think that these results point in the possible direction of the utility of fwm i don't believe the paper in its current form demonstrate that fwm exceeds state of the art in the chosen domains in which it was evaluated. that said, i believe this is a promising line of research and encourage the authors to try to address the issues raised.","this paper proposed a way to combine lstms with fast weights for associative inference.-----while reviewers had concerns about comparison with ba et al., and experimental results, the authors addressed all the concerns and convinced the reviewers. the revision strengthened the paper significantly. i recommend an accept.","or at least results on a version of babi where predictions are run for each story separately, so that mnm is not as penalized for not being able to deal with long sequences of text.-----in figure 2, what does the color represent?-----typos:-----page 2:-----*a biologically more plausible-----*stateful weights that can adapt-----most memory-augmented nns are based *on content-based-----page 3:-----becomes a part of the *model's output.-----figure 1: a simplified illustration of *our proposed method-----third-order tensor operations using *matrix multiplications-----page 4:-----wq  wn in equation (1)-----page 5:-----there is todo left summary:-----the authors present a working memory model composed of a recurrent neural network trained via gradient descent and an associative memory based on the approach taken by ba et al. (2016) in ""using fast weights to attend to the recent past"".","fast weight approaches allow us to learn to produce good state representations of the input sequence via slow weights (h_t) and where fast weights provide the associative mechanism to make important links across time.-----the authors propose a new model that combines a novel read-write mechanism that relies on a number of inference steps over the fast weights allowing a nice disentanglement of read/write operations taking advantage of the associative inference to both add new relevant associative information (v) while also filtering stale data (v_old).-----that said the overall form of the model doesn't seem fundamentally different from what is proposed by ba et al. (2016) who also used fast weights as a way to attend over past hidden states in combination with a ""slow"" weighted rnn trained via gradient descent optimization albeit some of the details differ.-----further it would be helpful if the authors could clarify more around the rationale around why particular architectural choices were made.",also in the second line after equation 4 there are some misprints in the formulas.-----i also have some questions:-----typically most associative memory models converge to a fixed point if one runs them for a long time.,this looks like an interesting paper with an original proposal.,"or at least results on a version of babi where predictions are run for each story separately, so that mnm is not as penalized for not being able to deal with long sequences of text.-----in figure 2, what does the color represent?-----typos:-----page 2:-----*a biologically more plausible-----*stateful weights that can adapt-----most memory-augmented nns are based *on content-based-----page 3:-----becomes a part of the *model's output.-----figure 1: a simplified illustration of *our proposed method-----third-order tensor operations using *matrix multiplications-----page 4:-----wq  wn in equation (1)-----page 5:-----there is todo left summary:-----the authors present a working memory model composed of a recurrent neural network trained via gradient descent and an associative memory based on the approach taken by ba et al. (2016) in ""using fast weights to attend to the recent past"".","fast weight approaches allow us to learn to produce good state representations of the input sequence via slow weights (h_t) and where fast weights provide the associative mechanism to make important links across time.-----the authors propose a new model that combines a novel read-write mechanism that relies on a number of inference steps over the fast weights allowing a nice disentanglement of read/write operations taking advantage of the associative inference to both add new relevant associative information (v) while also filtering stale data (v_old).-----that said the overall form of the model doesn't seem fundamentally different from what is proposed by ba et al. (2016) who also used fast weights as a way to attend over past hidden states in combination with a ""slow"" weighted rnn trained via gradient descent optimization albeit some of the details differ.-----further it would be helpful if the authors could clarify more around the rationale around why particular architectural choices were made.","they show that the lstm-fwm model generalizes better than without memory and similar models and with smaller capacity.-----======================================-----indeed, the fwm model is relevant to this community and involves current scientific discussion and challenges.","fast weight approaches allow us to learn to produce good state representations of the input sequence via slow weights (h_t) and where fast weights provide the associative mechanism to make important links across time.-----the authors propose a new model that combines a novel read-write mechanism that relies on a number of inference steps over the fast weights allowing a nice disentanglement of read/write operations taking advantage of the associative inference to both add new relevant associative information (v) while also filtering stale data (v_old).-----that said the overall form of the model doesn't seem fundamentally different from what is proposed by ba et al. (2016) who also used fast weights as a way to attend over past hidden states in combination with a ""slow"" weighted rnn trained via gradient descent optimization albeit some of the details differ.-----further it would be helpful if the authors could clarify more around the rationale around why particular architectural choices were made.",0.2083333333333333,0.0421052631578947,0.09375,0.09375,0.1990521327014218,0.0765550239234449,0.1327014218009478,0.1327014218009478,0.1647058823529411,0.0,0.0705882352941176,0.0705882352941176,0.175438596491228,0.0,0.1403508771929824,0.1403508771929824,0.2083333333333333,0.0421052631578947,0.09375,0.09375,0.1990521327014218,0.0765550239234449,0.1327014218009478,0.1327014218009478,0.1951219512195121,0.0,0.097560975609756,0.097560975609756,0.1990521327014218,0.0765550239234449,0.1327014218009478,0.1327014218009478,6.509149551391602,5.351520538330078,8.915169715881348,6.509149551391602,5.351520538330078,5.911861896514893,6.509149551391602,7.873849868774414,0.09232574345021112,0.47996655100801766,0.2792837576328789,0.35493082426697276,0.30834556841759897,0.19409977391327263,0.9344446472174369,0.9375511123989586,0.9182654462020919,0.9517187544761899,0.9569135838011269,0.9455038961168234,0.09232574345021112,0.47996655100801766,0.2792837576328789,0.35493082426697276,0.30834556841759897,0.19409970514239588,0.30117862629927744,0.5348884547461421,0.8946646946662071,0.35493082426697276,0.30834556841759897,0.19409977391327263
145,https://openreview.net/forum?id=U7-FJu0iE3t,"summary-----this paper introduces a new type of classification model called the ""cascading decision tree."" the cascading decision tree is a rule-based classifier designed to have an overlapping hierarchical structure between its nodes to produce succinct explanations. the paper introduces these models, presents an induction algorithm to learn them from data, and includes an empirical evaluation on three uci datasets as well as a propietary dataset. the submission includes code.-----pros-----the paper introduces a new kind of classification model. this model form is a contribution in and of itself (i.e., regardless of the algorithm used to fit cascading decision trees from data).-----the paper highlights an innovative approach to the design of machine learning models  i.e., training models that are constrained to have particular ""explainability"" properties.-----cons-----the cascading trees produced by the algorithm in this work have little to no formal guarantees regarding their optimality or generalization produces. it is unclear if this is the best way to learn cascading decision trees.-----the paper does not provide a pruning routine. the authors suggest that the algorithm can be paired with any generic pruning method. however, the empirical results do not showcase how the trees perform after pruning.-----the experimental section is lacking in multiple ways. ideally, this section should include comparisons on more than three datasets, and consider other baseline models such as ""rule lists"" (i.e., a special kind of decision tree) and sparse linear models (i.e., a type of model that does not require explanations). finally, i would recommend the authors to include a plot that shows the distribution of explanation depths for all the examples in a dataset. this would allow readers to have a far better understanding of how each method affects the explanation depth (as compared to a comparison of the means).-----the paper does not make a strong case to motivate why ""shorter explanations are better."" this is unfortunate given that succinct explanations are the primary motivation for using cascading decision trees. at a minimum, the paper should include a clear demonstration the advantages of using succinct explanations in a modern application. ideally, this would include: (i) comparisons of the explanations produced for the same point by competing methods; (ii) a study of how the properties of explanations change based on other relevant phenomena (e.g., explanation depth for seen/unseen points).-----rating-----overall, i was convinced that ""cascading decision trees"" were a valuable model class. i was also convinced that this work was valuable in that it highlights a novel approach for supervised learning (i.e., training models with explicit constraints on explainability such as in https://arxiv.org/abs/1703.03717)-----my current rating (3) is based on the fact that the submission fails to analyze, validate, or motivate cascading decision trees sufficiently. ideally, the paper should include a thorough analysis of the tree induction algorithm (as discussed in 3 and 4) as is standard in other work on decision trees. it should include more robust evidence that the proposed method produces succinct explanations (as discussed in 5), as well as a convincing demonstration of the utility of succinct explanations in modern applications (as discussed in 6).-----questions-----q1. how does one measure the ""quality of a cascading decision tree""? is it only in terms of ""explanation depth?""-----q2. did you use a pruning routine in your experiments? this work proposes to use cascade decision tree models to come up with shorter explanations for the predictions made by the decision trees.-----the proposed technique focuses on explaining one class in a binary classification task, i.e., positive samples. the idea is to build a decision tree with predefined depth to classify positive samples, remove those classified positive, as well as negative samples in leaf nodes that are dominated by negative samples, from the dataset, and then repeat this process until the sequence of tree models is built.-----explainability of ml models is an important topic, especially for medical scenarios. in addition, shortening the decision tree paths to improve explainability is a promising direction. the writing of the work is also clear and easy to follow.-----having said that, i have the following concerns about this work:-----w1. the application scenario is narrow. and it is not clear why the explanation path is not the concatenation of all the paths of the cascading trees.-----d1. the proposed approach only applies to binary classification task. it is not clear how this can extend to multi-class and regression models.-----d2. also, even for binary classification, it only explains one class. it is counterintuitive that you need to build different models to explain the classification of the two classes from the same dataset.-----d3. since the process of building a subtree is independent of the previous subtrees built, i.e., the features and splits of the tree being built does not consider the features and splits in previous subtrees, it seems to me that the subtree is pre-conditioned by the previous subtrees. it is not clear why it is correct to only use the subtree that the prediction ends for explanation instead of all the subtrees that are used before the prediction ends.-----w2. the complexity of this decision tree can be high, leading to high inference time. and the model can be overfit to positive samples.-----d4. because the cascading model tries to construct leaves with most pure positive samples, the total depth and the number of trees in the model can be quite high. this will especially impact the overhead in the inference.-----d5. also, given this tree is built for optimizing the classification of one class with very high accuracy, the model can be overfit to that class, and the prediction accuracy of the other class is not guaranteed.-----w3. the evaluation needs to be enhanced.-----d6. the average path length of classic decision trees is already small, i.e., less than 4. the evaluation needs to perform on more complex datasets.-----d7. in medical scenarios, we would expect the negative data samples are much more than positive data samples. it is already easy to overfit for the positive samples in this scenario, and as mentioned in d5, the technique itself also tends to overfit. overfitting may not be a good idea for this skewed dataset.-----d8. it is unclear if the proposed technique gives better explanation without doing a user study. as mentioned in w1 and d3, it is not entirely clear why the explanation does not need to concatenate all the paths in the subtrees before the prediction ends. this is especially a concern since the classic decision tree only has a single tree. it will be great to conduct some user study to understand why the proposed technique impacts the explainability. the authors presented in this submission a nice novel idea of building a tree ensemble in a cascading style so that any positive predictions are decided and explained by the first tree predicting them positively. the reviewer finds this idea very interesting and clearly elaborated in this paper. however, more theoretical and empirical justification is crucially necessary in order to make the claims in the submission convincing. the issues listed here are some questions that the reviewer believes should have been discussed or answered in the paper.-----major issues:-----despite the fact that the focus of cascading decision trees (cdts) is a short explanatory path, my major concern here is that it is very hard to justify them as a proper statistical model.-----practically, there are very simple adversarial cases that the cascading decision trees will fail to build. consider this example that x ~ unif([0,1]^2) and y|x == 1 if x \in [1/3, 2/3]^2 otherwise 0. if we apply the algorithm in the paper, using depth=2 trees and a mixed node threshold theta = 0.5 (which i believe are a reasonable choice), the positive-focused cdts will almost fail to construct the first tree with a decently large sample, let alone the cascading subsequence. on the other hand, since depth=2 trees can cover all rectangles (0,0) - (a,b) \in r^2, the counterpart random forests or gbdts are universal approximators.-----by also checking the negative predictions there might be ways to mitigate this issue. however relevant discussions are lacking in this paper in its current shape.-----from a more abstract perspective regarding the cdts depth: in a d-dim sample space, to separate out a d-dim cube, we are likely in the need of 2d splits. there might be fewer splits needed when the cube is touching the boundary of the support or there are other nodes made before as in an ensemble. but in general we should not make such assumptions, and it would be better if we could have more discussions in the paper.-----2 classic classification trees are asymptotically bayesian classifiers, whereas we can imagine asymptotically cdts assign the positive label to a region only when the true positive rate theta* within the region is larger than the constant threshold theta, which means cdts are intrinsically inaccurate - more specifically, low recall as mentioned by the authors. this situation is further worsened as cdts will take positive examples off from the sample. the fact that cdts are theoretically incapable of finding all positive examples is harming their credibility of giving short explanations to positive predictions. a bandage here might be to use adaptive thresholds, but no discussions are currently present in the submission.-----minor issues:-----classic carts are very sensitive towards outliers. the ""split one example out each time"" scenario being analyzed in the paper is likely to cause overfitting, chasing the outliers, and instability. it should be avoided.-----carts default greedy building algorithm uses entropy or gini index which are indifferent towards both positive and negative examples. since the focus in the paper is on positive predictions, it is worth discussing how the algorithm should be changed accordingly.-----it would be better if there were instructions in the paper regarding choosing the mixed node threshold theta, the tree depth, and hopefully the ensemble size.-----since cdts are still a tree ensemble, their capacity is expected to be larger than a single decision tree which can even be a bit deeper. the results pertaining to the accuracy, precision and recall in the empirical study session are therefore slightly unfair comparison - it would be better to benchmark against decision tree ensembles or rule-based models [1].-----[1] wang, fulton, and cynthia rudin. ""falling rule lists."" artificial intelligence and statistics. 2015.","this paper introduces the idea of cascading decision trees. the reviewers agree that this is a potentially novel and valuable idea, but they also agree that the paper fall short in execution. the paper would be substantially strengthened with more theoretical analysis, more discussion of why cascading decision trees are useful, and most importantly substantially more empirical evaluation, especially with more data sets and more baselines for comparison.","i was also convinced that this work was valuable in that it highlights a novel approach for supervised learning (i.e., training models with explicit constraints on explainability such as in https://arxiv.org/abs/1703.03717)-----my current rating (3) is based on the fact that the submission fails to analyze, validate, or motivate cascading decision trees sufficiently.","this model form is a contribution in and of itself (i.e., regardless of the algorithm used to fit cascading decision trees from data).-----the paper highlights an innovative approach to the design of machine learning models  i.e., training models that are constrained to have particular ""explainability"" properties.-----cons-----the cascading trees produced by the algorithm in this work have little to no formal guarantees regarding their optimality or generalization produces.","this work proposes to use cascade decision tree models to come up with shorter explanations for the predictions made by the decision trees.-----the proposed technique focuses on explaining one class in a binary classification task, i.e., positive samples.","summary-----this paper introduces a new type of classification model called the ""cascading decision tree.""","this work proposes to use cascade decision tree models to come up with shorter explanations for the predictions made by the decision trees.-----the proposed technique focuses on explaining one class in a binary classification task, i.e., positive samples.","this work proposes to use cascade decision tree models to come up with shorter explanations for the predictions made by the decision trees.-----the proposed technique focuses on explaining one class in a binary classification task, i.e., positive samples.",there might be fewer splits needed when the cube is touching the boundary of the support or there are other nodes made before as in an ensemble.,"this model form is a contribution in and of itself (i.e., regardless of the algorithm used to fit cascading decision trees from data).-----the paper highlights an innovative approach to the design of machine learning models  i.e., training models that are constrained to have particular ""explainability"" properties.-----cons-----the cascading trees produced by the algorithm in this work have little to no formal guarantees regarding their optimality or generalization produces.",0.2519685039370078,0.0639999999999999,0.1417322834645669,0.1417322834645669,0.3142857142857143,0.0579710144927536,0.1571428571428571,0.1571428571428571,0.2592592592592592,0.0754716981132075,0.1481481481481481,0.1481481481481481,0.216867469879518,0.0987654320987654,0.1927710843373493,0.1927710843373493,0.2592592592592592,0.0754716981132075,0.1481481481481481,0.1481481481481481,0.2592592592592592,0.0754716981132075,0.1481481481481481,0.1481481481481481,0.1684210526315789,0.0,0.1052631578947368,0.1052631578947368,0.3142857142857143,0.0579710144927536,0.1571428571428571,0.1571428571428571,11.777162551879885,11.777164459228516,17.32052230834961,9.673839569091797,8.34970760345459,11.777162551879885,9.673839569091797,3.2307205200195312,0.9573501489557881,0.9555111963912338,0.900772133571836,0.964964488990744,0.9745392289059204,0.7032994111304587,0.9812949082511002,0.9796670853412222,0.9411849941603658,0.9726808414877379,0.9634863741174925,0.9632118860682031,0.9812949082511002,0.9796670853412222,0.9411849170243441,0.9812949082511002,0.9796670853412222,0.9411849170243441,0.03716516761563992,0.06355998344679327,0.8633175798113913,0.964964488990744,0.9745392289059204,0.7032994111304587
146,https://openreview.net/forum?id=UoaQUQREMOs,"this paper presents a new cnn module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling. to achieve that, the authors propose to divide feature channels into several sub-dimensions (called channel tensorization) and then perform group convolutions at each sub-dimension sequentially to improve channel interactions. an se-like attention mechanism is also applied to further enhance feature representation. the proposed approach achieves competitive results on kinetics400 and something-something, compared to some existing sota results. the paper also provides detailed ablation studies on the approach.-----my understanding is that the main idea of the paper essentially performs channel shuffling followed by group convolutions at each sub-dimension. from this perspective, the idea is similar to shufflenet, which applies channel shuffling to enhance interactions between channel groups. while this provides interesting technical questions from the algorithmic perspective, from the point of view of the novelty, the paper does not appear as a strong technical contribution.-----another downside of the proposed approach is that it is not sufficiently validated by experiments. firstly, the contribution of te is not separated in table 3 and 4, so it remains unclear whether the performance improvement is due to the enhanced channel interactions or channel attention. secondly, the approach seems to be only effective when the number of dimensions is low, leaving it difficult to fully justify the advantages of high-dimensional channel tensorization claimed in the paper.-----the flops of the proposed approach in table 4 is somewhat misleading. as indicated in the work of x3d, the performance gain from using more clips (i.e. >5x10) in evaluation is small. nevertheless, most approaches are evaluated with 30 clips on kinetics. i would suggest sticking to this general practice for fair comparison.-----table 3 and 4 miss some existing sota approaches such as x3d [1], corrnet [3] and tam [3], which should be listed for reference.-----christoph feichtenhofer. x3d: expanding architectures for efficient video recognition. 2020 ieee conference on computer vision and pattern recognition (cvpr), pp. 200210, 2020.-----heng wang, du tran, lorenzo torresani, matt feiszli, video modeling with correlation networks, proceedings of the ieee/cvf conference on computer vision and pattern recognition (cvpr), 2020-----fan, q.; chen, c.-f. r.; kuehne, h.; pistoia, m.; and cox, d. 2019. more is less: learning efficient video representations by big-little network and depthwise temporal aggregation. in advances in neural information processing systems, 22612270. summary-----the paper proposes a new architecture for lightweight action classification networks, named channel tensorization network (ct-net). the idea of this architecture is the tensorization of mid-level input features in combination with an attention mechanism that allows to select relevant features. the channel tensorization can be used as intermediate building block e.g. in a resnet alternately with a res block. the ablation study is done on something-something v1 and the overall method is compared to state-of-the-art on something-something v1&v2 and kinetics-400 and is able to outperform other lightweight architecture with comparable size.-----paper strengths-----i think the ideas proposed in this paper are interesting and i would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3d convolution architecture.-----the paper shows competitive results on something-something as well as on kinetics-400-----the visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip.-----paper weakness-----the paper does not introduce a new technique per se, but manages to combine known elements. so there might be the issue of limited novelty.-----conclusion-----i think that the overall idea of reducing the computational load in 3d convolution architectures is a valid and important topic, as simply increasing networks and training data is not working for the majority of tasks. therefore, lightweight but powerful architectures have a need in the field. i understand that the proposed elements have only a limited novelty, but i think that the clever integration in this case might justify a publication in this case. the paper proposes a novel channel tensorized module (ct-module) to construct an efficient tensor separable convolution and learn the discriminative video representation. the proposed solution achieves a preferable balance between convolutional efficiency and feature-interaction sufficiency. the experiments were conducted on 3 benchmark video-classification datasets outperforming the state-of-the-art results.-----strengths:-----the paper's novelty is the first method for exploring the spatial/temporal tensor separable convolution along each sub-dimension.-----the ct-network framework is rigorously presented.-----experimental results outperforming state-of-the-art results.-----the paper is well written and easy to follow.-----weaknesses:-----the abstract is too long.-----results of the sport1m dataset (standard video classification dataset) are missing.-----table 2 looks clumsy and hard to understand the results of sub-tables.-----what are the reasons for choosing something-something v1 dataset for ablation studies? how about the ablation experiments on kinetic and something-something v2 dataset?.-----since earlier modes were having better performance on the resnet-152 model, the authors could have experimented with the resent-152 model as well. why only 50 and 101?.-----this paper needs careful proof-reading as there are major typos across the paper.-----major comments:-----why do authors adapt resnet as proposed ct-net? any reason for this?-----can we adapt other pre-trained imagenet architectures as ct-net?-----the dataset used for the comparison of visualization in fig.3 is missing.-----if authors can showcase the performance of proposed models on other domain datasets (say 4d fmri or 3d medical imaging datasets), it will be more interesting.-----validation plots are missing: the accuracy vs channel-interaction plot and the accuracy vs g-flops per clip.-----typos:-----page7: in our experiments, to ensure gflops is comparable with other methods. we crop the input to 256  256 during testing. (sentence continuity is missing, and check we).-----addressed concerns:-----corrected the typos.-----majority of the weaknesses are addressed.","the paper focuses on the task of learning efficient representation models for video classification. to avoid the excessive computational cost of performing 3d convolutions on video, the authors propose to break the channel dimension of video representations into sub-dimensions that are treated separately. this cuts down on computation and improves classification performance over many methods in the literature. extensive experiments were run on well-known benchmarks to justify the claims of the model. such backbone architectures can be very useful in the realm of video understanding. the authors should be commended for the amount of work they did in the rebuttal period to address the comments and inquiries brought up by the reviewers. extra experiments were done and more in-depth analysis was made possible.","so there might be the issue of limited novelty.-----conclusion-----i think that the overall idea of reducing the computational load in 3d convolution architectures is a valid and important topic, as simply increasing networks and training data is not working for the majority of tasks.","the ablation study is done on something-something v1 and the overall method is compared to state-of-the-art on something-something v1&v2 and kinetics-400 and is able to outperform other lightweight architecture with comparable size.-----paper strengths-----i think the ideas proposed in this paper are interesting and i would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3d convolution architecture.-----the paper shows competitive results on something-something as well as on kinetics-400-----the visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip.-----paper weakness-----the paper does not introduce a new technique per se, but manages to combine known elements.","the ablation study is done on something-something v1 and the overall method is compared to state-of-the-art on something-something v1&v2 and kinetics-400 and is able to outperform other lightweight architecture with comparable size.-----paper strengths-----i think the ideas proposed in this paper are interesting and i would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3d convolution architecture.-----the paper shows competitive results on something-something as well as on kinetics-400-----the visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip.-----paper weakness-----the paper does not introduce a new technique per se, but manages to combine known elements.","this paper presents a new cnn module to learn video feature representations for action recognition, with a particular focus on increasing channel interactions for spatio-temporal modeling.","in advances in neural information processing systems, 22612270. summary-----the paper proposes a new architecture for lightweight action classification networks, named channel tensorization network (ct-net).","the ablation study is done on something-something v1 and the overall method is compared to state-of-the-art on something-something v1&v2 and kinetics-400 and is able to outperform other lightweight architecture with comparable size.-----paper strengths-----i think the ideas proposed in this paper are interesting and i would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3d convolution architecture.-----the paper shows competitive results on something-something as well as on kinetics-400-----the visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip.-----paper weakness-----the paper does not introduce a new technique per se, but manages to combine known elements.","the proposed approach achieves competitive results on kinetics400 and something-something, compared to some existing sota results.","the ablation study is done on something-something v1 and the overall method is compared to state-of-the-art on something-something v1&v2 and kinetics-400 and is able to outperform other lightweight architecture with comparable size.-----paper strengths-----i think the ideas proposed in this paper are interesting and i would not be aware of any architecture that would use a similar arrangement of tensorization and attention in the mid-layer part to allow for a lightweight 3d convolution architecture.-----the paper shows competitive results on something-something as well as on kinetics-400-----the visualization supports the claim that the attention mechanism seems to learn to focus more on relevant parts of the video clip.-----paper weakness-----the paper does not introduce a new technique per se, but manages to combine known elements.",0.2209302325581395,0.0235294117647058,0.127906976744186,0.127906976744186,0.3574144486692015,0.0383141762452107,0.1673003802281369,0.1673003802281369,0.3574144486692015,0.0383141762452107,0.1673003802281369,0.1673003802281369,0.1437908496732026,0.0,0.0915032679738562,0.0915032679738562,0.1184210526315789,0.0133333333333333,0.0657894736842105,0.0657894736842105,0.3574144486692015,0.0383141762452107,0.1673003802281369,0.1673003802281369,0.0699300699300699,0.0,0.0699300699300699,0.0699300699300699,0.3574144486692015,0.0383141762452107,0.1673003802281369,0.1673003802281369,7.294039726257324,7.849519729614258,12.983991622924805,7.294039726257324,5.737417221069336,7.294039726257324,7.294039726257324,7.088937759399414,0.647738529908612,0.7745079866729017,0.8860330302873252,0.8896422620782032,0.9219561846263079,0.422969924493829,0.8896422620782032,0.9219561846263079,0.4229713126449467,0.9735280995019568,0.9705017169887002,0.8844167522658204,0.9770302128536883,0.9763062777201548,0.02401617330291224,0.8896422620782032,0.9219561846263079,0.422969924493829,0.9646177067906744,0.959311646347776,0.7133948935431341,0.8896422620782032,0.9219561846263079,0.422969924493829
147,https://openreview.net/forum?id=V4AVDoFtVM,"summary-----the paper proposes to learn a value function that takes as input both a state and a policy embedding (pevfa), which is used to design a new version of a generalized policy iteration (gpi) algorithm, named ppo-pevfa. the authors also introduce a new way of learning policy embeddings and show superior results relative to vanilla rl (ppo) on several mujoco tasks.-----strengths-----i found the proposed idea to be novel and interesting. considering the value of multiple policies is an interesting and neglected line of work, which could prove useful in many ways for rl, so i commend the authors for taking a step in this direction. i also particularly liked the theoretical analysis, as well as the experiments supporting the claim for local and global generalization. however, i believe there are a number of issues with the experiments and clarity of the paper, which i would like to see addressed.-----weaknesses-----my main concern about this paper is the significance of the results and the empirical evaluation. the performance gain from using pevfa does not seem very significant. after looking carefully at table 1 and figure 11 in the appendix, it seems like ppo-pevfa is typically within a standard deviation of ppo. in addition, some of the reported results for ppo are much lower than the ones reported in the original ppo paper (schulman et al. 2017). for example, on hopper-v1 the reported numbers after 1m training sweets are 1600 and 2200, ,while for walker2d-v1 they are 1500 and 3000 for the authors implementation and the original one, respectively.-----in addition, i found the description of ppo-pevfa to be quite confusing, so i think it can benefit from more details about the training procedure. does the policy network share any parameters with the pevfa network? usually, parameters are shared between the policy and value function to improve learning, but it doesnt seem like this is the case with ppo-pevfa. typically the policy and value function are updated at the same time in ppo but from algorithm 2, it seems like before a policy update, you first update the pevfa network with data from the current policy and then use the updated value to update the policy using the pg loss. is my understanding correct? if this is the case, then it seems like the method is using some kind of privileged information about the current policy so an ablation that uses the same training scheme of first updating the value function (without conditioning on a policy) using the monte carlo rollouts from the current policy and then updating the policy should be included.-----i also think it would be useful to include comparisons with other baselines that achieve strong results on mujoco such as sac or td3 or alternatively show that pevfa can improve over them as well.-----do you have an intuition for why the difference between pevfa and vfa is more significant for invertedpendulum compared to ant (and the other mujoco tasks) as illustrated in figure 3b? ive found these plots slightly worrying because it seems like the value approximation gains are rather small for most of these tasks. it would be interesting to better understand the relation between these value generalization errors and the performance improvement.-----have you looked into the sensitivity of pevfas generalization with respect to the set (and number) of policies it is trained on? one can imagine training it on the entire history of policies during training or on a subset of these (perhaps the most recent ones). it would be useful to analyze how this choice affects the results.-----have you tried combining the contrastive loss with the auxiliary loss for action action prediction? is this better than either of them or not?-----can you provide some quantitative metrics for the results in figure 3a such as the mse and ranking loss for the train and test values?-----why doesnt table 1 include results for invertedpendulum? can you add standard deviation for ppo and ran pr?-----minor points-----typos: which are beyond the scope of this paper instead of which beyond in section 3.3 in the last section instead of in last in section 4 to answer the above questions instead of ...answer above... in section 5-----recommendation-----while the proposed idea is interesting and novel, i believe more work is needed for publication (particularly in the empirical evaluation and algorithm description), so i lean towards rejection at this stage. the paper conditions the value function on a representation of the policy. the representation can be based on a batch of state-action pairs or based on the policy filters. when conditioning on the representation of a new policy, the value function can better approximate the value of the new policy. experiments show benefits on continuous control tasks.-----as mentioned in the paper, conditioning the value function on the policy was considered by some people before. it was unclear how to represent the policy. this paper makes it work and demonstrates clear benefits. the policy representation can be still improved, and this paper can encourage people to try better representations.-----the paper explains the ideas well. i would like to see a more detailed description of the surface policy representation (spr). the appendix d.5 is missing the review of the related works.-----pros:-----the value conditioned on the policy makes sense.-----proposed end-to-end training, contrastive learning and auxiliary distillation to train the policy embedding.-----tested raw policy representation, surface policy representation and origin policy representation.-----demonstrated benefits in experiments.-----visualization of the learned embeddings.-----cons:-----it is unclear how to practically encode policy representations for policies operating on images.-----minor typos:-----string ""which beyond"" should probably be ""which are beyond"".-----string ""together with pevfa end-to-end"" should be ""together with pevfa is end-to-end"".-----string ""policies along the policy improvement path naturally provides"" should be ""policies along the policy improvement path naturally provide"".-----string ""use ppr"" should probably be ""use spr"".","this paper proposes to consider value functions as explicit functions of policies, in order to allow generalization not only on the state(action) space, but also on the policy space. the initial reviews assessed that the paper was dealing with an important rl topic, but also raised many concerns about the position to previous works (pvn and pvf), the theoretical contributions and the experiments. the authors provided a rebuttal and revision that only partly addressed the initial concerns (check also the review of r3, updated to provide additional feedback following the authors rebuttal). the final discussion led to the assessment that the paper is not ready for publication. remaining concerns include clarity, claims being not fully supported by the experiments, theoretical aspects and missing baselines.",the authors also introduce a new way of learning policy embeddings and show superior results relative to vanilla rl (ppo) on several mujoco tasks.-----strengths-----i found the proposed idea to be novel and interesting.,"the appendix d.5 is missing the review of the related works.-----pros:-----the value conditioned on the policy makes sense.-----proposed end-to-end training, contrastive learning and auxiliary distillation to train the policy embedding.-----tested raw policy representation, surface policy representation and origin policy representation.-----demonstrated benefits in experiments.-----visualization of the learned embeddings.-----cons:-----it is unclear how to practically encode policy representations for policies operating on images.-----minor typos:-----string ""which beyond"" should probably be ""which are beyond"".-----string ""together with pevfa end-to-end"" should be ""together with pevfa is end-to-end"".-----string ""policies along the policy improvement path naturally provides"" should be ""policies along the policy improvement path naturally provide"".-----string ""use ppr"" should probably be ""use spr"".",the paper conditions the value function on a representation of the policy.,"summary-----the paper proposes to learn a value function that takes as input both a state and a policy embedding (pevfa), which is used to design a new version of a generalized policy iteration (gpi) algorithm, named ppo-pevfa.","the appendix d.5 is missing the review of the related works.-----pros:-----the value conditioned on the policy makes sense.-----proposed end-to-end training, contrastive learning and auxiliary distillation to train the policy embedding.-----tested raw policy representation, surface policy representation and origin policy representation.-----demonstrated benefits in experiments.-----visualization of the learned embeddings.-----cons:-----it is unclear how to practically encode policy representations for policies operating on images.-----minor typos:-----string ""which beyond"" should probably be ""which are beyond"".-----string ""together with pevfa end-to-end"" should be ""together with pevfa is end-to-end"".-----string ""policies along the policy improvement path naturally provides"" should be ""policies along the policy improvement path naturally provide"".-----string ""use ppr"" should probably be ""use spr"".","the appendix d.5 is missing the review of the related works.-----pros:-----the value conditioned on the policy makes sense.-----proposed end-to-end training, contrastive learning and auxiliary distillation to train the policy embedding.-----tested raw policy representation, surface policy representation and origin policy representation.-----demonstrated benefits in experiments.-----visualization of the learned embeddings.-----cons:-----it is unclear how to practically encode policy representations for policies operating on images.-----minor typos:-----string ""which beyond"" should probably be ""which are beyond"".-----string ""together with pevfa end-to-end"" should be ""together with pevfa is end-to-end"".-----string ""policies along the policy improvement path naturally provides"" should be ""policies along the policy improvement path naturally provide"".-----string ""use ppr"" should probably be ""use spr"".","can you add standard deviation for ppo and ran pr?-----minor points-----typos: which are beyond the scope of this paper instead of which beyond in section 3.3 in the last section instead of in last in section 4 to answer the above questions instead of ...answer above... in section 5-----recommendation-----while the proposed idea is interesting and novel, i believe more work is needed for publication (particularly in the empirical evaluation and algorithm description), so i lean towards rejection at this stage.","the appendix d.5 is missing the review of the related works.-----pros:-----the value conditioned on the policy makes sense.-----proposed end-to-end training, contrastive learning and auxiliary distillation to train the policy embedding.-----tested raw policy representation, surface policy representation and origin policy representation.-----demonstrated benefits in experiments.-----visualization of the learned embeddings.-----cons:-----it is unclear how to practically encode policy representations for policies operating on images.-----minor typos:-----string ""which beyond"" should probably be ""which are beyond"".-----string ""together with pevfa end-to-end"" should be ""together with pevfa is end-to-end"".-----string ""policies along the policy improvement path naturally provides"" should be ""policies along the policy improvement path naturally provide"".-----string ""use ppr"" should probably be ""use spr"".",0.1875,0.0126582278481012,0.1124999999999999,0.1124999999999999,0.28,0.032258064516129,0.128,0.128,0.145985401459854,0.0444444444444444,0.1021897810218978,0.1021897810218978,0.2073170731707317,0.0493827160493827,0.1341463414634146,0.1341463414634146,0.28,0.032258064516129,0.128,0.128,0.28,0.032258064516129,0.128,0.128,0.1809523809523809,0.0192307692307692,0.1333333333333333,0.1333333333333333,0.28,0.032258064516129,0.128,0.128,10.773268699645996,10.773268699645996,14.814868927001951,10.773268699645996,12.12621784210205,13.066047668457031,10.773268699645996,7.124772071838379,0.8600894867800748,0.9712730503632582,0.21431361442326669,0.2763581435024296,0.37680723952620804,0.8094224119427618,0.7293966465565442,0.8468624500625143,0.8473572082278655,0.9794931700052767,0.9748735672313553,0.947272663928787,0.2763581435024296,0.37680723952620804,0.8094221259764528,0.2763581435024296,0.37680723952620804,0.8094224119427618,0.09580669148803683,0.0897430919825798,0.006972555496029122,0.2763581435024296,0.37680723952620804,0.8094224119427618
148,https://openreview.net/forum?id=XvOH0v2hsph,"instead of using validation accuracy to determine the efficacy of a network, this paper recommends to use sum over training losses (sotl). sotl-e is a variant where the sum of training losses begins to be computed after the first e epochs. they also designed an early stopping mechanism based on baker et al's svr where they extrapolate sotl instead of validation accuracy.-----questions:-----how can training loss be used to identify a good network? it should theoretically lead to overfitting and poor generalization. going by this argument, if we apply any kind of regularization such as dropout or weight decay, the training loss would not be low while the test accuracy might still improve. it is surprising that sotl-e is able to rank the networks better than testl at 200 for cifar10 and cifar100. why do you think this is the case?-----darts experiment: (a) in figure 3(a), how is darts search replicated using just 100 random architectures? as it uses a supernet, it requires all possible architectures possible with the chosen operations. (b) in figure 3(b) and 3(c), the final architecture needs to be trained for 600 epochs. so it is natural that the rank correlation of sovl, sovl-e etc is poor for the first 100 epochs.-----what would be more interesting is, darts currently they perform a bi-level optimization. so instead of the architecture parameters----------trying to minimize the validation loss, can they also minimize the training loss (theoretically this should not generalize well)? if not this, can you devise a way to plug in sotl in the bi-level optimization and choosing the best architecture? if sotl performs well even in that case, then you could a stronger case.-----training very deep networks is not easy and takes more than 100 epochs to obtain good accuracy. so your observation might be a side effect of that too. as sotl could be applied to any deep learning networks, can you also repeat the experiment by training 100 networks sampled from a smaller search space, such as mobile search space (mobilenet, squeezenet, shufflenet etc), that takes less than 80 epochs to finish training, to see if it still holds true? then use this sotl and sotl-e to determine the best network. also compare with the baselines.-----in figure 4, how do sovl, sovl-e and validation accuracy fare? please include those too in the plots.-----what is the difference between the setup of 1 and 2 (a) to (c) apart from the fact that sovl-e and testl are not included in 2?-----if sotl-e is the average training loss of the final epoch, why not call it that? (i understand that it is still the sum of losses for all the batches but as it not across epochs it is misleading).-----as this is paper is proposing something that is fundamentally opposite to what has been studied widely thus far, it requires a lot more scrutiny. i do not think we can accept it with just empirical results and the theoretical motivation currently provided.-----post rebuttal:-----thank you for replying to all of my questions..-----plugging in your new metric to darts seems to be promising, especially if it alleviates the darts collapse problem. given that the community is more interested in one-shot nas algorithms, this might be worthwhile pursuing-----from the new plot in figure 4 and nas experiment in figure 5, it is evident that the sum of training loss is able to rank the networks more effectively in the first 50 epochs. so one could use sotl-e for early stopping rather than validation accuracy. this would also be effective in hyperband where the architectures are discarded after training them for very few epochs. this paper proposes a simple model-free method to estimate the generalization performance of deep neural architectures based on their early training losses. the proposed method uses the sum of training losses during training to estimate the performance and is motivated by recent empirical and theoretical results. the experimental results show that the proposed estimator outperforms the existing methods that predict the performance ranking among architectures.-----pros-----the proposed approach is simple yet shows better performance than the existing estimator.-----this paper is discussed from both theoretical and empirical perspectives.-----cons-----i am wondering how the proposed estimator can be used in recent gradient-based nas methods. for instance, darts optimizes architecture parameters during the search phase and pick up the top two operations that have the highest values after optimization. is it possible to use the proposed estimator for darts optimization (i.e. recent gradient-based methods) and to speed up the optimization? also, although one-shot nas based on random sampling (e.g [1]) achieves good performance, can we apply the proposed method to such methods? i would like to know the scope of the application of the proposed method.-----in the first experiment, this paper randomly samples 100 architectures from the darts search space and evaluate the proposed estimator based on them. i would like to see the behavior of the proposed method with more samples to get a more accurate understanding because the search space of darts is much large.-----regarding figure 5, in the case of the regularized evolution (re), i would like to know how much the speed of the optimization is improved. for instance, it would be nice to provide how much the proposed method can speed up the optimization to achieve the same performance reported in their paper. i am also interested in how fast it is compared with recent gradient-based nas methods. [1] g. bender+, understanding and simplifying one-shot architecture search, icml, 2018-----overall, this paper proposes a simple yet effective method to estimate the generalization performance among deep neural networks and it is motivated by both empirical and theoretical aspects. however, there are some unclear points to be clarified for the publication as described above. ###############################################################-----summary:-----this paper provides a new method for estimating the generalization performance of neural architectures. this method used the sum of training loss as a criterion. the paper gave some intuitions about the method from the perspective of bayeian model selection.-----###############################################################-----reason for score:-----using training loss to improve generalization performance is unreasonable. and the paper didn't give some convincing reason to this method. the method has no theoretical guarantee, and its analogy with bayesian model selection seems problematic.-----###############################################################-----cons:-----1, the method in this paper purely used training loss as a criterion for generalization performance. this is unreasonable. and the paper didn't give some convincing reason to this method.-----2, the analogy with the bayesian model selection is problematic. in bayesian model selection, the parameter in the following step is the posterior estimator based on previous data. this paper think of the sgd optimizer as a way to find the posterior estimator. this is problematic.","the paper proposes to use the sum of training losses during training, or a variant where the sum of training losses begins to be computed after the first e epochs, to estimate the generalization performance of the corresponding network. although the results seem promising for query-based nas strategies, the reviewers agree that as the paper proposes something that is fundamentally opposite to the common practice, it requires more careful and thorough analysis. besides, while the connection made by authors to the bayesian marginal likelihood is interesting, it's not a rigorous argument that convinces the audience about the applicability of the proposed method. i strongly encourage the authors to add more analysis and discussion to the revised version to strengthen their claim and clarify its scope.","[1] g. bender+, understanding and simplifying one-shot architecture search, icml, 2018-----overall, this paper proposes a simple yet effective method to estimate the generalization performance among deep neural networks and it is motivated by both empirical and theoretical aspects.",the experimental results show that the proposed estimator outperforms the existing methods that predict the performance ranking among architectures.-----pros-----the proposed approach is simple yet shows better performance than the existing estimator.-----this paper is discussed from both theoretical and empirical perspectives.-----cons-----i am wondering how the proposed estimator can be used in recent gradient-based nas methods.,"[1] g. bender+, understanding and simplifying one-shot architecture search, icml, 2018-----overall, this paper proposes a simple yet effective method to estimate the generalization performance among deep neural networks and it is motivated by both empirical and theoretical aspects.","instead of using validation accuracy to determine the efficacy of a network, this paper recommends to use sum over training losses (sotl).","[1] g. bender+, understanding and simplifying one-shot architecture search, icml, 2018-----overall, this paper proposes a simple yet effective method to estimate the generalization performance among deep neural networks and it is motivated by both empirical and theoretical aspects.",the proposed method uses the sum of training losses during training to estimate the performance and is motivated by recent empirical and theoretical results.,this is unreasonable.,the experimental results show that the proposed estimator outperforms the existing methods that predict the performance ranking among architectures.-----pros-----the proposed approach is simple yet shows better performance than the existing estimator.-----this paper is discussed from both theoretical and empirical perspectives.-----cons-----i am wondering how the proposed estimator can be used in recent gradient-based nas methods.,0.1916167664670658,0.0606060606060606,0.1437125748502994,0.1437125748502994,0.2780748663101604,0.0216216216216216,0.1390374331550802,0.1390374331550802,0.1916167664670658,0.0606060606060606,0.1437125748502994,0.1437125748502994,0.1610738255033557,0.0272108843537414,0.0939597315436241,0.0939597315436241,0.1916167664670658,0.0606060606060606,0.1437125748502994,0.1437125748502994,0.2649006622516556,0.1476510067114093,0.2251655629139073,0.2251655629139073,0.0153846153846153,0.0,0.0153846153846153,0.0153846153846153,0.2780748663101604,0.0216216216216216,0.1390374331550802,0.1390374331550802,12.041152954101562,6.913540363311768,16.380996704101562,5.661614418029785,6.913540363311768,6.913540363311768,5.661614418029785,2.2949440479278564,0.3500208236732888,0.5168634643060487,0.8679943973254151,0.8789829279940868,0.8966462999236299,0.9377760216449149,0.3500208236732888,0.5168634643060487,0.8679941797038346,0.8540108200887181,0.8662548834117574,0.935820003752763,0.3500208236732888,0.5168634643060487,0.8679943039821058,0.8979272412794816,0.8887461076226758,0.8066091294193467,0.30153863925670565,0.5524681677582298,0.8884361161607918,0.8789829279940868,0.8966462999236299,0.9377760076707167
149,https://openreview.net/forum?id=bGZtz5-Cmkz,"summary-----this paper proposed a method that can penalize collisions in latent space. to be more concrete, for a model which combines a neural network and a gaussian process, e.g. deep kernel learning, the learned latent features for two different inputs can be very close. in the following gp modeling, these two similar latent features will cause difficulties since the covariance between them will be large although these two inputs are quite different.-----cons-----the general idea presented in this work is very interesting. this is also a very realistic treatment, i.e. collisions in the latent space.-----although not a new technique, e.g. siamese network, triplet loss, i like the idea of penalizing close points in latent space combined with a gp. it implicitly incorporate prior knowledge in modeling the gp, which usually boost the performance of a gp.-----pros-----i am doubtful about the correctness of eq(1). without a treatment of stochastic variational inference, the marginal likelihood of gp cannot be factorized into a product of per data points. this means the batched update of gp in eq(1) will not produce a correct gp model, if i understand eq(1) correctly. can the authors explain this batched update?-----i think experimental results should be extended to include a comparison with smac and tpe, which are two strong baselines. although this work focus on gp based bo, empirical results of smac and tpe without considering collisions will make this work more convincing.-----the experimental settings used in this work are not detailed, e.g. how many units in each layer in the neural network, etc. empirical results are also not sufficient.-----in figure3, the line is the mean curve instead of median of at least 10 experiments. however, without a statistical test, it is hard to tell whether the proposed approach is better than other competing methods.-----questions-----it is not clear to me why the retrain interval-----t~-----is set to be 100 for 3c, 3a and 3d in figure 3. in algorithm 1, the latent model is updated every-----t~-----iterations. in figure 3, the total iteration numbers for 3c, 3a and 3d are 100, 200 and 100 respectively. this means for 3c and 3d, the latent model is updated only once and this update happens at the end of bo. after the update, the model will never be used. can the authors comment on this?-----overall speaking, i am afraid this paper doesn't contain necessary details and the theoretical results are not strong enough. this paper proposes a regularization technique in training a latent variable model so that points with different functions are pushed apart. its demonstrated that the proposed technique can boost regret bound and empirical performance.-----overall, i think its a nice paper, but i dont think the current presentation is good enough for publication at iclr.-----comments & questions:-----its a natural idea to add a lipchitz-like regularization loss to mitigate collision. the theoretical result seems a straightforward derivative of the srinivas et al. (2010), but i dont really see the novelty of the theoretical result, since lipchitz continuity is implicitly determined by the kernel function?-----the proposed method needs to pretrain the neural network with 100 or 200 points. its not clear to me what it means by pre-train. is it supervised or unsupervised? which 100 points are chosen for pretraining? if its supervised, did you count them in the optimization budget? that means if you pre-train on 100 labeled points, then perform 100 bo iterations, a fair comparison to standard bo would grant it a budget of 200 function evaluations.-----there are several parameters, such as----------,----------, how are they chosen exactly? how sensitive are these parameters? what exactly is the standard bo algorithm from nogueira (2014)? is it ucb? ei?-----seems all the benchmark functions have continuous domain with already low dimensions, e.g., the rastrigin 2d only has 2 dimensions. do you further reduce the dimension to one with the neural network? it would be great if you could plot the function on latent space. same for other benchmarks, since they are not very high dimensional.-----from the experiments, i dont really see if its true that the baselines lose because they have collision problems. is it possible to design some experiment to demonstrate that?-----to me seems the work could be more motivated by input domains such as graphs or other discrete structures, at least for the benchmarks in the experiments i dont see why they need this method despite the claimed superior performance. for your reference, some notable work on bayesian optimization in latent space for discrete objects:-----kusner et al. (icml 2017), grammar vae-----jin et al. (icml 2018), jt-vae-----zhang et al. (neurips 2019), d-vae-----...-----minor: the formula for posterior gp mean and covariance assumes zero prior mean, which was not explicitly pointed out. in 3.1, most popular acquisition should definitely include expected improvement-----there are many typos: in abstract: significant different -> significantly different in related work: taskss -> tasks in related work third paragraph: smooth(of -> add space (and many other places) in 3.1: wiggles first quote wrong direction in 3.1: the acquisition function---------- use it -> an acquisition function  uses it in 3.1: then use the sample as the acquisition function , need to add period in 4.1: base on -> based on in 5.1: promotse -> promotes? ... the paper proposes a method to avoid collision for the latent space based bayesian optimization method. the main idea is to add a regularization term into the training process. theoretical analysis is also conducted to understand the performance of the proposed method.-----although the idea of the proposed method is somewhat interesting, i do have many concerns for the paper.-----the writing is not good, so it makes it hard to understand the work. in particular, the english of the paper is frequently bad (wrong grammar, typos, unfinished sentences). the maths notations are occasionally not consistent. for example sometimes, the penalty is defined as p_[i,j}, sometimes it is denoted as p_{ij}.-----section 4.2 is too ambiguous. what are z_i, z_j in the equation in section 4.2? based on the notation of the latent space z, i can guess z_i, z_j are the values in the latent space, but this should be clearly mentioned in the paper. also, what does \lambda represent? and how to set it in practice? i went through the 2nd paragraph in section 6.2 and still feel unclear how to set this hyperparameter in practice.-----section 4.3 is also not clear. what is the intuition behind the weight \omega_{ij}? what do \gamma and \rho represent? how to set them? and what does gp_{kt}(m_t(x_i)) (in eq. (1)) denote?-----regarding the theoretical analysis, unless i miss something, it is just the standard theorem as in srinivas et al. (2010), but replace the assumption of the objective function f being a sample path from the gp, by the assumption of the latent space function h being a sample path from the gp? in which cases this assumption is satisfied? and what does it mean by comparing to theorem 2 in srinivas et al. (2010), the second part of the regret bound doesnt rely on \delta""? as much as i understand, the regret bound in theorem 1 is the same as the one in theorem 2 in srinivas et al. (2010).-----regarding the experiments, the experiments are only conducted on low-dimensional problems (2d, 6d, 3d, ), which is contradict with the motivation of the work (bo for high dimensional inputs). besides, what does it mean when the neural networks are pretrained on a number of data points? do we know the corresponding function values of these data points in advance? if yes, for the baseline methods the paper compares with, are these data points employed in these baseline optimization procedures? the paper proposes (1) a new regularization strategy for the latent space-based bo, (2) an optimization aware dynamic weighting for adjusting the collison penalty to improve bo, (3) theoretical analysis for the bo on the latent space. the idea for the regularization is to take in pairs of data points and penalizes those too close in the latent space compared to their target space distance-----the paper makes an interesting observation that the learned representation (for bo to deal with complex object or high-dimension) often leads to collision in the latent space: two points with significant different observations get too close in the learned latent space. collisions could be regarded as additional noise introduced by the traditional neural network, leading to degraded optimization performance-----the mapping by neural network to learn g: d->z is typically considered as the regression problem in which the neural network should learn the property that similar input should have similar output.-----a pair loss is integrated in learning the neural network. the dynamic weight improves the learned latent space by focusing on the potential high-value region.-----the idea of using constraint in the latent space has also been studied in [1].-----despite of the good motivation, the paper execution is not yet demonstrated the effectiveness of the proposed approach for three main reasons:-----(1) the experiments using 4 settings are quite simple and havenot yet satistisfactorily convinced why the proposed approach performs intuitively better. it can be improved further by demonstrating the collison-effect in more challenging task, such as automatic chemical design [2].-----(2) the theoretical analysis follows and extends from srinivas et al 2010.-----(3) fig 1 demonstrates the collision in 1d using the non-regularized latent space? it will be useful if you can add another figure in the same setting using the regularized latent space.-----the writing and presentation can be improved more.-----typo: section 5.1: promotse"" remark: why choosing is capitalized in the middle of the sentence? ucb use the upper => ucb uses the upper.-----[1] kusner, m. j., paige, b., & hernndez-lobato, j. m. (2017, june). grammar variational autoencoder. in proceedings of the 34 th international conference on machine learning, sydney, australia, pmlr 70, 2017 (vol. 70, pp. 1945-1954). acm.-----[2] griffiths, ryan-rhys, and jos miguel hernndez-lobato. ""constrained bayesian optimization for automatic chemical design using variational autoencoders."" chemical science 11.2 (2020): 577-586.","the reviewers liked the overall idea presented in this paper. although the idea as well as relevant tooling for incorporating constraints in the latent space has been studied a lot in the past, the authors differentiate their work by applying it in a new interesting problem. at the same time, some confusions about relation to prior work remain after rebuttal. firstly, the theoretical additions to prior work (srinivas et al. 2010) are still unclear in terms of significance - they feel more like observations made on top of an existing theorem rather than fresh significant insights. furthermore, even if prior work has not considered exactly the same set-up, it would still be needed to understand what the performance would be when considering prior models or prior datasets used in similar domains (e.g. suggestions by r2, r3). the latter would be desirable especially since the experimental set-up used in this paper is deemed by the reviewers too simple (while the motivation of the paper is to solve an issue essentially manifesting in complex scenarios).","its demonstrated that the proposed technique can boost regret bound and empirical performance.-----overall, i think its a nice paper, but i dont think the current presentation is good enough for publication at iclr.-----comments & questions:-----its a natural idea to add a lipchitz-like regularization loss to mitigate collision.",the idea for the regularization is to take in pairs of data points and penalizes those too close in the latent space compared to their target space distance-----the paper makes an interesting observation that the learned representation (for bo to deal with complex object or high-dimension) often leads to collision in the latent space: two points with significant different observations get too close in the learned latent space.,"that means if you pre-train on 100 labeled points, then perform 100 bo iterations, a fair comparison to standard bo would grant it a budget of 200 function evaluations.-----there are several parameters, such as----------,----------, how are they chosen exactly?",summary-----this paper proposed a method that can penalize collisions in latent space.,"although this work focus on gp based bo, empirical results of smac and tpe without considering collisions will make this work more convincing.-----the experimental settings used in this work are not detailed, e.g. how many units in each layer in the neural network, etc.",the idea for the regularization is to take in pairs of data points and penalizes those too close in the latent space compared to their target space distance-----the paper makes an interesting observation that the learned representation (for bo to deal with complex object or high-dimension) often leads to collision in the latent space: two points with significant different observations get too close in the learned latent space.,the maths notations are occasionally not consistent.,the idea for the regularization is to take in pairs of data points and penalizes those too close in the latent space compared to their target space distance-----the paper makes an interesting observation that the learned representation (for bo to deal with complex object or high-dimension) often leads to collision in the latent space: two points with significant different observations get too close in the learned latent space.,0.1333333333333333,0.0,0.0799999999999999,0.0799999999999999,0.2448979591836734,0.0576131687242798,0.1387755102040816,0.1387755102040816,0.1203703703703703,0.0,0.0555555555555555,0.0555555555555555,0.0638297872340425,0.021505376344086,0.0531914893617021,0.0531914893617021,0.1990950226244343,0.0547945205479452,0.1176470588235294,0.1176470588235294,0.2448979591836734,0.0576131687242798,0.1387755102040816,0.1387755102040816,0.0329670329670329,0.0,0.0329670329670329,0.0329670329670329,0.2448979591836734,0.0576131687242798,0.1387755102040816,0.1387755102040816,8.856575012207031,6.690919399261475,16.67423439025879,8.856575012207031,8.580191612243652,5.428956985473633,8.856575012207031,5.608941078186035,0.9591395311445505,0.9684672390014181,0.7759183439359633,0.7938789486217875,0.8424667352804066,0.8841041586079864,0.9707946511774272,0.9704636504959947,0.8601666931588469,0.9781869749398946,0.9768813238299765,0.9520652316722784,0.9403081418227103,0.9231530400577238,0.5646432536587587,0.7938789486217875,0.8424667352804066,0.8841041586079864,0.49545271467625734,0.5945405167171477,0.9154194569469775,0.7938789486217875,0.8424667352804066,0.884104224800553
150,https://openreview.net/forum?id=biH_IISPxYA,"this submission proposes a new method of learning from data with partially observed labels. in this problem, every instance has a label candidate set, which contains the true label. this submission introduces adversarial learning to improve the disambiguation of inexact labels. particularly, there are two adversarial learning component. in the first component, a generator tries to match the distribution of label candidate sets given the ""true"" label of an instance. in the second component, a generator tries to learn the distribution of instances give their ""true"" labels. since the the ""true"" label is not accessible, the ""true"" label is actually from a predictive model.-----the submission has done extensive experiments and shows that the proposed method outperforms several baseline methods.-----in a summary, the experiment results of this submission is strong, but i feel the motivation of the model design is not clear.-----my biggest question is the motivation behind adversarial training. to make it simple, adversarial training aims to match a generative distribution to the data distribution.-----loss (4) is somewhat reasonable to me. can i interpret it as: f(x) predict z, and z generate a label vector y'; if y' does not match the partial label y, then there is a loss? then the model needs to learn both the predictive distribution p(z | x) through f, and the label distribution p(y | z).-----if my interpretation is correct, have you considered to use alternative distributions for p(y | z) instead of a generator? there are many choices such as restricted boltzmann machine. since you treat y as continuous variables, there are even more choices such as a decoder in a conditional variational autoencoder. i don't mean to say that gan distribution is not a good choice, but if you could make this task clear so people may consider alternative choices.-----2.the generative distribution p(x | z) is more confusing. the structure in (5) seems like an encoder-decoder: decoding x, y gives z, and z should recover x. with this loss, the model has some flavor of generative modeling because it models the feature vector x. as a principle in machine learning, generative modeling is less powerful than discriminative models in classification tasks, so i don't see why we want to model input features.-----in the ablation study, have you tried to use only the classification model and losses from adversarial labels?-----since the model has better performance than baseline methods, there might be several explanations.-----differences in base models. what are base predictive models of f in your algorithm and equivalent in competing methods? is the difference significant? in the results from synthetic datasets, p(y | z) does not contain much information since noise labels are randomly sampled. the proposed method still outperform baseline methods by a good margin. is this an evidence that the model is still better without much contribution from the label generator? in another word, if you replace g_n(z, \epsilon) by your groundtruth distribution, the model should perform even better?-----the combination of the generative model gx(z, epsilon), which works like a generative model. when it works with f(x) and other loss terms, the entire model is like a combination of two neural networks. this might provide extra classification power?-----the loss in (4) is unnatural to me: the loss is computed from probabilities, and square loss may not be the best choice.-----i am asking a lot of questions, trying to explain why the experiment results. in another work, i wish the submission could answer some of these questions.-----the proposed model also have many parameters and neural network components. i don't how easy it is for others to tune the model.-----the writing of the submission also has many issues. a lot of symbols are not well defined. a few examples.-----n has two meaning: number of instances and ""noisy"" labels.-----what is p_z?-----the minimization problem in 2 is invalid since the true label z is unknown. this paper presents a multi-level generative model for partial label learning. the basic idea is to use a conditional noise label generation network to model the label noise. the noise label generator and the data feature generator are learned in an adversarial manner. experiments on synthesized and real-world data sets show competitive performance.-----overall the idea modeling conditional noise label is interesting. the paper is easy to follow. my concerns are mainly as following.-----it is not unclear for me for the motivation that previous methods are based on the separate label distribution estimation steps or the error-prone label confidence estimation process. such kinds of approaches do not mean that they are not good methods. actually using the separate label distribution estimation steps or the error-prone label confidence estimation process may be benefit to be more accurate and more efficient. therefore, such motivation is not so convincing, and leads to an important new contribution. -----the second motivation is based on non-random noise. this is an interesting observation. the proposed method involves a conditional probability to model the correlation. however, this might be sensitive and restrict. in practical, such kind of information may be not correct for infrequent patterns. for frequent patterns, this could be trivial to hold. overall, i like the idea of this paper and it is well-written. in this paper, the authors propose multi-level generative models for partial label learning with non-random label noise. it consists of five components: the conditional noise label generator which models the noise labels conditioning on the ground-truth label at the label level; the conditional data generator which generates data samples at the feature level conditioning on the denoised label vectors; the discriminator which separates the generated candidate label vectors from the observed candidate label vectors in the real training data; the discriminator which separates the generated samples from the real data in the feature space; and the prediction network which predicts the denoised label for each sample from its input features. with the proposed minmax adversarial loss, the proposed framework achieved state-of-the-art performance for partial label learning.----------i think this paper has the following advantages:-----it is novel to exploit multi-level generative models to model non-random noise labels for the partial label learning problem.-----the experiments in this paper are complete and thorough. the authors have tested the model in many datasets and designed the ablation study to verify the effect of each loss.-----the proposed model achieved the state-of-art results.-----despite the above advantages, i still have the following questions:-----is the true label vector-----z-----given for each training sample as the ground truth or it is sampled from the multinomial distribution?-----in algorithm 1, why the parameter----------needs to be limited in-----[c,c]-----?-----in equation 3, it is not clear what the -----n----- in-----yn-----stands for? in the previous context,-----n-----is the size of the training set.-----in fig. 1, the input for-----lc-----is-----z-----and-----f(x)-----, while in equation 4 it is-----f(x)-----and-----ygn(f(x),))","dear authors,-----thank you very much for your detailed feedback to the reviewers in the rebuttal phase. this certainly clarified some of the concerns raised by the reviewers and contributed highly to deepen their understanding of your work.-----we positively evaluated the novelty and the superior empirical performance of the proposed method. however, we still have concern about the justification since the proposed model is so complex that it is not clear what was the key for the good performance.-----for this reason, i suggest rejection of this submission, in comparison with many other strong submissions. i hope that the reviewers' feedback is useful for improving your work for future publication.-----best, ac",experiments on synthesized and real-world data sets show competitive performance.-----overall the idea modeling conditional noise label is interesting.,it consists of five components: the conditional noise label generator which models the noise labels conditioning on the ground-truth label at the label level; the conditional data generator which generates data samples at the feature level conditioning on the denoised label vectors; the discriminator which separates the generated candidate label vectors from the observed candidate label vectors in the real training data; the discriminator which separates the generated samples from the real data in the feature space; and the prediction network which predicts the denoised label for each sample from its input features.,this submission proposes a new method of learning from data with partially observed labels.,this submission proposes a new method of learning from data with partially observed labels.,it consists of five components: the conditional noise label generator which models the noise labels conditioning on the ground-truth label at the label level; the conditional data generator which generates data samples at the feature level conditioning on the denoised label vectors; the discriminator which separates the generated candidate label vectors from the observed candidate label vectors in the real training data; the discriminator which separates the generated samples from the real data in the feature space; and the prediction network which predicts the denoised label for each sample from its input features.,it consists of five components: the conditional noise label generator which models the noise labels conditioning on the ground-truth label at the label level; the conditional data generator which generates data samples at the feature level conditioning on the denoised label vectors; the discriminator which separates the generated candidate label vectors from the observed candidate label vectors in the real training data; the discriminator which separates the generated samples from the real data in the feature space; and the prediction network which predicts the denoised label for each sample from its input features.,this is an interesting observation.,it consists of five components: the conditional noise label generator which models the noise labels conditioning on the ground-truth label at the label level; the conditional data generator which generates data samples at the feature level conditioning on the denoised label vectors; the discriminator which separates the generated candidate label vectors from the observed candidate label vectors in the real training data; the discriminator which separates the generated samples from the real data in the feature space; and the prediction network which predicts the denoised label for each sample from its input features.,0.075187969924812,0.0,0.075187969924812,0.075187969924812,0.1835748792270531,0.0195121951219512,0.1352657004830918,0.1352657004830918,0.0944881889763779,0.016,0.0787401574803149,0.0787401574803149,0.0944881889763779,0.016,0.0787401574803149,0.0787401574803149,0.1835748792270531,0.0195121951219512,0.1352657004830918,0.1352657004830918,0.1835748792270531,0.0195121951219512,0.1352657004830918,0.1352657004830918,0.0338983050847457,0.0,0.0338983050847457,0.0338983050847457,0.1835748792270531,0.0195121951219512,0.1352657004830918,0.1352657004830918,8.159191131591797,8.159191131591797,12.270063400268556,8.159191131591797,7.823416709899902,12.270063400268556,8.159191131591797,0.8377377390861511,0.6941657228475904,0.7449656452747433,0.6042356122062905,0.5096459722583132,0.698117854003856,0.8426984540558672,0.9577162453041439,0.9575273221917442,0.9485562949401981,0.9577162453041439,0.9575273221917442,0.9485562843391577,0.5096459722583132,0.698117854003856,0.8426984540558672,0.5096459722583132,0.698117854003856,0.8426984414984332,0.49947806976564324,0.827572627146252,0.9546451675019632,0.5096459722583132,0.698117854003856,0.8426984540558672
151,https://openreview.net/forum?id=dYeAHXnpWJ4,"this paper examines gradient-based attribution methods that have been proposed in the explainability literature from a theoretical perspective motivated by a recent observation in energy-based generative models. first, the authors point out a general weakness of gradient-based attribution that derives from the fact that input-gradients do not provide well-defined explanations, since the shift-invariance of the softmax output makes them arbitrary. the authors then propose that the reason for the success of gradient-based attribution models can be explained by the fact that discriminative models ""contain an implicit"" class-conditional density model (the mentioned recent observation about energy-based generative models). they then go on to elaborate on this idea showing how aligning the implicit class-conditional generative model to the ""true"" generative model of the data would help provide relates to gradient-based attribution, how the alignment can be efficiently promoted with a novel implementation of score-matching, and how this mechanism can be practically realized as regularization costs. the authors then carry out empirical studies that convincingly confirm the prediction of their theoretical ideas. first, they show that samples generated with score-matching and the proposed gradient-norm regularization are better in the sense of being less noisy and in terms of their discriminative accuracy via a trained discriminative model as proposed by the ""gan-test approach"". finally, they show that the quality of gradient-based explanations are better according to a discriminative version of the pixel perturbation test, a method to evaluate gradient explanations by perturbing pixels ranked in increasing order of relevance. in conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations. in this work the authors explore the link between the explanatory power of input-gradients and the alignment between the ""implicit density model"" of the softmax-based deep model and the ""ground truth"" class-conditional data distribution. the authors propose using score-matching method to create models with varying degrees of alignment. the paper is full of interesting insights and ideas, such as soft-max shift invariance property and trivial input-gradient perturbation, connections between score-matching and adversarial training, and others. however, in the end, the paper's impact on how ml engineers will use interpretability tools is in my opinion limited. the authors successfully introduce some interesting heuristics to make the training of score-matching models more scalable and stable. however even with those heuristics ,gradient-norm regularized models are comparable if not superior in the 3 evaluations presented by the authors: gan-test score, pixel perturbation results, and perceptual alignment of input-gradients. the authors provide enough evidence to validate the main hypothesis of the paper. there are no inconsistencies or errors that i can see in the paper to the best of my knowledge. the paper is clearly written and well structured. to improve the paper, the authors could add some comments expanding on the practical impact that this results will have on the work of ml engineers who use input-gradients as a tool to improve model accuracy.","this paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction. the claim in this paper is that ""the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions"". the main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves. nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.","first, the authors point out a general weakness of gradient-based attribution that derives from the fact that input-gradients do not provide well-defined explanations, since the shift-invariance of the softmax output makes them arbitrary.","in conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.","in conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.",this paper examines gradient-based attribution methods that have been proposed in the explainability literature from a theoretical perspective motivated by a recent observation in energy-based generative models.,"in conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.","in conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.","first, they show that samples generated with score-matching and the proposed gradient-norm regularization are better in the sense of being less noisy and in terms of their discriminative accuracy via a trained discriminative model as proposed by the ""gan-test approach"".","in conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.",0.1643835616438356,0.0277777777777777,0.136986301369863,0.136986301369863,0.2372881355932203,0.0457142857142857,0.1581920903954802,0.1581920903954802,0.2372881355932203,0.0457142857142857,0.1581920903954802,0.1581920903954802,0.1884057971014493,0.0294117647058823,0.1594202898550724,0.1594202898550724,0.2372881355932203,0.0457142857142857,0.1581920903954802,0.1581920903954802,0.2372881355932203,0.0457142857142857,0.1581920903954802,0.1581920903954802,0.2105263157894737,0.0266666666666666,0.131578947368421,0.131578947368421,0.2372881355932203,0.0457142857142857,0.1581920903954802,0.1581920903954802,8.663311004638672,8.663311004638672,10.870464324951172,8.663311004638672,11.836925506591797,8.663311004638672,8.663311004638672,6.737070560455322,0.9253372954280698,0.9313136355646887,0.9389310377903329,0.981873284755889,0.9758629554468014,0.9416963002153557,0.981873284755889,0.9758629554468014,0.9416962721504311,0.9690283979455273,0.9637330511990576,0.9585174376526995,0.981873284755889,0.9758629554468014,0.9416963002153557,0.981873284755889,0.9758629554468014,0.9416963633614421,0.9479989899005629,0.9476224118444747,0.8260883151527437,0.981873284755889,0.9758629554468014,0.9416963002153557
152,https://openreview.net/forum?id=g75kUi1jAc_,"this paper proposes waffle for anonymized federated learning. the idea seems interesting, but i have a few concerns. in summary, the motivation/privacy claims are not clear, and the performance evaluation doesn't seem fair as the authors only considered single-model fl algorithms.-----motivation (why can't we just use secagg?): secure aggregation guarantees that each local model parameter is protected. see ""practical secure aggregation for privacy-preserving machine learning"" by bonawitz et al. thus, i am not sure about this problem's motivation. is there any reason one should employ this instead of secure aggregation? (note that the original secure aggregation protocol is computationally expensive, but the recent variations are not: for instance, see secagg+ [bell et al., ""secure single-server aggregation with (poly)logarithmic overhead""] and turboagg [so et al., ""turbo-aggregate: breaking the quadratic aggregation barrier in secure federated learning""].)-----missing privacy guarantees: while secure aggregation comes with a solid privacy guarantee, there is no theoretical guarantee that waffle indeed can protect clients' data. especially with the ibp prior that induces sparsity, each client will update only a small subset of the weight factor dictionary. this pattern may even reveal more information about the set of weight factors used by each client. (maybe the updated r also reveals this support information?)-----non-adaptive attack: while the authors have presented some experimental results to claim improved privacy in section 4.4, they only used the off-the-shelf attack algorithms. the authors should have designed an adaptive attack algorithm, which could have performed much better. the need for ""adaptive evaluations"" has been well described in [tramer et al., ""on adaptive attacks to adversarial example defenses""], though in an adversarial example context.-----multi-task/multi-center/personalization fl: the performance improvements in table 1 and table 2 seem mostly due to personalization, i.e., each model has its own model. however, all the baseline algorithms assume a single model. the authors should have compared the performance of waffle with other algorithms that also maintain individual local models or multiple global models. for instance, the authors may want to add mocha in [smith et al., ""federated multi-task learning""] (cited in the current work), and multi-center fl algorithms in [sattler et al., ""clustered federated learning: model-agnostic distributed multi-task optimization under privacy constraints""] and its follow-up works. the authors claimed that meta-learning-based approaches require sharing a small subset of data, but not all do. for instance, see [fallah et al., ""personalized federated learning: a meta-learning approach""]. -summary- the paper proposes a novel approach to federated learning which decomposes model parameters into two sub-modules with task-specific weight factors. the authors adopt the ibp process for sparse selections of the factors per client, which mitigates interferences across local clients. they validate the model with several baselines on two different non-iid settings.------pros------the proposed method largely outperforms baselines like fedavg, fedprox, and p-ffl.-----the paper is easy to follow and intuition/direction is reasonable enough.-----reasonable analysis of attacks and ablation studies (appendix) is described.------cons------lack of intuition about the architectural design of waffle. there will be several similar designs like w_i^l \leftarrow \lambda_i^l * w_i^l (without decomposition) where \lambda_i^l is an attention vector, or r^l can leave as local without communication. even weight factors can be regularized to l_0 norm. i agree that the method looks reasonable but there need more insights into the necessity of model components.-----only tiny networks are used. i recommend utilizing further modern complex cnn architectures. applicability of modified network designs of waffle on diverse modern cnns is not guaranteed.-----comparison with recent personalized fl/ bayesian fl approaches is required. since the method uses task-specific local parameters to mitigate inference from the naive aggregation of local parameters, the authors should compare with recent personalized fl/ bayesian fl methods rather than old fl baselines.","this paper proposes an anonymization method for federated learning based on the indian buffet process. the reviewers found the idea interesting, but raised the following main concerns (please see the reviews for more details):-----motivation and terminology needs clarification-----better comparison with secure aggregation methods-----missing privacy guarantees overall the reviewers of this paper are borderline. i hope the authors will take the reviewers' feedback into account when revising the paper.",applicability of modified network designs of waffle on diverse modern cnns is not guaranteed.-----comparison with recent personalized fl/ bayesian fl approaches is required.,"since the method uses task-specific local parameters to mitigate inference from the naive aggregation of local parameters, the authors should compare with recent personalized fl/ bayesian fl methods rather than old fl baselines.","the authors should have designed an adaptive attack algorithm, which could have performed much better.",this paper proposes waffle for anonymized federated learning.,"since the method uses task-specific local parameters to mitigate inference from the naive aggregation of local parameters, the authors should compare with recent personalized fl/ bayesian fl methods rather than old fl baselines.","since the method uses task-specific local parameters to mitigate inference from the naive aggregation of local parameters, the authors should compare with recent personalized fl/ bayesian fl methods rather than old fl baselines.","since the method uses task-specific local parameters to mitigate inference from the naive aggregation of local parameters, the authors should compare with recent personalized fl/ bayesian fl methods rather than old fl baselines.","since the method uses task-specific local parameters to mitigate inference from the naive aggregation of local parameters, the authors should compare with recent personalized fl/ bayesian fl methods rather than old fl baselines.",0.1052631578947368,0.021505376344086,0.0631578947368421,0.0631578947368421,0.1714285714285714,0.0194174757281553,0.1142857142857142,0.1142857142857142,0.0930232558139534,0.0238095238095238,0.0465116279069767,0.0465116279069767,0.1772151898734177,0.0779220779220779,0.1518987341772152,0.1518987341772152,0.1714285714285714,0.0194174757281553,0.1142857142857142,0.1142857142857142,0.1714285714285714,0.0194174757281553,0.1142857142857142,0.1142857142857142,0.1714285714285714,0.0194174757281553,0.1142857142857142,0.1142857142857142,0.1714285714285714,0.0194174757281553,0.1142857142857142,0.1142857142857142,6.542461395263672,6.542460441589356,13.036352157592772,6.542461395263672,7.914018630981445,8.582633972167969,6.542461395263672,6.542461395263672,0.4966354791397382,0.49622010548327977,0.8469046254895832,0.7029620789730762,0.7887815306990948,0.8815896869537804,0.9359536388420916,0.9462455332128394,0.9591287670123857,0.9585674581031888,0.9599727748495901,0.8292650696012495,0.7029620789730762,0.7887815306990948,0.8815896869537804,0.7029620789730762,0.7887815306990948,0.8815896869537804,0.7029620789730762,0.7887815306990948,0.8815897733225915,0.7029620789730762,0.7887815306990948,0.8815896869537804
153,https://openreview.net/forum?id=gl3D-xY7wLq,"the authors presented a comprehensive study on the role of background in image classification. they designed a new set of data and a lot of experiments to find answers to the following questions: (1) how much decrease in classification accuracy if the background signal is removed? (2) can a model successfully classify an image solely based on its background? (3) will an image be misclassified if the image's background is replaced by a different background? (4) with the advance of the model architecture, are the more advanced models like resnet more robust to background effect?-----the paper is well written, and the figures and tables are clearly presented. the newly created dataset imagenet-9, that contains background- and foreground-free images, are publicly available.-----comments:-----i am not sure if i understand the purpose for presenting the only-bg-t results in figure 7. in my opinion, by comparing the bg-required numbers in mixed-rand and original models, it is already clear enough to demonstrate that the background is a necessary component for many images to obtain correct classification.-----as shown in the section of related work, similar topics have been studied before. one of the main contributions of this paper is the newly created dataset. it can be generally useful for ml research in robustness and out-of-distribution detection. the submission performs similar foreground-background analysis for object recognition as in [1], but with more modern networks in mind. as such, the main takeaways indicate that this phenomenon still exists - networks today continue to suffer from background bias as they did four years ago with alexnet, although maybe to a lesser extent. this submission curates more careful evaluation setups by using segmentation of foreground objects, tiled backgrounds to create multiple datasets that serve to illustrate the trends in a more disambiguated way.-----is there some way to quantify the overall diversity of adversarial backgrounds? for example, is it possible that owing to strong correlations of a few objects with easily-learned backgrounds, these few backgrounds always cause misclassification for other objects? could there be a way to detect such backgrounds?-----the appendix says ""the original-trained model performs similarly on no-fg and only-bg-b, indicating that it does not use object shape effectively but there seems to be a 10% gap in table 5, indicating that the shape mask is fairly useful. the in-9l numbers seem 21% up instead of 13%. am i misreading this table?-----re. ""indeed, the only-bg trend observed in figure 8 suggests that, could an additional possibility be that around 20% of classes are fully correlated with their backgrounds? i.e. how can we know how much of the findings are to do with model ""failure"", and not dataset quirks?-----in summary,-----(+) while it is not particularly surprising to learn that backgrounds can be misleading even with the correct foreground or that there exists vulnerability to adversarially picked backgrounds, given the evidence we have of background biases already, it can be useful to have a quantification of the bg-gap for a range of modern models.-----(-) the takeaways are mostly already recognised from the many works that have pointed out reliance of object recognition models on backgrounds. the experiments provide a quantified view of how much modern networks trained on the particular datasets rely on backgrounds, but it is unclear how widely applicable this information is, given that this only analyses a specific dataset. the curated datasets might be useful for benchmarking progress; however, if one were to set up the goal of providing such a testset, then perhaps it might be more appropriate to curate an entire testset of adversarial backgrounds alone (rather than mixed-rand) across a range of modern networks and for all of imagenet, which, along with the usual test set would provide a background-robustness sanity check (with the caveat from the authors that backgrounds may actually be informative when the foreground is confusing).-----[1] object recognition with and without objects, zhu et al. 2017-----post rebuttal: i appreciate the authors' responses. the ""novelty"" over zhu et al. was never under question in my review, i was mostly confused about how to weigh the significance of the findings, how useful it is to know some numbers for the version of the dataset created by the authors (which is not really the original imagenet classification task), and if the submission actually does ""pinpoint"" what the problems are, how and when they manifest, to what extent the dataset is responsible vs. the training choices. having read the other reviews, responses, looked at the updates, i'm still unsure -- if there were something in this paper that was new or surprising and not more or less already known from existing works (perhaps not precise numbers, but then the paper is essentially using a synthetic, modified imagenet anyway), i'd be more enthusiastic about pushing up the rating. but as of now, i'm retaining my initial rating. ##########################################################################-----summary:-----the paper studies the effect of background noise on image classification tasks for neural networks. the paper suggests the following based on empirical results from imagenet classifiers.-----it is possible to achieve reasonable accuracy by just using the background information.-----image classification models suffer from a decrease in accuracy if inference images have a different background.-----image classification models have higher accuracy tends to depend on the background image less.-----the paper also introduces a toolkit for evaluating the imagenet classifiers' dependence on background images.-----##########################################################################-----pros:-----the paper provides a detailed quantitative analysis of the effects of using different backgrounds (both training and testing). it constructs various synthetic test dataset that analyzes different scenarios.-----the result sections (sections 3 and 4) are well structured and carefully study the impact of using different backgrounds.-----the proposed toolkit can be used to evaluate the model's reliance on the background.-----##########################################################################-----cons:-----the key concern about the paper is the lack of novelty. while the synthetic dataset was constructed to study the effects of background in detail, the findings from the paper are not new.-----the experiments do not study/relate how data augmentation techniques affect background reliance. the paper also mentions ood techniques such as distributionally robust optimization (sagawa et al, 2020), but does not study how these techniques affect background reliance. moreover, i cannot find a discussion on which factors in training might force the model to less rely on image backgrounds (or robust to foreground images).-----the dataset mainly used, in-9, is also a small dataset that contains less than 50,000 train images. moreover, the paper only considers the imagenet type dataset. some results may not hold for the other datasets. the authors do not address this in the manuscript.-----there are some minor concerns about the experimental set-up used in the paper that i describe in the section below.-----writings can be significantly improved.-----##########################################################################-----questions:-----on page 5, it says, ""mixed-rand models perform poorly on datasets with only backgrounds and no foregrounds."" what is the insight from this experiment? does this imply that the model might be learning shape features as it is doing better than random?-----for images processed with grabcut, wouldn't the model use shape-related features along with the background? if it is learning the shape features, can the positive correlation in section 4 mean that a stronger model might be learning more shape features?-----what is each point in figure 8 represent? is it using different architectures?-----appendix b.1, doesn't the change in the number of classes also result in a change in total dataset size?-----what is the main insight of this paper when training neural networks?-----what are the main contributions of the toolkit? how accurate is the segmentation?-----minor notes:-----on page 1, ""standard models misclassify 88% of images ..."" --> does this refer to 88% of test images?-----on page 1, i don't understand this sentence: ""tend to simultaneously exploit background correlations more."" is there a stronger correlation between backgrounds for more accurate models?-----for figure 2, what is the test accuracy for a model trained on the original dataset (original/original)?-----in table 2, ""on select test sets ..."" --> selected.-----in figure 3, i don't understand what it means by ""note that the gap decreases much more on the right side of the graph.""-----it isn't easy to interpret table 3. it will be easier to understand if it contains some illustrative examples.-----##########################################################################-----i raised my score based on the author's response.","the paper investigates the tendency of image recognition models to depend on image backgrounds, and propose a suite of datasets to study this phenomenon.-----all the reviewers agree that the paper investigates an important problem, is well-written and contains several interesting insights that should be of interest to the community. i recommend acceptance.","the authors do not address this in the manuscript.-----there are some minor concerns about the experimental set-up used in the paper that i describe in the section below.-----writings can be significantly improved.-----##########################################################################-----questions:-----on page 5, it says, ""mixed-rand models perform poorly on datasets with only backgrounds and no foregrounds.""",the paper suggests the following based on empirical results from imagenet classifiers.-----it is possible to achieve reasonable accuracy by just using the background information.-----image classification models suffer from a decrease in accuracy if inference images have a different background.-----image classification models have higher accuracy tends to depend on the background image less.-----the paper also introduces a toolkit for evaluating the imagenet classifiers' dependence on background images.-----##########################################################################-----pros:-----the paper provides a detailed quantitative analysis of the effects of using different backgrounds (both training and testing).,"is there a stronger correlation between backgrounds for more accurate models?-----for figure 2, what is the test accuracy for a model trained on the original dataset (original/original)?-----in table 2, ""on select test sets ..."" --> selected.-----in figure 3, i don't understand what it means by ""note that the gap decreases much more on the right side of the graph.""",the authors presented a comprehensive study on the role of background in image classification.,the paper suggests the following based on empirical results from imagenet classifiers.-----it is possible to achieve reasonable accuracy by just using the background information.-----image classification models suffer from a decrease in accuracy if inference images have a different background.-----image classification models have higher accuracy tends to depend on the background image less.-----the paper also introduces a toolkit for evaluating the imagenet classifiers' dependence on background images.-----##########################################################################-----pros:-----the paper provides a detailed quantitative analysis of the effects of using different backgrounds (both training and testing).,the paper suggests the following based on empirical results from imagenet classifiers.-----it is possible to achieve reasonable accuracy by just using the background information.-----image classification models suffer from a decrease in accuracy if inference images have a different background.-----image classification models have higher accuracy tends to depend on the background image less.-----the paper also introduces a toolkit for evaluating the imagenet classifiers' dependence on background images.-----##########################################################################-----pros:-----the paper provides a detailed quantitative analysis of the effects of using different backgrounds (both training and testing).,"the experiments provide a quantified view of how much modern networks trained on the particular datasets rely on backgrounds, but it is unclear how widely applicable this information is, given that this only analyses a specific dataset.",the paper suggests the following based on empirical results from imagenet classifiers.-----it is possible to achieve reasonable accuracy by just using the background information.-----image classification models suffer from a decrease in accuracy if inference images have a different background.-----image classification models have higher accuracy tends to depend on the background image less.-----the paper also introduces a toolkit for evaluating the imagenet classifiers' dependence on background images.-----##########################################################################-----pros:-----the paper provides a detailed quantitative analysis of the effects of using different backgrounds (both training and testing).,0.2777777777777778,0.0377358490566037,0.1296296296296296,0.1296296296296296,0.2797202797202797,0.0567375886524822,0.2097902097902098,0.2097902097902098,0.2413793103448275,0.0175438596491228,0.1379310344827586,0.1379310344827586,0.2352941176470588,0.0,0.1470588235294117,0.1470588235294117,0.2797202797202797,0.0567375886524822,0.2097902097902098,0.2097902097902098,0.2797202797202797,0.0567375886524822,0.2097902097902098,0.2097902097902098,0.2197802197802198,0.0,0.1538461538461538,0.1538461538461538,0.2797202797202797,0.0567375886524822,0.2097902097902098,0.2097902097902098,10.69018840789795,10.69018840789795,10.581680297851562,10.69018840789795,7.468487739562988,8.669885635375977,10.69018840789795,4.6854400634765625,0.1984062005527991,0.510349174691954,0.2654508262288406,0.9504907667996317,0.9540455392081589,0.9016559668472244,0.5655700729847823,0.6145397483460734,0.8470849755142321,0.9731220650027512,0.9742483047455082,0.8976825916806336,0.9504907667996317,0.9540455392081589,0.9016559668472244,0.9504907667996317,0.9540455392081589,0.9016559668472244,0.9712188141539035,0.9733175778094705,0.9206006577286604,0.9504907667996317,0.9540455392081589,0.9016559668472244
154,https://openreview.net/forum?id=i3Ui1Csrqpm,"the main goal of this paper is to introduce a simple methodology for optimizing transformer based models for efficiency and effectiveness.-----the paper introduces two main ideas:-----1)a top-down strategy for pruning components of a transformer model: given a specific focus, say speed, the strategy is to consider pruning large coarse-grained components first followed by smaller finer-grained components. the pruning decision is made based on a significance analysis -- a component is considered significant for pruning if it from the model does not result in a substantial increase in the models loss (as decided by a pruning threshold).-----pruning and approximating techniques for different components: for example feed-forward networks are pruned by removing weights in groups (determined via a hyperparameter). for approximating self-attention a sign-matching technique for deciding which top k keys to use for computing query x key dot products.-----the main strengths of this work are as follows:-----the techniques do not require training networks from scratch and can be applied directly during fine-tuning.-----the techniques are simple and should apply widely to most transformer-based models.-----the empirical results support the claim that the technique can yield significant speed-up and memory-reductions while maintaining accuracy and even provide improvements in accuracy if that is the pruning goal. they show that technique is orthogonal to other models explicitly designed for speed and memory footprint (q8bert, distillbert) and can provide further improvements in both efficiency and effectiveness.-----this is a practical and useful approach that should be widely applicable along with many useful insights about optimizing transformer-based systems.-----i appreciate that the experimental results are reported with averages across multiple runs!-----i dont see any major weaknesses in the paper. here are some areas that can be improved:-----the description of the pruning strategies was hard to follow and needed to be tightened up. possibly adding equations and some pseudo-code to the description should help.-----i am curious to know what components get pruned cross the different models that were optimized. i wonder if there are systematic differences between original and distilled models and between auto-regressive (gpt) and auto-encoding style models.-----also some level of ablation analysis on the strategies used will be helpful. for example if the elements were not ordered based on the granularity would the results be any different? since this is an iterative strategy the order should play an important role in selection and utility of the subsequent pruning steps. same goes for the set of pruning strategies. a related question would be what gives the biggest gains.-----what is the impact on the fine-tuning time? the baseline only requires one fine-tuning pass. does this method require multiple fine-tuning passes? or can the loss thresholds be computed on a smaller subset of the target data? this may be a good future work to look into for tasks where the training data is relatively large, where one cannot afford to exhaustively search through all the pruning strategies. after reading the rebuttal, some of my concerns are addressed by the additional experiments. but i also agree with other reviewers that the result is not very surprising. as r4 mentioned, the proposed method depends on the a specific downstream task where the ""small"" ""general"" bert can be further pruned. for a fair comparison to previous work, baselines that are applied to a specific fine-tuning task need to be compared.-----=====-----this paper presents a new framework for creating small fine-tuned pre-trained language models. the framework has 3 components:-----a set of transformer components to be pruned-----a significant analysis for identifying unimportant elements.-----a set of techniques to prune or approximate the transformer element.-----pros:-----the framework is very adaptive by considering different basic elements of the transformer.-----the framework is very efficient by removing large components (e.g., layers, attention blocks, ffd layers) at first and small components (e.g., weight group) later.-----the framework gathers multiple different pruning/approximation techniques and tries to explore the limit of pruning pre-trained models, which is appreciated.-----cons/questions:-----is the loss used in significant analysis computed using the development set? if the validation loss is used, the experiment results in table 1 are not reliable.-----there are many bert pruning papers. providing comparison to these papers is very important to evaluate the proposed method. can the model prune more weight at the same performance level? or can the model perform better at the same pruning ratio?-----it is also helpful to present how much computing resource is needed to prune the network. e.g., how many prune-finetune cycles are needed.-----lack of results of pruning bert-base on glue, which is a very standard and common setting.-----in figure 3, why q8bert + speed focus is even larger/slower than q8bert? with the same speed, q8bert + speed focus is significantly worse than q8bert.-----minor: page 5: less the minimum loss seen ==> less 'than' the minimum loss this paper presents a method for improving a fine-turned transformer in terms of a specific metric such as size, speed, or accuracy. the candidates of removed elements are considered hierarchically with some heuristics and are evaluated in terms of training and validation loss to determine whether they should actually be removed from the model. the authors apply their method to several state-of-the-art transformer models and show that they can produce fast and compact models without losing much accuracy.-----although the individual techniques employed to realize the whole pruning process are not particularly novel, the paper presents a well-thought-out approach to combine those and reports very promising experimental results. i think this is a nice contribution to the community, given that the computation cost is increasingly important in dealing with bert-like models.-----it seems to me that the authors used transformers whose weights are shared between different layers like universal transformers or albert. maybe i missed something, but i think the authors should clarify if this is really the case in the manuscript.-----the entire process of pruning is a bit vague and hard to replicate. would it be possible to describe the whole process in pseudo code? (is algorithm 1 the whole process?)-----i think the authors should also describe the computational cost (or maybe wallclock time) required to perform the proposed pruning processes. it seems to me that the search space is rather large and requires a considerable amount of computation.-----p.5  we prune the element only if the training/validation loss-----i think you should be more specific here. how did you actually use both the training and validation loss? why do you need to look at the training loss when you are interested in the generalization error?-----p.5  weight groups of (wn) -----why is this wn? i thought this should be w.-----minor comments: p.5 less the -> less than the? p.6 doesnt -> does not p.6 attention -> ``attention p.7 second order -> second-order?","this paper introduces a set of techniques that can be used to obtain smaller models on downstream tasks, when fine-tuning large pre-trained models such as bert. some reviewers have noted the limited technical novelty of the paper, which can be seen more as a combination of existing methods. this should not be a reason for rejection alone, but unfortunately, the results in the experimental section are also a bit weak (eg. see [1-4]), there are not very insightful analysis and it is hard to compare to existing work. for these reasons, i believe that the paper should be rejected.-----[1] dynabert: dynamic bert with adaptive width and depth-----[2] training with quantization noise for extreme model compression-----[3] mobilebert: a compact task-agnostic bert for resource-limited devices-----[4] squeezebert: what can computer vision teach nlp about efficient neural networks?","they show that technique is orthogonal to other models explicitly designed for speed and memory footprint (q8bert, distillbert) and can provide further improvements in both efficiency and effectiveness.-----this is a practical and useful approach that should be widely applicable along with many useful insights about optimizing transformer-based systems.-----i appreciate that the experimental results are reported with averages across multiple runs!-----i dont see any major weaknesses in the paper.","the framework has 3 components:-----a set of transformer components to be pruned-----a significant analysis for identifying unimportant elements.-----a set of techniques to prune or approximate the transformer element.-----pros:-----the framework is very adaptive by considering different basic elements of the transformer.-----the framework is very efficient by removing large components (e.g., layers, attention blocks, ffd layers) at first and small components (e.g., weight group) later.-----the framework gathers multiple different pruning/approximation techniques and tries to explore the limit of pruning pre-trained models, which is appreciated.-----cons/questions:-----is the loss used in significant analysis computed using the development set?",-----i think the authors should also describe the computational cost (or maybe wallclock time) required to perform the proposed pruning processes.,"the main goal of this paper is to introduce a simple methodology for optimizing transformer based models for efficiency and effectiveness.-----the paper introduces two main ideas:-----1)a top-down strategy for pruning components of a transformer model: given a specific focus, say speed, the strategy is to consider pruning large coarse-grained components first followed by smaller finer-grained components.","the main goal of this paper is to introduce a simple methodology for optimizing transformer based models for efficiency and effectiveness.-----the paper introduces two main ideas:-----1)a top-down strategy for pruning components of a transformer model: given a specific focus, say speed, the strategy is to consider pruning large coarse-grained components first followed by smaller finer-grained components.","the main goal of this paper is to introduce a simple methodology for optimizing transformer based models for efficiency and effectiveness.-----the paper introduces two main ideas:-----1)a top-down strategy for pruning components of a transformer model: given a specific focus, say speed, the strategy is to consider pruning large coarse-grained components first followed by smaller finer-grained components.","i think this is a nice contribution to the community, given that the computation cost is increasingly important in dealing with bert-like models.-----it seems to me that the authors used transformers whose weights are shared between different layers like universal transformers or albert.",the pruning decision is made based on a significance analysis -- a component is considered significant for pruning if it from the model does not result in a substantial increase in the models loss (as decided by a pruning threshold).-----pruning and approximating techniques for different components: for example feed-forward networks are pruned by removing weights in groups (determined via a hyperparameter).,0.2790697674418604,0.0469483568075117,0.1209302325581395,0.1209302325581395,0.28,0.0564516129032258,0.1359999999999999,0.1359999999999999,0.0975609756097561,0.0,0.0609756097560975,0.0609756097560975,0.2536585365853658,0.0394088669950738,0.1268292682926829,0.1268292682926829,0.2536585365853658,0.0394088669950738,0.1268292682926829,0.1268292682926829,0.2536585365853658,0.0394088669950738,0.1268292682926829,0.1268292682926829,0.202127659574468,0.010752688172043,0.1063829787234042,0.1063829787234042,0.2439024390243902,0.0197044334975369,0.1268292682926829,0.1268292682926829,13.456464767456056,13.456464767456056,13.456464767456056,8.704258918762207,8.743951797485352,9.786155700683594,11.96733570098877,6.508796691894531,0.9658116234974847,0.9758280361854551,0.7237244013349119,0.9476037421684594,0.9501664032091062,0.9402927865415578,0.5073926882898713,0.6447116130836589,0.9414041761610168,0.9887221278034498,0.9865056852716223,0.9559926651014536,0.9887221278034498,0.9865056852716223,0.9559926864697597,0.9887221278034498,0.9865056852716223,0.9559926864697597,0.4097223499210168,0.4285364566742899,0.9173041933004576,0.96578704994817,0.9675606464098042,0.767800342892278
155,https://openreview.net/forum?id=j5d9qacxdZa,"this paper explores the usage of ebms in continual learning for classification. although the application of ebms in continual learning is novel, the general idea is a special case of the usage of ebms for structured prediction, which has been widely studied. for instance, multi-class classification can be considered as a special version of multi-label classification, which has been studied in belanger and mccallum (2016) and a set of follow-up works. the main difference here is that multi-class classification is a simpler problem, and all possible classes can be enumerated in o(n), but in multi-label classification, more complicated inference such as gradient-descent based approaches must be used. the contrastive training can be seen as a special case of margin-based training (belanger and mccallum, 2016; rooshenas et al. 2019), where the margin is infinity. i believe the works in using ebms for structured prediction must be cited here as they are closely related.-----the authors also explored the effect of ml training on interference with past data and showed that using single sample ml approximation can significantly alleviate the catastrophic forgetting problem.-----i believe that this is an interesting observation.-----typo: ""current bath"" in section 5.1----belanger and mccallum (2016), structured prediction energy networks. gygli et al. (2017), deep value networks learn to evaluate and iteratively refine structured outputs. rooshenas et al. (2019), search-guided, lightly-supervised training of structured prediction energy networks ==========================-----before revision-----==========================-----review: motivated by the effectiveness and naturalness of energy-based models, this paper proposes to use energy-based learning framework for continual learning. empirical studies are performed to validate the proposed strategy on several continual learning benchmarks.-----strength:-----this paper applies ebms to the task of continual learning, which is interesting and relevant to iclr conference.-----the paper is well written and easy to follow.-----the paper is technically sound, since the formulation of the ebms are well derived by other prior works.-----concerns:-----the contribution of the paper is insufficient for publication. the energy-based learning framework for discriminative purpose has been developed for a long time, even though recently researchers in the field machine learning are enthusiastic about developing energy-based models for data generation.-----the underlying theory of the proposed method is developed by other papers, the only contribution of this paper is to apply the ebm to the continual learning, which is quite straightforward.-----missing key reference about ebm for discriminative learning. the core of this paper is mainly based on the finding of the transition between discriminative ebm and generative ebm, which is originally presented in reference [a]. the current paper misses to discuss and cite this paper.-----missing relevant reference about generative ebms in related work. even though this paper is not directly related to ebms for data generation, but it did discuss the development of it in its paper. the current related work about ebms for generative purpose is incomplete in the sense that it skipped some pioneering works and important application with ebms. for examples, [1] is the first paper to use convnet-parameterized ebms with langevin for image generation. training ebms with assisting networks can be found in [2] and [3]. also, writing a section of comprehensive related works about energy-based learning is not necessary but encouraging.-----references-----[1] a theory of generative convnet (icml 2016)-----[2] cooperative learning of descriptor and generator networks. ieee transactions on pattern analysis and machine intelligence (pami 2018).-----[3] divergence triangle for joint training of generator model, energy-based model, and inference model. (cvpr 2019)-----===================================-----after a revision-----=================================== thank you for your efforts to revise the paper. the revised parts about related work look good to me. i agree on that citing all those ebm application papers is not necessary. but doing so can provide a comprehensive and complete development of the deepnet-ebm. again, this is not required and it will not affect the rating.-----i also acknowledge the existing contributions in the current paper and admit that such a direction is promising, but i still feel that the current paper doesn't fully explore this area with more solid experiments. thus, the whole contribution is quite marginal. by taking into account all these concerns, i will change my rating from 4 to 5. summary: this work shows that energy-based models (ebms) are a promising model class for continual learning problems. according to the experiments, ebms outperform the baseline methods by several continual learning benchmarks.-----+ves:-----it is interesting to see that energy-based models are introduced for the classification continual learning problems. in the paper, the authors show that ebms achieve a significant improvement on four standard cl benchmarks. it is really surprising that the improvements are so large.-----some analysis, for example, energy landscape and interference with past data. seems interesting. it shows benefit of energy-based model for continual learning. both of them indicate that y ebms suffer less from catastrophic forgetting.-----concerns-----it seems like the gradient computation of equation (5) is not corresponding to equation (4).-----it is nice to see the improvement of ebms. in this work, a difference architecture is usually in ebms. the architecture is different from the baseline. it is nice that embs has a more flexible architecture to score the input x and output y. however, the improvement of your work is from architecture or from the training. it is better to do a clear claim. if the benefit is from the architecture, do you treat it as a black box? or it is from the training objective? it is possible to show some learned structure in your formulation. the architecture of ebm seems large.-----questions during the rebuttal period: please address and clarify the cons above:-----the experiment setting detail is unclear. the training details of the proposed method and baseline are unclear. and the baseline detail is a little confusing to me. in section 5.1.2, it says  all the baselines and ebms are based on the same model architecture. it seems the architecture of ebms is different.-----some related work on energy-based model: multiple label classification[1] , sequence labeling [2] and machine translation [3] [1] structured prediction energy networks, icml 2016 [2] benchmarking approximate inference methods for neural structured prediction. naacl 2019 [3] engine: energy-based inference networks for non-autoregressive machine translation acl 2020 the writing of the paper needs improvements. i am still unsure how the paper learns each new task. you talked about batches but never talked about how each new task is learned specifically. it creates doubts in my mind. e.g., does each batch contain some examples from old tasks? is this training like for multi-task learning?-----you wrote in the continual learning setting, we assume classes in y_b are uniformly distributed in every new batch. is y_b fixed for each task or each batch?-----you need a negative class in each batch. this means that your algorithm cannot work in the scenario where a task has one class only. most of existing techniques can handle this case although they use 2 or more classes in a task in their experiments. i think this is a serious limitation. it is only suitable for task-il, which is an easier problem to solve.-----i think the claim that existing techniques need to fix the number of classes beforehand is not correct. i know hat most of them fix the number in their code or experiments, but i dont see why they cannot use a large number or dynamically add new class heads when needed, say, using cross-entropy as the loss function.-----the experiments are not well described, and baselines are old. many newer baselines should be included, e.g.,-----learning a unified classifier incrementally via rebalancing. cvpr 2019. overcoming catastrophic forgetting for continual learning via model adaptation. iclr, 2019. random path selection for continual learning. neurips 2019 continuous learning of context-dependent processing in neural networks. nature machine intelligence, 2019 large scale incremental learning. cvpr 2019-----for each dataset, you used one setting for tasks only, e.g., cifar100, 10 tasks. more than one setting should be tried to show the generality of the approach.","there were opinions on both sides of this paper from the reviewers. reviewers were excited by the novel application of energy-based models (ebms) to continual learning and the resulting performance gains, but were concerned by the more direct application of ebms (which has been explored in other work, and here adapted to the continual learning setting, so its contribution is marginal) and with the depth of the evaluation, which they thought could be pushed farther. overall, the reviewers agreed that this paper could benefit from another round of revisions to strengthen its contribution, incorporating many of the excellent points made by the authors in their responses.","i believe the works in using ebms for structured prediction must be cited here as they are closely related.-----the authors also explored the effect of ml training on interference with past data and showed that using single sample ml approximation can significantly alleviate the catastrophic forgetting problem.-----i believe that this is an interesting observation.-----typo: ""current bath"" in section 5.1----belanger and mccallum (2016), structured prediction energy networks.","the energy-based learning framework for discriminative purpose has been developed for a long time, even though recently researchers in the field machine learning are enthusiastic about developing energy-based models for data generation.-----the underlying theory of the proposed method is developed by other papers, the only contribution of this paper is to apply the ebm to the continual learning, which is quite straightforward.-----missing key reference about ebm for discriminative learning.","rooshenas et al. (2019), search-guided, lightly-supervised training of structured prediction energy networks ==========================-----before revision-----==========================-----review: motivated by the effectiveness and naturalness of energy-based models, this paper proposes to use energy-based learning framework for continual learning.",this paper explores the usage of ebms in continual learning for classification.,"it seems the architecture of ebms is different.-----some related work on energy-based model: multiple label classification[1] , sequence labeling [2] and machine translation [3] [1] structured prediction energy networks, icml 2016 [2] benchmarking approximate inference methods for neural structured prediction.","it seems the architecture of ebms is different.-----some related work on energy-based model: multiple label classification[1] , sequence labeling [2] and machine translation [3] [1] structured prediction energy networks, icml 2016 [2] benchmarking approximate inference methods for neural structured prediction.","cvpr 2019-----for each dataset, you used one setting for tasks only, e.g., cifar100, 10 tasks.","the energy-based learning framework for discriminative purpose has been developed for a long time, even though recently researchers in the field machine learning are enthusiastic about developing energy-based models for data generation.-----the underlying theory of the proposed method is developed by other papers, the only contribution of this paper is to apply the ebm to the continual learning, which is quite straightforward.-----missing key reference about ebm for discriminative learning.",0.247191011235955,0.0227272727272727,0.1123595505617977,0.1123595505617977,0.3333333333333333,0.1123595505617977,0.1999999999999999,0.1999999999999999,0.2191780821917808,0.0833333333333333,0.1643835616438356,0.1643835616438356,0.1512605042016806,0.0512820512820512,0.134453781512605,0.134453781512605,0.1342281879194631,0.0408163265306122,0.0805369127516778,0.0805369127516778,0.1342281879194631,0.0408163265306122,0.0805369127516778,0.0805369127516778,0.0161290322580645,0.0,0.0161290322580645,0.0161290322580645,0.3333333333333333,0.1123595505617977,0.1999999999999999,0.1999999999999999,9.47803020477295,9.47803020477295,13.914600372314451,6.996518135070801,7.778794765472412,8.04840087890625,6.996518135070801,2.355487823486328,0.9682573339213458,0.9710218175907074,0.764217824957531,0.9782973880418006,0.9753748146875089,0.8615289512194984,0.97075925233382,0.7394768818126546,0.2413935933248353,0.9754044765899922,0.9722027935400196,0.8921871957304041,0.09783288119675972,0.13765331399330197,0.6860772477934739,0.09783288119675972,0.13765331399330197,0.6860771472934347,0.15939757103090166,0.2353116434221542,0.46731041716891064,0.9782973880418006,0.9753748146875089,0.8615289512194984
156,https://openreview.net/forum?id=jYVY_piet7m,"this paper's main topic is the actual usability of the current status of non-auto-regressive translation (nat) models.-----although previous papers have reported that the nat models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment. this is a proper investigation for the community since some researchers might believe that nat is always faster than standard auto-regressive models and became an excellent alternative to them.-----the ideas of inducing skip-at and skip-mt are really unique and somewhat innovative (since, i guess, no other researchers hardly think to employ such skip-decoding architecture). basically, this paper has several new findings that should be shared in the community for developing better technologies.-----the following are the questions/concerns of this paper.-----1,-----""ir-nat heavily relies on small batch size and gpu, which is rarely mentioned in prior studies."" i think this is an excellent investigation. however, this paper does not tell readers why this observation happens. please explain why the current nat models are not suitable to work on cpus and large batches.-----2,-----the intention of the statement, ""which indicates that the balanced distribution of deterministic tokens is necessary"" is unclear. please elaborate on what the authors try to tell by this statement.-----3,-----the proposed method consists of many new components. the authors provided the results of an ablation study in table 5. this is a really nice analysis. however, the performance differences in ft, rpr, and mixdistill are somewhat marginal. actually, we can easily observe such 00.4 bleu score difference by just changing random seeds for transformer models. are there any statistically significant differences among them? or any reasonable evidence that supports the difference?-----4,-----this is just a comment, and appreciate having the author's words.-----there is an opinion that fully tuned implementation for standard auto-regressive models outperforms both decoding speed and accuracy. see the following presentation slide on wngt-2020 (such as p33): https://kheafield.com/papers/edinburgh/wngt20overview_slides.pdf-----what would happen for the proposed method if we compared them on such a highly-tuned implementation? this paper proposes a hybrid-regressive machine translation (hrt) approachcombining autoregressive (at) and non-autoregressive (nat) translation paradigms: it first uses an at model to generate a gappy sketch (every other token in a sentence), and then applies a nat model to fill in the gaps with a single pass. as a result the at part latency is roughly reduced by half compared to a full at baseline. the at and nat models share a majority part of the parameters and can be trained jointly with a carefully designed curriculum learning procedure. experiments on several mt benchmarks show that the proposed approach achieves speedup over the full at baseline with comparable translation quality.-----the idea of combining at and nat is interesting, and the paper is very clearly written. the experiments and analysis are solid and well-designed. i vote for acceptance.-----pros:-----an interesting idea combining at and nat which can inspire future research.-----experiments and analysis support most of the claims.-----the proposed curriculum training and mixed distillation can be useful in future research.-----cons:-----it is a bit sad that the translation quality drops a lot when k > 2. this limits and maximum speedup that can be achieved.-----some of the claims on the hrts performance compared to the at baseline seems a bit misleading (see details below).-----details:-----from table 3 it seems that at models also benefit from mixed distillation. based on the numbers here im assuming the at models in table 2 are trained w/o mixed distillation. early on the paper claims that hrt can outperform at in terms of translation quality, but this doesnt seem to be the case if one compares hrt against at both trained with mixed distillation. can the authors clarify? if this is true, please tone down the claims on this point in the abstract, intro, and section 5.-----can the authors comment on the training cost of hrt in comparison to at in terms of, e.g., number of epochs, wall-clock time?-----does the at baselines in table 2 use the same relative positional encodings as hrt? if not, can the authors comment on how it may affect their performance?","this paper proposes a new method to combine non-autoregressive (nat) and autoregressive (at) nmt. compared with the original iterative refinement for non-autoregressive nmt, their method first generates a translation candidate using at and then fill in the gap using nat.-----all of the reviewers think the idea is interesting and this research topic is not well-studied. however, the empirical part did not convince all the reviewers. the revised version and response is good; however, it still does not solve some major concerns of reviewers.","experiments on several mt benchmarks show that the proposed approach achieves speedup over the full at baseline with comparable translation quality.-----the idea of combining at and nat is interesting, and the paper is very clearly written.","this paper's main topic is the actual usability of the current status of non-auto-regressive translation (nat) models.-----although previous papers have reported that the nat models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment.","this paper's main topic is the actual usability of the current status of non-auto-regressive translation (nat) models.-----although previous papers have reported that the nat models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment.","this paper's main topic is the actual usability of the current status of non-auto-regressive translation (nat) models.-----although previous papers have reported that the nat models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment.",i vote for acceptance.-----pros:-----an interesting idea combining at and nat which can inspire future research.-----experiments and analysis support most of the claims.-----the proposed curriculum training and mixed distillation can be useful in future research.-----cons:-----it is a bit sad that the translation quality drops a lot when k > 2. this limits and maximum speedup that can be achieved.-----some of the claims on the hrts performance compared to the at baseline seems a bit misleading (see details below).-----details:-----from table 3 it seems that at models also benefit from mixed distillation.,"this paper's main topic is the actual usability of the current status of non-auto-regressive translation (nat) models.-----although previous papers have reported that the nat models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment.","this is a proper investigation for the community since some researchers might believe that nat is always faster than standard auto-regressive models and became an excellent alternative to them.-----the ideas of inducing skip-at and skip-mt are really unique and somewhat innovative (since, i guess, no other researchers hardly think to employ such skip-decoding architecture).","this paper's main topic is the actual usability of the current status of non-auto-regressive translation (nat) models.-----although previous papers have reported that the nat models can achieve the same performance level with auto-regressive translation models while the decoding speed is much faster, like two to five times, this paper points out that it deeply relies on the batch size and computation environment.",0.3225806451612903,0.0655737704918032,0.1935483870967742,0.1935483870967742,0.2857142857142857,0.0394736842105263,0.1688311688311688,0.1688311688311688,0.2857142857142857,0.0394736842105263,0.1688311688311688,0.1688311688311688,0.2857142857142857,0.0394736842105263,0.1688311688311688,0.1688311688311688,0.3478260869565218,0.0219780219780219,0.1521739130434782,0.1521739130434782,0.2857142857142857,0.0394736842105263,0.1688311688311688,0.1688311688311688,0.2465753424657534,0.0277777777777777,0.1232876712328767,0.1232876712328767,0.2857142857142857,0.0394736842105263,0.1688311688311688,0.1688311688311688,17.02828025817871,11.176385879516602,17.02828025817871,17.02828025817871,12.92378044128418,17.02828025817871,17.02828025817871,11.186076164245604,0.9539905908520607,0.9602545070303719,0.8763951086048785,0.9829578459094259,0.9805632045536368,0.9523954967212216,0.9829578459094259,0.9805632045536368,0.9523954967212216,0.9829578459094259,0.9805632045536368,0.952395507365142,0.697122589556147,0.7548545269891551,0.43841764561880087,0.9829578459094259,0.9805632045536368,0.9523954967212216,0.9679508923104373,0.9626032412596256,0.9003200626666453,0.9829578459094259,0.9805632045536368,0.952395507365142
157,https://openreview.net/forum?id=kmqjgSNXby,"summary-----the paper studies offline policy evaluation (ope) and optimization in the model-based setting. the main methodological contribution of the paper is using autoregressive models for the next state and reward prediction. the authors demonstrate that autoregressive models achieve higher likelihood compared to feedforward models on 9 environments from rl unplugged [1] offline dataset. given that model likelihood is only a proxy quality metric in ope and control, they further demonstrate a positive correlation between likelihood and ope estimates. the paper shows quantitatively that using autoregressive models results in more accurate ope estimates than for feedforward models and model-free benchmarks. finally, the authors apply autoregressive models for offline control and achieve higher returns than for feedforward models.-----strengths-----the paper is written clearly and generally easy to follow.-----the proposed modification is simple, straightforward to implement, and demonstrates convincing results consistently on different environments. for example, the median rank correlation between ope and ground truth is the best against 7 ope baselines on 9 environments from rl unplugged.-----the experimental setup follows the standard practices (e.g. using a validation set for hyperparameter selection) and the details necessary for the reproduction of the results are provided (e.g. optimizers, learning rate schedules, number of epochs, architectures).-----weaknesses-----the authors claim that standard feedforward dynamics models assume that different dimensions of the next state and reward are conditionally independent given the current state and action. in other words, p(s,r|s,a) is claimed to be equal to p(s_1|s,a)  p(s_n|s,a) p(r|s,a) when using a feedforward model. the statement is incorrect unless we use a linear function approximator as a model. however, this mistake does not affect much the quality of the paper.-----using autoregressive models does not address aspects that are specific to the offline setting. providing results for the online setting will be helpful for understanding whether autoregressive models should be favored in general for model-based reinforcement learning.-----recommendation-----the reviewer votes for accepting the paper. the paper is well-written, the proposed extension is simple to implement and convincingly outperforms baselines on a variety of environments.-----notes-----appendix a.2 is identical to section 5.1 of another submission [2].-----the abbreviation fqe is used throughout the paper but expanded only in the appendix.-----references-----[1] gulcehre, caglar, ziyu wang, alexander novikov, tom le paine, sergio gmez colmenarejo, konrad zolna, rishabh agarwal et al. ""rl unplugged: benchmarks for offline reinforcement learning."" arxiv preprint arxiv:2006.13888 (2020).-----[2] anonymous. benchmarks for deep off-policy evaluation. in submission to international conference on learning representations, 2021. url https://openreview.net/forum?id=kwsegeehvf8. under review. the paper proposes extra conditioning in a dynamics model, wherein each dimension of the next state is generated on previous dimensions as well as previous state and action. this allows for a richer model (as a model without conditioning on previous dimensions is a special case).the paper claims that this additional conditioning adds a better inductive bias in certain tasks.-----the new models fit data better (deepmind suite, rl unplugged data) in terms of log-likelihood. the authors study the impact of better models on ope and on policy optimization. the paper is generally well written. the contribution seems straightforward.-----why is this relevant only in continuous control tasks? is this inductive bias really a general pattern observed in multiple tasks? what happens when there is no structure a priori (e.g., independence holds): do you lose in terms of sample efficiency?-----shouldn't any dynamics model be able to enrich replay buffer?-----the default ordering may be completely arbitrary, how is the new dynamics model able to cope with this in the experiments? in other words, it is unclear if p(s_i|s_{j<i},...) is appropriate at all without knowing the ordering.-----also since p(s|s_prev,a) = \prod_{i}p(s_i|s_prev,a,s_{j<i}) by definition, how is the feedforward model restrictive and making explicit conditional independence assumption? it seems that the feedforward model is not restrictive but too general and explicitly capturing this sequential dependency across states is useful.-----we ultimately care not about the log-likelihood numbers, but whether or not the dynamics models are faithful in policy evaluation and optimization.-----the above needs clarification. what is the faithfulness property? also, why would we not care about log-likelihood? summary-----the authors consider the usage of autoregressive dynamics models for batch model-based rl, where state-variable/reward predictions are performed sequentially conditioned on previously-predicted variables. extensive numerical results are provided in several continuous domains for both policy evaluation and optimization problems. the results showcase the effectiveness of autoregressive models and, in particular, their superiority over standard feed-forward models.-----pros-----the paper is very well-written and easy to follow. the experiments are described with sufficient details to understand the results-----the usage of these autoregressive models for model-based rl is, to my knowledge, novel-----the paper presents extensive experiments on several challenging domains. the results are convincing and significant. in particular, they show that autoregressive models are superior to feedforward ones-----cons-----the paper's sole contribution seems to be empirical since autoregressive models are (as acknowledged) not novel, though their application to this setting is.-----while the empirical results are very convincing, i did not find much intuition on where this big improvement over feedforward models comes from (see detailed comments below).-----the ordering of the state variables might be a limitation (again, see below).-----detailed comments-----as mentioned above, i did not find much intuition on the better performances of autoregressive models vs feedforward ones. as i am not entirely familiar with the system dynamics of the considered domains, do you think that they possess any property which makes autoregressive models more suitable than feedforward ones (e.g., strong correlations between next-state variables)? aren't the transition dynamics deterministic in most of the considered domains?-----since the reward in most of the considered domains is (i suppose) a function of state, action, and next-state, could it be that one of the reasons behind the worse performance of feedforward models is that they try to predict the reward as a function of state-action only? would their performance change if they explicitly modeled the reward as a function of s,a,s'?-----related to the previous point, the autoregressive model naturally predicts a reward as a function of s,a,s' since r is considered as the (n+1)-th state component. but what if we re-ordered the state variables with r as the first component instead of the last one? would the performance change?-----more generally, do you think that the ordering of the state variables might be a limitation? for instance, could there be an ordering of these variables that makes the model perform well and one that makes it perform poorly? while in, e.g., image/text generation problems where autoregressive models are applied we have a natural ordering between the variables involved (e.g., by space or time), here there seems to be no particular relationship between state variables with similar index. maybe some additional experiments could help in clarifying whether this could be a limitation or not.-----some minor comments/questions:-----in eq. 2, should the product be up to h-1?-----before sec. 3, a citation for ""behavioral cloning"" could be added-----sec. 5.3: the fqe acronym was not introduced-----fig. 4: what is ""r"" above each plot?","the paper is about the use of autoregressive dynamics models in the context of offline model-based reinforcement learning. after reading the authors' responses and the other reviews, the reviewers agree that this paper has several strengths (well written, easy to follow, the approach is novel and simple to implement, the empirical evaluation is well executed and the results are reproducible) and it deserves acceptance. the authors need to update their manuscript by keeping into considerations all the suggestions provided by the reviewers (clarifications and additional empirical comparisons).","finally, the authors apply autoregressive models for offline control and achieve higher returns than for feedforward models.-----strengths-----the paper is written clearly and generally easy to follow.-----the proposed modification is simple, straightforward to implement, and demonstrates convincing results consistently on different environments.","in particular, they show that autoregressive models are superior to feedforward ones-----cons-----the paper's sole contribution seems to be empirical since autoregressive models are (as acknowledged) not novel, though their application to this setting is.-----while the empirical results are very convincing, i did not find much intuition on where this big improvement over feedforward models comes from (see detailed comments below).-----the ordering of the state variables might be a limitation (again, see below).-----detailed comments-----as mentioned above, i did not find much intuition on the better performances of autoregressive models vs feedforward ones.",summary-----the paper studies offline policy evaluation (ope) and optimization in the model-based setting.,summary-----the paper studies offline policy evaluation (ope) and optimization in the model-based setting.,"finally, the authors apply autoregressive models for offline control and achieve higher returns than for feedforward models.-----strengths-----the paper is written clearly and generally easy to follow.-----the proposed modification is simple, straightforward to implement, and demonstrates convincing results consistently on different environments.","in particular, they show that autoregressive models are superior to feedforward ones-----cons-----the paper's sole contribution seems to be empirical since autoregressive models are (as acknowledged) not novel, though their application to this setting is.-----while the empirical results are very convincing, i did not find much intuition on where this big improvement over feedforward models comes from (see detailed comments below).-----the ordering of the state variables might be a limitation (again, see below).-----detailed comments-----as mentioned above, i did not find much intuition on the better performances of autoregressive models vs feedforward ones.",the paper shows quantitatively that using autoregressive models results in more accurate ope estimates than for feedforward models and model-free benchmarks.,"in particular, they show that autoregressive models are superior to feedforward ones-----cons-----the paper's sole contribution seems to be empirical since autoregressive models are (as acknowledged) not novel, though their application to this setting is.-----while the empirical results are very convincing, i did not find much intuition on where this big improvement over feedforward models comes from (see detailed comments below).-----the ordering of the state variables might be a limitation (again, see below).-----detailed comments-----as mentioned above, i did not find much intuition on the better performances of autoregressive models vs feedforward ones.",0.3484848484848484,0.1076923076923077,0.2727272727272727,0.2727272727272727,0.2580645161290322,0.0434782608695652,0.1505376344086021,0.1505376344086021,0.174757281553398,0.0594059405940594,0.116504854368932,0.116504854368932,0.174757281553398,0.0594059405940594,0.116504854368932,0.116504854368932,0.3484848484848484,0.1076923076923077,0.2727272727272727,0.2727272727272727,0.2580645161290322,0.0434782608695652,0.1505376344086021,0.1505376344086021,0.1818181818181818,0.0185185185185185,0.1454545454545454,0.1454545454545454,0.2580645161290322,0.0434782608695652,0.1505376344086021,0.1505376344086021,11.095091819763184,10.66461181640625,13.112421035766602,11.095091819763184,10.66461181640625,13.112420082092283,11.095091819763184,13.182333946228027,0.9839725963342509,0.9826693328613477,0.9311010476006324,0.3244134410544173,0.5937589688529006,0.41936058989750924,0.9735858370576245,0.9682964124198263,0.897345630292942,0.9735858370576245,0.9682964124198263,0.8973454770850872,0.9839725963342509,0.9826693328613477,0.9311012374608969,0.3244134410544173,0.5937589688529006,0.41936058989750924,0.9648981066026928,0.9621977522328132,0.8616584022995881,0.3244134410544173,0.5937589688529006,0.4193613051906799
158,https://openreview.net/forum?id=l-LGlk4Yl6G,"the authors propose a method to perform subspace splitting. that is, the task of clustering the entries of an input vector into sets of coherent subspaces. the contribution of the work is two-fold: (1) the theoretical characterization of the problem, and its well-posedness, and (2) the presentation of three algorithms for tackling the problem of subspace splitting. quantitative analysis of the performance of the three algorithms is provided by means of synthetic experiments.-----the paper is well written, with sound mathematical formulation. the contributions proposed by the authors seem to have enough novelty and relevance for the community, and both theoretical and practical contributions are thoroughly motivated and discussed.-----major concerns:-----i find the structure and content of the paper somehow unbalanced. a fairly large portion is dedicated to motivational applications, which feels like an extension of the related work section, but are not experimentally explored. in section 5 most discussion is devoted to approaches with clear drawbacks (ransas, greedys), while the final proposal (k-splits) is hardly discussed.-----while it is true that a large contribution of the work is strictly theoretical, the practical aspect of the paper could be further backed by experimental validation. the experiments section show limited results regarding the choice of the number of clusters (only 2 or 4). the performance of ransas and k-splits is saturated in all noise-free experiments which makes them hard to compare or identify failure cases.-----given the attention paid in the paper to motivating applications (a full section) i miss a section in the experiments where the proposed approach is validated with real data from any of the mentioned applications. i feel this would tremendously increase the value of the work and would legitimize the claim of practical importance that the authors make in introduction and conclusions.-----minor concerns:-----at the end of the first paragraph of related work, the authors claim that random sampling is not applicable to subspace splitting, with no further explanation of why this is the case. at the same time the algorithm presented (ransas) is based on random sampling which seems contradictory.-----as i mentioned earlier, i miss more discussion regarding the k-splits approach. did the authors find any clear drawbacks besides initialization? what about the k-means assumption of isotropic clusters?-----the choice of just l1 as a baseline for comparison is a bit arbitrary. i wonder why the other mentioned approaches (mixed-integer programming or random sampling) were not added to the evaluation.-----the paper could use further review to avoid typos and missing references.-----the paper doesnt follow the citation convention: authors last names and year. this paper introduced a new setting in which variables in feature vector are divided into different groups and in each group, variables are generated by sampling within linear subspace. they characterized the sufficient and necessary condition for the system to be identifiable. three algorithms are provided to cluster the variables into their generative subspace. they also provide motivating applications for this new setting in metagenomics, recommender systems, and robust learning. i think the new setting is potentially interesting and the proposed algorithms are good. however, compared with, for example, subspace clustering, the generative subspace {u_i} is known to the users a prior and the goal is to classify variables of a single data point, which might be rarely the case in practice.-----comments:-----my main concern is on the proof of theorem 1, which seems a little bit sloppy and not convincing enough to me. for example, on page 14, in the mid of eq.(1) and eq.(2), the authors argue u_\omega^k is invertible as subspace u^k is in so-called general space. please elaborate on that. i am confused as if we set u_\omega^k any full rank r-by-r matrix and let u_{r+1}^k be a copy of any row in u_\omega^k. and, do a row swap between r+1 and any row in [r]. then, the new r-by-r u_\omega^k cannot be invertible as it is not full rank in row. it is not clear to me the set of this type of subspace a zero-measure set on the uniform distribution of (d,r)-grassmannian. please consider rewriting the proof in a more rigorous way. summary: the paper introduces the problem of subspace spitting, in which an observed mixed-features vector is to be partitioned such that the identified partitions match with given subspaces. the main results of the paper lie in deriving sufficient and necessary conditions for identifiability of these partitions when the subspaces and the entries of the features are randomly positioned in the ambient dimension and the subspaces, respectively. the conditions simply require that there are more entries associated with each subspace than the dimension of the subspace. the paper also presents algorithms to perform the splitting.-----strengths:-----the problem statement is novel. i did not see previous formulations of this problem.-----the paper is generally well-written.-----the paper has a good balance of theory and algorithm development.-----weaknesses:-----the experimental results section is rather weak. it would be good to include some realistic examples from some concrete applications.-----while the paper has a dedicated section on motivating applications, they are not that convincing. the metagenomics application is more plausible, but i do not think this model applies well to recommender systems. the examples provided seem to be included to justify the proposed model. perhaps there are meaningful relevant applications, but for the current version the problem setup is not sufficiently justified and seems somewhat contrived.-----the assumptions about the subspaces need to be more explicit, especially when discussing the algorithms. for example, the random sampling algorithm seems to require full knowledge of the dimensions of these subspaces which may be impractical.-----i have not fully verified this argument, but i am under the impression that the result of the main theorem is trivial. isn't it obvious that the span of the restriction of a subspace to a given partition of size m, the whole r^m? i believe the main result can just follow from this simple observation.-----reproducibility: the authors did not include code for their developed algorithms in the supplementary material.-----impact: provided that the problem setup and model are better justified, i believe this work could open up new research questions in machine learning and data analysis, including (mixture) variants of well-studied problems on matrix completion and robust learning.-----overall i like the paper, especially that the problem setup itself seems novel. however, the model is not sufficiently justified, the assumptions are somewhat questionable, and the experimental section is lacking. as such i would rate this as a marginal acceptance.","the paper considers a new linear-algebraic problem motivated by applications such as metagenomics which requires the algorithm to partition the coordinates of a long noisy vector according to a few known subspaces. a number of theoretical questions were asked (e.g., identifiability; efficient algorithms and their error bounds; etc).-----the reviewers generally liked the paper for what it does. specific suggestions were raised by the reviewers, including how the paper went into length about the motivating applications but did not end up evaluating the proposed algorithms on any motivating applications; and that the main theoretical results were not technically challenging / nor surprising (although the authors provided a fair justification in their rebuttal).-----the ac finds the paper an outlier in terms of the topics among papers typically received by iclr, but liked the paper precisely because it is different. the authors are encouraged to discuss the connections of the specific problem to the context of representation learning and machine learning in general.-----overall, i believe the paper is a solid borderline accept.","in section 5 most discussion is devoted to approaches with clear drawbacks (ransas, greedys), while the final proposal (k-splits) is hardly discussed.-----while it is true that a large contribution of the work is strictly theoretical, the practical aspect of the paper could be further backed by experimental validation.","i believe the main result can just follow from this simple observation.-----reproducibility: the authors did not include code for their developed algorithms in the supplementary material.-----impact: provided that the problem setup and model are better justified, i believe this work could open up new research questions in machine learning and data analysis, including (mixture) variants of well-studied problems on matrix completion and robust learning.-----overall i like the paper, especially that the problem setup itself seems novel.","the contribution of the work is two-fold: (1) the theoretical characterization of the problem, and its well-posedness, and (2) the presentation of three algorithms for tackling the problem of subspace splitting.",the authors propose a method to perform subspace splitting.,the performance of ransas and k-splits is saturated in all noise-free experiments which makes them hard to compare or identify failure cases.-----given the attention paid in the paper to motivating applications (a full section) i miss a section in the experiments where the proposed approach is validated with real data from any of the mentioned applications.,"i believe the main result can just follow from this simple observation.-----reproducibility: the authors did not include code for their developed algorithms in the supplementary material.-----impact: provided that the problem setup and model are better justified, i believe this work could open up new research questions in machine learning and data analysis, including (mixture) variants of well-studied problems on matrix completion and robust learning.-----overall i like the paper, especially that the problem setup itself seems novel.","a fairly large portion is dedicated to motivational applications, which feels like an extension of the related work section, but are not experimentally explored.","the main results of the paper lie in deriving sufficient and necessary conditions for identifiability of these partitions when the subspaces and the entries of the features are randomly positioned in the ambient dimension and the subspaces, respectively.",0.1614349775784753,0.0361990950226244,0.1076233183856502,0.1076233183856502,0.300395256916996,0.0876494023904382,0.1422924901185771,0.1422924901185771,0.1844660194174757,0.0196078431372549,0.116504854368932,0.116504854368932,0.0659340659340659,0.0111111111111111,0.0439560439560439,0.0439560439560439,0.2327586206896552,0.0347826086956521,0.1379310344827586,0.1379310344827586,0.300395256916996,0.0876494023904382,0.1422924901185771,0.1422924901185771,0.1319796954314721,0.0205128205128205,0.1015228426395939,0.1015228426395939,0.2180094786729857,0.0382775119617224,0.1327014218009478,0.1327014218009478,6.493608951568604,6.950029850006104,18.18468475341797,6.493608951568604,6.73131275177002,12.20783805847168,7.541181087493896,3.9550528526306152,0.9343449542390928,0.8904830528571795,0.9334729161703744,0.13021795823764776,0.37509791386214714,0.9395642677975189,0.9497378980473903,0.9541382940478355,0.9462052110113535,0.9713882484867903,0.9670230680324884,0.9171126919038424,0.916673736099103,0.9382360087539432,0.9346201126022385,0.13021795823764776,0.37509791386214714,0.9395644431127005,0.9364382375949871,0.9403452121553965,0.013947813041310327,0.3370107669888005,0.4533978994850327,0.8986305392019989
159,https://openreview.net/forum?id=l3gNU1KStIC,"the authors propose to solve underspecified irl problem using an mcem approach. they claim it is the first succint, robust and transferrable solution and have some results on a gridworld-like environment. assessment: the mcem framework actually is a good fit to the irl problem and i dont recall if this has been explicitly called out in the literature before. but to me, the connection and resulting algorithm don't seem enough it terms of innovation and usefulness. arguably, both birl and maxentirl do essentially (sampled) em with different parametric forms. the experiments are unacceptably small scale and inconclusive.-----there were confusing parts in the exposition that i will outline below.-----going by section:-----intro: a good overview of the irl literature.-----sec 2: in the problem statement itself you need to say something about how the expert demonstrations are connected to r. otherwise they seem like there is no relation at all.-----2.2: i'm trying to understand the motivation behind the definition of c^e_\epsilon.-----in the objective formulation you are setting \theta to maximize the likelihood of a randomly choose subset of expert trajectories. i have never seen such a thing done before and it is interesting and possibly a route to robustness, but i would have liked more explanation and discussion. in particular, the choice of a uniform measure across subsets of different cardinalities seems bad.-----would it be simpler to just introduce a hidden variable for each trajectory, stating whether it is valid or not (i.e. drawn from the true reward function and should therefore be used in likelihood computation). i agree then with the 3rd point in 2.2.1, this could be a different way to model expert sub-optimality than birl.-----3.1.2: it took me a while to read this and i dont think i understand. you seem to set \theta_2 = w by the end. then this seems to be very complicated and redundant. in particular, it is very confusing how the dimensionality of the parameter space can depend on the number of monte carlo samples and changes at each iteration ! maybe i misunderstood something.-----3.1.3: this seems like a straightforward em derivation.-----3.2:-----what is the difference between the 2 stopping criteria? just the fact that the 2nd has a patience of 3? if so, seems trivial.-----3.3:-----what is being formally claimed here? that given the condition on n_t, convergence will hold and proof is in one of the references?-----4 (experiments):-----experiments are done on exactly 2 small instances of a variant of gridworld. this is much smaller scale than the experiments you would expect to see in an iclr paper (even granted that iclr is very theory-focused). previous papers have used irl for real world problems like robotics, vehicle routing etc.-----summation: a couple interesting starting point ideas in the theory part, but not completely fleshed out and ultimately the development of the em approach is partly straightforward and part of it is very confusing to me (sec 3.1.2). experiments are very unconvincing in terms of scope. the authors propose an approach to model-based inverse reinforcement learning which estimates a gaussian mixture model over reward-function parameters. the method uses mcem and samples reward functions from a current estimate of the gmm, updates them via a gradient-descent based maximum likelihood approach and then updates the gmm to fit the updated parameters. the authors evaluate the approach on objectworld.-----the paper is at times hard to follow and should be rewritten to be more clear. the contributions and assumptions could be stated more clearly and the paper would strongly benefit from proof-reading. it would also be helpful to disentangle the machinery of mcem from the novel algorithmic contributions of this paper.-----the choice of a gmm to represent the distribution of parameters is not motivated at all in the paper. intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial.-----dsirl is used as an acronym for a variant of the method but is not defined in the paper as far as i can tell.-----while the method can seemingly be applied to deep as well as linear representations, it is unclear what the chosen features and representation is in the experiments.-----the method appears to draw a set of weights w_i as well as a corresponding set of expert trajectories o_i at random in each iteration. the motivation for the use of o_i is that it may correspond to different modes in the expert set, e.g. demonstrations by different experts; however, the assignment of weights to object sets is not consistent between iterations so it is unclear to me how this would be able to handle different experts.-----in the beginning it is mentioned that irl methods require knowledge of the transition model. while many methods do, modern irl methods are model-free more often than not, so this claim is misleading.","this paper describes a method called 'stochastic' inverse reinforcement learning. it is somewhat unclear how this differs from other probabilistic approaches to irl. in particular bayesian approaches have been used in the past to obtain distributions over reward functions. however, sirl tries to estimate a generative model over such distributions. all the reviewers foudn the paper suffering from lack of clarity, in particular with respect to how the model/algorithm is constructed. there are some possible technical problems with respect to claims about inferring demonstrations by different experts (cf. work on multi-task irl). the experiments also seem to be insufficient.","in particular, the choice of a uniform measure across subsets of different cardinalities seems bad.-----would it be simpler to just introduce a hidden variable for each trajectory, stating whether it is valid or not (i.e. drawn from the true reward function and should therefore be used in likelihood computation).","intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial.-----dsirl is used as an acronym for a variant of the method but is not defined in the paper as far as i can tell.-----while the method can seemingly be applied to deep as well as linear representations, it is unclear what the chosen features and representation is in the experiments.-----the method appears to draw a set of weights w_i as well as a corresponding set of expert trajectories o_i at random in each iteration.","the motivation for the use of o_i is that it may correspond to different modes in the expert set, e.g. demonstrations by different experts; however, the assignment of weights to object sets is not consistent between iterations so it is unclear to me how this would be able to handle different experts.-----in the beginning it is mentioned that irl methods require knowledge of the transition model.",the authors propose to solve underspecified irl problem using an mcem approach.,"the method uses mcem and samples reward functions from a current estimate of the gmm, updates them via a gradient-descent based maximum likelihood approach and then updates the gmm to fit the updated parameters.","intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial.-----dsirl is used as an acronym for a variant of the method but is not defined in the paper as far as i can tell.-----while the method can seemingly be applied to deep as well as linear representations, it is unclear what the chosen features and representation is in the experiments.-----the method appears to draw a set of weights w_i as well as a corresponding set of expert trajectories o_i at random in each iteration.","they claim it is the first succint, robust and transferrable solution and have some results on a gridworld-like environment.","intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial.-----dsirl is used as an acronym for a variant of the method but is not defined in the paper as far as i can tell.-----while the method can seemingly be applied to deep as well as linear representations, it is unclear what the chosen features and representation is in the experiments.-----the method appears to draw a set of weights w_i as well as a corresponding set of expert trajectories o_i at random in each iteration.",0.2368421052631578,0.0533333333333333,0.1184210526315789,0.1184210526315789,0.287037037037037,0.0560747663551401,0.1481481481481481,0.1481481481481481,0.3411764705882353,0.0714285714285714,0.1647058823529411,0.1647058823529411,0.1061946902654867,0.0,0.0530973451327433,0.0530973451327433,0.2205882352941176,0.0149253731343283,0.1323529411764706,0.1323529411764706,0.287037037037037,0.0560747663551401,0.1481481481481481,0.1481481481481481,0.1322314049586777,0.0168067226890756,0.0826446280991735,0.0826446280991735,0.287037037037037,0.0560747663551401,0.1481481481481481,0.1481481481481481,7.264640808105469,6.77134370803833,12.292741775512695,7.264640808105469,4.669363021850586,6.239650249481201,7.264640808105469,8.244641304016113,0.929189793228087,0.9443251426888388,0.4160466188246657,0.6173170571497135,0.7516937626671399,0.9244138094599539,0.486813529748034,0.4617476241329569,0.4057536722153241,0.9601093453852174,0.9592885589397493,0.8942681373088032,0.957996950385065,0.9620141120587157,0.876931948276617,0.6173170571497135,0.7516937626671399,0.9244138966151796,0.9352588512559369,0.9535477646628634,0.7935105816366209,0.6173170571497135,0.7516937626671399,0.9244138966151796
160,https://openreview.net/forum?id=lU5Rs_wCweN,"this work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words. it is argued that the inadequate training of rare words slows down the pre-training. the authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words. this technique is applied to bert and electra and is shown to improve over the baseline.-----strength:-----this work proposes a simple approach to accelerate the pre-training, with only a small memory and compute cost during training. the empirical study on bert and electra supports the claimed improvements.-----it provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model.-----weakness:-----it is argued that the proposed approach helps with rare words problem. but it will help to add more experiments to see how much more benefit we can get from it. for example, maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words.-----specifically, regarding "" we define keys as those words with occurrences between 100 and 500 in the data corpus"", how are the range 100 to 500 chosen? have you tried it on words appearing lower than 100 or higher than 500? as mentioned above, it would be interesting to see if this approach can be applied to more words or subwords to get even more gains.-----some design choices needs more details or explanations.-----for example, why does the notedictionary use ""words"" instead of ""sub-words"" as keys? it seems using ""sub-words"" could cover a broader range of sentences with a notedictionary of the same size. it will also be easier to use during pre-training, for example, you could use the contextual embeddings to improve the word embeddings of the sub-words directly to avoid having an extra notedictionary.-----another example is how the window size is chosen, since it seems an important new hyperparameter. summary: this paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts (notes, stored in a note dictionary). the method considerably speeds up the convergence of pretraining bert and electra, and the authors furthermore show that these models perform better when fine-tuning on downstream glue tasks (likely because the models were undertrained to begin with, so converging faster alleviates this issue).-----strengths: the method is surprisingly simple and empirically quite effective. it's especially interesting to see that bert + tnf at 400k steps has better glue performance than bert at 1m steps.-----weaknesses: the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare words---im still not entirely sure why this works so well. do these rare words commonly show up in glue (and thus, the method is helping because your representations of rare words are better)? it seems like tnf is actually improving the representations of more-common words as well.-----recommendation: 7 despite the lack of clarity around why exactly this method works so well, the method seems empirically useful and straightforward to apply. i expect that this will be useful to practitioners interested in applying bert and similar pretraining strategies to new corpora and domains.-----questions:-----its a bit unclear to me that note-taking itself is required for this to work well...in the covid example presented in the introduction, if you see the sentence the covid-19 pandemic is an ongoing global crisis, isnt it possible that mlm itself is sufficient to associate the embedding of covid-19 with pandemic and global crisis? do you have further evidence to show that note-taking is actually improving the representations of rare words, besides glue score (which might not be very indicative, since the rare words might not show up in glue).-----the construction of note dictionary: does 3.47b refer to the number of types or the number of tokens? why not define keys with frequencies less than 100 in the dictionary as well (since you only use types that show up between 100 to 500 times)?-----it means that to reach the same performance, tnf can save 60% of pre-training time. if models are trained on 16 nvidia tesla v100 gpus, bert-tnf can reach berts final performance within 2 days while it takes bert 5.7 days.: is the 2 days vs 5.7 days an actual wallclock measurement? or, are you hypothesizing this based off of the loss curves?-----missing / erroneous citations:-----it is well-known that in a natural language data corpus, words follow a heavy-tail distribution (larson, 2010) this is more-commonly known in the nlp community as zipfs law. better cites would be:-----zipf g. the psychobiology of language. london: routledge; 1936.-----zipf g. human behavior and the principle of least effort. new york: addison-wesley; 1949.-----miscellaneous comments:-----moreover, completely removing those sentences with rare words is not an applicable choice either since it will significantly reduce the size of the training data and hurt the final model performance.: i agree that its a bad idea to remove sentences with rare words, but i disagree that the issue is reducing the size of the data---you can always go collect more data and filter it to not include rare words. its more likely that the issue is that removing sentences with rare words would reduce the diversity of the pretraining data, which would be harmful-----our method to solve this problem is inspired by how humans manage information.: i think the connection to human note-taking is tenuous at best, and would omit it; the motivation remains clear without this.","the authors propose an approach for pre-training that involves ""taking notes on the fly"" for rare words. the paper stirred a lively discussion on the reasons for the reported results, which the authors followed-up with new experiments and findings that convinced the reviewers that indeed their approach is valid and interesting. thus, i am recommending acceptance.","or, are you hypothesizing this based off of the loss curves?-----missing / erroneous citations:-----it is well-known that in a natural language data corpus, words follow a heavy-tail distribution (larson, 2010) this is more-commonly known in the nlp community as zipfs law.",the empirical study on bert and electra supports the claimed improvements.-----it provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model.-----weakness:-----it is argued that the proposed approach helps with rare words problem.,"for example, maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words.-----specifically, regarding "" we define keys as those words with occurrences between 100 and 500 in the data corpus"", how are the range 100 to 500 chosen?",this work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words.,"summary: this paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts (notes, stored in a note dictionary).","do these rare words commonly show up in glue (and thus, the method is helping because your representations of rare words are better)?","as mentioned above, it would be interesting to see if this approach can be applied to more words or subwords to get even more gains.-----some design choices needs more details or explanations.-----for example, why does the notedictionary use ""words"" instead of ""sub-words"" as keys?",the empirical study on bert and electra supports the claimed improvements.-----it provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model.-----weakness:-----it is argued that the proposed approach helps with rare words problem.,0.1359223300970873,0.0,0.0970873786407767,0.0970873786407767,0.3454545454545454,0.0185185185185185,0.1818181818181818,0.1818181818181818,0.2037037037037037,0.0188679245283018,0.1481481481481481,0.1481481481481481,0.1891891891891892,0.0833333333333333,0.1621621621621621,0.1621621621621621,0.2391304347826087,0.0222222222222222,0.1739130434782608,0.1739130434782608,0.1728395061728395,0.0253164556962025,0.1481481481481481,0.1481481481481481,0.0952380952380952,0.0,0.0761904761904762,0.0761904761904762,0.3454545454545454,0.0185185185185185,0.1818181818181818,0.1818181818181818,8.662797927856445,9.811473846435549,15.266488075256348,10.267534255981444,4.407358169555664,6.942501068115234,10.267536163330078,6.511813163757324,0.3077843536138093,0.6235753425020887,0.8950723147027525,0.9753333477440449,0.9758441491724792,0.6423048551669029,0.975361555987389,0.9736490199455309,0.9180393668932133,0.9752630427818028,0.9759965879532497,0.9167699187958701,0.9837905390448696,0.9835061412559567,0.9498364166350696,0.9134766101505493,0.93369956474189,0.8268247926999396,0.9800113369944281,0.9759526248886973,0.9562014461395917,0.9753333477440449,0.9758441491724792,0.6423041994986343
161,https://openreview.net/forum?id=m2ZxDprKYlO,"this paper proposes implicit process meta-learning (ipml) where each task is represented as a continuous latent vector-----z-----, and corresponding data points are described as function values evaluated at an implicit process conditioned on the task latent vector-----z-----. to conduct the intractable inference, a stochastic gradient hamiltonian monte carlo (sghmc) algorithm is employed. a vae-like network called x-net is trained simultaneously to generate synthetic tasks from the task latent vectors. the experimental results demonstrate that the proposed algorithm shows decent performances on few-shot classification tasks, and the task latent vectors indeed represent a meaningful space of the tasks on which measuring distances between tasks and detecting outlier tasks are possible.-----overall i find this paper is clearly written. various experiments are conducted to demonstrate the benefits of the proposed model. personally, i like the implicit process framework, and i guess this is a nice application of the idea to meta-learning.-----since the performance of the proposed algorithm is not overwhelming, i presume the main advantage of the proposed algorithm is its ability to model a proper latent space of tasks. my first concern is that whether this is a property only for the proposed ipml. for instance, consider a prototypical net. although not being as principled as ipml, a prototypical net can also represent a task via prototypical vectors and consider them as a latent representation of the task (e.g., just take the average of all prototype vectors to get a single representative of a task). is this space completely meaningless so measuring the distance metric doesn't reflect the semantic relationship between tasks at all?-----also, as far as i understood, the ability to construct a meaningful latent space of tasks is not solely from the implicit process itself but aided by the additional x-net learning a generative model over the data points in the tasks. what if we conduct measuring the mmd between tasks for the model without x-net (not between the generated tasks and an existing task, but between existing tasks semantically apart)? moreover, how the prototypical-like model with a generative model attached compare to the proposed one on synthetic task generation? the reason why i'm concerned about this is training a generative model and generating an image from it is usually quite expensive. for instance, one can easily imagine that training x-net for miniimagenet or tiredimagenet and generating images from it might be effective.-----some minor questions-----figure 3 shows the shift in the latent space when tasks data are manipulated. but is it really demonstrate the semantic difference between tasks? i guess a representation from an ordinary classifier might also show this because the input ""images"" were shifted using non-trivial operations. i guess this point may be made clear by having a look at the representations of semantically different tasks (e.g., classification of different classes) without being modified at the image level.-----for a binary classification task (figure 4), when generating a task from a task vector-----z-----, as far as i understood, each data-----x-----is generated conditioned on-----z-----and then corresponding label-----y-----is generated. are the generated data is well balanced between positive and negative classes?-----have you considered using sgld instead of sghmc? if so, did you observe any empirical degrade in the performance? === summary ===-----the paper proposes a meta-learning method based on implicit process (ip) framework in which each task is represented by a latent vector. the ip setup for meta-learning seems identical to that of neural processes [1]. in that, the key challenge for adaptation to a task based on a context/support set-----(xc,yc)-----is inferring the distribution-----p(z|xc,yc)-----over latent vectors. while [1] use amortized (variational) inference with a variational gaussian distribution to approximate the true-----p(z|xc,yc)-----, the paper proposes to use stochastic gradient hamiltonian monte carlo (sg-hmc) for sampling latent state vectors from-----p(z|xc,yc)-----.-----in the experiment section, the paper compares the proposed method (impl) against various previous methods, mostly gradient-based meta-learners, on common benchmarks, demonstrating competitive results. in addition, the paper provides a detailed inspection of the latent space and argues that impl could be used for active task selection and synthetic task generation.-----=== main argument ===-----the ip meta-learning setup of the paper is well-known in the meta-learning community (e.g. [1]) and thus not a contribution. unlike to previous work (e.g. [1]) which uses the ip framework for meta-learning, the paper proposes to use sg-hmc instead of amortized (variational) inference for approximating p(z|x_c, y_c), resulting in a new, sound meta-learning algorithm. this can be considered the methodological main contribution of the paper.-----the experiment section features standard, well-accepted meta-learning benchmarks and demonstrates that impl is not only theoretically sound but also performs well in practice. the analysis and visualization of the learned latent task representation is interesting and provides nice qualitative evidence that proximity of task representation vectors in the latent space reflects the human-perceived task similarity to a fair degree.-----unfortunately, the papers experiment section does not address some of the core questions: how does impl compare to neural processes (nps)? what are the pros and cons of replacing the amortized (variational) inference with sg-hmc? e.g. how much performance increase does the arguably much more complex and computationally demanding approximate inference method buy us? since, the key contribution of the paper is replacing the approximate inference method of nps with sg-hmc, an empirical in-detail comparison with nps would strengthen the paper a lot!-----in the related work section, the paper argues that a limitation of nps is the gaussian variational distribution in the latent space. however, since the latent space is meta-learned and thus quite arbitrary - this is not a very convincing argument. empirical evidence for the hypothesized shortcomings of nps would be much better.-----furthermore, it remains unclear why/how active task selection could be relevant in any real-world setting. motivating the setup with a real-world use case would strengthen this aspect of the paper. the real-world risk detection experimental results in the appendix are nice and interesting. putting more of the results in the main part (there is still some space) and briefly discussing them could give a nice argument for the benefits of the latent task representation inherent in impl - i.e. it provides a straightforward way for identifying tasks that are very dissimilar from the rest - excluding such outlier tasks may increase performance.-----=== overall assessment ===-----in the current form, i see the paper slightly above the acceptance threshold. the proposed meta-learning algorithm based on ips is sound and the extensive experiments demonstrate that it works well in practice. however, the only innovation of impl over neural processes (nps) is replacing variational inference by sg-hmc. thus, the algorithmic contribution is limited. what the advantage of impl over nps is, has not been addressed sufficiently. if a proper experimental comparison to nps is added to the paper, i am happy to increase my score.-----[1] garnelo, marta, et al. ""conditional neural processes."" arxiv preprint arxiv:1807.01613 (2018). this paper proposes an efficient meta-learning approach using implicit processes. specifically, authors represent each task as a continuous latent vector and use expectation-maximization algorithm to perform meta-learning. the e step performs task adaption using stochastic gradient hamiltonian monte carlo sampling method, while the m step optimizes meta-learning objective using these samples. their framework can measure principled distance between tasks by maximum mean discrepancy (mmd) and generate synthetic tasks by task-dependent distribution. finally, the authors validate their proposed framework on several benchmark datasets and real-world datasets. the novelty and originality of this paper is good by proposing new ideas and methods. in addition, the paper is well-organized and clearly written. we can quickly get to know what problem they are trying to solve, how they solve and what their results are.-----pros: 1.representing each task as a continuous latent vector is very suitable when ones need to measure the task similarity, synthesize new tasks or actively select a task without the assumption of known task contexts. 2.the expectation-maximization algorithm based on the stochastic gradient hamiltonian monte carlo sampling method can mitigate the enormous computation overhead by using first-order approximation without second-order derivative. 3.authors demonstrate the effectiveness of this ipml on benchmark datasets and real-world tasks.-----cons: 1.the results of ipml in table 2 do not outperform the baseline methods. i think authors could provide more analysis or reasons on results. 2.authors should add more ablation study and analysis on why the proposed method would work. for example, how much is the contribution of representing each task as a continuous latent vector? how much is the contribution of the expectation-maximization algorithm? 3.apart from the performance in the experiment, authors should also mention the efficiency of the ipml compared with other baseline methods. for example, the run time of the algorithm on computational expensive tasks.","this paper sits at the borderline: the reviewers agree it is a well-written and interesting paper, but have concerns about efficiency as well as a comparison with the neural process (the authors did include a revision with this comparison, though the numbers they report are worse than in the original neural processes paper on the same experiment). ultimately, this paper probably requires another round of reviews before it is ready for publication.","for instance, one can easily imagine that training x-net for miniimagenet or tiredimagenet and generating images from it might be effective.-----some minor questions-----figure 3 shows the shift in the latent space when tasks data are manipulated.","the experimental results demonstrate that the proposed algorithm shows decent performances on few-shot classification tasks, and the task latent vectors indeed represent a meaningful space of the tasks on which measuring distances between tasks and detecting outlier tasks are possible.-----overall i find this paper is clearly written.","while [1] use amortized (variational) inference with a variational gaussian distribution to approximate the true-----p(z|xc,yc)-----, the paper proposes to use stochastic gradient hamiltonian monte carlo (sg-hmc) for sampling latent state vectors from-----p(z|xc,yc)-----.-----in the experiment section, the paper compares the proposed method (impl) against various previous methods, mostly gradient-based meta-learners, on common benchmarks, demonstrating competitive results.","this paper proposes implicit process meta-learning (ipml) where each task is represented as a continuous latent vector-----z-----, and corresponding data points are described as function values evaluated at an implicit process conditioned on the task latent vector-----z-----.","while [1] use amortized (variational) inference with a variational gaussian distribution to approximate the true-----p(z|xc,yc)-----, the paper proposes to use stochastic gradient hamiltonian monte carlo (sg-hmc) for sampling latent state vectors from-----p(z|xc,yc)-----.-----in the experiment section, the paper compares the proposed method (impl) against various previous methods, mostly gradient-based meta-learners, on common benchmarks, demonstrating competitive results.","while [1] use amortized (variational) inference with a variational gaussian distribution to approximate the true-----p(z|xc,yc)-----, the paper proposes to use stochastic gradient hamiltonian monte carlo (sg-hmc) for sampling latent state vectors from-----p(z|xc,yc)-----.-----in the experiment section, the paper compares the proposed method (impl) against various previous methods, mostly gradient-based meta-learners, on common benchmarks, demonstrating competitive results.","this can be considered the methodological main contribution of the paper.-----the experiment section features standard, well-accepted meta-learning benchmarks and demonstrates that impl is not only theoretically sound but also performs well in practice.","the experimental results demonstrate that the proposed algorithm shows decent performances on few-shot classification tasks, and the task latent vectors indeed represent a meaningful space of the tasks on which measuring distances between tasks and detecting outlier tasks are possible.-----overall i find this paper is clearly written.",0.125,0.0181818181818181,0.0714285714285714,0.0714285714285714,0.2131147540983606,0.0166666666666666,0.1639344262295082,0.1639344262295082,0.1857142857142857,0.0144927536231884,0.1142857142857142,0.1142857142857142,0.2300884955752212,0.054054054054054,0.1592920353982301,0.1592920353982301,0.1857142857142857,0.0144927536231884,0.1142857142857142,0.1142857142857142,0.1857142857142857,0.0144927536231884,0.1142857142857142,0.1142857142857142,0.2385321100917431,0.0,0.146788990825688,0.146788990825688,0.2131147540983606,0.0166666666666666,0.1639344262295082,0.1639344262295082,6.987160682678223,6.987160682678223,13.901991844177246,6.812105178833008,8.0765380859375,6.987160682678223,6.812105178833008,6.394381046295166,0.915437621111773,0.9396966217933284,0.6575387432125763,0.9722311612587505,0.9463627807002556,0.8882681828977316,0.134329210485768,0.1469198078415458,0.5313323198382557,0.9733823295265444,0.9695510913324655,0.9343753997993872,0.134329210485768,0.1469198078415458,0.5313309488809665,0.134329210485768,0.1469198078415458,0.5313316641630305,0.6003658843087799,0.6400915841313422,0.9056901208039874,0.9722311612587505,0.9463627807002556,0.8882680240172183
162,https://openreview.net/forum?id=m4UCf24r0Y,"the knowledge distillation (kd) approach is a two-step procedure: first train the teacher model on the labeled data and then train the student model using the predicted class probabilities from the teacher model. a key theoretical question about kd is whether and how much this two-step approach can improve on the one-step approach that trains the student model directly on the labeled data. this paper casts kd as a semiparametric inference problem by treating the optimal student model as the parameter of primary interest and the true class probabilities as the nuisance parameter. building on the semiparametric framework, the paper makes two contributions: 1) develops theoretical guarantees for the vanilla kd algorithm; 2) proposes improved kd by using a first-order bias-corrected loss and a sample splitting procedure.-----overall, i find the paper novel, well-written, and thought-provoking. it bridges the two directions of kd and semiparametric inference, allowing for the possibility of borrowing theoretical and methodological tools from semiparametric inference to analyze and improve the kd approach.-----on the other hand, i think the paper somewhat oversells the semiparametric inference idea, since the theory presented in the paper (theorem 1) does not have much semiparametric flavor. the central questions in semiparametric inference are the information bounds and semiparametric efficiency for the target parameter. although theorem 1 is useful, it does not directly address these key questions. for example, what is the best possible performance for the student model? in many classical semiparametric inference problems, we are able to construct a semiparametrically efficient estimator. would this be possible for kd?-----more comments:-----the paper seems to treat-----f0-----as infinite dimensional (page 3, 3rd paragraph of section 3). however, classical semiparametric inference largely exploits the finite or low dimensionality of the target parameter to answer the questions mentioned above. in the kd framework, i still think the finite-dimensional case is more interesting and would provide more fundamental insights.-----there is a notable difference between the settings of kd and classical semiparametric inference: the target parameter-----f0-----in kd is ancillary in the sense that the data generating model depends only on the nuisance parameter-----p0-----. would this affect the applicability of semiparametric inference techniques?-----page 4, line 11, student should be teacher.-----update: i appreciate the authors response. the answer to my semiparametric efficiency question, however, is too terse and unclear. theorems 1 and 3 provide only error bounds; i dont see how they, together with the semiparametric efficiency of ols, directly imply semiparametric efficiency for the student model. semiparametric efficiency involves the optimal asymptotic variance. the authors might have confused the concept with rate optimality. nevertheless, the paper is a nice contribution, and i will keep my rating. this paper formulates knowledge distillation as a semi-parametric inference problem. then, the paper adapts techniques from semi-parametric inference to analyze the error of a student model and improve the performance of knowledge distillation. the presentation is overall of good quality and it is relatively easy for me to follow the logic flow. that being said, there is still some room of improving the clarity of the paper.-----p1: analyzing and improving knowledge distillation from a point view of semi-parametric inference is very interesting. the adaptation of semi-parametric inference techniques such as cross-fitting and orthogonal machine learning is novel.-----c1: the related work is not sufficient. after reviewing some pioneer studies in knowledge distillation before 2015, this paper jumps to very recent studies in 2020. as knowledge distillation is a hot research topic, there are a large number of related studies from 2016 to 2019. it is highly recommended for this paper to present the advance of knowledge distillation techniques from 2015 until now in a more comprehensive and organized way.-----c2: some contributions of this paper, cross-fitting and orthogonal machine learning, are largely adapted from semi-parametric inference literature. the contribution that is not adapted from semi-parametric inference literature is the proposed correction that balances the bias and variance. the correction is a linear combination of a bias term and a variance term. it is not clear how the bias and variance terms are related to the theoretical analysis in previous sections. it would be better if this paper could provide a clearer justification of the bias and variance terms by the theoretical analysis.-----c3: the hyper-parameter \alpha is introduced to balance the bias and variance of knowledge distillation in the experiment section. since one of the key ideas of the paper is to balance the bias and variance, the hyper-parameter should be important in determining the accuracy of the student model. this paper should do some ablation studies to investigate how the hyper-parameter affects the performance of the proposed knowledge distillation.","the paper studies knowledge distillation through the lens of semi-parametric inference. there has been a lot of work on knowledge distillation in the past, but, as the paper points out, most of it is heuristic or empirical in nature, and thus theoretical understanding on the subject is lacking. the reviewers generally agree that this paper makes a useful theoretical contribution towards a better understanding of knowledge distillation. however, the reviewers also raised some concerns, especially regarding the clarity of the text, and they felt that the paper might be overstating its contribution. still, all reviewers recommend acceptance, and on balance the merits of the paper seem to outweigh the weaknesses, so i'd be happy to recommend acceptance.","then, the paper adapts techniques from semi-parametric inference to analyze the error of a student model and improve the performance of knowledge distillation.","it bridges the two directions of kd and semiparametric inference, allowing for the possibility of borrowing theoretical and methodological tools from semiparametric inference to analyze and improve the kd approach.-----on the other hand, i think the paper somewhat oversells the semiparametric inference idea, since the theory presented in the paper (theorem 1) does not have much semiparametric flavor.",the contribution that is not adapted from semi-parametric inference literature is the proposed correction that balances the bias and variance.,the knowledge distillation (kd) approach is a two-step procedure: first train the teacher model on the labeled data and then train the student model using the predicted class probabilities from the teacher model.,the knowledge distillation (kd) approach is a two-step procedure: first train the teacher model on the labeled data and then train the student model using the predicted class probabilities from the teacher model.,"then, the paper adapts techniques from semi-parametric inference to analyze the error of a student model and improve the performance of knowledge distillation.",a key theoretical question about kd is whether and how much this two-step approach can improve on the one-step approach that trains the student model directly on the labeled data.,"it bridges the two directions of kd and semiparametric inference, allowing for the possibility of borrowing theoretical and methodological tools from semiparametric inference to analyze and improve the kd approach.-----on the other hand, i think the paper somewhat oversells the semiparametric inference idea, since the theory presented in the paper (theorem 1) does not have much semiparametric flavor.",0.1958041958041958,0.0709219858156028,0.1678321678321678,0.1678321678321678,0.247191011235955,0.0454545454545454,0.1573033707865168,0.1573033707865168,0.1857142857142857,0.0434782608695652,0.1285714285714285,0.1285714285714285,0.1699346405228758,0.0264900662251655,0.1437908496732026,0.1437908496732026,0.1699346405228758,0.0264900662251655,0.1437908496732026,0.1437908496732026,0.1958041958041958,0.0709219858156028,0.1678321678321678,0.1678321678321678,0.1456953642384106,0.0134228187919463,0.119205298013245,0.119205298013245,0.247191011235955,0.0454545454545454,0.1573033707865168,0.1573033707865168,9.493436813354492,13.669563293457031,13.669563293457031,8.793048858642578,9.493436813354492,5.407032012939453,8.793048858642578,10.46290683746338,0.9596460287800995,0.9553919166370434,0.9080368453799581,0.9506812563337604,0.9609108400583316,0.9095836435817968,0.8967078832970004,0.914406969816478,0.9010800605733247,0.9757477340912529,0.9783056028074696,0.9442613202926291,0.9757477340912529,0.9783056028074696,0.9442613308456704,0.9596460287800995,0.9553919166370434,0.9080367206874351,0.9671862277831605,0.961368196492055,0.9054698648662644,0.9506812563337604,0.9609108400583316,0.9095835718095326
163,https://openreview.net/forum?id=oev4KdikGjy,"in this work, authors provide an analysis of mutual information for msda and the develop a new variant of mixup. the effectiveness is demonstrated by experiments.-----strength 1. authors study the difference between masking msda and interpolative msda, which is helpful for understanding the power of mixup and its variants. 2. they develop a new augmentation method and improve the performance of masking msda.-----weakness 1. the proposed measurement is not helpful for designing new methods. note that the mutual information in mixup is lower than baseline while mixup still outperforms baseline. 2. compared to mixup and cutmix, the improvement reported in table 2 is marginal. 3. the experiments on imagenet is unconvincing. both of mixup and cutmix are worse than baseline, which contradicts the existing results. 4. there lacks the discussion for the saliency based mixup methods, e.g., puzzle mix [1]. it is closely related to fmix but equipped with a learnable strategy to obtain patches for mixing.-----[1] j-h kim, et al. puzzle mix: exploiting saliency and local statistics for optimal mixup this paper introduces a new mixup method that builds masks by first sampling a grey-scale mask from fourier space, which is subsequently transformed into a binary mask. this improves results against several baselines and achieves state-of-the-art on a few important vision benchmarks.-----my first remark is about the masking. the procedure seems fine, but why not compare to the way masks are sampled in context encoders [1]. this seems like an important baseline masking method to compare to. in addition, one could try sampling masks from a standard segmentation model, e.g., r-cnn.-----my second remark is about the mi bounds. on page 14 in the appendix, you state that the mi between z_a and x_hat is approximately equal to the kl divergence between the posterior and the normal distribution, but in general this wont be true as in training the gaussian mixture p_za wont match the normal distribution, so you have an upper bound. so you have a lower bound of an upper bound to the mi, not a lower bound.-----i'm curious though why not just use one of the recent neural estimators, e.g., found in [2] or [3]. in general, using vaes for mi estimators depends heavily on the quality of the generator, so these neural estimators might be better suited.-----other comments: p1:-----""'post-processing cannot increase information'"" if such processing is deterministic, no?-----p2:-----""cutmix imposes an unnecessary limitation"": what limitation? could you clarify?-----finally, do you plan to have updated results that compare to the 1024 batch size / 300 epoch settings?-----[1] pathak, deepak, et al. ""context encoders: feature learning by inpainting."" proceedings of the ieee conference on computer vision and pattern recognition. 2016. [2] belghazi, mohamed ishmael, et al. ""mine: mutual information neural estimation."" arxiv preprint arxiv:1801.04062 (2018). [3] poole, ben, et al. ""on variational bounds of mutual information."" arxiv preprint arxiv:1905.06922 (2019). the paper presents an interesting analysis of cutmix and mixup data augmentation techniques. it also presents an improvement to cutmix that removes the horizontal/vertical axis bias. the idea to use fourier noise to construct masks for a variant of cutmix is interesting and well-motivated.-----my main concern is whether the conclusions drawn by the analyses are fully grounded. the paper performs an analysis of the effect of the augmented data on learned representations by training unsupervised models on the augmented or clean data and measuring their mutual information. this analysis has the undesirable property of not matching the supervised case in a number of ways, such as different learning objectives, model architectures, etc. even ignoring this, if we take the result that mixup consistently reduces the amount of information that is learned about the original data, what then explains the improved generalization accuracy mixup showcases in their original paper?-----moreover, after claiming that the analysis indicates that mixup learns different representations, the paper asks whether these different representations learned from mixup give rise to practical differences other than just improved generalisation. the issue is that they do this via an adversarial attack analysis, rather than a more realistic non-worst-case robustness analysis. this leads to the conclusion mixup (...) does not correspond to a general increase in robustness. but it does not answer the original question of whether mixup gives rise to practical differences other than just improved generalisation. the finding that mixup yields greater imagenet-a robustness (presented later in the paper) also contradicts this early claim.-----the finding that mixup provides more compressed representations does not necessarily mean that masking augmentation methods are better than interpolation ones. the paper seems to acknowledge this as well, in the final paragraph of the introduction, where it describes an experiment in which combining fmix+mixup gives the best results (presumably because their representations of data are different and therefore combining them would yield the best of both worlds). this seems to contradict the previous adversarial analysis in which mixup was found to not yield significantly more robustness. further, the combination experiment has the two leading combination methods (fmix+mixup and cutmix+mixup) yield very similar results (within the margin of error), which opens the question of whether fmix meaningfully improves over cutmix.-----overall, i find the paper very easy to read and presenting some interesting ideas and even some exciting improvements in performance. i just wish the presentation and the claims made in the analysis of msda methods accounted for some of the inconsistencies described above.-----update after rebuttal: i appreciate the authors' response and clarifications. i maintain my original score.","the paper analyzes the space of mixed sample data augmentation approaches, and proposes a new variant, fmix, based on a new masking strategy. reviewers point to the fact that fmix is only marginally better than previous approaches, that the experimental setup is unconvincing, and that the proposed analysis might not be grounded. this is a really borderline paper but i see the issues as more important than the benefits, so i recommend rejection.","could you clarify?-----finally, do you plan to have updated results that compare to the 1024 batch size / 300 epoch settings?-----[1] pathak, deepak, et al. ""context encoders: feature learning by inpainting.""","even ignoring this, if we take the result that mixup consistently reduces the amount of information that is learned about the original data, what then explains the improved generalization accuracy mixup showcases in their original paper?-----moreover, after claiming that the analysis indicates that mixup learns different representations, the paper asks whether these different representations learned from mixup give rise to practical differences other than just improved generalisation.",the paper presents an interesting analysis of cutmix and mixup data augmentation techniques.,"in this work, authors provide an analysis of mutual information for msda and the develop a new variant of mixup.","even ignoring this, if we take the result that mixup consistently reduces the amount of information that is learned about the original data, what then explains the improved generalization accuracy mixup showcases in their original paper?-----moreover, after claiming that the analysis indicates that mixup learns different representations, the paper asks whether these different representations learned from mixup give rise to practical differences other than just improved generalisation.","even ignoring this, if we take the result that mixup consistently reduces the amount of information that is learned about the original data, what then explains the improved generalization accuracy mixup showcases in their original paper?-----moreover, after claiming that the analysis indicates that mixup learns different representations, the paper asks whether these different representations learned from mixup give rise to practical differences other than just improved generalisation.",the idea to use fourier noise to construct masks for a variant of cutmix is interesting and well-motivated.-----my main concern is whether the conclusions drawn by the analyses are fully grounded.,"even ignoring this, if we take the result that mixup consistently reduces the amount of information that is learned about the original data, what then explains the improved generalization accuracy mixup showcases in their original paper?-----moreover, after claiming that the analysis indicates that mixup learns different representations, the paper asks whether these different representations learned from mixup give rise to practical differences other than just improved generalisation.",0.0571428571428571,0.0194174757281553,0.0571428571428571,0.0571428571428571,0.2553191489361702,0.0287769784172661,0.1560283687943262,0.1560283687943262,0.1627906976744186,0.0476190476190476,0.1162790697674418,0.1162790697674418,0.1720430107526881,0.0439560439560439,0.1075268817204301,0.1075268817204301,0.2553191489361702,0.0287769784172661,0.1560283687943262,0.1560283687943262,0.2553191489361702,0.0287769784172661,0.1560283687943262,0.1560283687943262,0.2264150943396226,0.0,0.1509433962264151,0.1509433962264151,0.2553191489361702,0.0287769784172661,0.1560283687943262,0.1560283687943262,11.42223072052002,11.42223072052002,12.705946922302246,11.42223072052002,5.605071544647217,9.699380874633787,11.42223072052002,6.723340034484863,0.948435419992225,0.9257079384622064,0.807254065863855,0.24336602985089306,0.37166409524443056,0.9351353976394304,0.9679852137305667,0.965013805198152,0.9166364569854386,0.9680298190804343,0.9645581066762652,0.7375309582965046,0.24336602985089306,0.37166409524443056,0.9351353550700959,0.24336602985089306,0.37166409524443056,0.9351353976394304,0.9609671990772346,0.9595115019968018,0.9407704086075291,0.24336602985089306,0.37166409524443056,0.9351353690048176
164,https://openreview.net/forum?id=p8agn6bmTbr,"the authors contribute to the recent research on whether neural network training (in particular, sgd) favors minimal representations, in which irrelevant information is not represented by deeper layers. they do so by implementing a simple neuroscience-inspired task, in which the network is asked to make a decision by combining color and target information. importantly, the network's output is conditionally independent of the color information, given the direction decision, so the color information is in some sense irrelevant at the later stages. using this, the authors quantify the 'relevant' and 'irrelevant' information in different layers of the neural network during training. interestingly, the authors show that minimal representation are uncovered only if the network is started with random initial weights. information is quantified using a simple decoder network.-----the article is clearly written and has a simple (in a good way) and interesting message. however, i also have some criticisms, especially regarding the conceptual underpinnings.-----when any neural network is predicting a deterministic function f : x -> y, all input features are irrelevant to the output distribution when conditioned on the output itself. in other words, the minimal representation in a deterministic task is simply the output itself. (the situation is different when the task involves predicting a non-degenerate probability distribution p(y|x), in which case the minimal representation -- i.e., the sufficient statistics -- can have an arbitrary amount of information.) in the information bottleneck community, this was mentioned in https://arxiv.org/pdf/1703.00810.pdf (section 2.4) and explored in https://arxiv.org/abs/1808.07593.-----in motivating the paper, the authors appear to confuse two types of ""irrelevant features"": (1) when an input feature is useless for prediction, i.e., changing it does not change the predictions, and (2) when information about an input feature is independent of the output distribution, when conditioned on the output. for a deterministic prediction task, all features type 2, but not all features are type 1. the authors have the following text: ""we believe this task ... captures key structure from deep learning tasks. for example, in image classification, consider classifying an image as a car, which take on various colors. a representation in the last layer is typically conditionally independent of irrelevant input variations (i.e., the representation does not change based on differences in color)."" if i understand the example, this is building off the intuition that ""color of car"" is irrelevant because it is a type 1 feature (not useful for prediction). in fact, it can be conditionally independent because it is type 2. moreover, in the authors' task ""color of checkerboard"" is not type 1 (it is very relevant for the output -- changing it changes the output) but it can also be conditionally independent (since it is type 2).-----given the above arguments, the degree to which features are conditionally independent in middle layers does not necessarily reflect how useful they are for prediction.-----i have two other, more minor comments:-----the notion of ""direction information"" is somewhat confusing, as one can think about two kinds of direction information: (1) information about which targets (i.e., directions) correspond to which colors (which is provided as part of the input), and (2) information about the final reaching direction (i.e., the output). given the points made above, if i understand correctly, information about which targets correspond to which colors is just as irrelevant as the color information, when conditioned on the output. i would suggest referring to the second kind of information (the one mainly discussed in the paper) as ""output information"".-----the authors should probably cite (and may be interested in) https://arxiv.org/abs/2009.12789 (neuroips 2020), which also proposes to estimate mutual information using a practical family of decoders. this work introduces a notion of ""usable information"" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are ""minimal"" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations.-----pros:------- the definition of usable information is reasonable and likely useful for future analyses (though see below) -- the questions addressed by the paper are interesting / important and are in need of thorough empirical study-----cons:------- the tasks used in this paper are very simple, with even/odd mnist classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple xor). it is very hard to know whether the paper's conclusions would generalize to tasks of interest to the machine learning community, or even to other simple tasks with different structure------- it is not clear from the pretraining experiments whether the negative results are due to pretraining or just the scale of the weights. if networks were initialized randomly with mean / std taken from the pretrained network, would they also not learn minimal representations?------- it seems that the results of the paper must necessarily depend on several hyperparameters which were not explored. for instance, if the learning rate in early layers is set sufficiently small, the network should learn these simple tasks without minimal representations.------- the result about generalization correlating with minimality was not confirmed on mnist. it is not clear whether this is because the result does not hold on mnist or simply because the authors did not test it.------- transfer learning is known to be helpful in some practical settings. is this framework able to account for situations when transfer might be helpful, as well as harmful? more discussion of this is needed-----overall, given that this is an empirical paper (no new theory is provided), it is important for the experiments to be extensive and comprehensive. the experiments in this paper, though they touch on interesting ideas, are not thorough enough to convince a reader of the authors' broader claims. broadly, this work is an attempt to understand how neural networks can form generalizable representations while being severely overparameterized. this work proposes an information theoretic measure, called the ""usable information"", and use it to quantify the amount of relevant information in different layers of a neural network during training. the key idea is that, in order for the information represented in one layer to be ""usable"" by the next layer, it should be decodable by a simple transformation (affine + element-wise nonlinearity).-----pros:-----(significance) the ""usable information"" is a variant of mutual information, which replaces the expensive conditional entropy term with a cross-entropy loss that is more readily computable from a neural network. a computationally efficient measure for information has a potential for being a generally useful tool in the broad community.-----(quality) i like the approach of this study, which resembles how natural science tries to understand the function of a complex system (such as the brain, i.e., the real neural network) empirically. the choice of the task was also appropriate: it provides a good intuition about the relevant vs. irrelevant information, as well as a bridge to neuroscience studies that may lead to insightful discussion.-----(clarity) the paper is clearly written and easy to follow in most places.-----cons:-----(originality) one thing i expected to find in the paper was some review of other information theoretic measures that were proposed/used in the context of neural networks; proposing a cheaper alternative for the mutual information itself can't be a new idea (although if it is, that would be worth noting too). it would be fair to include a discussion along this line, and perhaps point out the properties of ""usable information"" that makes it particularly appealing.-----(clarity) the presentation of fig 4 is not clear to me. (i) which plots belong to which axis, and what is the third curve ""val"" in black? (ii) ""a positive correlation with the minimality of the representation and generalization performances"": this sounds vague. can you quantify?-----additional comments:-----""does sgd always lead to equivalent representations, or does sgd trace a path through parameter space that leverages structure present in the initialization?"": i don't understand this sentence in the introduction. also related, it would be nice to add a sentence or two to unpack the idea of ""implicit regularization through sgd"".-----in fig 2d, why do the four marker types appear somewhat separated (although not by a large margin), e.g., red x, green o, green x then red o?-----the observation about non-random initialization is very interesting. in this example, keeping the old information does not seem to compromise the performance of the network in the current task. do you think this is generally true for neural networks, or could there be a regime where retaining information about a previously relevant (but no longer relevant) information has an actual cost?-----overall, i think this is an interesting paper that presents a promising approach toward the understanding of how informative representations are formed by training, one of the most fundamental questions in deep learning.-----update: most of my questions/comments are addressed in the revised version of the paper and the author responses. i maintain my support for acceptance.","this paper proposes that we can understand the evolution of representations in deep neural networks during training using the concept of ""usable information"". this is effectively an indirect measure of how much information the network maintains about a given categorical variable, y, and the authors show that it is in fact a variational lower bound on the amount of mutual information that the network's representations have with y. the authors show that in deep neural networks the usable information that is maintained for different variables during training depends on the task, such that task irrelevant variables (but not task relevant variables) eventually have their usable information reduced, leading to ""minimal sufficient representations"".-----the initial reviews were mixed. a common theme in the critiques was the lack of evidence of the generalization and scalability of these results. the authors addressed these concerns by including new experiments on different architectures and the cifar datasets, leading one reviewer to increase their score. the final scores stood at 3, 7 ,7, 7. given the overall positive reviews, interesting subject matter, and relevance to understanding learned representations in deep networks, this paper seems appropriate for acceptance in the ac's opinion.","a computationally efficient measure for information has a potential for being a generally useful tool in the broad community.-----(quality) i like the approach of this study, which resembles how natural science tries to understand the function of a complex system (such as the brain, i.e., the real neural network) empirically.","this work introduces a notion of ""usable information"" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are ""minimal"" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations.-----pros:------- the definition of usable information is reasonable and likely useful for future analyses (though see below) -- the questions addressed by the paper are interesting / important and are in need of thorough empirical study-----cons:------- the tasks used in this paper are very simple, with even/odd mnist classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple xor).","this work introduces a notion of ""usable information"" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are ""minimal"" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations.-----pros:------- the definition of usable information is reasonable and likely useful for future analyses (though see below) -- the questions addressed by the paper are interesting / important and are in need of thorough empirical study-----cons:------- the tasks used in this paper are very simple, with even/odd mnist classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple xor).","the authors contribute to the recent research on whether neural network training (in particular, sgd) favors minimal representations, in which irrelevant information is not represented by deeper layers.","this work introduces a notion of ""usable information"" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are ""minimal"" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations.-----pros:------- the definition of usable information is reasonable and likely useful for future analyses (though see below) -- the questions addressed by the paper are interesting / important and are in need of thorough empirical study-----cons:------- the tasks used in this paper are very simple, with even/odd mnist classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple xor).","this work introduces a notion of ""usable information"" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are ""minimal"" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations.-----pros:------- the definition of usable information is reasonable and likely useful for future analyses (though see below) -- the questions addressed by the paper are interesting / important and are in need of thorough empirical study-----cons:------- the tasks used in this paper are very simple, with even/odd mnist classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple xor).","more discussion of this is needed-----overall, given that this is an empirical paper (no new theory is provided), it is important for the experiments to be extensive and comprehensive.","this work introduces a notion of ""usable information"" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are ""minimal"" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations.-----pros:------- the definition of usable information is reasonable and likely useful for future analyses (though see below) -- the questions addressed by the paper are interesting / important and are in need of thorough empirical study-----cons:------- the tasks used in this paper are very simple, with even/odd mnist classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple xor).",0.1927710843373494,0.0323886639676113,0.1124497991967871,0.1124497991967871,0.3694267515923567,0.0641025641025641,0.1847133757961783,0.1847133757961783,0.3694267515923567,0.0641025641025641,0.1847133757961783,0.1847133757961783,0.151111111111111,0.0269058295964125,0.0977777777777777,0.0977777777777777,0.3694267515923567,0.0641025641025641,0.1847133757961783,0.1847133757961783,0.3694267515923567,0.0641025641025641,0.1847133757961783,0.1847133757961783,0.158590308370044,0.0177777777777777,0.105726872246696,0.105726872246696,0.3694267515923567,0.0641025641025641,0.1847133757961783,0.1847133757961783,8.272295951843262,8.272295951843262,15.018484115600586,8.272295951843262,6.387746810913086,8.272295951843262,8.272295951843262,3.9344000816345215,0.2940239757214132,0.5083034461645607,0.6668526608256633,0.9060823290490608,0.9126139496730914,0.9529043635483675,0.9060823290490608,0.9126139496730914,0.9529043833621483,0.9742383653983623,0.9757276216995434,0.9250851173234796,0.9060823290490608,0.9126139496730914,0.9529043833621483,0.9060823290490608,0.9126139496730914,0.9529043635483675,0.4215239167197342,0.5344308483463333,0.7616269641014263,0.9060823290490608,0.9126139496730914,0.9529043833621483
165,https://openreview.net/forum?id=pAbm1qfheGk,"the authors of this manuscript proposed a generative dynamics system for the modelling and generation of 3d conformations of molecules. specifically, there are three components: (1) conditional graph continuous flow (cgcf) to transform random noise to distances, (2)a closed-form distribution p(r|d, g), and (3) an energy-based tilting model (etm) to capture long-range interactions and correct the position matrix distribution. the proposed framework was compared with two deep learning methods for conformation generations -- cvgae & graphdg, as well as the computational chemistry tool rdkit on geom-qm9, geom-drugs, and iso17 data sets. comparisons in terms of cov and mat scores show that the proposed method (particularly the one enhanced with etm) can outperform baselines. further comparisons of distances densities show that cgcf (but without etm) worked best over baselines.-----overall, i think it is an interesting work. the major novelty is the use of continuous flow to model the conditional distribution of the distances and an energy-based model to correct the conditional distribution of positions. however, i have the following concerns.-----the presentation of this paper can be significantly improved. a few typos need to be corrected: section x -? section x a optimization -> an optimization which a ...-> which is a... demotes -> denotes references should be further formatted-----a bit more precise description about the force-fields algorithms in rdkit is needed.-----from results in table 2, cgcf combined with the etm component does work better than graphdg, although the authors state that it is because the sharpness of the distance distribution. clear justifications should be given to show the benefits, if any, of this phenomenon. this paper presents an approach to generate diverse small molecule conformations given its graph by combining a conditional flow-based model with an energy-based model. sampling is performed in two separate stages: 1) a normalizing flow produces a distribution over interatomic distances (which is then postprocessed into cartesian coordinates), 2) sampled coordinates are refined by langevin dynamics with gradient signal produced from an energy-based model. the models are trained separately.-----i thought that this was an interesting, principled approach that advances the state of the art in generative models for molecular conformation sampling. this approach (cgcf+etm) appears to generate better samples than the vae-based baselines, though all methods are still improved by a final refinement step with traditional forcefields (esp. for larger drug-like molecules) which suggests room for improvement. the general sampling strategy is potentially relevant in many other domains.-----there were some aspects of the evaluation that could be improved/clarified:-----cov and mat do not appear to measure false positives  the cgcf+etm approach could be generating many junk conformations and this would not be captured by the cov/mat. can the authors report a statistic over the generated distribution (instead of over the test set)?-----the abstract/introduction focuses on the computational expense of competing approaches. can the authors comment on the computationally expense for generating each conformation?-----how do the molecular properties of the generated distribution compare to the test distribution (e.g. small molecule properties in simm & hernandez-lobato table 2)?-----can the authors clarify if any of the test set molecules from geom-qm9 or geom-drugs are in the training set? it was unclear from the writing.-----which bonds are in the set of interatomic distances? the authors defines distances as the set of all covalent bonds in the molecule in the methods section, but later mention auxiliary angle/dihedral bonds. this was confusing in the first read.-----typo in eq (9) == this paper combines flow-based and energy-based models to generate molecular conformations from a molecular graph.-----strengths-----the modeling is well-motivated and creative. a continuous flow model maps a graph-based molecular representation into a distribution over conformations. the continuous flow is well-motivated; it factors out the distance calculations to resolve concerns in previous works where pairwise distances are modeled independently.-----additionally, an energy-based model (ebm) is used to further help the model capture long-range atomic interactions. recent advancements in ebms are deployed for stable training, such as a lavengin dynamics.-----the paper is well-written, clear and easy to follow, despite the complexity of the model-----strong baselines: cvgae, graphdg, and rdkit. results are slightly better than cvgae for conformation generation, and slightly better for distance distribution.-----weaknesses-----while the model performs better than neural baselines, it does not yet approach rdkit, which is based on expert knowledge features. this means the proposed model is not yet practically useful.-----only two tasks were investigated, and only one or two datasets within each task. the empirical evaluation of this paper could be improved by including more experiments (e.g. along the lines of mansimov et al. 2019)-----the paper attributes the advantage in rdkit to its inclusion of a force field. therefore, the authors combine a force field with their model and assess the difference (good!) however, the conclusion in the paper seems misguided - looking at table 1, including the force field does not result in a significant change in their model's performance. in fact, this experiment was also done in the cvgae paper (mansimov et al. 2019).-----in table 1, the ebm improves performance on the conformation generation tasks. however, in table 2, the ebm decreases performance on the distribution over distances task. i would have liked to see more investigation or discussion of why this is the case.-----the rdkit baseline is not included in table 2. it is included in the graphdg paper and should be included here as well.-----overall, this paper presents a creative model that performs better than existing neural models. however, the evaluation feels preliminary compared to what we see in earlier works.","the paper combines flow-based and energy-based models to generate molecular conformations given a molecular graph. for this, a continuous flow model is used to map the graph-based molecular representation into a distribution over conformations. an energy-based model (ebm) is used to further help the model capture long-range atomic interactions. the proposed method is compared with strong baselines: cvgae, graphdg, and rdkit.-----the authors addressed most of the reviewers' concerns in the rebuttal.-----all the reviewers agree on acceptance.","recent advancements in ebms are deployed for stable training, such as a lavengin dynamics.-----the paper is well-written, clear and easy to follow, despite the complexity of the model-----strong baselines: cvgae, graphdg, and rdkit.",can the authors comment on the computationally expense for generating each conformation?-----how do the molecular properties of the generated distribution compare to the test distribution (e.g. small molecule properties in simm & hernandez-lobato table 2)?-----can the authors clarify if any of the test set molecules from geom-qm9 or geom-drugs are in the training set?,"further comparisons of distances densities show that cgcf (but without etm) worked best over baselines.-----overall, i think it is an interesting work.",the authors of this manuscript proposed a generative dynamics system for the modelling and generation of 3d conformations of molecules.,can the authors comment on the computationally expense for generating each conformation?-----how do the molecular properties of the generated distribution compare to the test distribution (e.g. small molecule properties in simm & hernandez-lobato table 2)?-----can the authors clarify if any of the test set molecules from geom-qm9 or geom-drugs are in the training set?,can the authors comment on the computationally expense for generating each conformation?-----how do the molecular properties of the generated distribution compare to the test distribution (e.g. small molecule properties in simm & hernandez-lobato table 2)?-----can the authors clarify if any of the test set molecules from geom-qm9 or geom-drugs are in the training set?,a few typos need to be corrected: section x -?,this paper presents an approach to generate diverse small molecule conformations given its graph by combining a conditional flow-based model with an energy-based model.,0.3193277310924369,0.1367521367521367,0.2184873949579832,0.2184873949579832,0.2676056338028169,0.0428571428571428,0.1971830985915492,0.1971830985915492,0.1132075471698113,0.0,0.0377358490566037,0.0377358490566037,0.233009708737864,0.0396039603960396,0.1359223300970873,0.1359223300970873,0.2676056338028169,0.0428571428571428,0.1971830985915492,0.1971830985915492,0.2676056338028169,0.0428571428571428,0.1971830985915492,0.1971830985915492,0.0434782608695652,0.0,0.0434782608695652,0.0434782608695652,0.3119266055045872,0.1308411214953271,0.2385321100917431,0.2385321100917431,7.980314254760742,7.980314254760742,11.498156547546388,7.980314254760742,6.168586730957031,5.981995582580566,8.898591995239258,-0.0058373212814331,0.08494964279210772,0.14454390978409418,0.22510155589998784,0.970789392821123,0.9593835364866488,0.15282214998393986,0.9554908206272122,0.9542762070814941,0.6603610202901273,0.9699673559703144,0.9704720553024807,0.9540619794719531,0.970789392821123,0.9593835364866488,0.1528217222911917,0.970789392821123,0.9593835364866488,0.15282215137149613,0.9361615275335632,0.9447010685526862,0.1083888054131359,0.9681059436649208,0.963090217842998,0.9046420880642446
166,https://openreview.net/forum?id=pHXfe1cOmA,"summary:-----this paper proposes an adaptive dynamics model based on the idea of hypernetworks. it is demonstrated that this approach compares favorably to other ways of adapting dynamics models such as conditioning on a separate feature input and meta learning by gradient-based model updates. the proposed approach is evaluated on pushing and locomotion tasks.-----pros:-----the proposed approach for conditioning dynamics models on rollouts to model system-specific properties using the hypernetworks idea seems novel and is interesting.-----paper is clearly written, the provided figures help understanding-----outperforms state-of-the-art adaptive dynamics modeling approaches [nagabandi et al., 2019], [sanchez-gonzalez et al., 2018b]-----reasonable baselines are used for comparison, such as fixed model (xyz), input feature conditioning (direct), expert ensemble, and state-of-the-art adaptive dynamics models-----cons:-----the paper does not explain training details for the architecture sufficiently well. how are the network components trained, especially the visual recognition part for object pushing? what kind of supervision with ground truth is required to train the components, for instance for object detection and shape representation? are components pretrained and how? which losses/data are used for training?-----its unclear why moving from a canonical to an oriented shape representation in sec. 3.1 should improve results. shouldnt this limit generalization and require more training data?-----giving standard deviations in addition to the average values in table 1-3 would complete the numerical results-----sec. 1) why is planet [hafner et al., 2019] listed as ""no adaptation"", although it contains a recurrent state representation?-----it appears magical that the approach performs better on novel than on seen objects during training for cheetah-slope or ant-slope in table 3. please discuss.-----recommendation:-----the paper reads well and proposes an interesting novel approach which could deserve acceptance. the paper should address the points raised in paper weaknesses.-----questions for rebuttal:-----please address points raised above in ""weaknesses"".-----typos:-----p2: ""they are are""-----p4: ""e_int then maps z_int to a 1-dimensional code z_int  r2"" - shouldn't this be ""e_int maps interactions to 2-dimensional code z_int  r2""?-----p4: ""which is typically comprised of an agent and its external environment"" -> ""which typically comprises / which is typically composed of""-----table 1: motion rediction error -> motion prediction error-----advice: in figure 1, the concatenation symbol is slightly misleading, as it could be interpreted as elementwise multiplication. maybe replace it by-----[,]-----.-----post-rebuttal comments:-----the authors' comments addressed my concerns on method and experimental details mostly well. i keep with my rating ""6: marginally above acceptance threshold"". == update ==-----thank you for your detailed response. the newly added clarifications and sanity checks have greatly improved the quality of the paper, and i am therefore increasing my score from 4 to 6. i believe the model capacity comparison (table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper.-----== original review ==-----the paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time. experiments are conducted on an object pushing and a locomotion task.-----strengths:-----the paper addresses an important question, namely, how a dynamics model may adapt to environments that don't fully match its training distribution.-----the proposed use of a hypernetwork is plausible and novel to my knowledge.-----the related work section appears comprehensive, and, to my knowledge, does not miss any major prior work.-----weaknesses:-----the main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model. i feel like the evaluation of this question is confounded by the choice of tasks and baselines. on the pushing benchmark, the xyz, vf, and densephysnet operate on different modalities than hyperdynamics (either no state information or no visual information), and are therefore difficult to compare. for the mb-maml baseline, this is not specified. the expert-ens model cannot be expected to generalize, since it is designed to overfit on individual objects. as a result, only the 'direct' baseline clearly operates in the same experimental regime as hyperdynamics. however, nothing is reported on the model architecture or the training method for that baseline, raising the question if its model capacity was competitive. my impression is that this experimental design blurs the effects of (a) using side-information to infer system properties, and (b) utilizing such information through a hypernetwork as opposed to a standard dynamics predictor. if the goal is to evaluate the new architecture, these should be disentangled.-----on the locomotion benchmark, the recurrent baseline is similarly unclear. sanchez-gonzalez et al. is cited, but that paper focusses on comparing recurrent models based on graph networks to those based on mlps, and it is unclear which model was used.-----no results are reported for the prediction accuracy on the locomotion task, which would have helped evaluate the performance of the dynamics models more directly than the task scores.-----many of these issues could have been avoided by testing on established benchmarks from the literature, for which results are available. if there is a simulator available, generalization ability could still have been tested by varying the physical constants of the dataset.-----the paper contains a decent amount of typos and grammatical errors.-----overall, while the paper presents an interesting idea, the experimental evaluation is not convincing in its current state: baseline architectures are not fully specified, many of them did not receive the same input, and no benchmark task with previously reported results has been used. as a result, i recommend rejection at this time.-----questions:-----in eq. 1, should omega be a parameter of h(.) instead of f(.)?-----section 3.1 introduces the ""1-dimensional code-----zintr2-----"". so is it one or two-dimensional?-----overall, the dimensionality of the latent codes and hidden layers seems incredibly small, e.g., only 1/2 numbers to encode prior interactions, and 8 to encode shape. is there really no benefit to using higher capacity models?","this paper proposes ""hyperdynamics"" a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction. these parameters are fed into a forward dynamics model, represented as a neural network, that is used for control.-----pros:-----addresses an important problem (adapting dynamics models to ""new"" environments) and provides strong baselines-----well written and authors have improved clarity even further based on reviewers comments-----cons:-----i agree with the reviewer that it is currently unclear how well this will transfer to the real world-----the idea of predicting physical parameters from a history of environment interactions is not not novel in itself (although the proposed framework is, as far as i know). the authors should include related work along the lines of (1) (this is just one paper that comes to mind, others exist)-----(1) preparing for the unknown: learning a universal policy with online system identification","experiments are conducted on an object pushing and a locomotion task.-----strengths:-----the paper addresses an important question, namely, how a dynamics model may adapt to environments that don't fully match its training distribution.-----the proposed use of a hypernetwork is plausible and novel to my knowledge.-----the related work section appears comprehensive, and, to my knowledge, does not miss any major prior work.-----weaknesses:-----the main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model.","the proposed approach is evaluated on pushing and locomotion tasks.-----pros:-----the proposed approach for conditioning dynamics models on rollouts to model system-specific properties using the hypernetworks idea seems novel and is interesting.-----paper is clearly written, the provided figures help understanding-----outperforms state-of-the-art adaptive dynamics modeling approaches [nagabandi et al., 2019], [sanchez-gonzalez et al., 2018b]-----reasonable baselines are used for comparison, such as fixed model (xyz), input feature conditioning (direct), expert ensemble, and state-of-the-art adaptive dynamics models-----cons:-----the paper does not explain training details for the architecture sufficiently well.","the newly added clarifications and sanity checks have greatly improved the quality of the paper, and i am therefore increasing my score from 4 to 6. i believe the model capacity comparison (table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper.-----== original review ==-----the paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time.",summary:-----this paper proposes an adaptive dynamics model based on the idea of hypernetworks.,"the proposed approach is evaluated on pushing and locomotion tasks.-----pros:-----the proposed approach for conditioning dynamics models on rollouts to model system-specific properties using the hypernetworks idea seems novel and is interesting.-----paper is clearly written, the provided figures help understanding-----outperforms state-of-the-art adaptive dynamics modeling approaches [nagabandi et al., 2019], [sanchez-gonzalez et al., 2018b]-----reasonable baselines are used for comparison, such as fixed model (xyz), input feature conditioning (direct), expert ensemble, and state-of-the-art adaptive dynamics models-----cons:-----the paper does not explain training details for the architecture sufficiently well.","the newly added clarifications and sanity checks have greatly improved the quality of the paper, and i am therefore increasing my score from 4 to 6. i believe the model capacity comparison (table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper.-----== original review ==-----the paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time.","the newly added clarifications and sanity checks have greatly improved the quality of the paper, and i am therefore increasing my score from 4 to 6. i believe the model capacity comparison (table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper.-----== original review ==-----the paper proposes a model for predicting the dynamics of a physical system based on hypernetworks: given some observed interactions and some visual input, the hypernetwork outputs the parameters of a dynamics model, which then predicts the evolution of the system's state over time.","the proposed approach is evaluated on pushing and locomotion tasks.-----pros:-----the proposed approach for conditioning dynamics models on rollouts to model system-specific properties using the hypernetworks idea seems novel and is interesting.-----paper is clearly written, the provided figures help understanding-----outperforms state-of-the-art adaptive dynamics modeling approaches [nagabandi et al., 2019], [sanchez-gonzalez et al., 2018b]-----reasonable baselines are used for comparison, such as fixed model (xyz), input feature conditioning (direct), expert ensemble, and state-of-the-art adaptive dynamics models-----cons:-----the paper does not explain training details for the architecture sufficiently well.",0.3658536585365853,0.0491803278688524,0.1951219512195122,0.1951219512195122,0.3333333333333333,0.0546875,0.1317829457364341,0.1317829457364341,0.367816091954023,0.0231660231660231,0.1685823754789272,0.1685823754789272,0.1387283236994219,0.0818713450292397,0.1387283236994219,0.1387283236994219,0.3333333333333333,0.0546875,0.1317829457364341,0.1317829457364341,0.367816091954023,0.0231660231660231,0.1685823754789272,0.1685823754789272,0.367816091954023,0.0231660231660231,0.1685823754789272,0.1685823754789272,0.3333333333333333,0.0546875,0.1317829457364341,0.1317829457364341,10.244596481323242,12.924978256225586,18.334278106689453,12.924978256225586,12.685818672180176,10.244596481323242,12.924978256225586,10.244596481323242,0.9133667254318845,0.9265454506982224,0.38726071633472664,0.9725404837797785,0.9707308011963004,0.704172648799685,0.9676454900326911,0.9621549832021452,0.9463551325698589,0.9707277871067777,0.9623188763426158,0.9448770582104468,0.9725404837797785,0.9707308011963004,0.704172648799685,0.9676454900326911,0.9621549832021452,0.9463552062768452,0.9676454900326911,0.9621549832021452,0.9463550873946283,0.9725404837797785,0.9707308011963004,0.704172648799685
167,https://openreview.net/forum?id=q-qxdClTs0d,"summary. this paper advances generalizable machine learning via addressing a major limitation of invariant risk minimization (irm). in particular, the author(s) identified and discussed the issue of strong----------spurious, where spurious features and class labels are strongly correlated due to common cause, causing unprotected irm to fail while trying to exclude such non-causal predictors. to avoid this. pitfall, the author(s) proposed to leverage conditional distribution matching (cdm) to regularize the representation, which effectively helps to alleviate this issue. two empirical solutions, respectively non-adversarial mmd and adversarial kl matching, have been presented and validated.-----quality & clarity. overall this paper is presented with clarity. the problem is well motivated and carefully discussed. what i found unsatisfactory is the proposed solution needs extra justifications, which is detailed in my weakness section below.-----originality & significance. the author(s) have identified a major weakness of irm that i also find concerning: while developed from the notation of invariant representations, based on which invariant predictors are defined, irm does not explicitly regularize the representation in its formulation. this view, provided fully developed, encapsulates sufficient novelties. on the significance side, while this submission indeed addresses a major concern of irm demonstrated by artificial examples, the author(s) fail to present a concrete real-world example to showcase this concern is a thing that we should actually worry about.-----main weakness. justification of cdm needs to be strengthened. the author(s) have provided an argument that explains the extreme case, where non-causal features perfectly predict domain. the discussion needs to be substantially enriched. cdm has been proposed for dealing with the label shift in domain adaption, and it relies on assumptions that should be reconciled with those made by irm. please clarify.-----theoretical results on pp 4. this is a totally misleading heading. what i have expected is some theoretical discussion, instead, the author(s) have provided a numerical table computed from ""theoretical computations"". i do not consider these as theoretical as they do not generalize beyond this particular example.-----insufficient experimental validation. this is what kills this paper. there is only one experiment performed on a semi-synthetic testbed, which does not serve to evidence the practical utility of this proposal.-----the domain adversarial neural net (dann) model is highly relevant to the proposal made here and should be carefully discussed and compared. in fact, dann also regularizes the representation to make it domain-agnostic.-----minor issues. the term out-of-distribution (ood) is a bit misleading, as this phrase is usually associated with the task of anomaly detection, where novel samples that are very different from the training examples are identified. i would suggest the author(s) replace ood to avoid confusion.-----the aspect ratio in fig 1 is off and it. makes readers very uncomfortable. please redo this figure. and it does not clearly depict what's different compared to the standard scenarios amendable to irm. this paper attacks the problem of ood learning from the angle of invariant causal feature learning. the key idea is to capture domain invariant causal features and use the extracted causality relation to convey domain-adaptive classification. in this work, domain invariant causal features are learned by irm, which imposes the consistency constraint between causal features and class labels across different domains. the core idea is to address the existence of spuriousness correlation by introducing the mmd and kl divergence based conditional distribution matching constraint to the irm learning process. the experimental study based on a crafted mnist data set demonstrates the superior performances of the regularised irm learning method in the domain invariant learning task.-----in general, the paper introduces an in-depth discussion of the limitation of the irm learning mechanism and points out the root cause of failure of irm (spuriousness correlation). this is interesting and potentially impactful for practical ood learning tasks. the paper is well-written and the proposed objective is novel to my knowledge. we tend to accept the paper.-----still, our concerns are as follows:-----though the results look promising on the toy data set, it would be better to have a real-world scenario as a testbed for the proposed method. domain transfer is a popular application. how would this method perform in a domain transfer learning task?-----following the first question, we would expect some discussion about the relation between the proposed method and other transfer learning methods, such as meta-learning methods. could domain invariant casual feature learning be considered as a way of conducting meta-learning?-----a minor issue in table.1: how many domain labels are there defined in the cmnist data set? how are they defined?","loosely, while irm aims to find a feature mapping phi s.t. response y given phi(x) is independent of the environment variables e, they suggest that when e is strongly correlated with y, then it is possible for phi obtained via irm to involve environment variables. they motivate this by suggesting that if there exists a feature mapping phi(x) = e, it would satisfy the irm aim, but that this is undesirable.-----they suggest instead requiring phi(x)|y being invariant to the environment.-----the reviewers bring up a couple of concerns. the first is that it is not clear outside some simple examples when y given phi(x) being independent of e does not suffice. the second is that the authors also do not empirically validate their fix outside a single simple dataset. moreover, what are the pitfalls of having phi(x) given y being independent of e?-----overall, this is an interesting kernel of an idea; it just needs to be fleshed out a bit more.","there is only one experiment performed on a semi-synthetic testbed, which does not serve to evidence the practical utility of this proposal.-----the domain adversarial neural net (dann) model is highly relevant to the proposal made here and should be carefully discussed and compared.","the experimental study based on a crafted mnist data set demonstrates the superior performances of the regularised irm learning method in the domain invariant learning task.-----in general, the paper introduces an in-depth discussion of the limitation of the irm learning mechanism and points out the root cause of failure of irm (spuriousness correlation).",this paper attacks the problem of ood learning from the angle of invariant causal feature learning.,summary.,"the experimental study based on a crafted mnist data set demonstrates the superior performances of the regularised irm learning method in the domain invariant learning task.-----in general, the paper introduces an in-depth discussion of the limitation of the irm learning mechanism and points out the root cause of failure of irm (spuriousness correlation).","the experimental study based on a crafted mnist data set demonstrates the superior performances of the regularised irm learning method in the domain invariant learning task.-----in general, the paper introduces an in-depth discussion of the limitation of the irm learning mechanism and points out the root cause of failure of irm (spuriousness correlation).","the author(s) have provided an argument that explains the extreme case, where non-causal features perfectly predict domain.","the experimental study based on a crafted mnist data set demonstrates the superior performances of the regularised irm learning method in the domain invariant learning task.-----in general, the paper introduces an in-depth discussion of the limitation of the irm learning mechanism and points out the root cause of failure of irm (spuriousness correlation).",0.1308411214953271,0.0188679245283018,0.1028037383177569,0.1028037383177569,0.1875,0.018018018018018,0.125,0.125,0.0756756756756757,0.0,0.054054054054054,0.054054054054054,0.0,0.0,0.0,0.0,0.1875,0.018018018018018,0.125,0.125,0.1875,0.018018018018018,0.125,0.125,0.0851063829787234,0.010752688172043,0.0425531914893617,0.0425531914893617,0.1875,0.018018018018018,0.125,0.125,8.083931922912598,8.083931922912598,5.669076442718506,8.083931922912598,5.426569938659668,9.256340980529783,8.083931922912598,8.370075225830078,0.9570913518909154,0.9495098836194437,0.9046318784347329,0.8586156364967354,0.8660641555285613,0.8940351980425415,0.9589628293666951,0.960378853406342,0.9133102252154857,0.8620793964687236,0.8906853365241585,0.04761715444887905,0.8586156364967354,0.8660641555285613,0.8940351980425415,0.8586156364967354,0.8660641555285613,0.8940351980425415,0.9466302411057684,0.9541797982454691,0.9259062360754406,0.8586156364967354,0.8660641555285613,0.8940351980425415
168,https://openreview.net/forum?id=qzBUIzq5XR2,"this paper introduces generative neuro-symbolic modelling, advertised as a probabilistic programming framework in which the distributions are modelled by neural networks. this is a very exciting idea, and well past its time. however, the work discussed here is limited to modelling the drawn characters in the omniglot dataset, rather than a general framework. it is clear there is not a usable tool that would allow for practical construction of a wide range of probabilistic programs, from the fact that the paper sticks with a simple model for the omniglot problem, rather than doing a lot of manipulations on the model, and, more importantly, the fact that the inference is specific to the omniglot model, and is presumably not very fast, given the description. thus, while the sales pitch to the paper is very exciting, compared to that, the content itself, which is really just a model of one problem, is a bit disappointing. that was also a property of the earlier lake et al paper - i never understood what the ""framework"" was there either. that is not to dismiss this paper. the insight for this problem is that, rather than relying on a database of character primitives, like the previous bayesian model of the problem, you can sequentially generate characters (types) non-independently using the expected cast of characters (convolutions and lstms - although there are also splines involved, so it's not exactly typical cnn either). the evaluations are not all that impressive, except for the unconditional character generation shown at the beginning, which is clearly outstripping the bpl model in terms of human-like ness. in table 3, i don't understand why different image sizes are used, and why bpl is not compared. the paper is very clear.-----in short, i really like the idea in this paper, and i think it is worth building on. the section in the appendix detailing the future work on 3d object modelling sounds very promising. however, the result itself is not all that significant beyond the idea; the evaluations are not particularly strong, except for the generation of novel characters, which would benefit from a formal evaluation, and perhaps a transfer to a less artificial problem. summary: this paper presents a generative neuro-symbolic model for learning the task-general representations. the model is inspired by the bpl approach, with less manually-defined prior or rules but more learning-based ingredients. i generally appreciate the ideas and improvements it made. however, compared with bpl, the claims of task-general representation, and neural-symbolic modeling does not match the actual contributions of the work.-----pros-----the paper is well-written with its motivations, methods, and corresponding experimental results. it shows a promising direction of learning a representation and model for multiple tasks, \ie, the omniglot challenge.-----the formulation of the proposed gns and the corresponding method of inference provides a well-established and holistic solution for the visual simple concepts learning and inference.-----cons-----the claimants of human-level visual understanding, and task-general representations are similar to the bpl methods, which is well-known enough. i think the author should put more effort into comparing the bpl and demonstrates the actual improvement compared with it.-----the current framework is still under the bayesian statistical modeling, i don't think it provides a task-general representation since it cannot be transferred to more complex domains and perceptual tasks. moreover, the symbolic representations and symbolic module in the proposed method are quite trivial. the proposed method does not provide deeper insights into the neural symbolic representations.-----the causal structures in the model are captures in an autoregressive way, which is similar to lots of existing modules. i don't see any actual causal problems are solved in the framework.-----my main concern is that it put too much space in telling the stories that bpl has already proposed and demonstrated, but not the detailed comparisons with bpl and how the technical contributions and novelty of this specific model can benefit the overall human-level understanding of visual concepts. i would like to raise my scores if the concern is properly addressed.-----i raised my scores from 4 to 6 after the author updated their draft and answered my questions. i appreciate their efforts in addressing my concerns and improve the paper. the current version is good enough to be accepted and it also compares with other approaches thoroughly. however, i still feel the symbolic module is too simple in this work and does not distinguish it from other works. motivated by few-shot learning challenges such as omniglot, the authors propose a human-like model for learning how to draw visual concepts that consists of three components: (1) a location model for picking the starting point of the next stroke given the current ""canvas"", (2) a stroke model that continues an existing stroke, (3) a termination model that decides when to stop drawing. the three components of this model are trained on example glyphs that are heuristically parsed into strokes. the authors evaluate performance of this method, called gns (generative neuro-symbolic) versus more classical program learning approaches based on pre-existing libraries of subroutines and more generic deep neural network architectures. this approach (1) represents an advance over methods based on pre-defined subroutines in the sense that primitives such as ""draw a stroke intersecting an existing stroke"" are learned rather than supplied by the programmer and (2) performs much better at few-shot learning of glyphs than generic approaches.-----quality: the idea behind the method makes sense. the authors evaluate the model on four different tasks, and compare against reasonable baselines. they also generalize beyond glyphs to learning 3d objects, although results are not shown. the authors are careful to describe details such preprocessing of the input into splines and candidate parses, although details of the actual architecture do not seem to be given. the weakest part of the approach is the image model. it seems to work well enough for performance purposes, but the image distribution involving randomly transforming parts does not seem intuitively plausible as a model of human-like image generation. as the authors note, this results in their approach underperforming against bpl. the approach would probably work better if the authors had included stroke variability as a part of the core model.-----clarity: i had to read the paper a few times to understand the approach. it would have helped if the authors could better explain the architecture of the various model components. besides this issue of vagueness on technical details, the paper is well-written.-----originality: to my knowledge the use of a separate location and lstm stroke model for glyphs is novel. the previous work section is adequate for contextualizing the paper.-----significance: the paper represents an important approach to mimicking human concept learning. (i base this on my own intuition; the authors could perhaps make a stronger case for this by including relevant references from the cognitive science literature.) chaining together multiple different neural networks to create a new type of ml algorithm has been a promising source of innovation in deep learning, e.g. alphago combining a policy and evaluation network, gans combining a generator and discriminator. this paper is yet another example via combining a location and stroke model for learning glyphs. on the other hand, the authors may have oversold the significance of the work by claiming that this model represents an example of fundamentally new approach in harmonizing artificial neural networks and symbolic program learning. i am not convinced that this combination of location+stroke model is doing hierarchical concept learning in the same way that humans do. if this is not what the authors are claiming, they should clarify this in the main paper.-----pros:-----novel and plausible model for human-like learning-----achieves reasonable performance on a variety of one-shot learning tasks-----qualitatively exhibits human-like performance at free glyph generation (in my opinion)-----cons:-----the type -> token part of the model seems underdeveloped-----paper is vague on the architectures used for each of the model components (location, stroke, termination)-----paper makes a claim that the model is doing symbolic modeling, but the term ""symbolic modeling"" is not well-defined and there is insufficient analysis showing that the model learns something analogous to existing methods such as bpl","this paper was reviewed by four experts in the field. based on the reviewers' feedback, the decision is to recommend the paper for acceptance to iclr 2021. the reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. the authors are encouraged to make the necessary changes and include the missing references.","the authors evaluate performance of this method, called gns (generative neuro-symbolic) versus more classical program learning approaches based on pre-existing libraries of subroutines and more generic deep neural network architectures.","if this is not what the authors are claiming, they should clarify this in the main paper.-----pros:-----novel and plausible model for human-like learning-----achieves reasonable performance on a variety of one-shot learning tasks-----qualitatively exhibits human-like performance at free glyph generation (in my opinion)-----cons:-----the type -> token part of the model seems underdeveloped-----paper is vague on the architectures used for each of the model components (location, stroke, termination)-----paper makes a claim that the model is doing symbolic modeling, but the term ""symbolic modeling"" is not well-defined and there is insufficient analysis showing that the model learns something analogous to existing methods such as bpl","if this is not what the authors are claiming, they should clarify this in the main paper.-----pros:-----novel and plausible model for human-like learning-----achieves reasonable performance on a variety of one-shot learning tasks-----qualitatively exhibits human-like performance at free glyph generation (in my opinion)-----cons:-----the type -> token part of the model seems underdeveloped-----paper is vague on the architectures used for each of the model components (location, stroke, termination)-----paper makes a claim that the model is doing symbolic modeling, but the term ""symbolic modeling"" is not well-defined and there is insufficient analysis showing that the model learns something analogous to existing methods such as bpl","this paper introduces generative neuro-symbolic modelling, advertised as a probabilistic programming framework in which the distributions are modelled by neural networks.","if this is not what the authors are claiming, they should clarify this in the main paper.-----pros:-----novel and plausible model for human-like learning-----achieves reasonable performance on a variety of one-shot learning tasks-----qualitatively exhibits human-like performance at free glyph generation (in my opinion)-----cons:-----the type -> token part of the model seems underdeveloped-----paper is vague on the architectures used for each of the model components (location, stroke, termination)-----paper makes a claim that the model is doing symbolic modeling, but the term ""symbolic modeling"" is not well-defined and there is insufficient analysis showing that the model learns something analogous to existing methods such as bpl","if this is not what the authors are claiming, they should clarify this in the main paper.-----pros:-----novel and plausible model for human-like learning-----achieves reasonable performance on a variety of one-shot learning tasks-----qualitatively exhibits human-like performance at free glyph generation (in my opinion)-----cons:-----the type -> token part of the model seems underdeveloped-----paper is vague on the architectures used for each of the model components (location, stroke, termination)-----paper makes a claim that the model is doing symbolic modeling, but the term ""symbolic modeling"" is not well-defined and there is insufficient analysis showing that the model learns something analogous to existing methods such as bpl","i don't see any actual causal problems are solved in the framework.-----my main concern is that it put too much space in telling the stories that bpl has already proposed and demonstrated, but not the detailed comparisons with bpl and how the technical contributions and novelty of this specific model can benefit the overall human-level understanding of visual concepts.","if this is not what the authors are claiming, they should clarify this in the main paper.-----pros:-----novel and plausible model for human-like learning-----achieves reasonable performance on a variety of one-shot learning tasks-----qualitatively exhibits human-like performance at free glyph generation (in my opinion)-----cons:-----the type -> token part of the model seems underdeveloped-----paper is vague on the architectures used for each of the model components (location, stroke, termination)-----paper makes a claim that the model is doing symbolic modeling, but the term ""symbolic modeling"" is not well-defined and there is insufficient analysis showing that the model learns something analogous to existing methods such as bpl",0.1505376344086021,0.0439560439560439,0.1075268817204301,0.1075268817204301,0.2988505747126436,0.0581395348837209,0.1839080459770115,0.1839080459770115,0.2988505747126436,0.0581395348837209,0.1839080459770115,0.1839080459770115,0.144578313253012,0.0246913580246913,0.1204819277108433,0.1204819277108433,0.2988505747126436,0.0581395348837209,0.1839080459770115,0.1839080459770115,0.2988505747126436,0.0581395348837209,0.1839080459770115,0.1839080459770115,0.2276422764227642,0.0165289256198347,0.1626016260162601,0.1626016260162601,0.2988505747126436,0.0581395348837209,0.1839080459770115,0.1839080459770115,6.697721004486084,6.697721004486084,11.714438438415527,6.697721004486084,6.296082496643066,6.697721004486084,6.697721004486084,6.260854244232178,0.24473476023882057,0.35937796823915924,0.8447566557460597,0.27460271369263267,0.8187630627664578,0.8474813692775577,0.27460271369263267,0.8187630627664578,0.8474813692775577,0.9702082340770805,0.9714648163793277,0.9266446755797099,0.27460271369263267,0.8187630627664578,0.8474813692775577,0.27460271369263267,0.8187630627664578,0.8474813692775577,0.926903800116066,0.9452767955383644,0.8652486885756066,0.27460271369263267,0.8187630627664578,0.8474813692775577
169,https://openreview.net/forum?id=r1MSBjA9Ym,"this paper shows that the training of deep relu neural networks will converge to a constant classifier with high probability over random initialization (symmetric weight distributions) if the widths of all hidden layers are too small.----------------overall, the paper is clearly written. i like the main message of the paper and the simplicity of its analysis. to some extent, i think that the results could add to our current understanding of the limitations of deep narrow networks, both theoretically and practically. ----------------on the other hand, my main concern at the moment is that the results seem to be informative only for low dimensional data and networks of small width. in particular, the bound on depth in eq (5) scales too fast with width. figure 6 shows that with width 16 the bound on depth is already too loose that it could be of any use in practice.----------------other comments and questions:--------in figure 6+7, it's not clear how many times each experiment is repeated in order to get the numerical estimations of probabilities, and which exactly weight distributions are used here?----------------the statement of theorem 1 and its proof looks a bit suspicious to me. this theorem first makes an assumption on a given network with fixed weights, but then makes some statement about random weights...this apparently does not make much sense to me because a given network has nothing to do with random weights, but the current proof is actually using the assumption made on the given network as a constant classifier to prove the probabilistic statement. i hope to see some clarification here.----------------it would be interesting to discuss the results of this paper with recent work [1,2] which also studied deep narrow networks but from other perspectives:--------[1] neural networks should be wide enough to learn connected decision regions. icml 2018--------[2] the expressive power of neural networks: a view from the width. nips 2017 the paper studies failure modes of deep and narrow networks. i find this research extremely valuable and interesting. in addition to that, the paper focuses on as small as possible models, for which the undesired behavior occurs. that is another great positive, too much of a research in dl focuses on the most complex and general cases in my opinion. i would be more than happy to give this paper a very strong recommendation, if not for numerous flaws in presentation. if those get improved, i am very eager to increase my rating. here are the things that i think need an improvement:--------1. the formulation of theorems.--------the paper strives for mathematical style. yet the formulations of the theorems are very colloquial. expression ""by assuming random weights"" is not what one wants to see in a rigorous math paper. the formulations of the theorems need to be made rigorous and easy to understand, the assumptions need to be clearly stated and all concepts used strictly defined.--------2. too many theorems--------9 (!) theorems is way too much. theorem is a significant contribution. i strongly suggest having 1-2 strong theorems, and downgrading more technical lemmas to a lemma and proposition status.--------in addition - the problem studied is really a study of bad local minimas for neural networks. more mentions of the previous work related to the topic would improve the scientific quality additionally, in my opinion.","the paper studies difficulties in training deep and narrow networks. it shows that there is high probability that deep and narrow relu networks will converge to an erroneous state, depending on the type of training that is employed. the results add to our current understanding of the limitations of these architectures. the main criticism is that the analysis might be very limited, being restricted to very narrow networks (of width about 10 or less) which are not very common in practice, and that the observed collapse phenomenon can be easily addressed by non symmetric initialization. there were some issues with the proofs that were covered in the discussed between authors and reviewers. the revision is relatively extensive. this is a borderline case. the paper receives one good rating, one negative rating, and a borderline accept rating. although the paper contributes interesting insights to a relevant problem that clearly needs contributions in this direction, the analysis presented in the paper and its applicability in practice seems to be very restrictive at this point.","----------------on the other hand, my main concern at the moment is that the results seem to be informative only for low dimensional data and networks of small width.","this theorem first makes an assumption on a given network with fixed weights, but then makes some statement about random weights...this apparently does not make much sense to me because a given network has nothing to do with random weights, but the current proof is actually using the assumption made on the given network as a constant classifier to prove the probabilistic statement.","this paper shows that the training of deep relu neural networks will converge to a constant classifier with high probability over random initialization (symmetric weight distributions) if the widths of all hidden layers are too small.----------------overall, the paper is clearly written.","this paper shows that the training of deep relu neural networks will converge to a constant classifier with high probability over random initialization (symmetric weight distributions) if the widths of all hidden layers are too small.----------------overall, the paper is clearly written.","this theorem first makes an assumption on a given network with fixed weights, but then makes some statement about random weights...this apparently does not make much sense to me because a given network has nothing to do with random weights, but the current proof is actually using the assumption made on the given network as a constant classifier to prove the probabilistic statement.","this theorem first makes an assumption on a given network with fixed weights, but then makes some statement about random weights...this apparently does not make much sense to me because a given network has nothing to do with random weights, but the current proof is actually using the assumption made on the given network as a constant classifier to prove the probabilistic statement.","----------------on the other hand, my main concern at the moment is that the results seem to be informative only for low dimensional data and networks of small width.","this theorem first makes an assumption on a given network with fixed weights, but then makes some statement about random weights...this apparently does not make much sense to me because a given network has nothing to do with random weights, but the current proof is actually using the assumption made on the given network as a constant classifier to prove the probabilistic statement.",0.16,0.0707070707070707,0.1,0.1,0.2033898305084745,0.0085470085470085,0.1101694915254237,0.1101694915254237,0.2523364485981308,0.0754716981132075,0.1401869158878504,0.1401869158878504,0.2523364485981308,0.0754716981132075,0.1401869158878504,0.1401869158878504,0.2033898305084745,0.0085470085470085,0.1101694915254237,0.1101694915254237,0.2033898305084745,0.0085470085470085,0.1101694915254237,0.1101694915254237,0.16,0.0707070707070707,0.1,0.1,0.2033898305084745,0.0085470085470085,0.1101694915254237,0.1101694915254237,6.648009300231934,6.648009300231934,15.676671028137209,6.648009300231934,10.62625789642334,15.676671028137209,6.648009300231934,10.626256942749023,0.9798112000412883,0.9718386091159621,0.9413622595595684,0.9602322077240124,0.9642018378894504,0.9568223189280249,0.9881709151007693,0.9859629537448181,0.9262066025999579,0.9881709151007693,0.9859629537448181,0.9262066025999579,0.9602322077240124,0.9642018378894504,0.9568223189280249,0.9602322077240124,0.9642018378894504,0.9568223189280249,0.9798112000412883,0.9718386091159621,0.9413622595595684,0.9602322077240124,0.9642018378894504,0.9568223189280249
170,https://openreview.net/forum?id=r1V0m3C5YQ,"the paper is well written and presented, giving a good literature review and clearly explaining the design decisions and trade-offs. the paper proposes a novel factorisation approach and uses recurrent networks. ----------------the evaluation is both quantitative and qualitative. the qualitative experiment is interesting, but there is no information given about the level of musical training the participants had. you would expect very different results from music students compared to the general public. how did you control for musical ability/ understanding?----------------the paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.----------------overall, while i am not an expert in musical composition and machine learning, the paper is clear, and appears to be advancing the art in a reliable fashion. composing polyphonic music is a hard computational problem. --------this paper views the problem as modelling a probability distribution --------over musical scores that is parametrized using convolutional and recurrent --------networks. emphasis is given to careful evaluation, both quantitatively and qualitatively. the technical parts are quite poorly written.----------------the introduction is quite well written and it is easy to follow. it provides a good review that is nicely balanced between older and recent literature. ----------------unfortunately, at the technical parts, the paper starts to suffer due to sloppy notation. the cross entropy definition is missing important details. what does s exactly denote? are you referring to a binary piano roll or some abstract vector valued process? this leaves a lot of guess work to the reader. --------even the footnote makes it evident that the authors may have a different mental picture -- i would argue that a piano roll does not need two bits. take a binary matrix: roll(note=n, time=t) = 1 (=0) when note n is present (absent) at time t. ----------------i also think the term factorization is sometimes used freely as a synonym for representation in last paragraphs of 4 and first two paragraphs of 5 -- i find this misleading without proper definitions.----------------the models, which are central to the message of the paper, are not described clearly. please--------define function a(\cdot) in (2), (3), (4), : this maybe possibly a typesetting issue (and a is highly likely a sigmoid) but what does x_p w_hp x x_pt etc stand for? various contractions? you have only defined the tensor as x_tpn. even there, the proposed encoding is difficult to follow -- using different names for different ranges of the same index (n and d) seems to be avoiding important details and calling for trouble. why not just introduce an order 4 tensor and represent everything in the product space as every note must have a duration? ----------------while the paper includes some interesting ideas about representation of relative pitch, the poor technical writing makes it not suitable to iclr and hard to judge/interpret the extensive simulation results.----------------minor:----------------for tensors, 'rank-3' is not correct use, please use order-3 here if you are referring to the number of dimensions of the multiway array. ----------------what is a non-linear sampling scheme? please be more precise.----------------the allan-williams citation and year is broken:--------moray allan and christopher k. i. williams. harmonising chorales by probabilistic inference. advances in neural information processing systems 17, 2005.","this paper proposes novel recurrent models for polyphonic music composition and demonstrates the approach with qualitative and quantitative evaluations as well as samples. the technical parts in the original write-up were not very clear, as noted by multiple reviewers. during the review period, the presentation was improved. unfortunately the reviewer scores are mixed, and are on the lower side, mainly because of the lack of clarity and quality of the results.","----------------while the paper includes some interesting ideas about representation of relative pitch, the poor technical writing makes it not suitable to iclr and hard to judge/interpret the extensive simulation results.----------------minor:----------------for tensors, 'rank-3' is not correct use, please use order-3 here if you are referring to the number of dimensions of the multiway array.","how did you control for musical ability/ understanding?----------------the paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.----------------overall, while i am not an expert in musical composition and machine learning, the paper is clear, and appears to be advancing the art in a reliable fashion.",are you referring to a binary piano roll or some abstract vector valued process?,"the paper is well written and presented, giving a good literature review and clearly explaining the design decisions and trade-offs.","the paper is well written and presented, giving a good literature review and clearly explaining the design decisions and trade-offs.","how did you control for musical ability/ understanding?----------------the paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.----------------overall, while i am not an expert in musical composition and machine learning, the paper is clear, and appears to be advancing the art in a reliable fashion.",harmonising chorales by probabilistic inference.,"how did you control for musical ability/ understanding?----------------the paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.----------------overall, while i am not an expert in musical composition and machine learning, the paper is clear, and appears to be advancing the art in a reliable fashion.",0.2461538461538461,0.015625,0.1692307692307692,0.1692307692307692,0.2677165354330708,0.064,0.1732283464566929,0.1732283464566929,0.0232558139534883,0.0,0.0232558139534883,0.0232558139534883,0.1935483870967741,0.0,0.1505376344086021,0.1505376344086021,0.1935483870967741,0.0,0.1505376344086021,0.1505376344086021,0.2677165354330708,0.064,0.1732283464566929,0.1732283464566929,0.0259740259740259,0.0,0.0259740259740259,0.0259740259740259,0.2677165354330708,0.064,0.1732283464566929,0.1732283464566929,16.594575881958008,8.159708023071289,8.159708023071289,16.594575881958008,8.527069091796875,7.684939861297607,16.594575881958008,9.46999740600586,0.9845308035705271,0.981160915622734,0.9113645399925654,0.974142754513194,0.9772618703933497,0.9193388386525909,0.9433250514567035,0.9481939220741056,0.8990561327033042,0.967955204857946,0.9686183383163189,0.959410517368453,0.967955204857946,0.9686183383163189,0.9594104709051103,0.974142754513194,0.9772618703933497,0.9193388386525909,0.9581078570923578,0.9684056347452448,0.11572565557432242,0.974142754513194,0.9772618703933497,0.9193388386525909
171,https://openreview.net/forum?id=r1eU1gHFvH,"this paper studies the emergence of local codes in neural networks on a synthetic dataset. from my understanding, a neuron is counted as a local code if there is a class a such that its activations of data points from a are linear separable from its activations of data points from all other classes. however, is this definition for data points in the training set, or in the test dataset, or union of them? i did not find the exact definition in the paper.----------it designed experiments to study the number of local codes. it made 7 empirical findings by the experiments on a synthetic dataset, listed in section 1.1. it's findings are purely empirical. the authors may clarify this work's novelty and importance.----------this paper seems to be finished in rush, because there is question masks, e.g., ""summary statistics and kolmogorov-smirnov hypothesis tests are reported in tables ?? in the appendix"" in page 8, ""results are shown in figure ?? and table 2."" in page 12, ""as can be in seen tables ?? and figure ?? low values of dropout are likely the same distribution"" in page 12. the paper is very difficult to read for me, partly due to its writing in a language (local codes) that i'm not familiar with. i think that its presentation can be greatly improved for general audience. ----------i'm not familiar with the concept of ""local codes"", and i do not understand part of the paper. i have a lot of questions about the data used in the experiments. they are created according to the method explained in data design (p.2). it is also summarized in the last paragraph of the first section as follows: there are 1/10 input bits that are always 1 for each class and these are the invariant bits, the 0s of each prototype are then filled in with a random mix of 1 and 0 of a known weight. what is the intention behind this way of creating data? how general are the data created in this way as well as the analyses based on them? it seems to me that the data and thus the analyses lack the generality needed for the purpose of understanding behaviors of neural networks on real tasks/data. ----------the same is true for neural network design (p.3), in which 13 experiments conducted in this study are explained. i think their explanations are too condensed; each explanation is very short and it is hard to understand the motivation and purpose of each experiment, i.e., what is the hypothesis to be verified and in what way it is verified? ----------in experiment-12, mnist is used as data unlike other experiments, and they are modified as with added 20 pixel invariants. what is the purpose of this modification? there is a statement in a footnote of p.5 no lcs were seen in the standard mnist runs, which agrees with the above concern about the lack of generality.----------additionally, i do not understand the statement in p.5 increasing the difficulty of the problem (by increasing n_x, . why does the use of more training data make the problem harder? it should usually be the opposite; the smaller, the harder.",this paper studies when hidden units provide local codes by analyzing the hidden units of trained fully connected classification networks under various architectures and regularizers. the reviewers and the ac believe that the paper in its current form is not ready for acceptance to iclr-2020. further work and experiments are needed in order to identify an explanation for the emergence of local codes. this would significantly strengthen the paper.,"the authors may clarify this work's novelty and importance.----------this paper seems to be finished in rush, because there is question masks, e.g., ""summary statistics and kolmogorov-smirnov hypothesis tests are reported in tables ?","from my understanding, a neuron is counted as a local code if there is a class a such that its activations of data points from a are linear separable from its activations of data points from all other classes.",this paper studies the emergence of local codes in neural networks on a synthetic dataset.,this paper studies the emergence of local codes in neural networks on a synthetic dataset.,this paper studies the emergence of local codes in neural networks on a synthetic dataset.,"from my understanding, a neuron is counted as a local code if there is a class a such that its activations of data points from a are linear separable from its activations of data points from all other classes.","there is a statement in a footnote of p.5 no lcs were seen in the standard mnist runs, which agrees with the above concern about the lack of generality.----------additionally, i do not understand the statement in p.5 increasing the difficulty of the problem (by increasing n_x, .","from my understanding, a neuron is counted as a local code if there is a class a such that its activations of data points from a are linear separable from its activations of data points from all other classes.",0.2264150943396226,0.0192307692307692,0.1509433962264151,0.1509433962264151,0.146788990825688,0.0186915887850467,0.110091743119266,0.110091743119266,0.2352941176470588,0.144578313253012,0.188235294117647,0.188235294117647,0.2352941176470588,0.144578313253012,0.188235294117647,0.188235294117647,0.2352941176470588,0.144578313253012,0.188235294117647,0.188235294117647,0.146788990825688,0.0186915887850467,0.110091743119266,0.110091743119266,0.2166666666666666,0.0,0.15,0.15,0.146788990825688,0.0186915887850467,0.110091743119266,0.110091743119266,13.12686824798584,13.085132598876951,13.085134506225586,13.126871109008787,4.264976501464844,13.085134506225586,13.12686824798584,3.7142910957336426,0.9506963293331787,0.957829513228994,0.46519766199214907,0.916897259258235,0.9154176471375812,0.9025925201530365,0.9412611311177049,0.9444093039268793,0.9320951662414643,0.9412611311177049,0.9444093039268793,0.9320950996911658,0.9412611311177049,0.9444093039268793,0.9320951552492043,0.916897259258235,0.9154176471375812,0.9025925201530365,0.9353246837196953,0.9350249780364867,0.2648828222382859,0.916897259258235,0.9154176471375812,0.9025925201530365
172,https://openreview.net/forum?id=r1egIyBFPS,"this paper presents a method for symbolic superoptimization  the task of simplifying equations into equivalent expressions. the main goal is to design a method that does not rely on human input in defining equivalence classes, which should improve scalability of the simplification method to a larger set of expressions. the solution uses a reinforcement learning method for training a neural model that transforms an equation tree into a simpler but equivalent one. the model consists of (i) a tree encoder, a recursive lstm that operates over the input equation tree, (ii) a sub-tree selector, a probability distribution over the nodes in the input equation tree, and (iii) a tree decoder, a two layer lstm that includes a tree layer and a symbol generation layer. the rl reward uses an existing method for determining soft equivalence between the output tree and the input tree along with a positive score for compressing. ----------the main strengths of the paper are that (i) it targets the scalability problem in simplifying arithmetic expressions by reducing the amount of human effort involved in the process, (ii) it provides some evaluation comparing against methods that use pre-defined rules for transforming equations, (iii) it provides a baseline method against which future scalable models can be compared against.----------the following are the main concerns i have with the paper----------1) the model description should include more details about the design choices. -----for instance, the specific reward function, the central component of the model, is left rather under-discussed. the function form is sensible but why does the negative case use -0.1 as the reward scaling constant? why not some other number? how is this tuned? ----------2) as far as i can see there is no real ablation analysis that shows which of the components are actually useful. is the sub-tree selector necessary? is the curriculum training necessary? how much does the sub-tree embedding similarity loss contribute to the results? even if each of these actually add value it will be useful to know how much. what about other design choices? if we trained a direct seq2seq model with linearized expressions instead of the tree structured inputs, would it work just as well? these are empirical questions that need to be answered to justify that the proposed model indeed is useful. ----------3) the experimental details are sparse. in particular, there is no mention of how hyper-parameters of the proposed method are tuned. are the performance numbers averages over a set of random seeds or is it simply the best performing number that has been reported? this is especially troublesome for a rl based model. ----------overall the paper presents a particular model and strategy for training but lacks appropriate experimentation to establish their utility. the authors present a framework for symbolic superoptimization using methods from deep learning. a deep learning approach operating on the expression tree structures is proposed based on a combination of subtree embeddings, lstm rnn structures, and an attention mechanism. ----------the approach avoids the exploitation of human-generated equivalence pairs thus avoiding human interaction and corresponding bias. instead, the approach is trained using random generated data. it remains somewhat unclear how the corresponding random data generation influences general applicability w.r.t. other tasks, as the authors apply constraints on the generation process for complexity reasons. a corresponding discussion would be valuable here.----------in secs. 3 & 4, the authors present their specific modeling and learning approach. however, they do not report on modeling or learning alternatives. it would be interesting for the audience to understand, how the authors reached these specific choices, and how (some of) these choice influence performance and learning stability. for example, in sec. 4.1, an additional loss term is introduced to further support the learning of embeddings. however, it might interesting to see comparative results quantitatively investigating the effect of this additional loss term. also, as far as i can see, no information on the choice of hyperparameters (e.g. lstm dimensions) are provided or analyzed w.r.t. their effect on the performance of the proposed approach. this paper provides a novel approach to the problem of simplifying symbolic expressions without relying on human input and information. to achieve this, they apply a reinforce framework with a reward function involving the number of symbols in the final output together with a probabilistic testing scheme to determine equivalence. the model itself consists of a tree-lstm-based encoder-decoder module with attention, together with a sub tree selector. their main contribution is that this framework works entirely independent of human-labelled data. in their experiments, they show that this deep learning approach outperforms their provided human-independent baselines, while sharing similar performance with human-dependent ones.-----overall, the work in this paper has the potential to be a contribution to iclr but lacks completeness and clarity. a number of experimental details are missing, making it difficult to understand the setup under which results were obtained. moreover, the paper does not seem to have been revised with many grammatical issues that make it hard to read.-----the following are major issues within the paper and should be addressed:----- the paper does not mention the amount of compute given to their model, nor the amount of time taken to train. as the reinforce framework is generally quite computation-heavy, these are significant details. without assessing the amount of compute and time allotted for training hiss, the comparisons to previous baselines lose a fair amount of meaning. the paper alludes to processes being extremely time consuming, but then does not provide any numbers.----- they do not mention the data used to train the model weights. in the comparisons sections, some details on datasets are given, but these seem to refer to data for inference.----- there are many grammatical errors that likely could have been detected with revision. a handful of such errors would not affect the score, but they are so numerous as to make the paper much more difficult to understand.-----additionally, these are comments that slightly detract from the quality of the paper:----- its unclear what to glean from section 5.1, as the dataset and baselines seem to be fairly trivial. if their claim is to have the first nontrivial human-independent approach to simplifying symbolic expressions, there is no need to compare to baselines that can only handle small expressions.----- sections 5.3 and 5.4 contribute little to the paper. for 5.3, the model was trained to embed equivalent expressions close together, using l2 regularization. it is therefore unsurprising that equivalent expressions are then closer together than non-equivalent ones. the paper also does not provide a comparison to the method without this regularization, and so its unclear if this embedding similarity helps in any way. for 5.4, the section is extremely short and contains very little content. moreover, just as many of the variables in their provided examples oppose their conjectures as support them.----- the most interesting figure provided is the rewrite rules discovered by the model. it would be even better if an additional column containing the rules discovered by halide (the main baseline) were provided.-----overall, in my understanding, the primary point in favor of the paper is in being the first nontrivial human-independent approach to simplifying symbolic expression. that said, this is not my area of expertise, so i cannot judge novelty or importance as well as other reviewers.","this work introduces a neural architecture and corresponding method for simplifying symbolic equations, which can be trained without requiring human input. this is an area somewhat outside most of our expertise, but the general consensus is that the paper is interesting and is an advance. the reviewer's concerns have been mostly resolved by the rebuttal, so i am recommending an accept.","in their experiments, they show that this deep learning approach outperforms their provided human-independent baselines, while sharing similar performance with human-dependent ones.-----overall, the work in this paper has the potential to be a contribution to iclr but lacks completeness and clarity.","----------the main strengths of the paper are that (i) it targets the scalability problem in simplifying arithmetic expressions by reducing the amount of human effort involved in the process, (ii) it provides some evaluation comparing against methods that use pre-defined rules for transforming equations, (iii) it provides a baseline method against which future scalable models can be compared against.----------the following are the main concerns i have with the paper----------1) the model description should include more details about the design choices.",this paper presents a method for symbolic superoptimization  the task of simplifying equations into equivalent expressions.,this paper presents a method for symbolic superoptimization  the task of simplifying equations into equivalent expressions.,"----------the main strengths of the paper are that (i) it targets the scalability problem in simplifying arithmetic expressions by reducing the amount of human effort involved in the process, (ii) it provides some evaluation comparing against methods that use pre-defined rules for transforming equations, (iii) it provides a baseline method against which future scalable models can be compared against.----------the following are the main concerns i have with the paper----------1) the model description should include more details about the design choices.","----------the main strengths of the paper are that (i) it targets the scalability problem in simplifying arithmetic expressions by reducing the amount of human effort involved in the process, (ii) it provides some evaluation comparing against methods that use pre-defined rules for transforming equations, (iii) it provides a baseline method against which future scalable models can be compared against.----------the following are the main concerns i have with the paper----------1) the model description should include more details about the design choices.","as the reinforce framework is generally quite computation-heavy, these are significant details.","----------the main strengths of the paper are that (i) it targets the scalability problem in simplifying arithmetic expressions by reducing the amount of human effort involved in the process, (ii) it provides some evaluation comparing against methods that use pre-defined rules for transforming equations, (iii) it provides a baseline method against which future scalable models can be compared against.----------the following are the main concerns i have with the paper----------1) the model description should include more details about the design choices.",0.2264150943396226,0.0,0.0943396226415094,0.0943396226415094,0.2758620689655172,0.0279720279720279,0.1517241379310344,0.1517241379310344,0.2564102564102564,0.0263157894736842,0.1538461538461538,0.1538461538461538,0.2564102564102564,0.0263157894736842,0.1538461538461538,0.1538461538461538,0.2758620689655172,0.0279720279720279,0.1517241379310344,0.1517241379310344,0.2758620689655172,0.0279720279720279,0.1517241379310344,0.1517241379310344,0.0799999999999999,0.0,0.0533333333333333,0.0533333333333333,0.2758620689655172,0.0279720279720279,0.1517241379310344,0.1517241379310344,9.234441757202148,9.234441757202148,16.087398529052734,9.234441757202148,6.488173007965088,16.0873966217041,9.234441757202148,8.501091957092285,0.18743264431045895,0.3105298832400546,0.859942206005112,0.9864216373812025,0.980327583421337,0.9407201169593711,0.9628979396890329,0.963180294450877,0.06671196884574648,0.9628979396890329,0.963180294450877,0.06671164995724087,0.9864216373812025,0.980327583421337,0.9407200608299159,0.9864216373812025,0.980327583421337,0.9407201169593711,0.12814341118430644,0.23239996337746555,0.8646775044861206,0.9864216373812025,0.980327583421337,0.9407200608299159
173,https://openreview.net/forum?id=r1fO8oC9Y7,"this is a wonderful paper as it seems to have brougth semantic role labelling (srl) in the context of dl and in the context of voice search. --------results are interesting but the paper has some major limitations. in fact, the paper totally disregard the work on semantic role labelling and on languages for expressing the general meaning of language in terms of relations and in terms of concepts. ----------------the first limitation is on the key idea. the key idea of the paper seems to be the existence of an intermediate representation language to encode meaning for utterances. yet, this intermediate language seems to be the final language with the same relation types (for example, searchaction) and without representations for the involved concepts (type that becomes alternatively film or weather according to the target final language). this seems to be srl where the first step is to recognize the relation and, then, the second step is to recognize the roles even if roles are slot filler types in the case of this papers.----------------the second limitation is on how the intermediate language has been choosen. what is the relation with framenet or verbnet? why the authors have not choosen something similar? what are the limitations of these two resources that have forced the authors to disregard them?----------------minor problems--------====--------- why there are not spaces between characters and opening brackets?--------- ""compositional graph based .... language"" is a really large noun compound this paper describes a two-stage encoder-decoder model for semantic parsing. the model first decodes a cross-domain schema (cds) representation from the input utterance, then decodes the final logial form from both the utterance and cds. the model outperforms other multitask seq2seq models on the snips (goo et al., 2018) dataset, but is still behind the traditional slot-filling models (goo et al., 2018).----------------my main concern is that it is unclear to me how cds (cross-domain schema) can be generalized to the other semantic parsing datasets, e.g., the overnight dataset (wang et al., 2015), which also contains multiple domains. ----------------i think it would be nice to have some details about the cds in the paper. for example, im wondering 1) how is this cds designed? 2) how are the cds annotations derived from the target output? ----------------there are other details missing regarding the comparisons and the evaluation metrics. in 4.2, the authors mentioned we use accuracy as the evaluation metric, does accuracy mean full logical form accuracy or accuracy on execution results?----------------* more minor comments:--------in the first paragraph of section 3, irrelevant to domain -> domain-general or domain-agnostic?----------------it will be nice to write something more specific than explore more ways to make it work better in the future work.----------------this paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).----------------* missing references:--------neural semantic parsing over multiple knowledge-bases, herzig and berant, acl 2017 <- this paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.----------------(concurrent) decoupling structure and lexicon for zero-shot semantic parsing, herzig and berant, emnlp 2018","interesting approach aiming to leverage cross domain schemas and generic semantic parsing (based on meaning representation language, mrl) for language understanding. experiments have been performed on the recently released snips corpus and comparisons have been made with multiple recent multi-task learning approaches. unfortunately, the proposed approach falls short in comparison to the slot gated attention work by goo et al. the motivation and description of the cross domain schemas can be improved in the paper, and for replication of experiments it would be useful to include how the annotations are extended for this purpose. experimental results could be extended to the other available corpora mentioned in the paper (atis and geo).","what are the limitations of these two resources that have forced the authors to disregard them?----------------minor problems--------====--------- why there are not spaces between characters and opening brackets?--------- ""compositional graph based .... language"" is a really large noun compound this paper describes a two-stage encoder-decoder model for semantic parsing.","in 4.2, the authors mentioned we use accuracy as the evaluation metric, does accuracy mean full logical form accuracy or accuracy on execution results?----------------* more minor comments:--------in the first paragraph of section 3, irrelevant to domain -> domain-general or domain-agnostic?----------------it will be nice to write something more specific than explore more ways to make it work better in the future work.----------------this paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).----------------* missing references:--------neural semantic parsing over multiple knowledge-bases, herzig and berant, acl 2017 <- this paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.----------------(concurrent) decoupling structure and lexicon for zero-shot semantic parsing, herzig and berant, emnlp 2018","this seems to be srl where the first step is to recognize the relation and, then, the second step is to recognize the roles even if roles are slot filler types in the case of this papers.----------------the second limitation is on how the intermediate language has been choosen.",this is a wonderful paper as it seems to have brougth semantic role labelling (srl) in the context of dl and in the context of voice search.,"in fact, the paper totally disregard the work on semantic role labelling and on languages for expressing the general meaning of language in terms of relations and in terms of concepts.","in 4.2, the authors mentioned we use accuracy as the evaluation metric, does accuracy mean full logical form accuracy or accuracy on execution results?----------------* more minor comments:--------in the first paragraph of section 3, irrelevant to domain -> domain-general or domain-agnostic?----------------it will be nice to write something more specific than explore more ways to make it work better in the future work.----------------this paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).----------------* missing references:--------neural semantic parsing over multiple knowledge-bases, herzig and berant, acl 2017 <- this paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.----------------(concurrent) decoupling structure and lexicon for zero-shot semantic parsing, herzig and berant, emnlp 2018",----------------i think it would be nice to have some details about the cds in the paper.,"in 4.2, the authors mentioned we use accuracy as the evaluation metric, does accuracy mean full logical form accuracy or accuracy on execution results?----------------* more minor comments:--------in the first paragraph of section 3, irrelevant to domain -> domain-general or domain-agnostic?----------------it will be nice to write something more specific than explore more ways to make it work better in the future work.----------------this paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).----------------* missing references:--------neural semantic parsing over multiple knowledge-bases, herzig and berant, acl 2017 <- this paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.----------------(concurrent) decoupling structure and lexicon for zero-shot semantic parsing, herzig and berant, emnlp 2018",0.1728395061728395,0.0124999999999999,0.0864197530864197,0.0864197530864197,0.3050847457627119,0.0256410256410256,0.1186440677966101,0.1186440677966101,0.2732919254658384,0.0251572327044025,0.1614906832298136,0.1614906832298136,0.1870503597122302,0.0291970802919708,0.1151079136690647,0.1151079136690647,0.2797202797202797,0.0141843971631205,0.1258741258741259,0.1258741258741259,0.3050847457627119,0.0256410256410256,0.1186440677966101,0.1186440677966101,0.140625,0.0634920634920634,0.125,0.125,0.3050847457627119,0.0256410256410256,0.1186440677966101,0.1186440677966101,7.117186069488525,12.006818771362305,15.25655746459961,7.117186069488525,9.928203582763672,10.989755630493164,7.117186069488525,5.695000648498535,0.9679838860624344,0.9641533289544123,0.09163402348223088,0.7518485738079079,0.9745488104847995,0.8148168101969591,0.9730670518409151,0.9692466639851804,0.6292161473538413,0.9669041374478309,0.9658805790595012,0.8714888580689097,0.9569617464365547,0.9541446988522714,0.7837693660705474,0.7518485738079079,0.9745488104847995,0.8148166836967914,0.9696554124467865,0.9642958311444694,0.9441220245326892,0.7518485738079079,0.9745488104847995,0.8148168101969591
174,https://openreview.net/forum?id=r1g6ogrtDr,"this paper describes an approach to applying attention in equivariant image classification cnns so that the same transformation (rotation+mirroring) is selected for each kernel. for example, if the image is of an upright face, the upright eyes will be selected along with the upright nose, as opposed to allowing the rotation of each to be independent. applying this approach to several different models on rotated mnist and cifar-10 lead to smaller test errors in all cases.----------overall, this is a good idea that appears to be well implemented and well evaluated. it includes an extensive and detailed bibliography of relevant work. the approach seems to be widely applicable. it could be applied to any deep learning-based image classification system. it can be applied to additional transformations beyond rotation and mirroring.----------the one shortcoming of the paper is that it takes a simple idea and makes it somewhat difficult to follow through cumbersome notation and over-mathmaticization. the ideas presented would be much clearer as an algorithm or more code-like representation as opposed to as equations. even verbal descriptions could suffice. the paper is also relatively long, going onto the 10th page. in order to save space, some of the mathematical exposition can be condensed.----------in addition, as another issue with clarity, the algorithm has one main additional hyperparameter, r_max, but the description of the experiments does not appear to mention the value of this hyperparameter. it also states that the rotated mnist dataset is rotated on the entire circle, but not how many fractions of the circle are allowed, which is equivalent to r_max. [update after rebuttal period]----------while i still find the paper somewhat hard to parse, the revision and responses have addressed most of my concerns. i think this paper should be accepted, because it presents a novel and non-trivial concept (rotation-equivariant self attention).---------------[original review]----------the authors propose a self-attention mechanism for rotation-equivariant neural nets. they show that introduction of this attention mechanisms improves classification performance over regular rotation-equivariant nets on a fully rotational dataset (rotated mnist) and a regular non-rotational dataset (cifar-10).----------strengths:-----+ states a clear hypothesis that is well motivated by figs. 1 & 2-----+ appears to accomplish what it claims as contributions-----+ demonstrates a rotation-equivariant attention mechanism-----+ shows that its introduction improves performance on some tasks----------weaknesses:------ unclear how the proposed attention mechanism accomplishes the goal outlined in fig. 2d------ performance of the authors' evaluations of the baselines is lower than reported in the original papers, casting some doubt on the performance evaluation------ the notation is somewhat confusing and cumbersome, making it hard to understand what exactly the authors are doing------ no visualisation or insights into the attention mechanism are provided----------there are three main issues detailed below that i'd like to see addressed in the authors' response and/or a revised version of the paper. if the authors can address these concerns, i am willing to increase my score.----------1. the motivation for the attention mechanism (as discussed in the introduction and illustrated in fig. 2) seems to be to find patterns of features which commonly get activated together (or often co-occur in the training set). however, according to eq. (9), attention is applied separately to orientations of the same feature ($a_i$ is indexed by i, the channel dimension), and not across different features. since the attention is applied at each spatial location separately, such mechanism only allows to detect patterns of relative orientations of the same feature appearing at the same spatial location. the motivation and utility of such formulation is unclear, as it appears to be unable to solve the toy problem laid out in fig. 2. please clarify how the proposed mechanism would solve the toy example in fig. 2.----------2. the only real argument that the proposed mechanism is useful are the numbers in table 1. however, the experimental results for cifar-10 are hard to compare to the baselines because of differences in reported and reproduced results. i would appreciate a clarification about the code used (was it published by the authors of other papers?) and discussion of why the relative improvement achieved by the proposed method is not an artefact of implementation or optimisation issues. ----------3. the exposition and notation in section 3.1 is very hard to follow and requires substantial improvement. for instance, the sections ""attention and self attention"" and ""compact local self attention"" seem to abstract from the specific case and use x and y, but it is unclear to me what x and y map to specifically. maybe also provide a visualization of how exactly attention is applied.---------------minor comments/questions:----------- if the attention is applied over the orientations of the same feature, why does it improve the performance on rotated mnist (which is rotation invariant)?----------- i assume the attention matrix $a_i$ is different for each layer, because the features in different layers are different and require different attention mechanisms. however, unlike f and k, a is not indexed by layer l.----------- it would be good to provide the standard deviation for the reported results on cifar-10 to see if the improvement is significant.","the paper proposes an attention mechanism for equivariant neural networks towards the goal of attending to co-occurring features. it instantiates the approach with rotation and reflection transformations, and reports results on rotated mnist and cifar-10. all reviewers have found the idea of using self-attention on top of equivariant feature maps technically novel and sound. there were some concerns about readability which the authors should try to address in the final version.",it can be applied to additional transformations beyond rotation and mirroring.----------the one shortcoming of the paper is that it takes a simple idea and makes it somewhat difficult to follow through cumbersome notation and over-mathmaticization.,"maybe also provide a visualization of how exactly attention is applied.---------------minor comments/questions:----------- if the attention is applied over the orientations of the same feature, why does it improve the performance on rotated mnist (which is rotation invariant)?----------- i assume the attention matrix $a_i$ is different for each layer, because the features in different layers are different and require different attention mechanisms.","maybe also provide a visualization of how exactly attention is applied.---------------minor comments/questions:----------- if the attention is applied over the orientations of the same feature, why does it improve the performance on rotated mnist (which is rotation invariant)?----------- i assume the attention matrix $a_i$ is different for each layer, because the features in different layers are different and require different attention mechanisms.",this paper describes an approach to applying attention in equivariant image classification cnns so that the same transformation (rotation+mirroring) is selected for each kernel.,they show that introduction of this attention mechanisms improves classification performance over regular rotation-equivariant nets on a fully rotational dataset (rotated mnist) and a regular non-rotational dataset (cifar-10).----------strengths:-----+ states a clear hypothesis that is well motivated by figs.,"maybe also provide a visualization of how exactly attention is applied.---------------minor comments/questions:----------- if the attention is applied over the orientations of the same feature, why does it improve the performance on rotated mnist (which is rotation invariant)?----------- i assume the attention matrix $a_i$ is different for each layer, because the features in different layers are different and require different attention mechanisms.","the only real argument that the proposed mechanism is useful are the numbers in table 1. however, the experimental results for cifar-10 are hard to compare to the baselines because of differences in reported and reproduced results.","maybe also provide a visualization of how exactly attention is applied.---------------minor comments/questions:----------- if the attention is applied over the orientations of the same feature, why does it improve the performance on rotated mnist (which is rotation invariant)?----------- i assume the attention matrix $a_i$ is different for each layer, because the features in different layers are different and require different attention mechanisms.",0.2342342342342342,0.036697247706422,0.1441441441441441,0.1441441441441441,0.3188405797101449,0.0441176470588235,0.2028985507246376,0.2028985507246376,0.3188405797101449,0.0441176470588235,0.2028985507246376,0.2028985507246376,0.2222222222222222,0.0,0.1414141414141414,0.1414141414141414,0.1896551724137931,0.0701754385964912,0.1551724137931034,0.1551724137931034,0.3188405797101449,0.0441176470588235,0.2028985507246376,0.2028985507246376,0.3214285714285714,0.0181818181818181,0.1964285714285714,0.1964285714285714,0.3188405797101449,0.0441176470588235,0.2028985507246376,0.2028985507246376,10.607624053955078,9.437740325927734,11.711305618286133,10.607624053955078,7.068275451660156,10.607624053955078,10.607624053955078,5.993968486785889,0.9636751741216526,0.971210128474958,0.5143895743829972,0.5898845881284022,0.8035442604514652,0.8596817922395693,0.5898845881284022,0.8035442604514652,0.8596819326264166,0.9722883249811417,0.9721390930531001,0.9331633029700447,0.9656229846088189,0.9687059545957417,0.6276779941662642,0.5898845881284022,0.8035442604514652,0.8596820262176412,0.829248534116759,0.8770463958581971,0.9201803508743173,0.5898845881284022,0.8035442604514652,0.8596817922395693
175,https://openreview.net/forum?id=r1gIdySFPH,"this paper introduced a very interesting idea to facilitate exploration in goal-conditioned reinforcement learning. the key idea is to learn a generative model of goal distribution to match the weighted empirical distribution, where the rare states receive larger weights. this encourages the model to generate more diverse and novel goals for goal-conditioned rl policies to reach.----------pros:-----the skew-fit exploration technique is independent of the goal-conditioned reinforcement learning algorithm and can be plugged in with any goal-conditioned methods. the experiments offer a comparison to several prior exploration techniques and demonstrate a clear advantage of the proposed skew-fit method. it is evaluated in a variety of continuous control tasks in simulation and a door opening task on a real robot. a formal analysis of the algorithm is provided under certain assumptions.----------cons:-----the weakest part of this work is the task setup. the method has only been evaluated on simplistic short-horizon control tasks. itd be interesting to see how this method is applied to longer-horizon multi-stage control tasks, where exploration is a more severe challenge. it is especially when the agent has no access to task reward and only explores the environment to maximize state coverage. it is unclear to me how many constraints are enforced in the task design in order for the robot to actually complete the full tasks through such exploration.----------i would also like to see how skew-fit works with different goal-conditioned rl algorithms, and how the performances of the rl policy in reaching the goals would affect the effectiveness of this method in exploring a larger set of states.----------section e: it seems that theres a logic jump before the conclusion goal-conditioned rl methods effectively minimize h(g|s). more elaboration on this point is necessary.----------minor:-----appendix has several broken references. the paper introduces skew-fit, an exploration approach that maximizes the entropy of a distribution of goals such that the agent maximizes state coverage. ----------the paper is well-written and provides an interesting combination of reinforcement learning with imagined goals (rig) and entropy maximization. the approach is well motivated and simulations are performed on several simulated and real robotics tasks.----------some elements were unclear to me:------ ""we also assume that the entropy of the resulting state distribution h(p(s | p)) is no less than the entropy of the goal distribution h(p(s)). without this assumption, a policy could ignore the goal and stay in a single state, no matter how diverse and realistic the goals are."" how do you ensure this in practice?------ in the second paragraph of 2.2, it is written ""note that this assumption does not require that the entropy of p(s | p) is strictly larger than the entropy of the goal distribution, p."" could you please clarify?---------------the experiments are interesting, yet some interpretations might be too strong (see below):------ in the first experiment, ""does skew-fit maximize entropy?"", it is empirically illustrated that the method does result in a high-entropy state exploration. however, it is only compared to one very naive way of exploring and it is not discussed whether other techniques also achieve the same entropy maximization. the last sentences seems to imply that only this technique ends up optimizing the entropy of the state coverage, while i believe that the claim (given the experiment) should only be about the fact it does so faster.------ on the comments of figure 6, the paper mentions that ""the other methods only rely on the randomness of the initial policy to occasionally pick up the object, resulting in a near-constant rate of object lifts."" i'm unsure about the interpretation of this sentence given figure 6 because other methods do not seem to fail entirely when given enough time.------ in the experiment ""real-world vision-based robotic manipulation"", it is written that ""a near-perfect success rate [is reached] after five and a half hours of interaction time"", while on the plot it is written 60% cumulative success after 5.5 hours and it is thus not clear where this ""5.5 hours"" comes from. summary : ----------the paper proposes an exploratory objective that can maximize state coverage in rl. they show that a formal objective for maximizing state coverage is equivalent to maximizing the entropy of a goal distribution. the core idea is to propose a method to maximize entropy of a goal distribution, or a state distribution since goals are full states. they show that the proposed method to maximize the state or goal distribution can lead to diverse exploration behaviour sufficient for solving complex image based manipulation tasks. ---------------comments and questions : ---------- - the core idea is to maximize the entropy of the state visitation frequency h(s). it is not clear from the paper whether the authors talk about the normalized discounted weighting of states (a distribution) or the stationary distribution? the entropy of the state visitation distribution only deals with valid states - but i am not sure what it means to maximize the entropy of this term exactly in terms of exploration, since it is neither the discounted weighting of states or the stationary distribution for an infinite horizon task? ----- - the authors do mention that maximizing the entropy of h(s) is not sufficient - so instead suggests for maxmizing entropy of h(s|g). but why is this even sufficient for exploration - if i do not consider new tasks at test time but only the training task? how is this a sufficient exploration objective? furthermore, since it is the conditional entropy given goal states, the fundamental idea of this is not clear from the paper. ----- - overall, i am not convinced that an objective based on h(s|g) is equivalent to an maximizing h(s), and why is this even a good objective for exploration? the meaning of h(s) to me is a bit vague from the text (due to reasons above) and therefore h(s|g) does not convince to be a good exploration objective either?----- - the paper then talks about the mi(s;g) to be maximized for exploration - what does this mi formally mean? i understand the breakdown from equation 1, but why is this a sufficient exploration objective? there are multiple ideas introduced at the same time - the mi(s;g) and talking about test time and training time exploration - but the idea itself is not convincing for a sufficient exploration objective. in light of this, i am not sure whether the core idea of the paper is convincing enough to me. ----- - i think the paper needs more theoretical insights and details to show why this form of objective based on the mi(s;g) is good enough for exploration. theoretically, there are a lot of details missing from the paper, and the paper simply proposes the idea of mi(s;g) and talks about formal or computationally tractable ways of computing this term. while the proposed solutuon to compute mi(s;g) seems reasonable, i don't think there is enough contribution or details as to why is maximizing h(s) good for exploration in the first place.----- - experimentally, few tasks are proposed comparing skew-fit with other baselines like her and autogoal gan - but the differences in all the results seem negligible (example : figure 5). ----- - i am not sure why the discussion of goal conditioned policies is introduced rightaway. to me, a more convincing approach would have been to first discuss why h(s) and the entropy of this is good for exploration (discounted weighting or stationary state distribution and considering episodic and infinite horizon tasks). if h(s) is indeed a difficult or not sufficient term to maximize the entropy for, then it might make sense to introduce goal conditioned policies? following then, it might be convincing to discuss why goal conditioned policies are indeed required, and then tractable ways of computing mi(s;g). ----- - experimentally, i think the paper needs significantly more work - especially considering hard exploration tasks (it might be simple setups too like mazes to begin with), and then to propose a set of new experimental results, without jumping directly to image based tasks as discussed here and then comparing to all the goal conditioned policy baselines. ----------overall, i would recommend to reject this paper, as i am not convinced by the proposed solution, and there are lot of theoretical details missing from the paper. it skips a lot of theoretical insights required to propose a new exploration based objective, and the paper proposes a very specific solution for a set a very specific set of experimental setups.","this paper tackles the problem of exploration in rl. in order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self-set goals. the empirically show that agents using this method uniformly visit all valid states under certain conditions. they also show that these agents are able to learn behaviours without providing a manually-defined reward function.----------the drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal-directed techniques. although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal-directed techniques do not, which would be expected from a purely empirical paper. this dampers the contribution, hence i recommend to reject this paper.",they show that the proposed method to maximize the state or goal distribution can lead to diverse exploration behaviour sufficient for solving complex image based manipulation tasks.,"the entropy of the state visitation distribution only deals with valid states - but i am not sure what it means to maximize the entropy of this term exactly in terms of exploration, since it is neither the discounted weighting of states or the stationary distribution for an infinite horizon task?","the paper introduces skew-fit, an exploration approach that maximizes the entropy of a distribution of goals such that the agent maximizes state coverage.",this paper introduced a very interesting idea to facilitate exploration in goal-conditioned reinforcement learning.,"it skips a lot of theoretical insights required to propose a new exploration based objective, and the paper proposes a very specific solution for a set a very specific set of experimental setups.","the core idea is to propose a method to maximize entropy of a goal distribution, or a state distribution since goals are full states.",more elaboration on this point is necessary.----------minor:-----appendix has several broken references.,"the entropy of the state visitation distribution only deals with valid states - but i am not sure what it means to maximize the entropy of this term exactly in terms of exploration, since it is neither the discounted weighting of states or the stationary distribution for an infinite horizon task?",0.165680473372781,0.0479041916167664,0.0946745562130177,0.0946745562130177,0.2291666666666666,0.0631578947368421,0.15625,0.15625,0.216867469879518,0.0121951219512195,0.1325301204819277,0.1325301204819277,0.1273885350318471,0.0258064516129032,0.0891719745222929,0.0891719745222929,0.1485714285714285,0.0115606936416184,0.0914285714285714,0.0914285714285714,0.1927710843373494,0.0121951219512195,0.0963855421686747,0.0963855421686747,0.0258064516129032,0.0,0.0258064516129032,0.0258064516129032,0.2291666666666666,0.0631578947368421,0.15625,0.15625,7.740300178527832,6.715386867523193,15.204370498657228,4.28336238861084,8.547657012939453,10.20847511291504,4.28336238861084,2.6747419834136963,0.08243511199841745,0.22218880444836847,0.8353762940263565,0.28685866496081697,0.42169983874219147,0.8629508606337595,0.9769061735846918,0.9703834692496103,0.8390899417001073,0.9670056686546904,0.9675377404773556,0.9464876815538543,0.11277706897825548,0.27585493569591774,0.05964765845454413,0.8436874225891975,0.8778549885261739,0.9324233829376403,0.9638119057652015,0.9609457397035395,0.8134671514034817,0.28685866496081697,0.42169983874219147,0.8629508606337595
176,https://openreview.net/forum?id=r1genAVKPB,"the author question an important aspect which is very often taken for granted in the rl community, that a good representation could lead to data-efficient rl. they show negative results, providing pessimistic lower bounds for both value-based and policy-based learning. ----------i believe the paper is an important contribution, in particular, it has the following advantages:---------- - well written, clear, and nicely structured. it is self contained, with main results and convincing sketch proofs provided in the main body of the document, and extended technical proofs made available in the supplementary material.---------- - authors provide the relevant related work and elegantly show how their work connects to the existing literature. ----------- the notation is consistent throughout the document, with necessary assumptions clearly stated. ----------- the discussion highlights important findings, in particular the difference between value-based and policy-based learning. additionally, it offers some hope for sample-efficient rl, by discussing the exponential separation between policy-based rl and imitation learning, reminding the community that sample-efficient rl can still be achieved by il even if it cant be achieved through good-but-not-perfect representation. ---------- minor comment: - phrasing of assumption 4.3 seems to be off. this paper presents theoretical lower bounds on sample complexities to learn good policies in reinforcement learning. the derived theorems show that there exists mdps which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning. these results constitute the first lower bounds for rl with linear function approximation.----------representation learning is an important area of research and this paper advances our theoretical understanding in a notable way, helping to elucidate the limits of representation learning alone. the lower bounds derived in the paper would be of particular interest to the community as they can apply to a wide range of function approximators, including neural networks. although this is not my area, the contributions are well-explained in the context of previous work and the theory was fairly easy to follow. the discussion also contained interesting points and summarized possible implications of the theoretical results. overall, i think this paper presents a solid contribution and recommend acceptance.----------although the paper was clear in general, i would like to have certain points clarified:-----1) just to check, is it still possible that there exists certain representations that are not perfect but do lead to sample efficient learning? if i'm interpreting the results correctly, the theorems only posit the existence of a representation that is good in the sense of approximation error, but bad in terms of sample complexity, which does not necessarily preclude the possibility of other efficient representations.-----2) more generally, why is it that there are few results for lower bounds when it seems like an obvious direction? are there technical barriers to proving such results?-----3) for value-based learning, the good representation has approximation error \omega(\sqrt(h / d)). could the authors explain why this assumption on the error is reasonable? -----3) in assumption 4.4, the features are assumed to have an l2-norm of 1. this seems like a fairly restrictive assumption. how important is this assumption and can it be relaxed?-----4) also, in theorem 4.2, it is assumed that the dimension of the features, d = h. this would seem to allow the possibility of policy-based learning being sample efficient when the number of features is much smaller. ----------the paper is well-polished with few noticeable typos.-----minor comments:------ p.5 sec3.3: ""knows the whole transition"" -> ""knows the whole transition function""------ p.8 sec5.1: ""our lower bound on policy-based learning thus demonstrates"" it may be worth reminding the reader that the bound applies to perfect representations -> ""our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates"" this paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods. the bound being exponential in the planning horizon is bad news, and has some implications with respect to further analysing sample complexity in rl.----------the gist of this paper is that one can craft a hard mdp which requires visiting every state at least once, and that since this mdp's state space is exponential in the mdp's horizon, then there exists a set of mdps which require an exponential (in the horizon) number of trajectories to be solved. as a consequence, further analysis of sample complexity in rl may need some much stronger assumptions.----------the writing of the paper is good, i was able to understand everything (i think). as far as i can tell, this is novel work. unfortunately i am currently unable to see why this contribution is valuable. i have set my score to weak reject but i am very open to having my mind changed, as i feel i may have missed some critical element.----------i have two criticisms:-----a- i don't understand why this bound is significantly different than previous bounds.-----b- i don't understand why this is bad news for representation learning, nor how this failure mode of linear features translates to the ""deep"" case.----------in the same spirit, i find rather odd the way the paper is introduced. discussions of representations usually involve some discussion of generalization, but that's not what this paper is about. deep neural networks/representation learning are only useful if there is an opportunity for generalization.---------------with respect to a, i am either grossly misunderstanding past bounds and/or your bounds, or something is wrong with the way complexities are compared:------ in wen & van roy, the ""polynomial"" sample complexity is in the number of states, it is related to |s|x|a|xh^2 (theorem 3 of wen & van roy)------ in this paper, theorem 4.1 states that the sample complexity is exponential because it is of the form 2^h. one *critical* assumption for this bound is precisely that |s| >= 2^h. thus the bound that you propose is still polynomial in |s|.-----i am thus puzzled, how is this bound significantly different?---------------with respect to b, i don't see how this bound has much to do with good representations, or even representations at all.-----in lemma a.1, you essentially craft a set of features that, being mutually orthogonal, are in some sense ""mutually linearly separable"", making learning the mapping from those features to a value function ""trivial"" once data is obtained. this is barely different from saying that you assume there is a magical learner that learns in o(1) given the data, because in either case, you need to visit _every_ of the 2^h state in order to solve the mdp, because by construction of your problem, there is _no hope_ of generalization*. since learning features or creating ""good"" features has everything to do with generalization (otherwise we'd just to tabular), i don't see how this bound is relevant to representations. (we already have wolpert's no free lunch theorem to tell us that there are always some problems that ml just can't be general enough to solve efficiently. what is more interesting is understanding how we can efficiently learn where there _is_ structure to a problem.)-----* there is no hope of generalization, unless something about the observation space (which is left undiscussed in the paper) contains *information* about the agent being to the unique path to the reward. in such a case, i can see a probabilistic argument being made where in the worst case the agent needs to visit all 2^h states, but in the average case, the agent may learn to ignore paths where it can generalize that there is no reward. this is not entirely unreasonable, think of e.g. alphago, where very few states end in victory, where there is an exponential number of states in the horizon, yet learning is totally reasonable because of structure in observation. this is where i don't agree with a statement like: ""since the class of linear functions is a strict subset of many more complicated function classes, including neural networks in particular, our negative results imply lower bounds for these more complex function classes as well.""","the authors challenge the idea that good representation in rl lead are sufficient for learning good policies with an interesting negative result -- they show that there exist mdps which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning. reviewers had some minor technical questions which were clarified sufficiently by the authors, leading to a consensus of the contribution and quality of this work. thus, i recommend this paper for acceptance.","this is barely different from saying that you assume there is a magical learner that learns in o(1) given the data, because in either case, you need to visit _every_ of the 2^h state in order to solve the mdp, because by construction of your problem, there is _no hope_ of generalization*.","----------the paper is well-polished with few noticeable typos.-----minor comments:------ p.5 sec3.3: ""knows the whole transition"" -> ""knows the whole transition function""------ p.8 sec5.1: ""our lower bound on policy-based learning thus demonstrates"" it may be worth reminding the reader that the bound applies to perfect representations -> ""our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates"" this paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods.",what is more interesting is understanding how we can efficiently learn where there _is_ structure to a problem.),"the author question an important aspect which is very often taken for granted in the rl community, that a good representation could lead to data-efficient rl.","----------the paper is well-polished with few noticeable typos.-----minor comments:------ p.5 sec3.3: ""knows the whole transition"" -> ""knows the whole transition function""------ p.8 sec5.1: ""our lower bound on policy-based learning thus demonstrates"" it may be worth reminding the reader that the bound applies to perfect representations -> ""our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates"" this paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods.","----------the paper is well-polished with few noticeable typos.-----minor comments:------ p.5 sec3.3: ""knows the whole transition"" -> ""knows the whole transition function""------ p.8 sec5.1: ""our lower bound on policy-based learning thus demonstrates"" it may be worth reminding the reader that the bound applies to perfect representations -> ""our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates"" this paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods.","deep neural networks/representation learning are only useful if there is an opportunity for generalization.---------------with respect to a, i am either grossly misunderstanding past bounds and/or your bounds, or something is wrong with the way complexities are compared:------ in wen & van roy, the ""polynomial"" sample complexity is in the number of states, it is related to |s|x|a|xh^2 (theorem 3 of wen & van roy)------ in this paper, theorem 4.1 states that the sample complexity is exponential because it is of the form 2^h.","----------the paper is well-polished with few noticeable typos.-----minor comments:------ p.5 sec3.3: ""knows the whole transition"" -> ""knows the whole transition function""------ p.8 sec5.1: ""our lower bound on policy-based learning thus demonstrates"" it may be worth reminding the reader that the bound applies to perfect representations -> ""our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates"" this paper's contribution is a sample complexity lower bound for linear value-based learning and policy-based learning methods.",0.2567567567567567,0.0136986301369863,0.1216216216216216,0.1216216216216216,0.3575418994413407,0.0677966101694915,0.1899441340782122,0.1899441340782122,0.1071428571428571,0.0181818181818181,0.0892857142857142,0.0892857142857142,0.2644628099173554,0.0672268907563025,0.1652892561983471,0.1652892561983471,0.3575418994413407,0.0677966101694915,0.1899441340782122,0.1899441340782122,0.3575418994413407,0.0677966101694915,0.1899441340782122,0.1899441340782122,0.3260869565217391,0.0439560439560439,0.1521739130434782,0.1521739130434782,0.3575418994413407,0.0677966101694915,0.1899441340782122,0.1899441340782122,9.4960355758667,9.4960355758667,13.645858764648438,9.4960355758667,4.862439632415772,7.483307838439941,9.4960355758667,8.806550979614258,0.4258588015387247,0.5171689780452808,0.9242526106174046,0.8321523345259304,0.8552803411170359,0.9212105508856858,0.1993852653623345,0.28689787762233426,0.8139574656438049,0.6646231429758361,0.6830357966433671,0.8762301902083626,0.8321523345259304,0.8552803411170359,0.9212105508856858,0.8321523345259304,0.8552803411170359,0.921210514038683,0.12917138559961652,0.1432212481438029,0.9154816802995261,0.8321523345259304,0.8552803411170359,0.9212105508856858
177,https://openreview.net/forum?id=r1glDpNYwS,"this paper proposes a method to create adversarial perturbations whose target labels are similar to their ground truth. the target labels are selected using an existing perceptual similarity measure for images. perturbations are generated using a deepfool-like algorithm. human evaluation supports that the pair of the generated images and target labels are more natural to humans than prior attack algorithms.----------this paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically. attackers can manually select target labels and apply targeted attacks. in the target label selection, attackers can choose less detectable labels if necessary. it is encouraged to provide some applications where attackers want to create less detectable adversarial examples in label space without manually assigning target labels.----------==========-----update:----------after reading the authors' responses, the motivation of the paper became clearer. i will not get surprised if this paper is accepted. however, all reviewers still share concerns about the importance of the problem tackled. i think the paper needs to suggest more applications and emphasize the value of the goal in the main paper before being published. this paper describes a technique for creating adversarial images where the added perturbations are not only imperceptible to machines, but also to human observers. the authors describe why this might be beneficial. the method works by finding labels that are not too far from the source image's ground-truth labels, and moving the source image in that direction. to find the target label, the authors use a threshold on the confidence of predicted ground-truth labels. the authors test their algorithm using a newly proposed metric of how much a method allows imperceptibility for a human observer. they show that their method creates images whose perturbations are more impercetible to humans, compared to other methods, but are also imperceptible to machines.----------my concern is as follows: if the misclassification is between a and b, and they are related classes, is the attack so bad? and what are the scenarios in practice when a user simply wants to create an attack, without regards to the target label chosen? i imagine normally the attacker has a target label in mind, so the part of the paper that chooses a target label is not very useful; and this is the main element of novelty, since the rest of the method is from deepfool, as the authors explain. some specific use cases of this methods should be discussed. ----------minor suggestion: it would be useful to see examples like in fig. 5 but with the classes (true/target) listed. this paper proposes a method for constructing adversarial attacks that are less detectable by humans, by changing the target class to be a class similar to the original class of the image. the resulting attack methodology is then studied in terms of its imperceptibility in label space, and shown to be less perceptible in label space to human observers, while not coming at a cost in image space.----------the paper presents compelling evaluation of the method and does seem to succeed in proving that their proposed attack satisfies the stated goal. however, it appears as though this goal is somewhat counter to the main point of adversarial examples---indeed, if the label is reasonable to a human, then what makes the adversarial example adversarial? the main threat in adversarial examples research seems to be that it is possible to induce predictions that are arbitrarily different from humans' on natural-looking in puts. thus, changing the label to something that a human actually agrees with would actually reduce the impact of the adversarial attack.----------in order to improve the paper, i would suggest applying the same (or similar) methodologies to other areas of ml security where imperceptibility in label space is commonly desired---for example, in data poisoning attacks or backdoor attacks. in general, such attacks are much more likely to be ""inspected"" by humans, and so imperceptibility in both label and image space is very desirable. however, i suspect that this would require significant effort and changes to the paper, and so for now i recommend rejection.","thanks for the discussion with reviewers, which improved our understanding of your paper significantly.-----however, we concluded that this paper is still premature to be accepted to iclr2020. we hope that the detailed comments by the reviewers help improve your paper for potential future submission.","it is encouraged to provide some applications where attackers want to create less detectable adversarial examples in label space without manually assigning target labels.----------==========-----update:----------after reading the authors' responses, the motivation of the paper became clearer.",human evaluation supports that the pair of the generated images and target labels are more natural to humans than prior attack algorithms.----------this paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically.,"to find the target label, the authors use a threshold on the confidence of predicted ground-truth labels.",this paper proposes a method to create adversarial perturbations whose target labels are similar to their ground truth.,attackers can manually select target labels and apply targeted attacks.,this paper proposes a method to create adversarial perturbations whose target labels are similar to their ground truth.,"in the target label selection, attackers can choose less detectable labels if necessary.",human evaluation supports that the pair of the generated images and target labels are more natural to humans than prior attack algorithms.----------this paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically.,0.1951219512195122,0.0,0.1463414634146341,0.1463414634146341,0.2528735632183908,0.0470588235294117,0.1839080459770115,0.1839080459770115,0.1587301587301587,0.0,0.0952380952380952,0.0952380952380952,0.1269841269841269,0.0327868852459016,0.1269841269841269,0.1269841269841269,0.0,0.0,0.0,0.0,0.1269841269841269,0.0327868852459016,0.1269841269841269,0.1269841269841269,0.0344827586206896,0.0,0.0344827586206896,0.0344827586206896,0.2528735632183908,0.0470588235294117,0.1839080459770115,0.1839080459770115,13.843677520751951,15.606243133544922,13.84367561340332,13.114309310913086,11.676132202148438,10.151517868041992,13.11430835723877,12.60582160949707,0.9433448478081342,0.9697118618732588,0.8504795157257968,0.9711651153390227,0.9728104814892747,0.7596912844346969,0.925776282386581,0.9566673316466948,0.8577113424045468,0.9793729031328036,0.9770342532764456,0.9478949769624923,0.9628787806003211,0.97240016908694,0.8637648715757863,0.9793729031328036,0.9770342532764456,0.9478950255484831,0.9385935015581983,0.9615712657149292,0.6907595851755067,0.9711651153390227,0.9728104814892747,0.7596912844346969
178,https://openreview.net/forum?id=r1lIKlSYvH,"this paper tries to establish an explanation for the posterior collapse by linking the phenomenon to local minima. if i am understanding correctly, the final conclusion reads along the lines of 'if the reconstruction error is high, then the posterior distribution will follow the prior distribution'. they also provide some experimental data to suggest that when the reconstruction error is high, the distribution of the latents tend to follow the prior distribution more closely. ----------although i really liked section 3 where authors establish the different ways in which `posterior collapse' can be defined, overall i am not sure if i can extract a useful insight or solution out of this paper. when the reconstruction error is large, the vae is practically not useful. ----------also, i don't think i am on board with continuing to use the standard gaussian prior. several papers (such as the cited vampprior paper) showed that one can very successfully use gmm like priors, which reduces the burden on the autoencoder. even though i liked the exposition in the first half of the paper, i don't think i find the contributions of this paper very useful, as one can actually learn the prior and get good autoencoder reconstructions while obtaining a good match between the prior and the posterior, without having degenerate posterior distribution which is independent from the data distribution. all in all, i think using a standard gaussian prior is not a good idea, and that fact renders the explanations provided in this paper obsolete in my opinion. is there any reason why we would want to utilize a simplistic prior such as the standard gaussian prior? do you have any insights with regards to whether the explanations in this paper would still hold with more expressive prior distributions? ----------in general, i have found section 5 hard follow. and to reiterate, the main arguments seem to be centered around autoencoders which cannot reconstruct well, as the authors also consider the deterministic autoencoder. if the autoencoder can not reconstruct well, it is not reasonable to expect a regularized autoencoder such as vae to reconstruct, better, and therefore the vae is already is a regime where it is not useful anyhow. i think the authors should think about the cases where the reconstruction error is low, and see if there is an issue of posterior collapse in those setups. 1. summary-----the paper theoretically investigates the role of local optima of the variational objective in ignoring latent variables (leading to posterior collapse) in variational autoencoders. the paper first discusses various potential causes for posterior collapse before diving deeper into a particular cause: local optima. the paper considers a class of near-affine decoders and characterise the relationship between the variance (gamma) in the likelihood and local optima. the paper then extends this discussion for deeper architecture and vanilla autoencoders and illustrate how this can arise when the reconstruction cost is high. the paper considers several experiments to illustrate this issue.----------2. opinion and rationales-----i thank the authors for a good discussion paper on this important topic. however, at this stage, im leaning toward weak reject, due to the reasons below. that said, im willing to read the authors clarification and read the paper again during the rebuttal to correct my misunderstandings if there is any. the points below are all related.----------a. i would like to understand the use of local optima here. i think the paper specifically investigate local optima of the likelihood noise variance, and there are potentially other local optima. wouldnt this be an issue with hyperparameter optimisation in general? for example, for any regression tasks, high observation noise can be used to explain the data and all other modelling components can thus be ignored, so people have to initialise this to small values or constrain it during optimisation.----------b. i think there is one paper that the paper should discuss: two problems with variational expectation maximisation for time-series models by turner and sahani. in this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the bias issue of the bound towards high observation noise.----- -----c. i think it would be good to think about the intuition of this as well: unavoidably high reconstruction errors, this implicitly constrains the corresponding vae model to have a large optimal gamma value: isnt this intuitive to improve the likelihood of the hyperparameter gamma given the data?----------d. if all above are sensible and correct, i would like to understand the difference between this class of local minima and that of (ii). arent they the same?----------e. the experiments consider training aes/vaes with increasingly complex decoders/encoders and suggest there is a strong relationship between the reconstruction errors in aes and vaes, and this and posterior collapse. but are these related to the minima in the decoders/encoders parameter spaces and not the hyperparameter space? so that is the message that the paper is trying to convey here?----------3. minor:----------sec 3-----(ii) assumi -> assuming-----(v) fifth -> four, forth -> fifth","this manuscript investigates the posterior collapse in variational autoencoders and seeks to provide some explanations from the phenomenon. the primary contribution is to propose some previously understudied explanations for the posterior collapse that results from the optimization landscape of the log-likelihood portion of the elbo.----------the reviewers and ac agree that the problem studied is timely and interesting, and closely related to a variety of recent work investigating the landscape properties of variational autoencoders and other generative models. however, this manuscript also received quite divergent reviews, resulting from differences in opinion about the technical difficulty and importance of the results. in reviews and discussion, the reviewers noted issues with clarity of the presentation and sufficient justification of the results. there were also concerns about novelty. in the opinion of the ac, the manuscript in its current state is borderline, and should ideally be improved in terms of clarity of the discussion, and some more investigation of the insights that result from the analysis.","in this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the bias issue of the bound towards high observation noise.----- -----c. i think it would be good to think about the intuition of this as well: unavoidably high reconstruction errors, this implicitly constrains the corresponding vae model to have a large optimal gamma value: isnt this intuitive to improve the likelihood of the hyperparameter gamma given the data?----------d.","in this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the bias issue of the bound towards high observation noise.----- -----c. i think it would be good to think about the intuition of this as well: unavoidably high reconstruction errors, this implicitly constrains the corresponding vae model to have a large optimal gamma value: isnt this intuitive to improve the likelihood of the hyperparameter gamma given the data?----------d.","in this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the bias issue of the bound towards high observation noise.----- -----c. i think it would be good to think about the intuition of this as well: unavoidably high reconstruction errors, this implicitly constrains the corresponding vae model to have a large optimal gamma value: isnt this intuitive to improve the likelihood of the hyperparameter gamma given the data?----------d.",this paper tries to establish an explanation for the posterior collapse by linking the phenomenon to local minima.,"in this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the bias issue of the bound towards high observation noise.----- -----c. i think it would be good to think about the intuition of this as well: unavoidably high reconstruction errors, this implicitly constrains the corresponding vae model to have a large optimal gamma value: isnt this intuitive to improve the likelihood of the hyperparameter gamma given the data?----------d.","in this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the bias issue of the bound towards high observation noise.----- -----c. i think it would be good to think about the intuition of this as well: unavoidably high reconstruction errors, this implicitly constrains the corresponding vae model to have a large optimal gamma value: isnt this intuitive to improve the likelihood of the hyperparameter gamma given the data?----------d.","if the autoencoder can not reconstruct well, it is not reasonable to expect a regularized autoencoder such as vae to reconstruct, better, and therefore the vae is already is a regime where it is not useful anyhow.","even though i liked the exposition in the first half of the paper, i don't think i find the contributions of this paper very useful, as one can actually learn the prior and get good autoencoder reconstructions while obtaining a good match between the prior and the posterior, without having degenerate posterior distribution which is independent from the data distribution.",0.2343096234309623,0.0253164556962025,0.1506276150627615,0.1506276150627615,0.2343096234309623,0.0253164556962025,0.1506276150627615,0.1506276150627615,0.2343096234309623,0.0253164556962025,0.1506276150627615,0.1506276150627615,0.1098901098901098,0.0555555555555555,0.0989010989010989,0.0989010989010989,0.2343096234309623,0.0253164556962025,0.1506276150627615,0.1506276150627615,0.2343096234309623,0.0253164556962025,0.1506276150627615,0.1506276150627615,0.1094527363184079,0.0,0.0895522388059701,0.0895522388059701,0.1866666666666666,0.0358744394618834,0.1422222222222222,0.1422222222222222,7.058314323425293,7.058314323425293,13.007157325744627,7.058314323425293,7.058314323425293,7.058314323425293,9.296326637268066,1.856268048286438,0.4834243081059853,0.5254045921831199,0.8689519365301099,0.4834243081059853,0.5254045921831199,0.8689519494787955,0.4834243081059853,0.5254045921831199,0.8689519494787955,0.9700394689256158,0.9736324563295212,0.9326413993241128,0.4834243081059853,0.5254045921831199,0.8689519494787955,0.4834243081059853,0.5254045921831199,0.8689520635533347,0.9467494808985721,0.954826981256836,0.2774049687712065,0.9444507197520812,0.9557087394031734,0.9384423375192993
179,https://openreview.net/forum?id=r1luCsCqFm,"this paper describes an approach for automated curriculum learning in a deep learning classification setup. the main idea is to weigh data points according to the current value of the loss on these data points. a naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples). this last part is implemented by caching hidden representations and classification loss values during training and fetching nearest neighbours in the feature space whenever a hard data point is encountered. the final loss takes the form of a linear combination of the classification loss and the representation loss.----------------the idea is interesting in the sense that it tries to use information about how difficult classification of a given data point is to improve learning. the proposed representation loss can lead to forming tight cluster of similar data point in the feature space and can make classification easier. it is related to student-teacher networks, where a student is trained to imitate the teacher in generated similar feature representations.----------------the authors justify the method by introducing the notion of inverse internal covariate shift. however, it is not defined formally, nor is it supported empirically, and is based on the (often criticized [1]) notion of internal covariate shift. for this reason, it is hard to accept the presented argumentation in its current state.----------------moreover, there seems to be a mistake in equation (2) in 4.2. the equation defines the method of computing loss weighting for a given datapoint. the authors note that it converges to the value of one with increasing training iterations, but for correctness it should be \in [0, 1]. if it is > 1, one of the losses in equation (3) is negated and is therefore maximised (instead of being minimised), which can lead to unexpected behaviour. current parameterization allows it to be \in [0, + infinity].----------------experimental evaluation consists of quantitative evaluation of random sampling (usual sgd) and the proposed approach in training a classification model on mnsit, cifar-10 and cifar-100. the proposed approach outperforms random sampling. this is encouraging, but the method should be compared to state of the art in curriculum learning in order to gauge how useful this approach is.----------------the paper is poorly written, with many grammatical (lack of s at the end of verbs used in singular 3rd person, many places in the paper) and spelling mistakes (e.g. 3.26 tough instead of through, i think). some descriptions are unclear (e.g. 4.22), while some parts of the paper seem to be irrelevant to the problem at hand (3.1 describes training on a single minibatch for multiple iterations as if it were a separate task and motivates random sampling, which is just sgd).----------------to summarize, the paper presents a very interesting idea. in its current state it is hard to read, however. it also contains a number of unsupported claims and can be misleading. it could also benefit from a more extensive evaluation. with this in mind, i suggest rejecting this paper.----------------[1] rahimi, a (2017). test of time award talk, nips. this paper proposes a curriculum that encourages training on easy examples first and postpones training on hard examples. however, contrary to common ideas, they propose to keep hard examples contribute to the loss and only forcing them to have internal representations similar to a nearby easy example. the proposed objective is hence biased at the beginning but they dampen it over time to converge to the true objective at the end.----------------positives:--------- there is not much work considering each example as an individual subtask.--------- the observation that an under-fitted classifier can destroy a good feature extractor is good.----------------negatives:--------- in the intro it says [update rule of gradient descent] assumes the top layer, f2, to be the right classifier.. this seems like a fundamental misunderstanding of gradient descent and the chain rule. the term d output/d f1 takes into account the error in f2.--------- the caption of figure 2 says the ... they cannot separate from its neighbors. if the loss of all examples in a cluster is high, all are being misclassified. a classifier then might have an easy job fixing them if all their labels are the same or have a difficult job if their labels are random. the second scenario is unlikely if based on the claim of this figure, the entropy has decreased during training. in short, the conclusion made in fig 2 does not necessarily hold given that figure.--------- this method is supposed to speed up training, not necessarily improve the final generalization performance of the model. the figures show the opposite outcomes. its not clear why. the improvement might be due to not tuning the hyperparameters of the baselines.--------- figure 3 does not necessarily support the conclusion. the fluctuations might be caused by any curriculum that forces a fixed ordering across training epochs. often on mnist, the ordering of data according to the loss does not change significantly throughout training.","this paper attempts to address a problem they dub ""inverse"" covariate shift where an improperly trained output layer can hamper learning. the idea is to use a form of curriculum learning. the reviewers found that the notion of inverse covariate shift was not formally or empirically well defined. furthermore the baselines used were too weak: the authors should consider comparing against state-of-the-art curriculum learning methods.","it is related to student-teacher networks, where a student is trained to imitate the teacher in generated similar feature representations.----------------the authors justify the method by introducing the notion of inverse internal covariate shift.","a naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples).","a naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples).",this paper describes an approach for automated curriculum learning in a deep learning classification setup.,the proposed representation loss can lead to forming tight cluster of similar data point in the feature space and can make classification easier.,"a naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples).","however, contrary to common ideas, they propose to keep hard examples contribute to the loss and only forcing them to have internal representations similar to a nearby easy example.","a naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples).",0.3300970873786407,0.099009900990099,0.2135922330097087,0.2135922330097087,0.2337662337662338,0.0526315789473684,0.1558441558441558,0.1558441558441558,0.2337662337662338,0.0526315789473684,0.1558441558441558,0.1558441558441558,0.1686746987951807,0.0493827160493827,0.144578313253012,0.144578313253012,0.1318681318681318,0.0,0.1098901098901099,0.1098901098901099,0.2337662337662338,0.0526315789473684,0.1558441558441558,0.1558441558441558,0.1237113402061855,0.0,0.1030927835051546,0.1030927835051546,0.2337662337662338,0.0526315789473684,0.1558441558441558,0.1558441558441558,12.206300735473633,9.750622749328612,11.629362106323242,12.206300735473633,6.760275840759277,12.206300735473633,12.206300735473633,7.0449724197387695,0.9554709637403738,0.9682213793607855,0.08977679680054527,0.9465795895208953,0.9564465241540965,0.9298600478062321,0.9465795895208953,0.9564465241540965,0.9298600478062321,0.9649551723094588,0.9648836158100074,0.9173154967394817,0.9561234963513855,0.954721850573111,0.8934610364347118,0.9465795895208953,0.9564465241540965,0.9298600048043466,0.9283824086773884,0.9247999532935578,0.026704908074152225,0.9465795895208953,0.9564465241540965,0.9298600048043466
180,https://openreview.net/forum?id=r1rhWnZkg,"summary: the paper presents low-rank bilinear pooling that uses hadamard product (commonly known as element-wise multiplication). the paper implements low-rank bilinear pooling on an existing model (kim et al., 2016b) and builds a model for visual question answering (vqa) that outperforms the current state-of-art by 0.42%. the paper presents various ablation studies of the new vqa model they built.----------------strengths:----------------1. the paper presents new insights into element-wise multiplication operation which has been previously used in vqa literature (such as antol et al., iccv 2015) without insights on why it should work. ----------------2. the paper presents a new model for the task of vqa that beats the current state-of-art by 0.42%. however, i have concerns about the statistical significance of the performance (see weaknesses below).----------------3. the various design choices made in model development have been experimentally verified. ----------------weaknesses/suggestions:----------------1. when authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of vqa).----------------2. the authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. so, could the authors please explain how does the reduction in number of parameters help experimentally? does the training time of the model reduce significantly? can we train the model with less data? ----------------3. one of the contributions of the paper is that the proposed model outperforms the current state-of-art on vqa by 0.42%. however, i am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. i would like the authors to explicitly mention the differences between mrn, marn and mlb. it is not very clear from reading the paper.----------------5. in the caption for table 1, fix the following: have not -> have no ----------------review summary: i like the insights about low-rank bilinear pooling using hadamard product (element-wise multiplication) presented in the paper. however, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. it does lead to reduction in number of parameters but it is not clear how much that helps experimentally. so, to be more convinced i would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. results on the vqa task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. ----------------missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. since the main contribution of the--------paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. if you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible? how is the question encoded? is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? the meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9? this work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the hadamard product (element-wise product). --------this formulation is evaluated on the visual question answering (vqa) task together with several other model variants.----------------strength:--------1. the paper discusses how the hadamard product can be used to approximate the full outer product.--------2. the paper provides an extensive experimental evaluation of other model aspect for vqa.--------3. the full model archives a slight improvement over prior state-of-the-art on the challenging and large scale vqa challenge.----------------weaknesses:--------1. novelty: the paper presents only a new interpretation of the hadamard product which has previously been widely used for pooling, including for vqa.--------2. experimental evaluation:--------2.1. an experimental direct comparison with mcb missing. although the evaluated model is similar to fukui et al. several other changes have been made, including question encoding (gru vs. lstm), normalization (tanh vs. l2 vs. none). the small difference in performance (0.44% om table 1) could easily be attributed to these differences.--------2.2. an experimental comparison to the full outer product (e.g. for a lower dimension) is missing. it remains unclear how good the proposed approximation for the full outer product is. while a comparison to mcb is presented this seems insufficient as mcb is a very different model.--------2.3. one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?--------2.4. comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.--------3. no theoretical analysis or properties of the approximation are presented.--------4. the paper seems to be general at the beginning, but the claim of the benefit of the hadamard product is only shown experimentally on the vqa dataset.--------5. related work: the comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.------------------------minor--------- it is not clear why the lu et al, 2015 is cited rather than the published paper from antol et al.--------- sect 2, first sentence: every pairs -> every pair------------------------summary:--------while the paper provides a new best performance and an interesting interpretation of hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.----------------to be more convincing i would like to see the following experiments --------- comparison with outer product in the identical model--------- comparison with mcb in the identical model--------- comparison with elementwise sum instead of elementwise product--------- one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?","the program committee appreciates the authors' response to concerns raised in the reviews. while there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at iclr.","in the caption for table 1, fix the following: have not -> have no ----------------review summary: i like the insights about low-rank bilinear pooling using hadamard product (element-wise multiplication) presented in the paper.",it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.----------------to be more convincing i would like to see the following experiments --------- comparison with outer product in the identical model--------- comparison with mcb in the identical model--------- comparison with elementwise sum instead of elementwise product--------- one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?,it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.----------------to be more convincing i would like to see the following experiments --------- comparison with outer product in the identical model--------- comparison with mcb in the identical model--------- comparison with elementwise sum instead of elementwise product--------- one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?,summary: the paper presents low-rank bilinear pooling that uses hadamard product (commonly known as element-wise multiplication).,it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.----------------to be more convincing i would like to see the following experiments --------- comparison with outer product in the identical model--------- comparison with mcb in the identical model--------- comparison with elementwise sum instead of elementwise product--------- one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?,"hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of vqa).----------------2.","the paper implements low-rank bilinear pooling on an existing model (kim et al., 2016b) and builds a model for visual question answering (vqa) that outperforms the current state-of-art by 0.42%.",it is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the hadamard product) but of unrelated aspects which are important to achieve high performance in the vqa challenge.----------------to be more convincing i would like to see the following experiments --------- comparison with outer product in the identical model--------- comparison with mcb in the identical model--------- comparison with elementwise sum instead of elementwise product--------- one of the most important hyper parameters for the hadamard product seems to be the dimension of the lower dimensional embedding d. what effect does changing this have?,0.2222222222222222,0.050632911392405,0.1481481481481481,0.1481481481481481,0.2297297297297297,0.0547945205479452,0.1621621621621621,0.1621621621621621,0.2297297297297297,0.0547945205479452,0.1621621621621621,0.1621621621621621,0.123076923076923,0.0317460317460317,0.0923076923076922,0.0923076923076922,0.2297297297297297,0.0547945205479452,0.1621621621621621,0.1621621621621621,0.16,0.0273972602739726,0.1066666666666666,0.1066666666666666,0.1481481481481481,0.0253164556962025,0.1234567901234567,0.1234567901234567,0.2297297297297297,0.0547945205479452,0.1621621621621621,0.1621621621621621,11.622441291809082,8.601181030273438,9.96413803100586,8.601181030273438,10.184797286987305,8.601181030273438,8.601181030273438,10.290664672851562,0.875199204041407,0.8768928791268583,0.33462203331250007,0.8765478663779264,0.897887643246872,0.9418591267326314,0.8765478663779264,0.897887643246872,0.9418591618198325,0.9528428133166333,0.9516501307312425,0.943652791221196,0.8765478663779264,0.897887643246872,0.9418591618198325,0.9229405371871837,0.9169294832877657,0.3860743728130004,0.9219024388292191,0.9105335901772657,0.8941372977591231,0.8765478663779264,0.897887643246872,0.9418591267326314
181,https://openreview.net/forum?id=rJ8uNptgl,"this paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters. the authors assume that the network is already pruned and aim for compressing the non-pruned parameters. the problem of network compression is a well-motivated problem and of interest to the iclr community. ----------------the main drawback of the paper is its novelty. the paper is heavily built on the results of han 2015 and only marginally extends han 2015 to overcome its drawbacks. it should be noted that the proposed method in this paper has not been proposed before. ----------------the paper is well-structured and easy to follow. although it heavily builds on han 2015, it is still much longer than han 2015. i believe that there is still some redundancy in the paper. the experiments section starts on page 12 whereas for han 2015 the experiments start on page 5. therefore, i believe much of the introductory text is redundant and can be efficiently cut. ----------------experimental results in the paper show good compression performance compared to han 2015 while losing very little accuracy. can the authors mention why there is no comparison with hang 2015 on resnet in table 1?----------------some comments:--------1) it is not clear whether the procedure depicted in figure 1 is the authors contribution or has been in the literature.--------2) in section 4.1 the authors approximate the hessian matrix with a diagonal matrix. can the authors please explain how this approximation affects the final compression? also how much does one lose by making such an approximation?----------------minor typos (these are for the revised version of the paper):--------1) page 2, parag 3, 3rd line from the end: fined-tuned -> fine-tuned--------2) page 2, one para to the end, last line: assigned for -> assigned to--------3) page 5, line 2, same as above--------4) page 8, section 5, line 3: explore -> explored this paper proposes a novel neural network compression technique.--------the goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.--------it assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.--------in contrast to previous work (han et al. 2015a, gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation. ----------------unfortunately, the submitted paper is 20 pages, rather than the 8 recommended. the length of the paper seems unjustified to me, since the first three sections (first five pages) are very generic and redundant can be largely compressed or skipped (including figures 1 and 2). although not a strict requirement by the submission guidelines, i would suggest the authors to compress their paper to 8 pages, this will improve the readability of the paper.----------------to take into account the impact on the networks loss the authors propose to use a second order approximation of the cost function of the loss. in the case of weights that originally constitute a local minimum of the loss, this leads to a formulation of the impact of the weight quantization on the loss in terms of a weighted k-means clustering objective, where the weights are derived from the hessian of the loss function at the original weights.--------the hessian can be computed efficiently using a back-propagation algorithm similar to that used to compute the gradient, as shown in cited work from the literature. --------the authors also propose to alternatively use a second-order moment term used by the adam optimisation algorithm, since it can be loosely interpreted as an approximate hessian. ----------------in section 4.5 the authors argue that with their approach it is more natural to quantise weights across all layers together, due to the hessian weighting which takes into account the variable impact across layers of quantisation errors on the network performance. --------the last statement in this section, however, was not clear to me: --------in such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.--------perhaps the authors could elaborate a bit more on this point?----------------in section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process. --------the first method described by the authors (based on previous work), is uniform quantisation of the weight space, which is then further optimised by their hessian-weighted clustering procedure from section 4. --------for the case of nonuniform codeword lengths to encode the cluster indices, the authors develop a modification of the hessian weighted k-means algorithm in which the code length of each cluster is also taken into account, weighted by a factor lambda. different values of lambda give rise to different compression-accuracy trade-offs, and the authors propose to cluster weights for a variety of lambda values and then pick the most accurate solution obtained, given a certain compression budget. ----------------in section 6 the authors report a number of experimental results that were obtained with the proposed methods, and compare these results to those obtained by the layer-wise compression technique of han et al 2015, and to the uncompressed models. --------for these experiments the authors used three datasets, mnist, cifar10 and imagenet, with data-set specific architectures taken from the literature. --------these results suggest a consistent and significant advantage of the proposed method over the work of han et al. comparison to the work of gong et al 2014 is not made.--------the results illustrate the advantage of the hessian weighted k-means clustering criterion, and the advantages of the variable bitrate cluster encoding. ----------------in conclusion i would say that this is quite interesting work, although the technical novelty seems limited (but im not a quantisation expert).--------interestingly, the proposed techniques do not seem specific to deep conv nets, but rather generically applicable to quantisation of parameters of any model with an associated cost function for which a locally quadratic approximation can be formulated. it would be useful if the authors would discuss this point in their paper. the paper has two main contributions:----------------1) shows that uniform quantization works well with variable length (huffman) coding----------------2) improves fixed-length quantization by proposing the hessian-weighted k-means, as opposed to standardly used vanilla k-means. the hessian weighting is well motivated, and it is also explained how to use an efficient approximation ""for free"" when using the adam optimizer, which is quite neat. as opposed to vanilla k-means, one of the main benefits of this approach (apart from improved performance) is that no tuning on per-layer compression rates is required, as this is achieved for free.----------------to conclude, i like the paper: (1) is not really novel but it doesn't seem other papers have done this before so it's nice to know it works well, and (2) is quite neat and also works well. the paper is easy to follow, results are good. my only complaint is that it's a bit too long.----------------minor note - i still don't understand the parts about storing ""additional bits for each binary codeword for layer indication"" when doing layer-by-layer quantization. what's the problem of just having an array of quantized weight values for each layer, i.e. q[0][:] would store all quantized weights for layer 0, q[1][:] for layer 1 etc, and for each layer you would have the codebook. so the only overhead over joint quantization is storing the codebook for each layer, which is insignificant. i don't understand the ""additional bit"" part. but anyway, this is really not a important as i don't think it affects the paper at all, just authors might want to additionally clarify this point (maybe i'm missing something obvious, but if i am then it's likely some other people will as well).","the paper proposes using quantization schemes to compress the weights of a neural network. the paper carries out a methodical study of first deriving the objective function for optimizing the quantization, and then uses various quantization schemes. experiments show competitive performance in terms of compression and accuracy tradeoff. i am happy to go with the reviewers' recommendations to accept the paper. a minor comment: it is important to mention other frameworks that compress neural networks, e.g. https://arxiv.org/abs/1509.06569 although the weights are not quantized above, the number of parameters is reduced. similarly there are other works looking into network compression. it will be good to mention them.","--------the last statement in this section, however, was not clear to me: --------in such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.--------perhaps the authors could elaborate a bit more on this point?----------------in section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process.","also how much does one lose by making such an approximation?----------------minor typos (these are for the revised version of the paper):--------1) page 2, parag 3, 3rd line from the end: fined-tuned -> fine-tuned--------2) page 2, one para to the end, last line: assigned for -> assigned to--------3) page 5, line 2, same as above--------4) page 8, section 5, line 3: explore -> explored this paper proposes a novel neural network compression technique.--------the goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.--------it assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.--------in contrast to previous work (han et al. 2015a, gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation.","--------the last statement in this section, however, was not clear to me: --------in such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.--------perhaps the authors could elaborate a bit more on this point?----------------in section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process.","this paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters.","the paper has two main contributions:----------------1) shows that uniform quantization works well with variable length (huffman) coding----------------2) improves fixed-length quantization by proposing the hessian-weighted k-means, as opposed to standardly used vanilla k-means.","also how much does one lose by making such an approximation?----------------minor typos (these are for the revised version of the paper):--------1) page 2, parag 3, 3rd line from the end: fined-tuned -> fine-tuned--------2) page 2, one para to the end, last line: assigned for -> assigned to--------3) page 5, line 2, same as above--------4) page 8, section 5, line 3: explore -> explored this paper proposes a novel neural network compression technique.--------the goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.--------it assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.--------in contrast to previous work (han et al. 2015a, gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation.","but anyway, this is really not a important as i don't think it affects the paper at all, just authors might want to additionally clarify this point (maybe i'm missing something obvious, but if i am then it's likely some other people will as well).","also how much does one lose by making such an approximation?----------------minor typos (these are for the revised version of the paper):--------1) page 2, parag 3, 3rd line from the end: fined-tuned -> fine-tuned--------2) page 2, one para to the end, last line: assigned for -> assigned to--------3) page 5, line 2, same as above--------4) page 8, section 5, line 3: explore -> explored this paper proposes a novel neural network compression technique.--------the goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.--------it assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.--------in contrast to previous work (han et al. 2015a, gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation.",0.2797927461139896,0.0418848167539267,0.1450777202072538,0.1450777202072538,0.3460207612456747,0.0418118466898954,0.1453287197231833,0.1453287197231833,0.2797927461139896,0.0418848167539267,0.1450777202072538,0.1450777202072538,0.2499999999999999,0.044776119402985,0.1911764705882352,0.1911764705882352,0.1733333333333333,0.0135135135135135,0.0933333333333333,0.0933333333333333,0.3460207612456747,0.0418118466898954,0.1453287197231833,0.1453287197231833,0.175,0.0253164556962025,0.1,0.1,0.3460207612456747,0.0418118466898954,0.1453287197231833,0.1453287197231833,6.138729095458984,8.753893852233887,13.385844230651855,6.138729095458984,8.772687911987305,8.772687911987305,6.138729095458984,4.967426300048828,0.7503894415350718,0.8208979780044389,0.951118705744544,0.9739289577220779,0.5012250870106936,0.23632612093651487,0.7503894415350718,0.8208979780044389,0.9511186951149064,0.9759310062377363,0.9719483733507888,0.9614670124290633,0.9199419415515845,0.9314779939217118,0.9254613315279981,0.9739289577220779,0.5012250870106936,0.23632612636120198,0.24297497242234004,0.5645344387188691,0.4147127521199769,0.9739289577220779,0.5012250870106936,0.23632612093651487
182,https://openreview.net/forum?id=rJIN_4lA-,"about the first point, it does not present a clear problem definition. the paper continues stating what it should do (e.g. ""our agents only live once at at test time and must maintain cooperation by behaving intelligently within the confines of a single game rather than threats across games."") without any support for these desiderata. it then continues explaining how to achieve these desiderata, but at this point it is impossible to follow a coherent argument without understanding why are the authors making these strong assumptions about the problem they are trying to solve, and why. without this problem description and a good motivation, it is impossible to assess why such desiderata (which look awkward to me) are important. the paper continues defining some joint behavior (e.g. cooperative policies), but then construct arguments for individual policy deviations, including elements like \pi_a and \pi_2^{a_k} that, as you see, a is used sometimes as subindex and sometimes as supperindex. could not follow this part, as such elements lack definition. d_k is also not defined. ----------------experiments are uninteresting and show same results as many other rl algorithms that have been proposed in the past. no comparison with such other approaches is presented, nor even recognized. the paper should include a related work section that explain such similar approaches and their difference with this approach. the paper should continue the experimental section making explicit comparisons with such related work.----------------**detailed suggestions**--------- on page 2 you say ""this methodology cannot be directly applied to our problem"" without first defining what the problem is.--------- when authors talk about the agent, it is unclear what agent they refer to--------- \delta undefined--------- you say selfish reward schedule each agent i treats the other agent just as a part of their environment. however, you need to make some assumption about its behavior (e.g. adversarial, cooperative, etc.) and this disregarded. this paper studies learning to play two-player general-sum games with state (markov games). the idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. in this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. this is fun but i did not find it particularly surprising from a game-theoretic or from a deep learning point of view. ----------------from a game-theoretic point of view, the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising. it is basically a straightforward generalization of the idea of punishing, which is common in ""folk theorems"" from game theory, to give a particular equilibrium for cooperating in markov games. many markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. when the game is symmetric, this might be ""the natural"" solution but in general it is far from clear why all players would want to maximize the total payoff. ----------------the paper follows with some fun experiments implementing these new game theory notions. unfortunately, since the game theory was not particularly well-motivated, i did not find the overall story compelling. it is perhaps interesting that one can make deep learning learn to cooperate, but one could have illustrated the game theory equally well with other techniques.----------------in contrast, the paper ""coco-q: learning in stochastic games with side payments"" by sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. i would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.----------------it should also be noted that i was asked to review another iclr submission entitled ""consequentialist conditional cooperation in--------social dilemmas with imperfect information--------"" which amazingly introduced the same ""pong players dilemma"" game as in this paper. ----------------notice the following suspiciously similar paragraphs from the two papers:----------------from ""maintaining cooperation in complex social dilemmas using deep reinforcement learning"":--------we also look at an environment where strategies must be learned from raw pixels. we use the method--------of tampuu et al. (2017) to alter the reward structure of atari pong so that whenever an agent scores a--------point they receive a reward of 1 and the other player receives 2. we refer to this game as the pong--------players dilemma (ppd). in the ppd the only (jointly) winning move is not to play. however, a fully--------cooperative agent can be exploited by a defector.----------------from ""consequentialist conditional cooperation in social dilemmas with imperfect information"":--------to demonstrate this we follow the method of tampuu et al. (2017) to construct a version of atari pong --------which makes the game into a social dilemma. in what we call the pong players dilemma (ppd) when an agent --------scores they gain a reward of 1 but the partner receives a reward of 2. thus, in the ppd the only (jointly) winning--------move is not to play, but selfish agents are again tempted to defect and try to score points even though--------this decreases total social reward. we see that ccc is a successful, robust, and simple strategy in this--------game.","the reviewers found numerous issues in the paper, including unclear problem definitions, lack of motivation, no support for desiderata, clarity issues, points in discussion appearing to be technically incorrect, restrictive setting, sloppy definitions, and uninteresting experiments. unfortunately, little note of positive aspects was mentioned. the authors wrote substantial rebuttals, including an extended exchange with reviewer2, but this had no effect in terms of score changes. given the current state of the paper, the committee feels the paper falls short of acceptance in its current form.","it then continues explaining how to achieve these desiderata, but at this point it is impossible to follow a coherent argument without understanding why are the authors making these strong assumptions about the problem they are trying to solve, and why.","i would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.----------------it should also be noted that i was asked to review another iclr submission entitled ""consequentialist conditional cooperation in--------social dilemmas with imperfect information--------"" which amazingly introduced the same ""pong players dilemma"" game as in this paper.","about the first point, it does not present a clear problem definition.","about the first point, it does not present a clear problem definition.","the paper continues defining some joint behavior (e.g. cooperative policies), but then construct arguments for individual policy deviations, including elements like \pi_a and \pi_2^{a_k} that, as you see, a is used sometimes as subindex and sometimes as supperindex.","however, a fully--------cooperative agent can be exploited by a defector.----------------from ""consequentialist conditional cooperation in social dilemmas with imperfect information"":--------to demonstrate this we follow the method of tampuu et al. (2017) to construct a version of atari pong --------which makes the game into a social dilemma.",the idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains.,"i would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.----------------it should also be noted that i was asked to review another iclr submission entitled ""consequentialist conditional cooperation in--------social dilemmas with imperfect information--------"" which amazingly introduced the same ""pong players dilemma"" game as in this paper.",0.1587301587301587,0.0161290322580645,0.0952380952380952,0.0952380952380952,0.1702127659574468,0.0,0.0992907801418439,0.0992907801418439,0.0824742268041237,0.0210526315789473,0.0618556701030927,0.0618556701030927,0.0824742268041237,0.0210526315789473,0.0618556701030927,0.0618556701030927,0.09375,0.0158730158730158,0.0625,0.0625,0.1353383458646616,0.0,0.0902255639097744,0.0902255639097744,0.0792079207920792,0.0,0.0792079207920792,0.0792079207920792,0.1702127659574468,0.0,0.0992907801418439,0.0992907801418439,8.304327011108398,5.023373603820801,7.457691669464111,7.247946262359619,10.032943725585938,7.457691669464111,7.247946262359619,6.821824073791504,0.9369945310381574,0.9495617984650127,0.9231795512616381,0.8236739191443317,0.8176498552129953,0.9408336676093128,0.945780045415252,0.9512026542851763,0.0916910804986976,0.945780045415252,0.9512026542851763,0.09169121454685104,0.9417068862969408,0.9526079398253817,0.2885079869754463,0.6769900941517234,0.3743957141610421,0.2794297309220272,0.9335859273670534,0.941890958726938,0.911412864723516,0.8236739191443317,0.8176498552129953,0.9408336676093128
183,https://openreview.net/forum?id=rJPcZ3txx,"the paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for cnns.----------------the first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.----------------the second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. the authors are very methodical in how they build the model and evaluate it very thoroughly.----------------it seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. this could make for a nice direction for future work.----------------one point that is missing is some discussion of how transferable the performance model is to gpus. this would make the technique easier to adopt broadly.----------------other areas for improvement: the points in figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5). this paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass. as i understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity). they evaluate their method on both alexnet and googlenet as well as on various platforms. the authors make code available online. the paper is well written and does a good job of putting this work in the context of past model reduction techniques.----------------my main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work. the authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.","while the core ideas explored in this paper are quite limited in algorithmic novelty (e.g., the direct sparse convolutions), the reviewers largely feel that the paper is well written, experiments are carefully done on multiple architectures and system issues are discussed in-depth. given the interest in the iclr community around performance characterization and acceleration of cnns in particular, this paper offers an interesting perspective.","this would make the technique easier to adopt broadly.----------------other areas for improvement: the points in figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5).","the paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for cnns.----------------the first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.----------------the second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains.","the paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for cnns.----------------the first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.----------------the second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains.","the paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for cnns.----------------the first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.----------------the second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains.","this would make the technique easier to adopt broadly.----------------other areas for improvement: the points in figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5).","this would make the technique easier to adopt broadly.----------------other areas for improvement: the points in figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5).",the authors make code available online.,"the paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for cnns.----------------the first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.----------------the second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains.",0.2290076335877862,0.0155038759689922,0.1526717557251908,0.1526717557251908,0.2424242424242424,0.0153846153846153,0.1515151515151515,0.1515151515151515,0.2424242424242424,0.0153846153846153,0.1515151515151515,0.1515151515151515,0.2424242424242424,0.0153846153846153,0.1515151515151515,0.1515151515151515,0.2290076335877862,0.0155038759689922,0.1526717557251908,0.1526717557251908,0.2290076335877862,0.0155038759689922,0.1526717557251908,0.1526717557251908,0.0277777777777777,0.0,0.0277777777777777,0.0277777777777777,0.2424242424242424,0.0153846153846153,0.1515151515151515,0.1515151515151515,6.9284491539001465,6.9284491539001465,16.65447998046875,16.65447998046875,6.9284491539001465,16.65447998046875,16.65447998046875,5.021022796630859,0.9694354018420152,0.960160100760967,0.3366871266233957,0.9931082024741593,0.9913901283419811,0.9403927063185341,0.9931082024741593,0.9913901283419811,0.9403927868938667,0.9931082024741593,0.9913901283419811,0.9403927868938667,0.9694354018420152,0.960160100760967,0.33668699563608145,0.9694354018420152,0.960160100760967,0.3366871266233957,0.9560719448323357,0.978890214002998,0.6708963052573632,0.9931082024741593,0.9913901283419811,0.9403927063185341
184,https://openreview.net/forum?id=rJbbOLcex,"this paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. the authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. there is also some analysis as to topics learned by the model and its ability to generate text. overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. i have 2 potentially major questions i would ask the authors to address:----------------1 - lda topic models make an exchangability (bag of words) assumption. the discussion of the generative story for topicrnn should explicitly discuss whether this assumption is also made. on the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. not clear how this clean exposition of the generative model relates to what is actually done. in the generating sequential text section its clear the topic model cant generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. this needs to be shown in the paper and made clear to have a complete paper.------------------------2 - the topic model only allows for linear interactions of the topic vector theta. it seems like this might be required to keep the generative model tractable but seems like a very poor assumption. we would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why its not such a bad assumption as one might imagine)----------------------------------------figure 2 colors very difficult to distinguish. this paper presents topicrnn, a combination of lda and rnn that augments traditional rnn with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. --------experiments on two tasks are performed: language modeling on ptb, and sentiment analysis on imbd. --------the authors show that topicrnn outperforms vanilla rnn on ptb and achieves sota result on imdb.----------------some questions and comments:--------- in table 2, how do you use lda features for rnn (rnn lda features)? --------- i would like to see results from lstm included here, even though it is lower perplexity than topicrnn. i think it's still useful to see how much adding latent topics close the gap between rnn and lstm.--------- the generated text in table 3 are not meaningful to me. what is this supposed to highlight? is this generated text for topic ""trading""? what about the imdb one?--------- how scalable is the proposed method for large vocabulary size (>10k)?--------- what is the accuracy on imdb if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). i think this is a fairer comparison to bow, lda, and svm methods presented as baselines. this work combines a lda-type topic model with a rnn and models this by having an additive effect on the predictive distribution via the topic parameters. a variational auto-encoder is used to infer the topic distribution for a given piece of text and the rnn is trained as a rnnlm. the last hidden state of the rnnlm and the topic parameters are then concatenated to use as a feature representation.----------------the paper is well written and easy to understand. using the topic as an additive effect on the vocabulary allows for easy inference but intuitively i would expect the topic to affect the dynamics too, e.g. the state of the rnn. the results on using this model as a feature extractor for imdb are quite strong. is the rnn fine-tuned on the labelled imdb data? however, the results for ptb are weaker. from the original paper, an ensemble of 2 lstms is able to match the topicrnn score. this method of jointly modelling topics and a language model seems effective and relatively easy to implement.----------------finally, the imdb result is no longer state of the art since this result appeared in may (miyato et al., adversarial training methods for semi-supervised text classification).----------------some questions:--------how important is the stop word modelling? what do the results look like if l_t = 0.5 for all t?----------------it seems surprising that the rnn was more effective than the lstm. was gradient clipping tried in the topiclstm case? do grus also fail to work?----------------it is also unfortunate that the model requires a stop-word list. is the link in footnote 4 the one that is used in the experiments?----------------does factoring out the topics in this way allow the rnn to scale better with more neurons? how reasonable does the topic distribution look for individual documents? how peaked do they tend to be? can you show some examples of the inferred distribution? the topics look odd for imdb with the top word of two of the topics being the same: 'campbell'. it would be interesting to compare these topics with those inferred by lda on the same datasets.----------------minor comments:--------below figure 2: ghz -> gb--------\gamma is not defined.","in the generative model in sec 3 you're drawing a k-dimensional ""topic vector"" theta from a normal distribution n(0,i). if these are intended to be topic proportions, how is a normal distribution suited to this, since the components of theta can be negative? i would have thought maybe a logistic-normal distribution would be more suitable for this?",we would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document.,"this method of jointly modelling topics and a language model seems effective and relatively easy to implement.----------------finally, the imdb result is no longer state of the art since this result appeared in may (miyato et al., adversarial training methods for semi-supervised text classification).----------------some questions:--------how important is the stop word modelling?",in the generating sequential text section its clear the topic model cant generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification.,this paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models.,this paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models.,"this paper presents topicrnn, a combination of lda and rnn that augments traditional rnn with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word.",the discussion of the generative story for topicrnn should explicitly discuss whether this assumption is also made.,"this method of jointly modelling topics and a language model seems effective and relatively easy to implement.----------------finally, the imdb result is no longer state of the art since this result appeared in may (miyato et al., adversarial training methods for semi-supervised text classification).----------------some questions:--------how important is the stop word modelling?",0.2325581395348837,0.0,0.1395348837209302,0.1395348837209302,0.2413793103448275,0.0,0.1206896551724137,0.1206896551724137,0.1538461538461538,0.0674157303370786,0.1318681318681318,0.1318681318681318,0.1463414634146341,0.0,0.073170731707317,0.073170731707317,0.1463414634146341,0.0,0.073170731707317,0.073170731707317,0.2083333333333333,0.0,0.1041666666666666,0.1041666666666666,0.1772151898734177,0.0259740259740259,0.1012658227848101,0.1012658227848101,0.2413793103448275,0.0,0.1206896551724137,0.1206896551724137,9.341991424560549,15.748290061950684,15.748289108276367,8.428607940673828,12.992475509643556,9.634115219116213,8.428607940673828,8.930139541625977,0.9467900938202836,0.9517740410102931,0.9416173663218889,0.32504343602316443,0.37964047521222366,0.9002771796760276,0.951726117844148,0.9473561769442809,0.8224664363816595,0.9704131303224212,0.9702182554670596,0.9498880309203075,0.9704131303224212,0.9702182554670596,0.9498879683281886,0.9759545235752282,0.9712932682572297,0.9398110167835045,0.7921538094161291,0.9028526568979846,0.8709838233174847,0.32504343602316443,0.37964047521222366,0.9002771796760276
185,https://openreview.net/forum?id=rJe8pxSFwr,"this paper discusses various approaches to predict missing values in the input (filling/inpainting task).-----they define an energy function equal to the squared l2 distance between the input and its reconstruction by various kinds of neural nets. they show slightly better performance compared to a pca-based baseline.----------positive things about this work------ the topic is interesting------ the last application is interesting (water temperature prediction) ----------negative things about this work------ this work is very poorly written and lacks sufficient clarity. this work needs a major rewrite. i do not even know where to start, but to give an example:-----1st sentence of the abstract reads: for numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often irregular spacetime sampling patterns and large missing data rates., a sentence that misses the verb.-----2nd sentence of the abstract reads: ""these sampling properties is which is not grammatically correct-----and so on so forth. speaking of which, the authors make excessive use of ""...."".------ because of the lack of clarity throughout the paper, i have had hard time figuring out what exactly the authors do. from the limited understanding i got after reading this draft twice, i think they consider a few different variants of auto-encoder and a ""log-prior""/energy function equal to the squared l2 distance between input and its reconstruction. however, this formulation has barely any novelty. for instance, see old work like:-----s. roth and m. j. black, fields of experts: a framework for learning image priors, in proc. of the ieee conference on computer vision and pattern recognition (cvpr), vol. 2, san diego, california, june 2005, pp. 860867-----where they used a different log-prior, but essentially the same optimization process. if the novelty is the use of auto-encoders, then the comparison should be against methods that do not auto-encode (like the above or more modern versions of it).------ several choices made by the authors seem not well motivated. for instance, it's written that computing gradients is too expensive and therefore these are replaced by the g network. however, doesn't the g network also need gradients to be updated?------ the terminology is not standard and confusing. hidden state usually refers to the encoder output, not to the decoder output.------ the authors never introduce the metrics they use, reconstruction and interpolation scores.------ in general, the motivation is unclear. why is it a problem if the data does not come from a grid? in vision, inpainting on unconstrained masks has been standard for decades. -----more recently, transformer architectures and gnns seem quite good at representing sets and graph structured data.-----so considering this context, the current motivation provided by the authors needs some refinement. in fact, the authors could consider building on top of these other more modern approaches. the paper proposes an end-to-end learning framework for interpolation problems, motivated by problems such as irregularly-sampled images or time-series.----------it was not clear after reading the paper where the key novelty of the proposal lies. the energy formulations for u, namely an autoencoder and gibbs model, are not to my knowledge new in this context. i gather the novelty must then be in section 3.2, which uses a neural-network based interpolation scheme. this builds on an existing update scheme (equation 8), but replaces the full operator  with an iterative update. the key idea is then to replace the full gradient for the energy function with a learned function, based on a cnn. while this is not my area of expertise, i am not sure as to technical significance or novelty of such a proposal.----------the paper emphasises their view of an interpolation operator i, which is a little confusing in an autoencoder context. typically, we pick \hat{x} to be the solution attaining minimal squared error compared to the reference y. it was not clear why one needs a further invocation of a minimisation between y^i and i(u, y^i, ^i).----------the writing could be improved, as there are a number of grammatical issues, as well as missing section or equation #'s. in terms of presentation, the introduction delves far too soon into a detailed discussion of related work in the area. i would suggest instead to provide a crisp high-level overview of the limitations of existing work, and how they are overcome in the present paper.","this work looks at ways to fill in incomplete data, through two different energy terms.-----reviewers find the work interesting, however it is very poorly written and nowhere near ready for publication. this comes on top of poorly stated motivation and insufficient comparison to prior work.-----authors have chosen not to answer the reviewers' comments.-----we recommend rejection.","typically, we pick \hat{x} to be the solution attaining minimal squared error compared to the reference y. it was not clear why one needs a further invocation of a minimisation between y^i and i(u, y^i, ^i).----------the writing could be improved, as there are a number of grammatical issues, as well as missing section or equation #'s.","the paper proposes an end-to-end learning framework for interpolation problems, motivated by problems such as irregularly-sampled images or time-series.----------it was not clear after reading the paper where the key novelty of the proposal lies.","the paper proposes an end-to-end learning framework for interpolation problems, motivated by problems such as irregularly-sampled images or time-series.----------it was not clear after reading the paper where the key novelty of the proposal lies.",this paper discusses various approaches to predict missing values in the input (filling/inpainting task).-----they define an energy function equal to the squared l2 distance between the input and its reconstruction by various kinds of neural nets.,this paper discusses various approaches to predict missing values in the input (filling/inpainting task).-----they define an energy function equal to the squared l2 distance between the input and its reconstruction by various kinds of neural nets.,"i do not even know where to start, but to give an example:-----1st sentence of the abstract reads: for numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often irregular spacetime sampling patterns and large missing data rates., a sentence that misses the verb.-----2nd sentence of the abstract reads: ""these sampling properties is which is not grammatically correct-----and so on so forth.","-----more recently, transformer architectures and gnns seem quite good at representing sets and graph structured data.-----so considering this context, the current motivation provided by the authors needs some refinement.","the paper proposes an end-to-end learning framework for interpolation problems, motivated by problems such as irregularly-sampled images or time-series.----------it was not clear after reading the paper where the key novelty of the proposal lies.",0.1512605042016806,0.0,0.1008403361344537,0.1008403361344537,0.1649484536082474,0.0,0.1030927835051546,0.1030927835051546,0.1649484536082474,0.0,0.1030927835051546,0.1030927835051546,0.2083333333333333,0.0,0.1458333333333333,0.1458333333333333,0.2083333333333333,0.0,0.1458333333333333,0.1458333333333333,0.1860465116279069,0.0,0.0930232558139534,0.0930232558139534,0.2045454545454545,0.0,0.1136363636363636,0.1136363636363636,0.1649484536082474,0.0,0.1030927835051546,0.1030927835051546,4.5978217124938965,12.758411407470703,12.758410453796388,6.775884628295898,5.560998439788818,6.775884628295898,6.775886535644531,6.2284345626831055,0.10672579103932704,0.25434485988065686,0.8706735653115505,0.9721265595199367,0.9681245787299594,0.9327370003395197,0.9721265595199367,0.9681245787299594,0.9327370003395197,0.9553263749137894,0.9546612889997932,0.9524022286363821,0.9553263749137894,0.9546612889997932,0.9524022463764529,0.9738716162346117,0.9707735014160341,0.06091722728764426,0.955880348941499,0.96063272885093,0.9392075160369434,0.9721265595199367,0.9681245787299594,0.9327370361560641
186,https://openreview.net/forum?id=rJeB36NKvB,"this paper studies whether and how position information is encoded in cnns. on top of vgg and resnet, it constructs an additional posenet to recover position information. by analyzing how well posenet recovers position information, this paper provides several interesting findings: cnns indeeds encode position information and zero-padding is surprisingly important here.----------[pros]----------1. i enjoy reading this paper: probing cnns is not easy, but it designs experiments in an intuitive way and rigorously performs ablation studies and analysis.-----2. the observations and findings are interesting and helpful to the community.----------[cons]----------1. a weakness of this paper is that it ignores the impact of training process while probing posenet: in table 1, vgg/resnet perform much better than posenet, but it could be because vgg/resnet is easier to train (kind of fine-tuning posenet only) than posenet. would be nice to show the training curve and train posenet longer.-----2. zero-padding seems to play a surprisingly important role in encoding position information (table 5), but it is still unclear why it is so important and how it helps.----------overall, i think this is a good paper. this paper studied the problem of the encoded position information in convolution neural networks. the hypothesis is that cnn can implicitly learn to encode the position information. the author tests the hypothesis with lots of experiments to show how and where the position information is encoded.----------clarity:-----this paper is interesting for me. it tries to understand the encoded position information that is easily ignored by researchers. i like adequate experiments with learned position information and position illustrations.----------experiments:-----1. the paper mainly discussed the zero-padding and found it is the source of position information. how about other padding modes like constant-padding, reflection-padding, and replication-padding?----------2. the partial convolution-based padding method [1] (padded regions are masked out) shows that its recognition accuracy is higher than the traditional zero-padding approach. can you help investigate where the position information comes from for this case?----------[1] partial convolution based padding, https://arxiv.org/pdf/1811.11718.pdf.---------------some of my concerns are well addressed by the author thus i upgrade my score.","this paper analyzes the weights associated with filters in cnns and finds that they encode positional information (i.e. near the edges of the image). a detailed discussion and analysis is performed, which shows where this positional information comes from. ----------the reviewers were happy with your paper and found it to be quite interesting. the reviewers felt your paper addressed an important (and surprising!) issue not previously recognized in cnns.","can you help investigate where the position information comes from for this case?----------[1] partial convolution based padding, https://arxiv.org/pdf/1811.11718.pdf.---------------some of my concerns are well addressed by the author thus i upgrade my score.","by analyzing how well posenet recovers position information, this paper provides several interesting findings: cnns indeeds encode position information and zero-padding is surprisingly important here.----------[pros]----------1.","by analyzing how well posenet recovers position information, this paper provides several interesting findings: cnns indeeds encode position information and zero-padding is surprisingly important here.----------[pros]----------1.",this paper studies whether and how position information is encoded in cnns.,"by analyzing how well posenet recovers position information, this paper provides several interesting findings: cnns indeeds encode position information and zero-padding is surprisingly important here.----------[pros]----------1.",this paper studies whether and how position information is encoded in cnns.,"can you help investigate where the position information comes from for this case?----------[1] partial convolution based padding, https://arxiv.org/pdf/1811.11718.pdf.---------------some of my concerns are well addressed by the author thus i upgrade my score.","by analyzing how well posenet recovers position information, this paper provides several interesting findings: cnns indeeds encode position information and zero-padding is surprisingly important here.----------[pros]----------1.",0.2,0.0555555555555555,0.109090909090909,0.109090909090909,0.2857142857142857,0.0833333333333333,0.183673469387755,0.183673469387755,0.2857142857142857,0.0833333333333333,0.183673469387755,0.183673469387755,0.2195121951219512,0.075,0.1951219512195121,0.1951219512195121,0.2857142857142857,0.0833333333333333,0.183673469387755,0.183673469387755,0.2195121951219512,0.075,0.1951219512195121,0.1951219512195121,0.2,0.0555555555555555,0.109090909090909,0.109090909090909,0.2857142857142857,0.0833333333333333,0.183673469387755,0.183673469387755,16.744136810302734,17.141712188720703,16.744136810302734,17.141712188720703,7.327362060546875,17.141712188720703,17.141712188720703,7.327361106872559,0.9799678147101246,0.9767452109372927,0.7934665123710805,0.9835447102751023,0.9797737615788314,0.8039436923456476,0.9835447102751023,0.9797737615788314,0.8039431202466539,0.971477722929902,0.9715543006545858,0.9011249821656965,0.9835447102751023,0.9797737615788314,0.8039436923456476,0.971477722929902,0.9715543006545858,0.9011248822753477,0.9799678147101246,0.9767452109372927,0.7934659640950433,0.9835447102751023,0.9797737615788314,0.8039436923456476
187,https://openreview.net/forum?id=rJeQYjRqYX,"this paper proposes a method for the detection of adversarial examples based on identification of critical paths (called ""effective paths"") in dnn classifiers. borrowing from the analysis of execution paths of control-flow programs, the authors use back-propagation from the neuron associated from the final class decision to identify a minimal subset of input synapses accounting for more than a threshold proportion (""theta"") of the total input weight. the identification process is then recursively applied at the preceding layer for those neurons associated with the selected minimal subset of synapses, forming a tree of synapses (the ""effective path""). the authors then propose to compare the effective paths (actually, unions of paths) of different examples using simple structural dissimilarity measures, which they extend to allow comparison to a typical (aggregated) path for multiple examples drawn from a common class.----------------in their experimentation with their measure, they noted that examples generated by a number of adversarial attacks tend to be less similar to their first-ranked estimated class than normal examples are to their own first-ranked classes. similarly, they note that these same adversarial attacks tend to be *more* similar to their second-ranked classes than normal examples are to their own second-ranked classes (as the authors point out, this is likely due to the increased likelihood of the second-ranked class of adversarial examples being the true class for the original example from which it was perturbed). the authors then propose the difference between these two similarities (that is, first-ranked dissimilarity minus second-ranked dissimilarity) as a characterization of adversarial examples.----------------the idea of using critical paths in the dnn to detect adversarial examples is interesting, and the authors deserve credit for showing that these critical paths (as defined in this paper) do show differences from those of normal examples. however, the originality of the approach is undercut by the recent work of wang et al. (cvpr, 2018), which the authors acknowledge only in the discussion of experimental results. although the details are different as to how critical paths are identified, and how adversarial examples can be detected using them, the strategies are definitely related - a more detailed explanation of this should have been given in the introduction of the paper. more troubling is the fact that a head-to-head experimental comparison is not provided, neither with wang et al. nor with other state of the art detectors, other than a qualitative assessment of the capabilities of some detectors in table 1. note that even this qualitative discussion does not include some of the recent detection approaches, such as bpda (athalye et al., icml 2018) or lid (ma et al., iclr 2018).----------------the question of how best to define critical paths and their similarities is still very much open - the authors' approach is rather simplistic and straightforward. for example, is their similarity measure biased towards the contributions from early layers? can a layer-by-layer weighting of contributions improve the performance?----------------the authors do not always interpret their own experimental results correctly. for example, their results in figures 7i and 7j don't really support their conclusion that performance ""remains almost unchanged"" when theta is in the range 0.5 - 1.0. also, figure 4 does not show that their effective path similarity is not *directly* ""a great metric to distinguish between normal and adversarial"" examples, because a large proportion of adversarial examples have scores that fall in the typical range for normal examples (however, there are differences in tendency which can be exploited, as the authors do show).----------------the organization of the paper is in some need of improvement. for example, the discussion of densities of ""effective paths"" (section 2) comes well before the details of the choice of threshold value theta used to generate them (section 4.1).----------------to summarize:----------------pros:--------* a good case is made for the use of critical paths as a way of differentiating adversarial examples from normal examples.--------* the reported improvement in similarity of adversarial examples with respect to their second-ranked classes is particularly intriguing.--------* the paper is generally well written and easy to follow.----------------cons:--------* the experimental treatment is insufficient; in particular, a more carefully considered experimental justification is needed with respect to other detection strategies.--------* the question of how best to define critical paths and their similarities is still very much open.--------* the authors do not always interpret their own experimental results correctly.--------* the organization of the paper is in some need of improvement. the authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.--------overall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.--------the experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesnt really show how the proposed work is adding value to the field.--------it lacks the experimental comparison with previous methods but only include discussion in texts.--------this paper could turn out to be a stronger paper but it is not ready yet.----------------below are some more detailed comments.--------1) the authors motivate by stating that the vulnerability of nn to input perturbations is due to the lack of interpretability (section introduction & abstract). i can understand that we want more interpretability, and we want less vulnerability, but i cant agree that vulnerability is caused by lack of interpretability. also, the authors are trying to accomplish both tasks, interpretability and adversary detection, by showing data analysis of how the findings coincide with prior knowledge (eg. class of digit 1 is the most different from other classes in mnist task), and by showing detecting adversary images. however, neither has valid quantitative comparison with previous work; actually for the interpretability topic, the authors didnt really provide a tool or a generalizable method. thus, i would suggest to choose one of the two topics (ie. adversarial image detection) and focus on it by adding thorough comparison with other methods; in the discussion and result section, include the interpretability analysis to justify why the proposed adversary detection method is behaving in certain ways.----------------2) one topic that is missing from the paper is the time complexity of the proposed method. at a nave estimate, it would require tracking and finding the minimum set of effective neurons with threshold \theta and thus per instance, at least o(m log m) is required at prediction phase, where m is the number of features; for n instances, the asymptotic complexity is o(nm log m) how does it compare to the other adversary detection methods?----------------3) page 3 mentions that the work for critical routing path (wang et al. 2018) requires re-training for every image; this statement is not really true without more context. also authors discuss this work again very briefly in page 8 due to the high similarity in methods and motivation with the proposed method, but the authors dont show any quantitative comparison. after all, both methods are trying to identify neurons that contribute the most to the prediction, some more concrete comparison would be nice.----------------4) page 3 mentions that the derived overall effective path is highly sparse compared to the original network and the effective path density for five trained models ranges from 13% to 42% which conforms with the 80% claim from another paper. together with the other similar statements, it would be really nice to note what \theta is used for such statements; how does such statement change with different \theta. also some discussion would be nice about what such sparsity implies. specifically, does the sparsity suggest the opportunity for feature selection, or does it suggest a way for detecting overfitting?----------------5) page 5 shows the path similarity between the normal and the adversary examples; from the figure 5a and 5b, we can see the on the first layer, the mean deviate between normal and others but why the last layer they almost reach to the same point? it seems it is the middle layer that distinguish the normal from the adversary examples the most. some more discussion would be good.----------------6) some justification of why \theta=0.5 is chosen would be good on page 6.----------------7) on page 7, the authors are discussing the performance of the proposed method, however, there is no really comparison with other methods. but rather, the authors stated better accuracy, auc is better by comparing different evaluation scenarios. i dont find such discussion helpful in showing the contribution of the proposed method. also in the parameter sensitivity, it would be nice to add the analysis for the effective path density and see if it still conforms with the 80% claim with different \theta.----------------8) page 1, need to add citations for the statement  and even outperformed human beings.----------------9) minor issue: page 1 such computer vision should be such as computer vision. this paper proposes a measure (effective path) of which units and weights were most important for classification of a particular input or input class. using the effective path, the authors analyze the overlap between paths across classes for cnns and between adversarially modified and unmodified images. finally, the paper proposes an adversarial defense method based on effective path which detects adversarially manipulated images with high accuracy and generality to a variety of settings. ----------------overall, this paper is interesting and provides several novel observations. the clarity of the exposition is generally good, but can be improved in several places (mentioned below). as for significance, effective path is likely to inform future analyses of neural networks, and the adversarial defense may prove impactful, though ultimately, its impact will depend on if and when the defense is broken. ----------------however, there are several important controls missing from the analysis, several claims which are unsubstantiated, and experimental details are lacking in a few places. as such, in its current form, i can only weakly recommend this paper for acceptance. if in the revision the controls requested below are included, additional evidence is provided for the unsubstantiated claims (or if those claims are toned down), and exposition of missing experimental details is included, id be happy to raise my score. ----------------major points:----------------1) while the observation regarding path specialization is very interesting, one cannot gauge whether or not the degree of overlap observed between class-specific paths signals path specialization or simply high input-to-input path variance (which is similar both within and across classes). in order to distinguish between these possibilities, a measure of intra-class path similarity is necessary. in addition, an experiment similar to that in figure 2 with cifar-10 would be quite helpful in evaluating whether this phenomenon exists in more natural datasets (the imagenet results are difficult to interpret due to the large number of classes).----------------2) several claims in the path specialization section are unsubstantiated. ----------------2a) in particular, the claim that 1 has the highest degree of specialization because of its unique shape is made without evidence as is the similarity between 5 and 8. 6 is also similar to 8 and yet does not show the same similarity in the path specialization. these differences may very well simply be due to chance.----------------2b) the claim that the path specialization in imagenet matches the class hierarchy is made only based on the rough non-linearity of figure 3. please either measure the overlap within and across class categories or soften this claim.----------------3) the similarity analysis for adversarial images is also very interesting, but a comparison between unmodified and randomly perturbed images with matched norms to the adversarially perturbed images is necessary to establish whether this effect is due to noise generally or adversarial noise.--------its unclear how the effective path is calculated when negative weights are involved. further exposition of this aspect would be helpful.----------------minor points/typos: ----------------1) there are several places where confusing concepts are introduced in one paragraph but explained several paragraphs later. in particular, the distinction between synapses and weights is introduced halfway through page 2 but explained on page 3 and the fact that the coefficients for the defense metric are learned is unclear until page 4 even though theyre introduced on page 3.----------------2) typos: ----------------2a) section 1, fourth paragraph: ...and adversarial images, we uncover... should be ...and adversarial images, and we uncover...----------------2b) section 1, fourth paragraph: ...by small perturbation, the network should be ...by small perturbations, the network----------------2c) section 2, first paragraph: ...the black-boxed neural should be ...the black-box neural----------------2d) section 2, first paragraph: in the high level should be at a high level----------------2e) section 4, first paragraph: ...as it does no modify should be ...as it does not modify----------------2f) title, should be ""neural network""?","the paper presents an approach to estimate the ""effective path"" of examples in a network to reach a decision, and consider this to analyze if examples might be adversarial. reviewers think the paper lacks some clarity and experiments. they point to a confusion between interpretability and adversarial attacks, they ask questions about computational complexity, and point to some unsubstanciated claims. authors have not responded to reviewers. overall, i concur with the reviewers to reject the paper.","in addition, an experiment similar to that in figure 2 with cifar-10 would be quite helpful in evaluating whether this phenomenon exists in more natural datasets (the imagenet results are difficult to interpret due to the large number of classes).----------------2) several claims in the path specialization section are unsubstantiated.","the authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.--------overall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.--------the experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesnt really show how the proposed work is adding value to the field.--------it lacks the experimental comparison with previous methods but only include discussion in texts.--------this paper could turn out to be a stronger paper but it is not ready yet.----------------below are some more detailed comments.--------1) the authors motivate by stating that the vulnerability of nn to input perturbations is due to the lack of interpretability (section introduction & abstract).","the authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.--------overall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.--------the experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesnt really show how the proposed work is adding value to the field.--------it lacks the experimental comparison with previous methods but only include discussion in texts.--------this paper could turn out to be a stronger paper but it is not ready yet.----------------below are some more detailed comments.--------1) the authors motivate by stating that the vulnerability of nn to input perturbations is due to the lack of interpretability (section introduction & abstract).","this paper proposes a method for the detection of adversarial examples based on identification of critical paths (called ""effective paths"") in dnn classifiers.","in particular, the distinction between synapses and weights is introduced halfway through page 2 but explained on page 3 and the fact that the coefficients for the defense metric are learned is unclear until page 4 even though theyre introduced on page 3.----------------2) typos: ----------------2a) section 1, fourth paragraph: ...and adversarial images, we uncover... should be ...and adversarial images, and we uncover...----------------2b) section 1, fourth paragraph: ...by small perturbation, the network should be ...by small perturbations, the network----------------2c) section 2, first paragraph: ...the black-boxed neural should be ...the black-box neural----------------2d) section 2, first paragraph: in the high level should be at a high level----------------2e) section 4, first paragraph: ...as it does no modify should be ...as it does not modify----------------2f) title, should be ""neural network""?","the authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.--------overall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.--------the experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesnt really show how the proposed work is adding value to the field.--------it lacks the experimental comparison with previous methods but only include discussion in texts.--------this paper could turn out to be a stronger paper but it is not ready yet.----------------below are some more detailed comments.--------1) the authors motivate by stating that the vulnerability of nn to input perturbations is due to the lack of interpretability (section introduction & abstract).","however, neither has valid quantitative comparison with previous work; actually for the interpretability topic, the authors didnt really provide a tool or a generalizable method.","the authors propose the notion of effective path, for the purpose of identifying neurons that contributes to the predictions and being able to detect adversarial images in the context of image classification.--------overall the paper is well written except that the authors are mixing two highly related but still different topics: explanation and adversary detection so that the motivation is confusing.--------the experimental results indeed show promises that effective path can help understand class similarities and network efficiencies but doesnt really show how the proposed work is adding value to the field.--------it lacks the experimental comparison with previous methods but only include discussion in texts.--------this paper could turn out to be a stronger paper but it is not ready yet.----------------below are some more detailed comments.--------1) the authors motivate by stating that the vulnerability of nn to input perturbations is due to the lack of interpretability (section introduction & abstract).",0.2519685039370078,0.0,0.1574803149606299,0.1574803149606299,0.3070175438596491,0.0265486725663716,0.175438596491228,0.175438596491228,0.3070175438596491,0.0265486725663716,0.175438596491228,0.175438596491228,0.202020202020202,0.0206185567010309,0.101010101010101,0.101010101010101,0.1626794258373205,0.0096618357487922,0.0861244019138756,0.0861244019138756,0.3070175438596491,0.0265486725663716,0.175438596491228,0.175438596491228,0.1386138613861386,0.0,0.0792079207920792,0.0792079207920792,0.3070175438596491,0.0265486725663716,0.175438596491228,0.175438596491228,9.59050464630127,7.206217288970947,12.314945220947266,9.59050464630127,4.635542869567871,9.59050464630127,9.59050464630127,4.206328392028809,0.24562723880862244,0.27933660905615054,0.7230338903369137,0.6601008829117461,0.6533480965109356,0.9285649571444222,0.6601008829117461,0.6533480965109356,0.928564998654933,0.9778049134151245,0.9755198258182213,0.9039131530951526,0.6093892457818181,0.7560747190050182,0.7074386895376227,0.6601008829117461,0.6533480965109356,0.9285649571444222,0.16760088753913532,0.3895775839765346,0.7306045289714856,0.6601008829117461,0.6533480965109356,0.9285649571444222
188,https://openreview.net/forum?id=rJfUCoR5KX,"the paper systematically studies the training of binary neural networks, where binary in this case refers to single bit weight elements in the network. in particular, different existing training methods are tested and compared for training both mlps and cnns.----------------the main findings of the paper are:--------- using methods such as adagrad, adadelta, rmsprop and adam yields better performance than simpler momentum-based methods such as vanilla momentum and nesterov momentum, which in turn are much better than vanilla sgd--------- when training binary models, it is common to clip weights and/or gradients for the proxy weights in the network. in the paper it is however shown that these methods hinder using a fast learning rate in the beginning of training, while the methods are required in later stages of training in order to achieve good results--------- pre-training the model with full-precision training works well in speeding up training----------------for a practitioner, the paper presents a very useful reference for what methods work well when training binary networks. although there are some proposals and hypotheses for reasons behind the results, i see the paper as a review paper of existing methods for training binary networks, showing experiments where the methods are tested using the same benchmark and training procedure in order to give a fair comparison.----------------as a practical guide, the paper therefore has clear value. what is lacking compared to typical iclr papers is rigorously presenting new findings. the authors present a hypothesis for why different batch sizes are needed in the beginning compared to the end of training, but i found neither the justification nor the results very convincing with respect to the hypothesis. the way i see it, the actual novel proposals that are made in the paper are two two-stage training methods: one in which the tricks of weight and gradient clipping are only used towards the end of training, and one where the first stage of training is done using a full precision model. it is however quite well known that some training schemes with different stages can lead to improved performance: for instance with adam, even if it is an adaptive method, lowering the learning rate towards the end of training is often beneficial. it might therefore be fair to compare the methods to other multi-stage training methods. in addition, i could not find the training curves or final performance figures of the method where clipping is only activated towards the end of training.----------------to put it all together, the paper is clearly useful for the community as it provides a useful summary of the performance of different methods for training binary neural networks. in addition, it presents two two-stage training schemes that seem to make training even faster. what the paper lacks is rigorous theoretical justifications and clearly novel ideas.----------------small comments:--------- how are the training lengths decided for the different methods? if i am not mistaken, in figure 2, it seems like the sgd and momentum methods have not yet converged when training is halted. is there a budget for wall clock time or is early stopping used or something similar? considering the nature of the paper, i would see this kind of decisions as important to report.--------- in the abstract, you might want to refer to binary weights somewhere. based on the abstract it is easy to mix the binary networks in this paper with stochastic binary networks that can also be trained using the ste estimator--------- are the differences in the performances in table 3 statistically significant? the authors made several claims and provide suggestions on training binary networks, however, they are not proved or theoretically analyzed. the empirical verification of the proposed hypothesis was viewed as weak as the only two datasets used are small datasets mnist and cifar-10, and the used network architectures are also limited. much more rigorous and thorough testing is required for an empirical paper which proposes new claims. ----------------take the first claim ""end-to-end training of binary networks crucially relies on the optimiser taking advantage of second moment gradient estimates"" as an example. as it is known that choice of optimizer is highly dependent on the specific dataset and network structure, it is not convincing to jump to this conclusion using the observations on two small datasets and limited network architectures. e.g, many binarization papers use momentum for imagenet dataset with residual networks. does adam also outperforms momentum in this case? similarly, it is also hard for me to judge whether the other conclusions made about weight/gradient clipping, the momentum in batch normalization and learning rate, are correct or not.----------------some minor issues are:--------1. in figure 4, different methods are not run to convergence, and the comparison may not be fair.--------2. the second paragraph in section 4: ""it can be seen that not clipping weights when learning rates are large can completely halt the optimisation (red curve in figure 5)."" however, in figure 5, the red curve is ""clipping gradients"", which one is correct?--------3. the authors propose a recipe for faster training of binary networks, is there experiments supporting that training networks with the proposed recipe is faster than the original counterpart?","the paper summarizes existing work on binary neural network optimization and performs an empirical study across a few datasets and neural network architectures. i agree with the reviewers that this is a valuable study and it can establish a benchmark to help practitioners develop better binary neural network optimization techniques. ps: how about ""an empirical study of binary neural network optimization"" as the title?","similarly, it is also hard for me to judge whether the other conclusions made about weight/gradient clipping, the momentum in batch normalization and learning rate, are correct or not.----------------some minor issues are:--------1. in figure 4, different methods are not run to convergence, and the comparison may not be fair.--------2.","in the paper it is however shown that these methods hinder using a fast learning rate in the beginning of training, while the methods are required in later stages of training in order to achieve good results--------- pre-training the model with full-precision training works well in speeding up training----------------for a practitioner, the paper presents a very useful reference for what methods work well when training binary networks.","in addition, i could not find the training curves or final performance figures of the method where clipping is only activated towards the end of training.----------------to put it all together, the paper is clearly useful for the community as it provides a useful summary of the performance of different methods for training binary neural networks.","the paper systematically studies the training of binary neural networks, where binary in this case refers to single bit weight elements in the network.","the authors propose a recipe for faster training of binary networks, is there experiments supporting that training networks with the proposed recipe is faster than the original counterpart?","in the paper it is however shown that these methods hinder using a fast learning rate in the beginning of training, while the methods are required in later stages of training in order to achieve good results--------- pre-training the model with full-precision training works well in speeding up training----------------for a practitioner, the paper presents a very useful reference for what methods work well when training binary networks.","the second paragraph in section 4: ""it can be seen that not clipping weights when learning rates are large can completely halt the optimisation (red curve in figure 5).""","in the paper it is however shown that these methods hinder using a fast learning rate in the beginning of training, while the methods are required in later stages of training in order to achieve good results--------- pre-training the model with full-precision training works well in speeding up training----------------for a practitioner, the paper presents a very useful reference for what methods work well when training binary networks.",0.1538461538461538,0.0,0.0854700854700854,0.0854700854700854,0.253731343283582,0.0151515151515151,0.1194029850746268,0.1194029850746268,0.25,0.0508474576271186,0.1666666666666666,0.1666666666666666,0.2954545454545454,0.0930232558139534,0.2045454545454545,0.2045454545454545,0.2391304347826087,0.0444444444444444,0.1739130434782608,0.1739130434782608,0.253731343283582,0.0151515151515151,0.1194029850746268,0.1194029850746268,0.1075268817204301,0.0219780219780219,0.086021505376344,0.086021505376344,0.253731343283582,0.0151515151515151,0.1194029850746268,0.1194029850746268,11.299428939819336,13.853326797485352,12.983028411865234,11.299428939819336,8.916067123413086,8.784899711608887,11.299428939819336,6.268589973449707,0.4199185236380695,0.6286964458906547,0.9194923229917241,0.9590758424900104,0.9516581360448932,0.9353322100112755,0.9753133732921588,0.9673318302286991,0.9243184648011181,0.9707920986136145,0.97433843860392,0.9183958071450209,0.6731852010977369,0.8098896684504256,0.8913106148110789,0.9590758424900104,0.9516581360448932,0.9353322100112755,0.3460976906399687,0.50854702943445,0.8940954960126243,0.9590758424900104,0.9516581360448932,0.9353322100112755
189,https://openreview.net/forum?id=rJgD2ySFDr,"this paper is out of my research area. i could understand that the paper studies the message transformation with bandwidth-limited channels. it seems naturally the message transformation could be represented as a autoencoder model. the paper proposed variational model for this problem and it seems to me the paper employs the popular models in neural networks for example vae, etc. technically, what's new of this paper? was it the auxiliary variable decoders? is it that this class of algorithms/models firstly applied to this problem domain? to be honest the paper mentioned most of the terminologies in ml and seems that the paper wanted to connect to them, for example, elbo, vae, gan, re-parameterization, etc. the paper provides experimental results on the designed model for bandwidth-limited channel. this paper focuses on transmitting messages reliably by learning joint coding with the bandwidth-limited channel. the authors justify joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators. their experiments show the advantage of their design decisions via improved distoration and fid scores.----------pros:----------1. this paper is clearly written and well-structured in logic. for example, the authors use figures 1 and 2 assist readers to catch the difference between joint communication system and separate communication system.----------2. this paper gives a reliazation of joint source-channel coding, especially to give auxilary latent variable decoders.----------3. this paper has been verified in both gaussian channel and bandwidth-limited channel. the empirical results show the advantage of joint coding.----------cons:----------1. intuitively, you section 4.3 should be better than section 4.2. however, i don't see any difference or major items to justify this kind of benefits. could you please explain why techniques in section 4.3 can outperform these in section 4.2.----------2. although the authors verified their work on celeba, it seems that the proposed method has very limited applications. if possible, the authors should do more datasets to verify their proposed method, which will be more useful to boarder readers.","there was some support for this paper, but it was on the borderline and significant concerns were raised. it did not compare to the exiting related literature on communications, compression, and coding. there were significant issues with clarity.",the authors justify joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators.,"the paper proposed variational model for this problem and it seems to me the paper employs the popular models in neural networks for example vae, etc.",the paper provides experimental results on the designed model for bandwidth-limited channel.,this paper is out of my research area.,the authors justify joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators.,the authors justify joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators.,"if possible, the authors should do more datasets to verify their proposed method, which will be more useful to boarder readers.","the paper proposed variational model for this problem and it seems to me the paper employs the popular models in neural networks for example vae, etc.",0.0714285714285714,0.0,0.0714285714285714,0.0714285714285714,0.25,0.032258064516129,0.1874999999999999,0.1874999999999999,0.196078431372549,0.0408163265306122,0.1176470588235294,0.1176470588235294,0.0869565217391304,0.0454545454545454,0.0869565217391304,0.0869565217391304,0.0714285714285714,0.0,0.0714285714285714,0.0714285714285714,0.0714285714285714,0.0,0.0714285714285714,0.0714285714285714,0.0677966101694915,0.0,0.0677966101694915,0.0677966101694915,0.25,0.032258064516129,0.1874999999999999,0.1874999999999999,5.514167785644531,5.514167785644531,6.041100025177002,14.359556198120115,5.514167785644531,11.313709259033203,14.359554290771484,6.924870014190674,0.9700655612237742,0.9651093531611451,0.821078232647099,0.9727580414990676,0.9697823095459847,0.8824663353171078,0.9765284202870573,0.9782352605844512,0.9093339222393688,0.9714684334165495,0.9633547011571825,0.8705804399393638,0.9700655612237742,0.9651093531611451,0.821078529109962,0.9700655612237742,0.9651093531611451,0.821078529109962,0.9709288320783999,0.9684170211438797,0.9285902968356347,0.9727580414990676,0.9697823095459847,0.8824663353171078
190,https://openreview.net/forum?id=rJgWiaNtwH,"this paper proposes to modify the elbo loss for vae in order to learn better latent representation of the data. there is a large literature on this topic and the related work section should be written with more care and details. some references are missing:-----[a] fixing a broken elbo by alemi et al.-----[b] learning disentangled joint continuous and discrete representations by dupont-----on a general level, the motivation for this work is not completely clear. as mentioned in the related work section, one could use a more complex prior such as a mixture of gaussians or an architecture like [b] to do clustering. at least, in the experiment section, results shuold be compared with these natrual approaches. it should be easy as [b] for example provided his code and gives results on the same datasets as those used here.-----another remark is that the title is quite misleading. the connection with mmd is very weak in my opinion. indeed modifing the kl term in the loss by using mmd has already been proposed in infovae (last reference of this paper) and in infovae a mmd-vae is also introduced. this connection should be explained and comparison should be given with this work. this paper proposes to modify the kl term () in vae so that instead of requiring q(z|x) to be close to p(z), we require the aggregated posterior \int q(z|x)p(x)dx to be close to p(z). it is argued that the original vae objective leads to posterior collapse because it requires q(z|x) to be close to p(z), leading to similar latent code across different data points. ----------i find the motivation in this paper rather weak, and the authors not sufficiently familiar with prior work. therefore i could not recommend acceptance at this time.----------more specifically, -----1. the fact that minimizers of e_p(x)[kl(q(z|x)||p(z))] are posterior collapse solutions does not imply this term is a bad regularizer, since in general, the optimization process will not end up in such parameters becaues of the data fitting term. in fact, [1,2] showed that this regularizer should have a desirable pruning effect, removing unneeded latent dimensions while keeping the useful dimensions intact. see also the information-theoretic interpretations of elbo in e.g. [3,4]. [2] also gives a more rigorous discussion on the relation between elbo and posterior collapse.-----2. the authors propose to match the marginal distributions in latent space. this idea is explored in previous work such as wasserstein and adversarial auto-encoders. none of this type of work is discussed in the text or included as baselines.-----3. the proposed regularizer, as shown in eq (6), do not fully emulate the behavior of mmd (or any other divergence measure between the aggregated posterior and prior) as claimed: e.g. mu^{(i)}_d can be arbitrarily far from origin. thus justifying (6) as such is not convincing.----------regarding the experiments, there is a notable lack of baselines as mentioned above; also, for sample quality it is desirable if quantative measures (e.g. fid or inception score) are included.----------for future improvements, the proposed objective could be worth exploring if a new justification is found. also, the empirical evaluation should be more thorough.----------# references----------[1] diagnosing and enhancing vae models, iclr 19.-----[2] understanding posterior collapse in generative latent variable models, neurips 19.-----[3] an information theoretic analysis of deep latent-variable models. arxiv 1711.00464.-----[4] elbo surgery: yet another way to carve up the variational evidence lower bound. aabi workshop, nips 16.","the authors introduce a method for disentangling effects of correlated predictors in the context of high dimensional outcomes. while the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of iclr due to its limitations in terms of limited applicability and experiments. the paper will benefit from a revision and resubmission to another venue.","thus justifying (6) as such is not convincing.----------regarding the experiments, there is a notable lack of baselines as mentioned above; also, for sample quality it is desirable if quantative measures (e.g. fid or inception score) are included.----------for future improvements, the proposed objective could be worth exploring if a new justification is found.","this paper proposes to modify the kl term () in vae so that instead of requiring q(z|x) to be close to p(z), we require the aggregated posterior \int q(z|x)p(x)dx to be close to p(z).","this paper proposes to modify the kl term () in vae so that instead of requiring q(z|x) to be close to p(z), we require the aggregated posterior \int q(z|x)p(x)dx to be close to p(z).",this paper proposes to modify the elbo loss for vae in order to learn better latent representation of the data.,"it is argued that the original vae objective leads to posterior collapse because it requires q(z|x) to be close to p(z), leading to similar latent code across different data points.","therefore i could not recommend acceptance at this time.----------more specifically, -----1. the fact that minimizers of e_p(x)[kl(q(z|x)||p(z))] are posterior collapse solutions does not imply this term is a bad regularizer, since in general, the optimization process will not end up in such parameters becaues of the data fitting term.","at least, in the experiment section, results shuold be compared with these natrual approaches.","this paper proposes to modify the kl term () in vae so that instead of requiring q(z|x) to be close to p(z), we require the aggregated posterior \int q(z|x)p(x)dx to be close to p(z).",0.1639344262295081,0.0,0.0983606557377049,0.0983606557377049,0.146788990825688,0.0,0.110091743119266,0.110091743119266,0.146788990825688,0.0,0.110091743119266,0.110091743119266,0.1839080459770115,0.0,0.1379310344827586,0.1379310344827586,0.08,0.0,0.08,0.08,0.192,0.016260162601626,0.1279999999999999,0.1279999999999999,0.074074074074074,0.0253164556962025,0.074074074074074,0.074074074074074,0.146788990825688,0.0,0.110091743119266,0.110091743119266,5.518154621124268,6.290799617767334,14.655255317687988,6.795121192932129,7.55299186706543,6.795121192932129,6.795120716094971,6.256712913513184,0.962313546710727,0.950169901490955,0.32033388242962946,0.9773121147972755,0.9740739266003215,0.8271697476213167,0.9773121147972755,0.9740739266003215,0.82716968599122,0.9732137687937467,0.9702783923895462,0.8769691299681232,0.959766497341849,0.9611503923075158,0.91438855205142,0.9610847615549978,0.9576789465057968,0.7790480260172282,0.9564565762315103,0.9626920760312336,0.8949542094826461,0.9773121147972755,0.9740739266003215,0.8271697476213167
191,https://openreview.net/forum?id=rJl0r3R9KX,"authors proposes a new algorithm for improving the stability of class importance weighting estimation procedure (lipton et al., 2018) with a two-step procedure. the reparamaterization of w using the weight shift theta and lambda allows authors develop a generalization upperbound with terms rely on theta, sigma and lambda. ----------------the problem of label shift is a known important issue in transfer learning but has been understudied.----------------the paper is very well written and the algorithm is well-motivated (introducing regularization to avoid the singularity) and post processing step looks sound (using lambda to de-biase). i only have a few minor questions: ----------------1. how realistic it is to assume we have prior knowledge on theta and sigma_min? ----------------2. if i understand correctly, the only experiment where lambda is varied is sec 3.3? it would be interesting if authors also included bbse in sec 3.3 as a baseline. ----------------3. the authors mentioned in the discussion that the generalization guarantee is obtained with no prior knowledge q/p is needed. however, doesn't theta implicitly represent the knowledge in p/q? --------------------------------------------------------------------------------i have read authors' comments. - the authors consider the problem of learning under label shifts, where the label proportions p(y) and q(y) of the training and test distributions differ, while the conditionals p(x|y) and q(x|y) are equal. they build upon the work by lipton et al. 18 on estimating label proportion weights q(y)/p(y) using the confusion matrix, by proposing an improved estimator with regularization. they show that their estimator provides better weight estimates compared to the unregularized version, and it also gives better prediction accuracies under large label shift scenarios. ----------------- one question i have about this approach is the choice of h in the confusion matrix estimation. since the theory holds for any fixed hypothesis h, is there any guidance on how we should pick h? the authors seem to use the same model class for the weight estimation and predictions in the experiments. how would using a simpler h for weight estimation (e.g., linear logistic regression) affect the results presented here? ----------------- the dirichlet shifts described with only the parameter alpha is not particularly intuitive in conveying the size of shifts. the cifar10 and mnist datasets contain about 6000 examples per class. how would a large shift with alpha=0.01 change the distribution, especially for the smallest class how many samples are retained? this can help the readers judge when the correction of label shifts are helpful. ----------------- to clarify, in the experiments for figure 4 using minority-class shifts, with p=0.001, is it true that there are less than 100 training examples for each of the minority classes in the training set? this seems like a very extreme shift. ----------------- i also have trouble understanding figure 3. resnet-18 should give >90% accuracy on the original cifar10, but in 3b we see accuracies around 75% for small shifts. also how is the f1-score in 3c computed? is it micro-averaged or macro-averaged f1? either way an f1 score below 20% is very low for the unweighted classifier, since resnet-18 should give fairly good classification accuracy on each class separately if it has >90% overall accuracy. ----------------- the paper is quite solid in motivating the need for better weight estimators for reweighing label proportions and their derivations, and manage to show improvements over the unregularized estimator. details on the experiments should be improved to give the readers better ideas on when correcting for label shifts help. right now it looks like it only helps for cases with fairly extreme shifts.","the paper gives a novel algorithm for transfer learning with label distribution shift with provably guarantees. as the reviewers pointed out, the pros include: 1) a solid and motivated algorithm for a understudied problem 2) the algorithm is implemented empirically and gives good performance. the drawback includes incomplete/unclear comparison with previous work. the authors claimed that the code of the previous work cannot be completed within a reasonable amount of time. the ac decided that the paper could be accepted without such a comparison, but the authors are strongly urged to clarify this point or include the comparison for a smaller dataset in the final revision if possible.",----------------the problem of label shift is a known important issue in transfer learning but has been understudied.----------------the paper is very well written and the algorithm is well-motivated (introducing regularization to avoid the singularity) and post processing step looks sound (using lambda to de-biase).,"they show that their estimator provides better weight estimates compared to the unregularized version, and it also gives better prediction accuracies under large label shift scenarios.","----------------2. if i understand correctly, the only experiment where lambda is varied is sec 3.3?","authors proposes a new algorithm for improving the stability of class importance weighting estimation procedure (lipton et al., 2018) with a two-step procedure.","authors proposes a new algorithm for improving the stability of class importance weighting estimation procedure (lipton et al., 2018) with a two-step procedure.",----------------the problem of label shift is a known important issue in transfer learning but has been understudied.----------------the paper is very well written and the algorithm is well-motivated (introducing regularization to avoid the singularity) and post processing step looks sound (using lambda to de-biase).,the cifar10 and mnist datasets contain about 6000 examples per class.,"they show that their estimator provides better weight estimates compared to the unregularized version, and it also gives better prediction accuracies under large label shift scenarios.",0.2709677419354839,0.0522875816993464,0.1419354838709677,0.1419354838709677,0.1037037037037037,0.0,0.0592592592592592,0.0592592592592592,0.064,0.0,0.048,0.048,0.1203007518796992,0.015267175572519,0.0902255639097744,0.0902255639097744,0.1203007518796992,0.015267175572519,0.0902255639097744,0.0902255639097744,0.2709677419354839,0.0522875816993464,0.1419354838709677,0.1419354838709677,0.05,0.0,0.05,0.05,0.1037037037037037,0.0,0.0592592592592592,0.0592592592592592,10.743432998657228,11.966079711914062,11.966079711914062,9.297893524169922,10.743432998657228,7.699456214904785,9.297895431518556,4.885494232177734,0.9880050988404296,0.9847974927275168,0.9504741340149535,0.9624980497499915,0.9653653475256588,0.7999126994000006,0.973131267672853,0.9688475344659774,0.779711863823854,0.9747529215165234,0.974523056562062,0.7754992551837498,0.9747529215165234,0.974523056562062,0.7754990051979699,0.9880050988404296,0.9847974927275168,0.9504741340149535,0.9020786206022098,0.9555697397148977,0.9078557236825822,0.9624980497499915,0.9653653475256588,0.7999126994000006
192,https://openreview.net/forum?id=rJl2E3AcF7,"the present paper proposes a fast approximation to the softmax computation when the number of classes is very large. this is typically a bottleneck in deep learning architectures. the approximation is a sparse two-layer mixture of experts.----------------the paper lacks rigor and the writing is of low quality, both in its clarity and its grammar. see a list of typos below.----------------an example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation. also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.----------------algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.----------------how are the lambda and threshold parameters tuned? the authors mention a validation set, are they just exhaustively explored on a 3d grid on the validation set?----------------the results only compare with shim et al. why only this method? why would it be expected to be faster than all the other alternatives? wouldn't similar alternatives like the sparsely gated moe, d-softmax and adaptive-softmax have chances of being faster?----------------the column ""flops"" in the result seems to measure the speedup, whereas the actual flops should be less when the speed increases. also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.----------------all in all, the results show that the proposed method provides a significant speedup with respect to shim et al., but it lacks comparison with other methods in the literature.----------------a brief list of typos:----------------""sparse mixture of sparse of sparse experts""--------""if we only search right answer""--------""it might also like appear""--------""which is to design to choose the right""--------sparsly--------""will only consists partial""--------""with  is a lasso threshold""--------""an arbitrarily distance function""--------""each 10 sub classes are belonged to one""--------""is also needed to tune to achieve"" in this paper the authors introduce a new technique for softmax inference. in a multiclass setting, the idea is to take the output of a nn and turn it into a gating function to choose one expert. then, given the expert, output a particular category. the first level of sparsity comes from the first expert. the second level of sparsity comes from every expert only outputting a limited set of output categories.----------------the paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. ""search right"" -> ""search for the right"", ""predict next word"" -> ""predict the next word"", ...) in section 3, can you be more specific about the gains in training versus inference time? i believe the results all relate to inference but it would be good to get an overview of the impact of training time as well. you motivate some of the work by the fact that the experts have overlapping outputs. maybe in section 3.7 you can address how often that occurs as well?----------------nits:--------- it wasn't clear how the sparsity percentage on page 3 was defined?--------- can you motivate why you are not using perplexity in section 3.2?","this work proposes a new approximation method for softmax layers with large number of classes. the idea is to use a sparse two-layer mixture of experts. this approach successfully reduces the computation requires on the ptb and wiki-2 datasets which have up to 32k classes. however, the reviewers argue that the work lacks relevant baselines such as d-softmax and adaptive-softmax. the authors argue that they focus on training and not inference and should do worse, but this should be substantiated in the paper by actual experimental results.","also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.----------------algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.----------------how are the lambda and threshold parameters tuned?","also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.----------------all in all, the results show that the proposed method provides a significant speedup with respect to shim et al., but it lacks comparison with other methods in the literature.----------------a brief list of typos:----------------""sparse mixture of sparse of sparse experts""--------""if we only search right answer""--------""it might also like appear""--------""which is to design to choose the right""--------sparsly--------""will only consists partial""--------""with  is a lasso threshold""--------""an arbitrarily distance function""--------""each 10 sub classes are belonged to one""--------""is also needed to tune to achieve"" in this paper the authors introduce a new technique for softmax inference.","then, given the expert, output a particular category.",the present paper proposes a fast approximation to the softmax computation when the number of classes is very large.,"also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.----------------all in all, the results show that the proposed method provides a significant speedup with respect to shim et al., but it lacks comparison with other methods in the literature.----------------a brief list of typos:----------------""sparse mixture of sparse of sparse experts""--------""if we only search right answer""--------""it might also like appear""--------""which is to design to choose the right""--------sparsly--------""will only consists partial""--------""with  is a lasso threshold""--------""an arbitrarily distance function""--------""each 10 sub classes are belonged to one""--------""is also needed to tune to achieve"" in this paper the authors introduce a new technique for softmax inference.","also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.----------------all in all, the results show that the proposed method provides a significant speedup with respect to shim et al., but it lacks comparison with other methods in the literature.----------------a brief list of typos:----------------""sparse mixture of sparse of sparse experts""--------""if we only search right answer""--------""it might also like appear""--------""which is to design to choose the right""--------sparsly--------""will only consists partial""--------""with  is a lasso threshold""--------""an arbitrarily distance function""--------""each 10 sub classes are belonged to one""--------""is also needed to tune to achieve"" in this paper the authors introduce a new technique for softmax inference.","also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.----------------all in all, the results show that the proposed method provides a significant speedup with respect to shim et al., but it lacks comparison with other methods in the literature.----------------a brief list of typos:----------------""sparse mixture of sparse of sparse experts""--------""if we only search right answer""--------""it might also like appear""--------""which is to design to choose the right""--------sparsly--------""will only consists partial""--------""with  is a lasso threshold""--------""an arbitrarily distance function""--------""each 10 sub classes are belonged to one""--------""is also needed to tune to achieve"" in this paper the authors introduce a new technique for softmax inference.","also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.----------------all in all, the results show that the proposed method provides a significant speedup with respect to shim et al., but it lacks comparison with other methods in the literature.----------------a brief list of typos:----------------""sparse mixture of sparse of sparse experts""--------""if we only search right answer""--------""it might also like appear""--------""which is to design to choose the right""--------sparsly--------""will only consists partial""--------""with  is a lasso threshold""--------""an arbitrarily distance function""--------""each 10 sub classes are belonged to one""--------""is also needed to tune to achieve"" in this paper the authors introduce a new technique for softmax inference.",0.253731343283582,0.0303030303030303,0.1492537313432835,0.1492537313432835,0.3474178403755868,0.0663507109004739,0.1596244131455399,0.1596244131455399,0.0606060606060606,0.0,0.0404040404040404,0.0404040404040404,0.2727272727272727,0.0555555555555555,0.1454545454545454,0.1454545454545454,0.3474178403755868,0.0663507109004739,0.1596244131455399,0.1596244131455399,0.3474178403755868,0.0663507109004739,0.1596244131455399,0.1596244131455399,0.3474178403755868,0.0663507109004739,0.1596244131455399,0.1596244131455399,0.3474178403755868,0.0663507109004739,0.1596244131455399,0.1596244131455399,11.478896141052246,11.478896141052246,15.515891075134276,11.478896141052246,10.397821426391602,4.498291492462158,11.478896141052246,11.478896141052246,0.9112679634233246,0.9314289835121841,0.887966641990873,0.9713979090756694,0.9546720218500877,0.0923389533208443,0.9470065382966535,0.9501101631984137,0.45156630366782935,0.9643308424340998,0.9601299136719349,0.9373515245021768,0.9713979090756694,0.9546720218500877,0.0923389533208443,0.9713979090756694,0.9546720218500877,0.0923389533208443,0.9713979090756694,0.9546720218500877,0.0923389533208443,0.9713979090756694,0.9546720218500877,0.0923389533208443
193,https://openreview.net/forum?id=rJl4BsR5KX,"this work uses sequence-to-sequence and memory network neural nets to learn a--------network that not only predicts a label, but also predicts nearest neighbors and--------their label. the intuition is that by training on a related but harder task,--------the network is forced to learn not just about sampled points but about the--------behavior in regions around the actual training examples.----------------their work shows that imposing these additional requirements on the model does--------result in better performance on unseen data, where only the label of the unseen--------data point is required as output. they show that learning more about the --------global {feature,label} distribution improves f1 scores, and suggest that their--------methods can be used as an example generator for datasets with class imbalance.----------------the writing was clear. i had only 1 misunderstanding that cause me trouble,--------namely the first sentence of ""classification"", where i might put the word--------'benchmark' up front rather than at the end of this somewhat long sentence.--------by the time 'knn benchmark' appeared some paragraphs later, i realized i--------had missed something, and had to backtrack.----------------figs 1--3 are only viewable on screen with fairly extreme magnification. i--------found different viewers varied in legibility at these extreme zoom settings.--------however, when expanded so y-axis numbers could be deciphered, it turned out--------that actual improvements in f1 score were rather modest.----------------for 4 datasets, their f1 scores of their best method, v2vsls, improved by ~ 1-------to 6%. they found that their methods were less affected by out-of-core--------computation than the benchmark knn f1 scores.----------------i did have some questions about the problem formulation. i did not really--------understand why the models wanted to reproduce the exact ordering of the--------nearest neighbors, other than this is easy to formulate. this makes sense--------for n.n. label sequences, where further points might be in some neighboring--------class. but for predicting the feature vectors of n.n., it seems that the--------exact order is not robust, in the sense very small distance changes can cause--------abrupt shifts in the target nearest-neghbor ordering. is there some way--------for me to understand why this does not pose a problem? or is there perhaps--------a way to make the loss function a bit less dependent on the precise order--------of the generated feature vector sequence?----------------in the computational experiments, with the choice of datasets, it was often--------hard for me to judge how much different aspects of their 4 network structures--------were really being excercised. the main issue is the all problems used only--------2--3 classes. i could not guess what fraction of data points had actual label--------changes within the 5 n.n.. for many datasets, ""same class"" might be a pretty--------good predictor of nearest neighbor label.----------------alternatively, one can consider the other extreme, of very many classes.--------does it still make sense to try to predict the order of nearest neighbor labels in--------such a setting?----------------the authors provide evidence of some performance improvement, but i would--------encourage them to provide some intuition about what the networks are actually--------learning. some of this can be done with their existing data.----------------for example, on average, what are the distances from predicted feature vectors--------to the actual nn feature vectors? if the feature sequence is typically badly--------predicted, then this might allow the authors to propose that the network is--------*actually* learning some simpler features of the the distributions underlying--------an actual {feature,label} sequence. this might allow simplified training losses,--------based, on things like direction and distance to same-label cluster center,--------direction+distance to average same-class nn.s, direction and distance to--------closest differently labeled cluster, etc. or does their data suggest that--------their models are actually learning the precise nearest neighbor ordering?----------------such considerations might be able to improve the ooc training, since--------""global"" aspects of the distribution features (like ""cluster center"")--------remain approx. valid as training batches changes.----------------the other question i had was with the oversampling proposition. the extent--------of class imbalance in the datasets is not described. perhaps it belongs in--------table 1. their approach seems a lot of work for modest gains usually available--------with oversampling. can the authors provide any guideline for how many members a--------minority class should have before using their sequence-to-sequence technique?----------------pros: they improve generalization to unseen data.--------cons: their models are considerably more complex, and they do not analyze their--------data in enough detail to suggest whether their complexity is necessary, or perhaps--------could be reduced. figures are too small (many unreadable in printed copy).--------datasets have very few classes and the extent to which nearest neighbors are--------of different class not reported. i had a hard time understanding this paper. the approach is clearly about combining knn with neural networks, but it wasnt clear how it is done. after reading the whole paper, my guess is that knn is done on raw data first, and then its results are used for training a neural network. in particular, a network is trained to predict the labels of neighboring samples, which are obtained by knn beforehand. a simple figure explaining it in the introduction would be very helpful since the idea is not that complex. ----------------also, the authors also fail to give an adequate explanation on why the method works. the only reason i can think of is that this regularization forces the model to detect if a sample near a class boundary. this is because when a sample is far from boundaries and surrounded by samples of the same class, the model would simply predict that class label. the same is true when predicting out-of-sample vectors because the average position of k'th neighbor is likely to overlap with the input sample due to the randomness of sampling. ----------------i dont really see why a memory-based model is introduced. the external memory is used for holding random samples. it is not clear how the model can use such random samples for making predictions. also, the authors give no explanation to why it should help. the results also dont show the benefit of a memory-based model. maybe the authors should look into models that output a set instead of a sequence since neighbors are more like a set in their structure.----------------the experimental results show clear improvements over basic baselines, so the method is doing some regularization. however, i'm not very familiar with datasets used here and their state-of-art. they are relatively low dimensional compared to usual datasets used in deep learning. it is not clear if the method can scale to high dimensional data such as images. the vanilla neural network is not really a strong baseline here. since the authors proposed a regularization technique, it should be compared with other regularization techniques in neural networks.----------------pros:--------- a simple idea--------- encouraging experimental results----------------cons:--------- confusing read--------- no clear intuition is given--------- restricted to low-dimensional datasets--------- strong baselines needed--------- the plots are too small to see (impossible to see when printed)----------------other comments:--------- the authors are using the term ""feature vector"" to refer to a data point. however, in the context of neural networks, ""feature vector"" often means a hidden representation of a neural network. --------- why repeat ""randomly draw b samples"" r times? why not directly sample rxb samples?--------- ""it is quite implausible that only affine ..."" any evidence to support this?--------- the model is not really ""sequence-to-sequence"" since the input is not a sequence. to exploit the near neighbor/manifold features, this paper proposes to combine k-nearest neighbors of each training data point into the neural network models. specifically, the authors propose two families of models built on the popular sequence to sequence neural network models and memory network models, which mimic the k-nearest neighbors model in model learning. besides, the final label of the classification task will be learned, a sequence of nearest neighbor labels and a sequence of out-of-sample feature vectors (for oversampling) will be also learned in the same time, similar with the multi-task approaches. since the proposed models are based k-nearest neighbor calculations, which is time-consuming, they also design an algorithm for the out-of-core situation, say load a small portion of data each time to approximately calculate the neighbors. experiments show that some proposed models work better than baselines in classification and oversampling.--------strong points:--------(1) as similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.--------(2) the proposed models work well on the out-of-core situation, which shows that the models are robust.--------concerns or suggestions:--------(1) the training data is just one data point, it is not a sequence of data. so the idea to model it in a sequence to sequence setting does not make sense.--------(2) k-nearest neighbors are a set but not a sequence. to model them as a sequence is also strange. the i-th nearest neighbor does not necessarily dependent on the i-1-th nearest neighbor. for example, we consider the one-dimensional case, the focus data may lie between its first and second nearest neighbors. in this case, there is no clear sequence dependence from the second neighbor to the first neighbor. --------(3) the experiments are not sufficient. they only compare with some weak baselines, such as knn. as the classification task, there are many state-of-the-art models. besides of these standard classification models, we strongly suggest comparing with the previous method, wang et al. (2017), which also proposes to combine the k-nearest neighbors into memory network models. i am surprised that the authors did not compare with this very related work. in my opinion, the idea of utilizing nearest neighbors as external memory in wang et al. (2017) makes more senses.--------(4) the experimental results of some proposed sub-models (key parts of final models) are even worse than the basic knn model. i should say that the results are not good enough to support the proposed methods.","the proposed approach of predicting k nearest neighbouring examples as an auxiliary task is an interesting idea. however, the submission should have studied further on how those examples are predicted (e.g., sequence prediction is one, but you could try set prediction, or so on) rather than how sequential prediction of nearest neighbours is done together with different types of classifiers (many of which are arguably not necessarily suitable for classification,) which was a sentiment shared by all the reviewers. more careful investigation of different ways in which nearest neighbour prediction could be incorporated and more careful/thorough analysis on how the incorporation of this auxiliary task changes the behaviours or properties of the representation would make it a much better paper (also with clearer writing.)","or is there perhaps--------a way to make the loss function a bit less dependent on the precise order--------of the generated feature vector sequence?----------------in the computational experiments, with the choice of datasets, it was often--------hard for me to judge how much different aspects of their 4 network structures--------were really being excercised.","experiments show that some proposed models work better than baselines in classification and oversampling.--------strong points:--------(1) as similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.--------(2) the proposed models work well on the out-of-core situation, which shows that the models are robust.--------concerns or suggestions:--------(1) the training data is just one data point, it is not a sequence of data.","experiments show that some proposed models work better than baselines in classification and oversampling.--------strong points:--------(1) as similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.--------(2) the proposed models work well on the out-of-core situation, which shows that the models are robust.--------concerns or suggestions:--------(1) the training data is just one data point, it is not a sequence of data.","this work uses sequence-to-sequence and memory network neural nets to learn a--------network that not only predicts a label, but also predicts nearest neighbors and--------their label.","experiments show that some proposed models work better than baselines in classification and oversampling.--------strong points:--------(1) as similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.--------(2) the proposed models work well on the out-of-core situation, which shows that the models are robust.--------concerns or suggestions:--------(1) the training data is just one data point, it is not a sequence of data.","experiments show that some proposed models work better than baselines in classification and oversampling.--------strong points:--------(1) as similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.--------(2) the proposed models work well on the out-of-core situation, which shows that the models are robust.--------concerns or suggestions:--------(1) the training data is just one data point, it is not a sequence of data.",i had a hard time understanding this paper.,"specifically, the authors propose two families of models built on the popular sequence to sequence neural network models and memory network models, which mimic the k-nearest neighbors model in model learning.",0.2762430939226519,0.0111731843575419,0.1215469613259668,0.1215469613259668,0.2885572139303482,0.0100502512562814,0.1293532338308458,0.1293532338308458,0.2885572139303482,0.0100502512562814,0.1293532338308458,0.1293532338308458,0.1419354838709677,0.0,0.0774193548387096,0.0774193548387096,0.2885572139303482,0.0100502512562814,0.1293532338308458,0.1293532338308458,0.2885572139303482,0.0100502512562814,0.1293532338308458,0.1293532338308458,0.044776119402985,0.0,0.044776119402985,0.044776119402985,0.1518987341772152,0.0128205128205128,0.1012658227848101,0.1012658227848101,12.104480743408203,12.104480743408203,16.093063354492188,12.104480743408203,12.899940490722656,12.104480743408203,9.400521278381348,5.406184196472168,0.9668597797356541,0.9652718013808756,0.8803478538034412,0.8789264644222675,0.9002127758794687,0.824368157561098,0.8789264644222675,0.9002127758794687,0.8243686344120695,0.982530935314858,0.9810100083487144,0.9344799962698938,0.8789264644222675,0.9002127758794687,0.824368157561098,0.8789264644222675,0.9002127758794687,0.824368157561098,0.6719416823996821,0.654060133175482,0.8713951536851117,0.7442415542726457,0.838871880821659,0.40465549995915434
194,https://openreview.net/forum?id=rJlHIo09KQ,"the authors state a clear summary of their contribution to slow feature analysis (sfa) in section 5: ""the key idea for gradient-based sfa is that a whitening layer can be applied subsequently to any differentiable architecture (such as deep neural networks) to enforce outputs that approximately obey the sfa constraints while still being a differentiable architecture. as a result, the proposed method can replace the previous sfa pipeline of [fixed, non-linear feature extraction + learned linear feature extraction] with simply end-to-end [learned, non-linear feature extraction]. the resulting method is thus more flexible and expressive and less hand-crafted than traditional sfa approaches. the paper shows experiments that validate that the method can learn meaningful representations in practice.----------------the writing is difficult to follow, unclear in several places, and has grammatical mistakes. as a result, it is more challenging to understand the proposed method, its motivation, and its precise place among related literature. for example, the authors discuss spin, and from that discussion, it seems that spin are quite similar; spin was submitted to arxiv 3-4 months before the iclr deadline and thus is not concurrent as the authors claim (on my understanding, from reading the iclr and other ml conference guidelines).----------------the approach does seem somewhat incremental, though i would be open to changing my mind on author response. on one view, this method can be seen as simply replacing sfas fixed, non-linear feature extraction with learned non-linear feature extraction, with a trick to make it work. deep learning consistently improves over fixed non-linear feature extractors, so it is not surprising a surprising place to incorporate neural networks.----------------based on the method, this paper could go either way, but given my concerns on its novelty and writing quality/clarity, i lean slightly towards reject. summary--------the manuscript proposes to use power iterations in an approximate ""whitening layer"" to optimize the slowness objective of sfa in a very general setting. a set of experiments illustrates that this way of doing nonlinear sfa is meaningful.----------------quality--------although the idea is pretty straight forward and the paper shows qualitative results on a number of datasets, the relative merit of the approach is empirically not well characterized.----------------clarity--------the manuscript is in general well written and the technical content is well accessible. however the description of the whitening layer implementation needs some more details.----------------originality--------the idea of using a whitening layer together with the slowness objective has not been explored before. there is a second iclr 2019 submission (pfau et al.) with a very similar idea, though.----------------empirical evaluation--------the approximate whitening should lead to a trade-off between whitening and slowness optimization. i miss an experiment illustrating that trade-off. also the comparison to nonlinear sfa using expansion or kernelization of hierarchical sfa is empirically not properly characterized. in the end, if one takes the slowness objective seriously, one would use the method yielding slower results.----------------significance--------the manuscript introduces a way of running nonlinear sfa with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.----------------reproducibility--------the data is either synthetic or publicly available. the keras implementation of the powerwhitening layer as well as the entire neural network along with its optimization schedule is not shared. hence, there should be some effort involved to reproduce the experiments.----------------pros and cons--------1+) the idea of an approximate whitening layer is conceptually simple and clear.--------2-) the description of the practical implementation of the power iteration is slightly imprecise.--------3-) the algorithm scales badly in the number of output dimensions. this scaling is bad in a computational sense and also in a statistical sense.----------------details--------a) section 6.1: why do you need to add the noise term? what is the statistical meaning of this added noise?--------b) section 6.1: the solutions if comparable -> the solutions is comparable--------c) references: shaham -> iclr 2018 paper--------d) references: nystrm -> nystrm--------e) the name for the algorithm ""power sfa"" is a little bit bold.","this paper proposes to unroll power iterations within a slow-feature-analysis learning objective in order to obtain a fully differentiable slow feature learning system. experiments on several datasets are reported. this is a borderline submissions, with reviewers torn between acceptance and rejection. they were generally positive about the clarity and simplicity of the presentation, whereas they raised concerns about the relative lack of novelty (especially related to the recent spin model), as well as the current limitations of the approach on large-scale problems. reviewers also found authors to be responsive and diligent during the rebuttal phase. the ac agrees with this assessment, and therefore recommends rejection at this time, encouraging the authors to resubmit to the next conference cycle after addressing the above points.","in the end, if one takes the slowness objective seriously, one would use the method yielding slower results.----------------significance--------the manuscript introduces a way of running nonlinear sfa with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.----------------reproducibility--------the data is either synthetic or publicly available.","the authors state a clear summary of their contribution to slow feature analysis (sfa) in section 5: ""the key idea for gradient-based sfa is that a whitening layer can be applied subsequently to any differentiable architecture (such as deep neural networks) to enforce outputs that approximately obey the sfa constraints while still being a differentiable architecture.","summary--------the manuscript proposes to use power iterations in an approximate ""whitening layer"" to optimize the slowness objective of sfa in a very general setting.","the authors state a clear summary of their contribution to slow feature analysis (sfa) in section 5: ""the key idea for gradient-based sfa is that a whitening layer can be applied subsequently to any differentiable architecture (such as deep neural networks) to enforce outputs that approximately obey the sfa constraints while still being a differentiable architecture.","as a result, the proposed method can replace the previous sfa pipeline of [fixed, non-linear feature extraction + learned linear feature extraction] with simply end-to-end [learned, non-linear feature extraction].","in the end, if one takes the slowness objective seriously, one would use the method yielding slower results.----------------significance--------the manuscript introduces a way of running nonlinear sfa with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.----------------reproducibility--------the data is either synthetic or publicly available.",i miss an experiment illustrating that trade-off.,"the authors state a clear summary of their contribution to slow feature analysis (sfa) in section 5: ""the key idea for gradient-based sfa is that a whitening layer can be applied subsequently to any differentiable architecture (such as deep neural networks) to enforce outputs that approximately obey the sfa constraints while still being a differentiable architecture.",0.227027027027027,0.0109289617486338,0.0864864864864864,0.0864864864864864,0.2076502732240437,0.0331491712707182,0.1202185792349726,0.1202185792349726,0.1721854304635761,0.0268456375838926,0.119205298013245,0.119205298013245,0.2076502732240437,0.0331491712707182,0.1202185792349726,0.1202185792349726,0.1518987341772152,0.0,0.0759493670886076,0.0759493670886076,0.227027027027027,0.0109289617486338,0.0864864864864864,0.0864864864864864,0.0149253731343283,0.0,0.0149253731343283,0.0149253731343283,0.2076502732240437,0.0331491712707182,0.1202185792349726,0.1202185792349726,13.552080154418944,11.869443893432615,17.34523582458496,17.34523582458496,13.552080154418944,15.077024459838867,17.34523582458496,0.6223118305206299,0.9855178055468887,0.9825860703926271,0.9261776910253257,0.9751669681699385,0.9745102743135888,0.9310132493863743,0.9774182618347168,0.9785642305828632,0.9441617360739787,0.9751669681699385,0.9745102743135888,0.9310131717957207,0.9777950434775277,0.966995109419657,0.9423338333061403,0.9855178055468887,0.9825860703926271,0.9261776270198812,0.9312795099938713,0.946310597762284,0.7202151082105227,0.9751669681699385,0.9745102743135888,0.9310132493863743
195,https://openreview.net/forum?id=rJlcLaVFvB,"this work proposed a new model called sparse deep predictive coding, which introduced top-down connections between consecutive layers, to improve the solutions to hierarchical sparse coding (hsc) problems. instead of decomposing the hsc problem into independent subproblems, the proposed model added a new term to the loss function, which represents the influence of the latter-layer on the current layer.----------#pros:------- the proposed model adopted the idea from predictive coding and came up with a relatively novel idea for hsc problems.------- the experiments are solid. the experiments evaluated the proposed methods with different hyper-parameter settings and three real-world datasets.------- the figures in the result sections are well designed and concise.----------#cons:------- the mathematical description of the main problem and the proposed model is not clear. for example, the dimensionality for the variables in eq.(1) is not clarified.------- the test procedure is not clear. how the internal state variables are obtained for the test set is not clarified.------- the proposed model was only compared with a basic hierarchical lasso network. there are not any state-of-art methods included as baseline methods.----------#detailed comments:----------(1) the proposed model is named as sparse deep predictive coding, however, the experiments only considered the sdpc and hi-la networks with 2 layers. i am wondering if a deeper structure will improve the performance?----------(2) for the structure shown in fig.1, the decoding dictionaries are -----, but i am confused why the encoding dictionaries are reciprocal to encoding dictionaries. does it come from the optimization updates shown in eq.(3)?----------(3) according to eq.(1), is a vector and is a 2d matrix. however, the real inputs in the experiments are images and is a convolutional filter with 4 dimensions. how are the matrices reshaped?----------(4) for section 2.2 and 2.3, the number/index of samples is not shown in the loss function for training. the loss should be over the whole training set. besides, the test procedure is not clarified.----------(4) the number of iterations using fista of sdpc and hi-la networks is shown to compare the rate of convergence. however, considering both models are solving a lasso-type regression problem, i would suggest using coordinate descent for optimization.----------(5) for the main result of prediction error, why is the global prediction error more important than the reconstruction error? is the first-layer prediction error the reconstruction error? if yes, fig.2 shows that hi-la has a lower prediction error compared to sdpc for the first layer.----------(6) two minor comments on writing:-----(a) it would be better to have a separate section for 2.5 since it describes the dataset and is not related to the proposed model.-----(b) a typo of neuronal implementation exists in the introduction section. this paper presents a study that compares two techniques for hierarchical sparse coding. this is the problem of learning sparse representations in multi-layer (but not necessarily deep) models. the first method applies lasso at each layer in a bottom up fashion, while the second method  introduced in the submitted work  adds an additional term which propagates information in a top-down manner. it is found that the top-down term is beneficial in terms of reducing predictive error and can learn faster. in terms of novelty the new term, the paper does not make a breakthrough contribution, but i consider this to be sufficient. moreover, the paper is well written and presents some interesting results. however i am not entirely sure that the impact of these results will attract the interest of a broad audience since the experiments presented are on small datasets and with some rather shallow neural nets. this paper proposes sparse deep predictive coding (sdpc) to access the impact of the inter layer feedback, which is suggested by neuro-scientific evidence. the sdpc model is compared with hila on 2 different databases, and the experimental results show that sdpc achieved lower prediction error, faster converge rate. ----------although the paper is well-motivated and well-written, the potential impact of this work is limited in the community. moreover, the experiments are not sufficient to valid the advantage of this model. for example, there are only 3 dataset adopted in the experiments and the all these three datasets are quite small. moreover, there is only one baseline, which seems too weak to show the advantage of the model.----------based on the above points, i tend to reject the paper.----------**updated comments**-----thanks for the response and i really appreciate the hard work during the rebuttal. however, the stl-10 is still a small scale dataset, and the baselines provided in the rebuttal are still limited. i will not change the original score.",this paper introduces a new architecture for sparse coding.----------the reviewers gave long and constructive feedback that the authors in turn responded at length on. there is consensus among the reviewers that despite contributions this paper in its current form is not ready for acceptance.----------rejection is therefore recommended with encouragement to make updated version for next conference.,"there are not any state-of-art methods included as baseline methods.----------#detailed comments:----------(1) the proposed model is named as sparse deep predictive coding, however, the experiments only considered the sdpc and hi-la networks with 2 layers.","instead of decomposing the hsc problem into independent subproblems, the proposed model added a new term to the loss function, which represents the influence of the latter-layer on the current layer.----------#pros:------- the proposed model adopted the idea from predictive coding and came up with a relatively novel idea for hsc problems.------- the experiments are solid.","there are not any state-of-art methods included as baseline methods.----------#detailed comments:----------(1) the proposed model is named as sparse deep predictive coding, however, the experiments only considered the sdpc and hi-la networks with 2 layers.","this work proposed a new model called sparse deep predictive coding, which introduced top-down connections between consecutive layers, to improve the solutions to hierarchical sparse coding (hsc) problems.","this work proposed a new model called sparse deep predictive coding, which introduced top-down connections between consecutive layers, to improve the solutions to hierarchical sparse coding (hsc) problems.","there are not any state-of-art methods included as baseline methods.----------#detailed comments:----------(1) the proposed model is named as sparse deep predictive coding, however, the experiments only considered the sdpc and hi-la networks with 2 layers.",how the internal state variables are obtained for the test set is not clarified.------- the proposed model was only compared with a basic hierarchical lasso network.,"there are not any state-of-art methods included as baseline methods.----------#detailed comments:----------(1) the proposed model is named as sparse deep predictive coding, however, the experiments only considered the sdpc and hi-la networks with 2 layers.",0.2061855670103093,0.0,0.1030927835051546,0.1030927835051546,0.208695652173913,0.0176991150442477,0.1565217391304347,0.1565217391304347,0.2061855670103093,0.0,0.1030927835051546,0.1030927835051546,0.1839080459770115,0.0470588235294117,0.160919540229885,0.160919540229885,0.1839080459770115,0.0470588235294117,0.160919540229885,0.160919540229885,0.2061855670103093,0.0,0.1030927835051546,0.1030927835051546,0.1904761904761905,0.024390243902439,0.119047619047619,0.119047619047619,0.2061855670103093,0.0,0.1030927835051546,0.1030927835051546,10.280664443969728,12.82310676574707,12.82310676574707,14.711311340332031,10.28066635131836,10.280664443969728,10.280665397644045,8.29073715209961,0.9407867195687646,0.9598611992983999,0.9066128627878153,0.9786927635683838,0.9782438845050665,0.938433064219743,0.9407867195687646,0.9598611992983999,0.9066128751090864,0.9760870852715763,0.9721778158300132,0.9268670349173772,0.9760870852715763,0.9721778158300132,0.9268671748457643,0.9407867195687646,0.9598611992983999,0.906612902128633,0.9578484748959267,0.9603287639005409,0.6180583689289959,0.9407867195687646,0.9598611992983999,0.906612902128633
196,https://openreview.net/forum?id=rJleFREKDr,"the authors build on work regarding few-shot learning with memory augmented networks, specifically [kaiser, et al., iclr17] where the goal is to learn a memory address mapping such that generalization is achieved by finding the nearest neighbor memory address when predicting the label. for correct predictions, the memory key is updated to include the associated (predicted) address while new memory locations are written for mistakes. whereas [kaiser, et al., iclr17] follows a lru-like procedure for replacing memory, the current work proposes performing policy-gradient rl where the action space is the memory locations and the reward is reduction of entropy over the memory address assignment distribution over the memory locations. this approach is empirically studied for an rnn approach to ner, specifically considering few-shot learning for ner in the stanford task-oriented dialogue (stod) dataset  showing non-negligible improvements over memory augmented networks [santoro, et al., icml16] and matching networks [vinyals, et al., neurips16].----------from a high-level perspective, memory networks have played an important role in building dialogue managers (and in several applications) and their proposed expansion over [kaiser, et al., iclr17] in the context of few-shot, sparse memory architectures seems sensible. additionally, the empirical results o ner seem promising. however, in my opinion, this paper is making contributions along two dimensions, but neither of them convincingly as detailed below. ----------first, one potential point of emphasis is the learning to control mechanism through pg rl being better than [kaiser, et al., iclr17] using with a lru-like policy. to do this, it would make sense to both compare to the datasets in the previous work (omniglot, wmt  even if it does require additional architectures that are pretty straightforward expansions) and to include this architecture in the submitted paper as there is a publicly available dataset (https://github.com/himani-arora/learning_to_remember_rare_events). while the synthetic dataset probably isnt necessary, one could easily think of synthetic datasets that would clearly contrast with the existing method and conduct ablation/configuration studies regarding different number of neighbors, size of memory, exact vs. approximate nearest neighbor for lookup, etc. ----------a second point of interest is the ner task. from this perspective, there is some existing work (e.g., [fritzler, logacheva & kretov, sac19; hou, et al., https://arxiv.org/pdf/1906.08711.pdf (not required as not accepted yet); hofer, et al., https://arxiv.org/pdf/1811.05468.pdf (not clear if accepted)]. while these are largely preliminary works, they do provide datasets (and there are many others) for ner that would potentially make a convincing case for this being a state-of-the-art approach for few-shot learning in ner. however, i am not sure that being the state-of-the-art for few-shot ner would be a sufficient contribution for iclr, even if done convincingly. if there was something in the algorithm specific to sequence prediction, then i think few-shot sequence prediction would be a sufficient contribution, but given the current architecture, i would think that the bar is a methodological contribution to adding sparse memory units (via rl in this case) is the acceptable level of contribution.----------finally, my belief that the contribution is too narrow and not sufficiently developed is supported by the writing seeming rushed. the paper makes much more sense after reading [kaiser, et al., iclr17] as sections 2, 3 of the submission are missing the aspects of a clear constrast to [kaiser, et al., iclr17], a working example to provide readers a more intuitive understanding, a clear example showing dimensionality and notation (beyond what is in figure 1). there are also many typos (e.g., mammals exel, trainale controller, rathern then, amongst others) and general need for proofreading. however, this is not the determining factor in the paper. basically, i think the contribution is insufficient in terms of scope and convincingness of the contribution. thus, i recommend rejecting in its current form  even if the underlying idea is promising and worth developing further. the paper proposes a memory network architecture with a sparse memory. each memory entry contains a key (vector) and a value (class label). the memory is addressed using a policy pi_theta, which selects a single memory entry to be updated at each time step. the policy pi_theta is trained using policy gradient with the reward being the increase in the policy's certainty (measured as entopy). the model is evaluated on an online ner task that mimics the meta-learning setting of http://proceedings.mlr.press/v48/santoro16.html. in that work, the examples are provided in episodes. the labels are renamed at the beginning of each episode (to prevent fitting to the labels). the model has to predict the label of each example in sequence, and the correct label is given after each prediction.----------the paper suffers from the following issues:----------- the method in the paper is very similar to the existing discrete addressing scheme (https://www.mitpressjournals.org/doi/full/10.1162/neco_a_01060), which naturally also uses rl for training. ----------- the method description is either incomplete or incorrect. the task label does not seem to influence the loss function at training time. the current training loss only encourages the entropy to reduce, so unless the lstm is pre-trained, there is no guarantee that the key s will address the right memory entry. please correct me if i am mistaken here.----------- the writing is unclear. task descriptions and notations are not properly set up. the experiment section does not explain the details of the experiments, leaving the reader to look at santoro et al., 2016 for context. even then, some statements in the experiment section are confusing. for instance, the numbers of classes in the description (5 and 20) do not match the number of ner classes (8).----------- the task used in the paper is non-standard. the paper could have used meta-learning tasks from previous papers, including the ones in santoro et al., 2016.----------other comments:----------- the citations should be written as ""xxx (author, year)"" and not ""xxx, author (year)"".------ the paper could benefit from proofreading. in this paper, the authors present a method, learning to control (ltc), that enables a reinforcement learning agent to learn to read and write external memory. they follow the intuition that human has two degrees of plasticity for memory, which leads to the dense-sparse memory design in this paper. the proposed method can be applied to a few-shot setting. ----------writing: the writing is ok. readers can obtain the essence by a reasonable amount of guessing. ---------------strength: although memory augmented neural networks and reinforcement learning are both well-known, the proposed method combined them in a novel way. the presented approach outperforms two baselines on the stanford dialogue dataset. ----------weakness: there is no ablation study for the proposed method to fully understand why the proposed method is superior. the authors use reinforce as the rl algorithm which is no longer useful in most of the complex rl tasks. more advanced rl algorithms are encouraged to be tried. the proposed title is ""learning to control"", which i don't see where the control part is. it seems the ltc only refers to the reinforce agent. the experiments are very minimal which did not provide enough information to distinguish the proposed method from other methods. ----------overall, the general direction is promising. however, the paper is not yet fully finished. i believe the authors need major changes to the experiments section as well as the method description section. hence, i cannot recommend this paper to acceptance.----------minor: -----exel -> excel -----figure 1 needs more clarification.","this work proposes to use policy-gradient rl to learn to read and write actions over memory locations using as reward the entropy reduction of memory location distribution. the authors perform experiments on ner in stanford dialogue task, that are framed though as few-shot learning. the reviewers have pointed out shortcomings of the paper with regards to its novelty, narrow contribution in combination thin experimental setup (the authors only look into one dataset and one task with minimal comparison to previous work and no ablation studies as to understand the behaviour of the model) and clarity (method description seems to be lacking some crucial components of the model). as such, i cannot recommend acceptance but i hope the authors will use the reviewers comments to transform this into a strong submission for a later conference.","the current training loss only encourages the entropy to reduce, so unless the lstm is pre-trained, there is no guarantee that the key s will address the right memory entry.","this approach is empirically studied for an rnn approach to ner, specifically considering few-shot learning for ner in the stanford task-oriented dialogue (stod) dataset  showing non-negligible improvements over memory augmented networks [santoro, et al., icml16] and matching networks [vinyals, et al., neurips16].----------from a high-level perspective, memory networks have played an important role in building dialogue managers (and in several applications) and their proposed expansion over [kaiser, et al., iclr17] in the context of few-shot, sparse memory architectures seems sensible.","the authors build on work regarding few-shot learning with memory augmented networks, specifically [kaiser, et al., iclr17] where the goal is to learn a memory address mapping such that generalization is achieved by finding the nearest neighbor memory address when predicting the label.","the authors build on work regarding few-shot learning with memory augmented networks, specifically [kaiser, et al., iclr17] where the goal is to learn a memory address mapping such that generalization is achieved by finding the nearest neighbor memory address when predicting the label.","this approach is empirically studied for an rnn approach to ner, specifically considering few-shot learning for ner in the stanford task-oriented dialogue (stod) dataset  showing non-negligible improvements over memory augmented networks [santoro, et al., icml16] and matching networks [vinyals, et al., neurips16].----------from a high-level perspective, memory networks have played an important role in building dialogue managers (and in several applications) and their proposed expansion over [kaiser, et al., iclr17] in the context of few-shot, sparse memory architectures seems sensible.","this approach is empirically studied for an rnn approach to ner, specifically considering few-shot learning for ner in the stanford task-oriented dialogue (stod) dataset  showing non-negligible improvements over memory augmented networks [santoro, et al., icml16] and matching networks [vinyals, et al., neurips16].----------from a high-level perspective, memory networks have played an important role in building dialogue managers (and in several applications) and their proposed expansion over [kaiser, et al., iclr17] in the context of few-shot, sparse memory architectures seems sensible.",more advanced rl algorithms are encouraged to be tried.,"this approach is empirically studied for an rnn approach to ner, specifically considering few-shot learning for ner in the stanford task-oriented dialogue (stod) dataset  showing non-negligible improvements over memory augmented networks [santoro, et al., icml16] and matching networks [vinyals, et al., neurips16].----------from a high-level perspective, memory networks have played an important role in building dialogue managers (and in several applications) and their proposed expansion over [kaiser, et al., iclr17] in the context of few-shot, sparse memory architectures seems sensible.",0.1437125748502994,0.0121212121212121,0.0958083832335329,0.0958083832335329,0.2443438914027149,0.0365296803652968,0.1266968325791855,0.1266968325791855,0.2111111111111111,0.0449438202247191,0.1333333333333333,0.1333333333333333,0.2111111111111111,0.0449438202247191,0.1333333333333333,0.1333333333333333,0.2443438914027149,0.0365296803652968,0.1266968325791855,0.1266968325791855,0.2443438914027149,0.0365296803652968,0.1266968325791855,0.1266968325791855,0.0551724137931034,0.0139860139860139,0.0551724137931034,0.0551724137931034,0.2443438914027149,0.0365296803652968,0.1266968325791855,0.1266968325791855,7.888613700866699,7.888613700866699,12.544357299804688,7.888613700866699,8.035771369934082,12.544357299804688,7.888613700866699,8.871756553649902,0.2958970661056469,0.37816413599168824,0.8239318781983206,0.9585530408220524,0.9683841443263902,0.9038756039875273,0.9559859538545046,0.9569535733918758,0.9125595931403016,0.9559859538545046,0.9569535733918758,0.9125595523449154,0.9585530408220524,0.9683841443263902,0.9038754992499167,0.9585530408220524,0.9683841443263902,0.9038756039875273,0.15974824075163988,0.24157237325561653,0.8576827209002236,0.9585530408220524,0.9683841443263902,0.9038756039875273
197,https://openreview.net/forum?id=rJlnfaNYvB,"the paper mostly reads well. it proposes to use statistics from previous activations to compute and adaptive scaling of the loss such that the amount of underflow is minimized. the scaling is defined per layer. experiments are carried for various model sizes and datasets. ----------if anything i think the paper can do a better job at centralizing (maybe in an appendix) the gritty details (e.g. how the stats are computed etc). unfortunately, the best way of doing this might be in the form of code or maybe pseudocode, but being quite explicit in all technical details. ----------right now this is mentioned in the text (same way as batch norm stats if i understood correctly, based on the current minibatch). though is not clear how the variance on w is treated. how is the variance on dirac delta (backpropagated error) is converted into a scalar (that will be for the entire loss). in this paper, the authors propose a method to train models in fp16 precision. the authors show that the key reason of training performance drop is the overflow or underflow of back propagation information. instead of using a fixed value or dynamic value proposed by a previous work, this paper adopts a more elaborate way to minimize underflow-----in every layer simultaneously and automatically based on the current layer statistics. experiment results on cifar10, imagenet and object detection models are conducted to demonstrate the effectiveness of the proposed method.----------there are some concerns about this paper:-----1. this paper tries to solve a very practical problem which is good, however, the stability of this method which is very important for real applications remains unclear. more networks such as vgg/resnet/depthwise-conv based networks, more initialization methods (such as gaussian, xavier, kaiming), w/o bn layers, and more tasks such as segmentation and detection with different batch sizes are strongly recommended to make this work more solid.-----2. in the experiments, it seems that dynamic loss scaling method works well too except on the configuration of ssd batchsize=8. why dynamic loss scaling fails on this case? more detailed analysis are recommended to show the advantage of the proposed method.-----3. in many experiments, it seems adaptive loss scaling with fp16 is even better than fp32, is this stable? could we further improve the fp32 results if using dynamic / adaptive loss scaling? the authors propose an adaptive loss scaling method during the backpropagation stage for the mix precision training to reduce the underflow. compared with the previous work, which scales the loss by human design, and needs to be consistent in all layers. the authors state that they can decide the scale rate layer by layer automatically to reduce the underflow in a low precision situation. ----------they calculate the scale rate using the statistic information of the layer weight and gradient. by adaptively scale each layers loss and gradient, this method can reduce the underflow rate better than the previous work. additionally, the authors claim that the computation overhead is not significant, so it is efficient to use rather than searching from a set of fix scale rates. ----------the experiments present on image classification and objective detection benchmarks. from the result, we can see that the adaptive loss scale reaches a relatively high point on all the tasks.----- -----pros:----- ------ the method is straight forward and easy to understand. the motivation is good. they get some impressive results on resnet110 and ssd512 comparing with the fixed scaling method. besides, they give some analysis of their advantages and disadvantages in different networks, which looks promising to me.----- -----cons:----------- one question in section 3.2.1, the assumption that w, g, p can be treated as the random variable with gaussian distribution seems not natural in the training process. especially p is a zero-mean distribution. the cited paper uses this assumption in a more convincing case, such as the weight initialization task. notice that he et al., 2015 claim that the product of weight and gradient can be a zero-mean normal distribution is based on the weight is a symmetric distribution around zero, which is not true in neither this papers assumption nor the real training situation. ----------- in the objective detection part, i can not find which dataset the authors use. though the author state that they follow liu et al., 2016 s work, there are also several tasks in lius paper, and i can not directly match the resulting point with any of those tasks, which makes me hard to confirm the experiment result.----------- the experiment setting is unclear. here are two questions. 1, what is the initial scale at the last layer? it should be manually designed in the experiment, and i think this value may affect the other layers scale as well. if the algorithm is robust for this scale, it is better to show some study on that. 2, what update frequency is used in the experiment? the authors say that the overhead can be reduced by reducing the frequency, but they do not clearly show which frequency they use in their experiment, if the frequency does not affect the performance, it is also better to claim or show some study on that.----------minor comments:------ figures can use a larger font. figures 4a and 4b can be aligned better.","this work proposes to improve mixed precision training by adaptively scaling the loss based on statistics from previous activations to minimize underflow during training. however, the method is designed rather heuristically and can be improved with stronger theoretical support and improved representation of the paper.","besides, they give some analysis of their advantages and disadvantages in different networks, which looks promising to me.----- -----cons:----------- one question in section 3.2.1, the assumption that w, g, p can be treated as the random variable with gaussian distribution seems not natural in the training process.","by adaptively scale each layers loss and gradient, this method can reduce the underflow rate better than the previous work.","the authors say that the overhead can be reduced by reducing the frequency, but they do not clearly show which frequency they use in their experiment, if the frequency does not affect the performance, it is also better to claim or show some study on that.----------minor comments:------ figures can use a larger font.",the paper mostly reads well.,"by adaptively scale each layers loss and gradient, this method can reduce the underflow rate better than the previous work.","by adaptively scale each layers loss and gradient, this method can reduce the underflow rate better than the previous work.",it proposes to use statistics from previous activations to compute and adaptive scaling of the loss such that the amount of underflow is minimized.,"by adaptively scale each layers loss and gradient, this method can reduce the underflow rate better than the previous work.",0.2127659574468085,0.0217391304347826,0.1276595744680851,0.1276595744680851,0.3999999999999999,0.0634920634920634,0.2153846153846153,0.2153846153846153,0.1818181818181818,0.0206185567010309,0.101010101010101,0.101010101010101,0.0799999999999999,0.0416666666666666,0.0799999999999999,0.0799999999999999,0.3999999999999999,0.0634920634920634,0.2153846153846153,0.2153846153846153,0.3999999999999999,0.0634920634920634,0.2153846153846153,0.2153846153846153,0.4927536231884057,0.2388059701492537,0.2898550724637681,0.2898550724637681,0.3999999999999999,0.0634920634920634,0.2153846153846153,0.2153846153846153,9.088837623596191,9.088837623596191,5.432003021240234,9.088838577270508,6.514286994934082,6.841044425964356,9.088837623596191,12.550336837768556,0.9198398905455396,0.9311557069826928,0.8246319341074916,0.9546643837724305,0.9525858047876916,0.645746720987498,0.5639425772683199,0.7627047789246976,0.9207089676925815,0.9232452661427599,0.9185283719013998,0.83744906524249,0.9546643837724305,0.9525858047876916,0.645746720987498,0.9546643837724305,0.9525858047876916,0.645746720987498,0.9265235731257648,0.940017577641341,0.132717603002952,0.9546643837724305,0.9525858047876916,0.645746720987498
198,https://openreview.net/forum?id=rJxRJeStvB,"the authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [dai et al. (2017)]. the main contribution is to consider each of the steps taken by dai et al. to solve combinatorial problems on graphs, and adapt them to the considered scheduling problem.----------not being an expert in rl, my assessment should be discounted. however, i am not sure i follow properly the main idea of the paper. the point of dai et al. was to use rl to solve a wide family of combinatorial problems. now, the authors claim to build upon these ideas to solve... what looks essentially like a far more standard rl problem, and not necessarily a combinatorial optimization problem. the main insight by dai et al. was to highlight the fact that combinatorial problems are usually solved (or approximated) without ""warm starts"", i.e. they do not consider distributions on problem instances to learn from. the problem considered by the authors is, quite on the contrary, a typical rl problem where information is extracted from the problem's structure (here a maze). therefore, i feel there is something of a fundamental contradiction going on at a fairly high-level, in the sense that the paper ""uses rl to solve a subset of combinatorial problems that were studied by rl before"". the absence of other baselines in experiments make this even more suspicious. therefore i believe the paper's presentation could be greatly improved if it were better ""located"" within the rl literature (which is almost non-existent in the very brief bibliographic section) and that the authors were able to show that their proposals are original, within an rl context.----------minor points:-----* the comment ""while learning-based methods are generally believed to suffer exponentially increasing training requirements as problem size (number of robots and tasks) increases, our methods training requirement is empirically shown not to scale while maintaining near-optimal performance"" --> this is too loose a statement. provide more evidence or references. paper addresses the problem of centralized multi-machine task assignment in an rl setting (""multi-robot reward collection""). claim is that this has not been successfully done in a rl setting before, so a new problem is proposed (multi-agent pac-man) and results are presented on this problem. approach proposed extends prior work from dai 2017 and 2016 (which i am a priori unfamiliar with), and it seems to me that the exposition of this method leans a bit too heavily on presumed familiarity with those works. an auction-consensus approach is proposed whereby each machine makes a bid for each unclaimed task, then the coordinator picks the highest bid and assigns that task-machine pairing, after which the remaining machines make bids for the remaining tasks, and so forth.----------as it stands, part of me leans toward rejecting for a couple reasons.-----1. the exposition of the method needs to be improved to assume less background knowledge of the heuristic pgm and structure2vec methods, investing some text introducing them. appendix c seems to do part of this, and probably should be integrated into the body of the paper.-----2. another view of ""random graphical models"" is the sampling trace of a universal ppl. this is studied in, e.g. https://cocolab.stanford.edu/papers/daipptr.pdf so it seems like this deserves at least a brief additional literature review as opposed to simply diving into mfi. appendix d looks ok: since the action space is discrete, then a fixed point approach becomes feasible.----------on the other hand, the experiments are good, the auction approach is a nice idea/novel. the ablation experiment is good, and the comparison against or tools is also good to have. insofar as the structure2vec is representation-oriented, it seems like a decent fit to the venue.----------on balance, i think the paper needs too much polish and revision to accept at this time.----------minor nits:-----the word ""seminar"" is used a couple times, where from context i think ""seminal"" is intended.-----some figure refs are broken. in this paper, the authors propose a reinforcement learning method for multi-robot scheduling problems. they state the method's scalable performance and transferability. my major concerns are as follows.----------1. the paper is not easy to read. in my understanding, multi-robot scheduling is a very important problem and is very similar to many scheduling problems in complex platforms such as the dispatch system for ride sharing and package delivery. however, i did see any real application in this paper. it is very difficult to understand how this proposed method works and what is the benefit under non trivial environment.----------2. the experiments (2~8 robots, 20~50 tasks) cannot support the scalable performance or large problems very well. how about thousands and millions of robots/tasks, e.g. routing planning or dispatching for vehicles in a ride sharing platform? ----------3. it is not convincing without comparison with necessary baseline methods.----------4. there is no in-depth analyses for the transferability.----------5. there are many typos, such as the missing figure citation with figure ??.","unfortunately, the reviewers of the paper are all not certain about their review, none of them being rl experts. assessing the paper myselfnot being an rl expert but having experiencethe authors have addressed all points of the reviewers thoroughly.","the authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [dai et al. (2017)].","therefore i believe the paper's presentation could be greatly improved if it were better ""located"" within the rl literature (which is almost non-existent in the very brief bibliographic section) and that the authors were able to show that their proposals are original, within an rl context.----------minor points:-----* the comment ""while learning-based methods are generally believed to suffer exponentially increasing training requirements as problem size (number of robots and tasks) increases, our methods training requirement is empirically shown not to scale while maintaining near-optimal performance"" --> this is too loose a statement.","now, the authors claim to build upon these ideas to solve... what looks essentially like a far more standard rl problem, and not necessarily a combinatorial optimization problem.","the authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [dai et al. (2017)].","the authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [dai et al. (2017)].","the authors study a combinatorial multi-robot scheduling problem (in fact the robot part is a bit inflated, since the experiments only involve agents in a simulated discrete state-space maze) using a method that builds upon recent advances from [dai et al. (2017)].",there is no in-depth analyses for the transferability.----------5.,"therefore i believe the paper's presentation could be greatly improved if it were better ""located"" within the rl literature (which is almost non-existent in the very brief bibliographic section) and that the authors were able to show that their proposals are original, within an rl context.----------minor points:-----* the comment ""while learning-based methods are generally believed to suffer exponentially increasing training requirements as problem size (number of robots and tasks) increases, our methods training requirement is empirically shown not to scale while maintaining near-optimal performance"" --> this is too loose a statement.",0.0963855421686747,0.0,0.072289156626506,0.072289156626506,0.2238805970149253,0.0303030303030303,0.1343283582089552,0.1343283582089552,0.1194029850746268,0.0,0.0597014925373134,0.0597014925373134,0.0963855421686747,0.0,0.072289156626506,0.072289156626506,0.0963855421686747,0.0,0.072289156626506,0.072289156626506,0.0963855421686747,0.0,0.072289156626506,0.072289156626506,0.0408163265306122,0.0,0.0408163265306122,0.0408163265306122,0.2238805970149253,0.0303030303030303,0.1343283582089552,0.1343283582089552,11.08249282836914,11.08249282836914,11.08249282836914,6.4989213943481445,11.082491874694824,8.414852142333984,6.4989213943481445,3.6509082317352295,0.9772281033196125,0.8767646350118752,0.35959404903744213,0.9502013869957027,0.9473323745091855,0.9393656827551887,0.9401991962881416,0.9478187815979572,0.92239882985189,0.9772281033196125,0.8767646350118752,0.3595940513444085,0.9772281033196125,0.8767646350118752,0.3595941662836884,0.9772281033196125,0.8767646350118752,0.3595940513444085,0.17236089388481796,0.23281819002752896,0.031724715400425145,0.9502013869957027,0.9473323745091855,0.9393656827551887
199,https://openreview.net/forum?id=rJxok1BYPr,"this paper presents a novel approach to generating molecules using black box recurrent translation.-----the authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.-----then, the recursion takes the top k best molecules and runs another iteration to generate even better molecules, ad infinitum.-----the authors use the newly introduced selfies strings as vocabulary for generation.-----the authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.-----relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.-----the authors also show that this technique can optimize multiple properties at once.----------i am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.------ recursive black box translation seems to be widely applicable to new models.------ the model seems to reach a significantly better state of the art on the metrics proposed.------ none of the baselines seem to use selfies as the string of choice.----- this means it's difficult to tell how much the ""blackbox recursive"" part of the algorithm adds to the model.----- an ablation experiment without bbrt might inform us of how much of the benefit is due to the molecule representation (fig 4a reports the mean, but it would be good to have the same metric as table 1).------ the authors provide an in depth discussion about how having molecular traces would hhelp in drug design.----- this makes the tool seem more widely appealing and useful.----------a few questions would clear up the strengths of the paper:------ is there a connection to the backtranslation work in lample 2018? (phrase-based & neural unsupervised machine translation)----- it seems like a similar idea - except in this domain, the target language and source language are the same.------ how can there be multiple scoring functions? ----- were they combined in one run, or were these separately optimized runs? are these only used in figure 4?------ why would beam search do less well than stochastic? ----- is it because during recursive translation, the beam search variants have low diversity?----- then, training with stochastic decoding and generation with a beam search should do even better, right?----- this would highlight that the advantage of stochastic decoding is really online in the context of recursive translation, not generally.------ what is the point of fig 4a right? why do we expect that maximizing non-logp properties will increase mean logp? the paper builds on existing translation models developed for molecular optimization, making an iterative use of sequence to sequence or graph to graph translation models by wrapping them in a meta-procedure. the primary contribution is really just to apply the translation models iteratively, i.e., feeding translation outputs from the models back in as inputs for retranslation. a few strategies are introduced to score / rank candidates before they are chosen for retranslation. the overall idea is very simple, and is likely to work in some basic cases where the property has a natural ""additive"" nature, e.g., logp that you can improve by adding functional groups. this is recognized but not really controlled in the paper except for selecting for input similarity before retranslating. moreover, i don't think that you really ever want to just maximize logp for any drug so this particular task is a bit artificial in the first place. other properties are not additive in the same sense, e.g., drug likeness or qed, and the method doesn't appear to improve it (though, to be fair, there may be a ceiling effect for qed in particular). ----------one of the main ways that one can control the final output in the iterated translation process is by judiciously selecting or ranking candidates for retranslation. the authors use essentially the score from the model itself, similarity to input, and some basic chemistry metrics to do that. wouldn't it be much better to train a separate ranking method to guide the iterative steps? ----------the empirical results are clean though not convincing (see the logp discussion above). additional properties should be included to demonstrate that the method might actually have some practical value, i.e., generalize beyond additive logp. multi-property optimization would be one possible setting since de novo models have a hard time to reach intersections of different property constraints. abstractly, one could imagine that an iterative, successively guided approach could work well. the proposed approach in the paper is somewhat undeveloped. it merely uses a translation model for the primary property, and ranks candidates by the other. this is unlikely to get you to any challenging intersections. also, since logp was always one of the properties effectiveness in this regard is not really demonstrated either. a slightly more sophisticated approach might use relaxed, separately trained ranking models in intermediate steps, successively tightened towards the intersection as the iteration progresses. e.g.,----------brookes et al., design by adaptive sampling, arxiv:1810---------the paper is clearly written but for such a simple method one would need really convincing results and experiments. maybe better as a workshop submission?","this paper presents a simple method for improving molecular optimization with a learned model. the method operates by repeatedly feeding generated molecules back through an encoder decoder pair trained to maximize a desired property. reviewers liked the simplicity of the method, and found it interesting but ultimately there were concerns about the metrics used to evaluate the method. reviewers 3 and 4 both noted issues with the log p (and penalized log p) metric, noting that it is possible to artificially increase both metrics in a way that isn't useful in practice. during the discussion phase, reviewer 4 constructed a specific example where simply adding long carbon chains to a molecule would yield a linear increase the penalized log p metric, and noted that the ""best molecules"" found by the method in figure 3 also have extremely long carbon chains (long carbon chains are not generally desirable for drug discovery). -----i recommend the authors resubmit after finding a better way to evaluate that their method generates molecules with more useful properties for drug discovery.","a slightly more sophisticated approach might use relaxed, separately trained ranking models in intermediate steps, successively tightened towards the intersection as the iteration progresses.","this paper presents a novel approach to generating molecules using black box recurrent translation.-----the authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.-----then, the recursion takes the top k best molecules and runs another iteration to generate even better molecules, ad infinitum.-----the authors use the newly introduced selfies strings as vocabulary for generation.-----the authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.-----relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.-----the authors also show that this technique can optimize multiple properties at once.----------i am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.------ recursive black box translation seems to be widely applicable to new models.------ the model seems to reach a significantly better state of the art on the metrics proposed.------ none of the baselines seem to use selfies as the string of choice.----- this means it's difficult to tell how much the ""blackbox recursive"" part of the algorithm adds to the model.----- an ablation experiment without bbrt might inform us of how much of the benefit is due to the molecule representation (fig 4a reports the mean, but it would be good to have the same metric as table 1).------ the authors provide an in depth discussion about how having molecular traces would hhelp in drug design.----- this makes the tool seem more widely appealing and useful.----------a few questions would clear up the strengths of the paper:------ is there a connection to the backtranslation work in lample 2018?",wouldn't it be much better to train a separate ranking method to guide the iterative steps?,"this paper presents a novel approach to generating molecules using black box recurrent translation.-----the authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.-----then, the recursion takes the top k best molecules and runs another iteration to generate even better molecules, ad infinitum.-----the authors use the newly introduced selfies strings as vocabulary for generation.-----the authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.-----relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.-----the authors also show that this technique can optimize multiple properties at once.----------i am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.------ recursive black box translation seems to be widely applicable to new models.------ the model seems to reach a significantly better state of the art on the metrics proposed.------ none of the baselines seem to use selfies as the string of choice.----- this means it's difficult to tell how much the ""blackbox recursive"" part of the algorithm adds to the model.----- an ablation experiment without bbrt might inform us of how much of the benefit is due to the molecule representation (fig 4a reports the mean, but it would be good to have the same metric as table 1).------ the authors provide an in depth discussion about how having molecular traces would hhelp in drug design.----- this makes the tool seem more widely appealing and useful.----------a few questions would clear up the strengths of the paper:------ is there a connection to the backtranslation work in lample 2018?","this paper presents a novel approach to generating molecules using black box recurrent translation.-----the authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.-----then, the recursion takes the top k best molecules and runs another iteration to generate even better molecules, ad infinitum.-----the authors use the newly introduced selfies strings as vocabulary for generation.-----the authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.-----relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.-----the authors also show that this technique can optimize multiple properties at once.----------i am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.------ recursive black box translation seems to be widely applicable to new models.------ the model seems to reach a significantly better state of the art on the metrics proposed.------ none of the baselines seem to use selfies as the string of choice.----- this means it's difficult to tell how much the ""blackbox recursive"" part of the algorithm adds to the model.----- an ablation experiment without bbrt might inform us of how much of the benefit is due to the molecule representation (fig 4a reports the mean, but it would be good to have the same metric as table 1).------ the authors provide an in depth discussion about how having molecular traces would hhelp in drug design.----- this makes the tool seem more widely appealing and useful.----------a few questions would clear up the strengths of the paper:------ is there a connection to the backtranslation work in lample 2018?","this paper presents a novel approach to generating molecules using black box recurrent translation.-----the authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.-----then, the recursion takes the top k best molecules and runs another iteration to generate even better molecules, ad infinitum.-----the authors use the newly introduced selfies strings as vocabulary for generation.-----the authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.-----relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.-----the authors also show that this technique can optimize multiple properties at once.----------i am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.------ recursive black box translation seems to be widely applicable to new models.------ the model seems to reach a significantly better state of the art on the metrics proposed.------ none of the baselines seem to use selfies as the string of choice.----- this means it's difficult to tell how much the ""blackbox recursive"" part of the algorithm adds to the model.----- an ablation experiment without bbrt might inform us of how much of the benefit is due to the molecule representation (fig 4a reports the mean, but it would be good to have the same metric as table 1).------ the authors provide an in depth discussion about how having molecular traces would hhelp in drug design.----- this makes the tool seem more widely appealing and useful.----------a few questions would clear up the strengths of the paper:------ is there a connection to the backtranslation work in lample 2018?","----- is it because during recursive translation, the beam search variants have low diversity?----- then, training with stochastic decoding and generation with a beam search should do even better, right?----- this would highlight that the advantage of stochastic decoding is really online in the context of recursive translation, not generally.------ what is the point of fig 4a right?","this paper presents a novel approach to generating molecules using black box recurrent translation.-----the authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.-----then, the recursion takes the top k best molecules and runs another iteration to generate even better molecules, ad infinitum.-----the authors use the newly introduced selfies strings as vocabulary for generation.-----the authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.-----relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.-----the authors also show that this technique can optimize multiple properties at once.----------i am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.------ recursive black box translation seems to be widely applicable to new models.------ the model seems to reach a significantly better state of the art on the metrics proposed.------ none of the baselines seem to use selfies as the string of choice.----- this means it's difficult to tell how much the ""blackbox recursive"" part of the algorithm adds to the model.----- an ablation experiment without bbrt might inform us of how much of the benefit is due to the molecule representation (fig 4a reports the mean, but it would be good to have the same metric as table 1).------ the authors provide an in depth discussion about how having molecular traces would hhelp in drug design.----- this makes the tool seem more widely appealing and useful.----------a few questions would clear up the strengths of the paper:------ is there a connection to the backtranslation work in lample 2018?",0.0804020100502512,0.0,0.050251256281407,0.050251256281407,0.32409381663113,0.0513918629550321,0.1535181236673773,0.1535181236673773,0.09375,0.0,0.0520833333333333,0.0520833333333333,0.32409381663113,0.0513918629550321,0.1535181236673773,0.1535181236673773,0.32409381663113,0.0513918629550321,0.1535181236673773,0.1535181236673773,0.32409381663113,0.0513918629550321,0.1535181236673773,0.1535181236673773,0.1982758620689655,0.0260869565217391,0.1034482758620689,0.1034482758620689,0.32409381663113,0.0513918629550321,0.1535181236673773,0.1535181236673773,16.060014724731445,16.060014724731445,16.060014724731445,16.060014724731445,3.5558652877807617,4.576749801635742,16.060014724731445,7.692692279815674,0.1672380868930397,0.3793735365712782,0.9130808200210628,0.9959870851867169,0.9957961371164715,0.9348330811303229,0.9237004639269347,0.9428954157617153,0.9407031232069041,0.9959870851867169,0.9957961371164715,0.9348330811303229,0.9959870851867169,0.9957961371164715,0.9348330811303229,0.9959870851867169,0.9957961371164715,0.9348330811303229,0.9730687951386564,0.9784325853636763,0.900136842895972,0.9959870851867169,0.9957961371164715,0.9348330811303229
200,https://openreview.net/forum?id=rkMhusC5Y7,"the authors proposed a variant of ensemble method in reinforcement learning for query reformulation. they train multiple specialized sub-agents on disjoint partitions of the training data, and use a meta-agent, which can see all the training data, to decide the final answer. this can speed up the training thanks to parallelization. they observed that this can improve the diversity of learnt reformulations and the overall performance in some cases. ------------------------strengths--------1. the paper is clear and easy to follow.--------2. multiple evaluation metrics and baseline models are considered----------------weaknesses--------1. the proposed method is simple and lacks novelty.--------2. the performance improvement is marginal and some empirical results are not carefully analyzed. ------------------------significance--------exploring a diverse set of strategies is beneficial in reinforcement learning. this paper focuses on two aspects of this problem. one is how to learn diverse agents and the other one is how to efficiently learn these agents efficiently, which are important concerns in practice. ------------------------originality--------the model learning approach they proposed is merely a simple variant of ensemble learning. the main difference is they train sub-agents on disjoint partitions of the training data, which seems a trivial modification although this shows to improve the overall model performance.------------------------technical quality--------overall, the experiments are well-thought, but the following questions need to be explained:--------1. in the introduction, the authors claim three contributions they made in this paper. my question is, if the third one is really an important contribution, why didnt the authors demonstrate it in detail in the main text? attaching it to the appendix could make the reader confused about its significance.--------2. in table-1, the authors claim that their proposed architectures can outperform the baseline rl-10-ensemble with only 1/10 time. the sub-agents are trained on a partition of the training set. my question is, are these sub-agents trained in parallel on different machine? if so, why cannot the rl-10-ensemble be trained in parallel through some multithread or distributed computation? the implementation of the proposed model and the baseline seems not that fair.--------3. the main architecture is described in section 3.3 and 3.4, including the sub-agents and the aggregator. however, in appendix c.1, the authors claim that the gains the proposed method comes mostly from the pool of diverse reformulators, and not from the simple use of a re-ranking function (aggregator). this is confusing because if it is true, the proposed method is really reduced to an ensemble of the baseline model.--------4. in table-2, some of the results are worse than the baseline methods like re-ranker. although the authors claim the re-ranking is a post-processing, re-ranker performs significantly better than the proposed model. if the authors want to better demonstrate the advantages of the proposed model, a comparison between the proposed model with re-ranking and the re-ranker is required.--------5. in table 10, why the proposed method fails to produce the right answer whereas the other methods perform well? summary:--------the authors propose to train multiple distinct agents, each over a different subset of the training set. a meta-agent, known as the aggregator, groups and scores answers from the sub-agents for any given input. ----------------each agent produces a unique reformulation that is applied to the environment, producing an answer for the reformulated query. the aggregator receives the original query and the answers provided by the environment and produces a relevance score for each answer with respect to the original query that is a function of both components.----------------the final answer is select using this relevance score, as well as an aggregate ranking score for over the space of reformulations for each answer.----------------the aggregator is trained to minimize the cross-entropy of the relevance score. each reformulation agent is trained using recall@40 as a reward for retrieving the correct answer from the environment given their reformulation. ----------------the authors argue that learning multiple specialized sub-agents is easier than learning a generalist agent responsible for being able to model the entire training data. authors shows that this strategy is even more generalizable than training an ensemble for the same number of agents over the entire training set. authors apply the approach to query reformulation for document retrieval and qa.----------------review:----------------pros:---------the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset. empirical result show that both the addition of the aggregator and the exclusivity of the agents contributes to this effect. baselines are considerable and in-depth (though it seems like the hui et al., 2017 model that is sota on trec-car could be shown in table 1 as well)---------the paper is well written and easy to understand in the approach.----------------cons:---------the authors could do a better job explaining a couple of unclear points. first, how did the authors come up with equation 2 for computing the relevance score? while the empirical investigation in table 8 indicates it does better than other simpler formulations, its not clear why the authors were motivated to try this one.---------i dont come away with an idea of why the authors proposed approach works better. while the empirical investigation is a contribution in it of itself, the results seem slightly counterintuitive. its not clear why a random partition should be better than a semantically-motivated partition. its also not clear why training the reformulating agents individually on these partitions would do better than an ensemble. i find the paper interesting, but the analysis of these results is missing.----------------questions:--------why does the function for z_j in equation 2 need to be so complicated? why are the cnn features of the query concatenated twice in the first part. what does the dot operator in the second part of the equation correspond to? in this paper, authors proposed an ensemble approach for query reformulation (qr). the basic idea is that 1) train a bunch of models/sub-agents on subsets, e.g., randomly partitioned, of the training data; 2) and then train an additional meta model/meta-agent to aggregate the results from the step 1). they conduct experiments on document retrieval and question answering tasks to show the effectiveness of the proposed model.----------------this paper is well written and easy to follow. --------however there are several my concerns. ----------------1. it is counter intuitive, e.g., why sub-agents trained on full training dataset obtain worse results than on its subset. regarding diversity, one may use different random seeds or different dropout rates instead of sample a subset of training data. ----------------2. the baseline is much lower than the current sota systems. such as the best result on searchqa in this paper is 50.5 in terms of f1 score. however r3 and re-ranker obtains 55.3 and 60.6 respectively. could the proposed approach be adapted on those models? note that those sota systems are released.----------------3. the proposed system is quite similar to nogueira& cho 2017 and buck et al. 2018. i'm not very sure the contribution of this work and its novelty. ----------------questions:--------1. why the authors didn't use beam search during the sub-agent training? --------2. it seems that the proposed framework is a pipeline model: firstly it trains a bunch of sub-agents; and then trains meta-agent. is it possible to fine-tune the model jointly?--------3. what is extra budget in table 1?",pros: - the paper is clear and easy to read - both reviewer 1 and reviewer 2 found the empirical evaluation to be good cons: - some of the reviewers felt that the proposed approach lacked novelty (e.g. with respect to nogueira and cho) - some of the architecture choices seem complicated and it was not fully clear to the reviewers (even after the rebuttal) how and why things were working better in this approach than in other similar ones. i think this is a good paper but it doesn't quite meet the bar for acceptance at this time.,"baselines are considerable and in-depth (though it seems like the hui et al., 2017 model that is sota on trec-car could be shown in table 1 as well)---------the paper is well written and easy to understand in the approach.----------------cons:---------the authors could do a better job explaining a couple of unclear points.",authors apply the approach to query reformulation for document retrieval and qa.----------------review:----------------pros:---------the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.,"however, in appendix c.1, the authors claim that the gains the proposed method comes mostly from the pool of diverse reformulators, and not from the simple use of a re-ranking function (aggregator).",the authors proposed a variant of ensemble method in reinforcement learning for query reformulation.,authors apply the approach to query reformulation for document retrieval and qa.----------------review:----------------pros:---------the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.,authors apply the approach to query reformulation for document retrieval and qa.----------------review:----------------pros:---------the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.,"my question is, are these sub-agents trained in parallel on different machine?",authors apply the approach to query reformulation for document retrieval and qa.----------------review:----------------pros:---------the paper provides convincing empirical evidence that training multiple distinct agents on different partitions of a dataset to learn to reformulate queries for environment feedback is a more efficient and accurate approach than training single or ensemble model on the whole dataset.,0.3006535947712418,0.0529801324503311,0.1437908496732026,0.1437908496732026,0.261437908496732,0.0529801324503311,0.1699346405228758,0.1699346405228758,0.2137404580152671,0.0310077519379844,0.1526717557251908,0.1526717557251908,0.1081081081081081,0.0,0.0900900900900901,0.0900900900900901,0.261437908496732,0.0529801324503311,0.1699346405228758,0.1699346405228758,0.261437908496732,0.0529801324503311,0.1699346405228758,0.1699346405228758,0.0363636363636363,0.0,0.0363636363636363,0.0363636363636363,0.261437908496732,0.0529801324503311,0.1699346405228758,0.1699346405228758,12.627943992614746,12.627943992614746,17.625816345214844,12.627943992614746,7.939052104949951,6.54733943939209,12.627943992614746,12.65310764312744,0.10490151233648423,0.12525775816691423,0.5728049332699618,0.8103388595582959,0.8000061393734635,0.23473814852038793,0.968387861362494,0.9489210770104559,0.9081498878720193,0.976552511127197,0.9769551573674633,0.9249345027024864,0.8103388595582959,0.8000061393734635,0.23473814852038793,0.8103388595582959,0.8000061393734635,0.23473814852038793,0.9721008860665512,0.9716024557752525,0.9074200352361996,0.8103388595582959,0.8000061393734635,0.23473814852038793
201,https://openreview.net/forum?id=rkeH6AEYvr,"summary:----------paper presents a progressive attention mechanism for multi-scale image classification. the network contains a module that prunes non-important regions progressively by a factor of 2, then extract the features through a cnn. after all level features are extracted, the network later aggregates them and produce a final prediction. since bernoulli sampling is used to binarize the region mask, reinforce algorithm is used to update the region network. the paper shows slight improvements on top of a simple cnn (with only 4 layers), on a synthetic mnist and imagenet, with some visualization examples of proposed region network.----------this idea to use a region pruning seems interesting. however, as detailed later, the motivation and experiments are not sufficient to show that it brings new insight to the classification. lacking proper comparisons with earlier methods also undercuts the effectiveness. it is unfortunate but i could not recommend this paper to be accepted in the current version.----------strength----------+ informative graphics to help understand the method-----+ detailed methodology and discussion on experiments----------weakness------ question about the motivation-----in the second paragraph of the introduction, the paper claims ""problem is that computational resources are allocated uniformly to all image regions, no matter how important they are for the task at hand."" will this actually be a problem in training cnn nowadays? it is safe to assume the different regions could be described by some statistics, and the reason why cnn successes in the past years is its capability to capture such statistics during the training with gradient descent. could the author provide more justification for this motivation?---------------- experiments are not sufficient-----as the paper indicated in related work ""multi-scale representations"", this work belongs to the image pyramid method category. however, there is no empirical comparison shown.-----in addition, the paper only tests its proposed method on top of a simple cnn, with 4 convolutional layers as in table 4, and the top-1 accuracy on imagenet is merely 40%. i suggest the author deploy their method to at least vgg-16 and resnet-50 to further strengthen its impact. otherwise, it is hard to convince people this method can generalize to cnn models.-----i suggest authors to conduct additional experiments in their future submission.----------- localization metrics-----since the key contribution of this method is the network learns to prune those not useful regions, should the author provide some numerical results, e.g. the mean intersection of union between the proposed method's region prediction and the ground-truth bounding box? this data should be available in your synthetic mnist and the imagenet one. only showing some example in figure 7 is not that convincing.---------------minor comments:----------1. what's the number in figure 4, 6, next to the marks in figure?-----2. while this kflops per image is informative, should the author consider to put their results in a table, indicates by the level 1,2,3 for a better visualization? the current figure 4 and figure 6 are not straight-forward to grasp in the beginning. this paper proposes a multiscale cnn variant that processes images in a coarse-to-fine setting (low to high resolution). at each resolution, the network both extracts features and computes an attention map to use in processing the next higher resolution. features extracted from all resolutions are merged together and used in the classification decision. experiments show results of the attention mechanism on an mnist-based synthetic dataset, and show classification accuracy on imagenet.----------unfortunately, the paper has major shortcomings.----------experimental results are simply not convincing as imagenet classification performance is far below modern baselines. figure 6 reports top-1 accuracy on imagenet in the 50-55% range. modern cnn baselines such as vgg [simonyan and zisserman, very deep convolutional networks for large-scale image recognition, 2014] or residual networks [he et al, deep residual learning for image recognition, 2015] achieve top-1 accuracy in the 70-80% range. this is an enormous performance gap.----------the paper should be using a modern network architecture as a baseline and modifying it to include the proposed multiscale feature extraction, location, aggregation, and merging modules. then test if those modifications improve imagenet classification performance. the baseline cnn architecture currently used is too impoverished to allow comparison to recent results in other work.----------furthermore, this is not the first paper to propose multiscale methods or adaptive computation. though some work is cited, experimental comparison to competing methods in both of these areas is entirely lacking. this alone is sufficient cause for rejection.----------for example, how does the proposed attention mechanism compare to autofocus [najibi et al]? how does the multiscale approach compare to feature pyramid networks [lin et al] or multi-scale dense networks [huang et al]?----------there is also highly relevant work that is not cited by the paper, including:----------(1) sbnet: sparse blocks network for fast inference----- mengye ren, andrei pokrovsky, bin yang, raquel urtasun----- cvpr 2018----------this paper proposes an architecture with internal dynamic attention masks to reduce computation at inference.----------(2) multigrid neural architectures----- tsung-wei ke, michael maire, stella x. yu----- cvpr 2017----------this paper present a multiscale network design that processes all scales simultaneously (an alternative to the proposed coarse-to-fine approach), yielding improved results on imagenet. it also conducts experiments on a synthetic mnist dataset to demonstrate implicit attentional capabilities of its network architecture.----------both of these recent papers highly overlap the central themes (multiscale, attention) in the submitted work and should likely be included in citation, discussion, and experimental comparison. in this paper, a coarse-to-fine pyramidal scheme, along with learned attention/routing mechanism, is proposed to handle classification in large images, whereby the receptive field of attends to a much smaller part of the image than would be useful for classification. to overcome this problem, the authors propose a mechanism by which attention mechanisms are propagated from coarse to fine levels. a few experiments are provided to show the benefits of the proposed mechanism over convolutional network baselines, especially in situations where the input size is relatively large compared to the effective receptive field. the paper is written in a straightforward manner and is mostly easy to read. ----------while the overall idea is interesting, the authors seem to have completely missed reference [1] from cvpr 2017, which provides a very similar set of ideas but in a more formal and potentially more powerful setting of multigrid networks, incorporating both multiscale and attention mechanisms. ----------furthermore the experiments on imagenet have extremely weak baselines; if i understand correctly, the top-1 accuracy is not even 55% for the best model, which is a far cry from modern architectures and baselines. it is well known that results on toy models hardly translate to real-world networks, and this makes me wonder about how applicable the results of this paper would be for large scale resnet networks. it would have been much more convincing if the authors would have taken modern baselines and then shown that using their top-down approach allows scaling to even larger images, for example. as it is, the largest image size considered is 256x256, which is exactly what can already be well handled by modern resnet's. for these very weak empirical results, i feel that the paper is not yet ready.----------some specific questions:----------1. what interpolation/downsampling algorithm was used to create the coarser resolutions? is the performance sensitive to this algorithm?----------2. for figure 4, please provide the details about the figures in the caption itself, to avoid needing to scroll back and forth between the text and the figure. i found figure 4 quite confusing to parse. similarly for figure 6. fixing these would be very helpful.----------[1] multigrid neural architectures (http://openaccess.thecvf.com/content_cvpr_2017/papers/ke_multigrid_neural_architectures_cvpr_2017_paper.pdf)","this paper studies generalizations of variational autoencoders to non-euclidean domains, modeled as products of constant curvature riemannian manifolds. the framework allows to simultaneously learn the latent representations as well as the curvature of the latent domain. ----------reviewers were unanimous at highlighting the significance of this work at developing non-euclidean tools for generative modeling. despite the somewhat preliminary nature of the empirical evaluation, there was consensus that the paper puts forward interesting tools that might spark future research in this direction. given those positive assessments, the ac recommends acceptance.","a few experiments are provided to show the benefits of the proposed mechanism over convolutional network baselines, especially in situations where the input size is relatively large compared to the effective receptive field.","how does the multiscale approach compare to feature pyramid networks [lin et al] or multi-scale dense networks [huang et al]?----------there is also highly relevant work that is not cited by the paper, including:----------(1) sbnet: sparse blocks network for fast inference----- mengye ren, andrei pokrovsky, bin yang, raquel urtasun----- cvpr 2018----------this paper proposes an architecture with internal dynamic attention masks to reduce computation at inference.----------(2) multigrid neural architectures----- tsung-wei ke, michael maire, stella x. yu----- cvpr 2017----------this paper present a multiscale network design that processes all scales simultaneously (an alternative to the proposed coarse-to-fine approach), yielding improved results on imagenet.","experiments show results of the attention mechanism on an mnist-based synthetic dataset, and show classification accuracy on imagenet.----------unfortunately, the paper has major shortcomings.----------experimental results are simply not convincing as imagenet classification performance is far below modern baselines.",summary:----------paper presents a progressive attention mechanism for multi-scale image classification.,"experiments show results of the attention mechanism on an mnist-based synthetic dataset, and show classification accuracy on imagenet.----------unfortunately, the paper has major shortcomings.----------experimental results are simply not convincing as imagenet classification performance is far below modern baselines.","how does the multiscale approach compare to feature pyramid networks [lin et al] or multi-scale dense networks [huang et al]?----------there is also highly relevant work that is not cited by the paper, including:----------(1) sbnet: sparse blocks network for fast inference----- mengye ren, andrei pokrovsky, bin yang, raquel urtasun----- cvpr 2018----------this paper proposes an architecture with internal dynamic attention masks to reduce computation at inference.----------(2) multigrid neural architectures----- tsung-wei ke, michael maire, stella x. yu----- cvpr 2017----------this paper present a multiscale network design that processes all scales simultaneously (an alternative to the proposed coarse-to-fine approach), yielding improved results on imagenet.","as it is, the largest image size considered is 256x256, which is exactly what can already be well handled by modern resnet's.","how does the multiscale approach compare to feature pyramid networks [lin et al] or multi-scale dense networks [huang et al]?----------there is also highly relevant work that is not cited by the paper, including:----------(1) sbnet: sparse blocks network for fast inference----- mengye ren, andrei pokrovsky, bin yang, raquel urtasun----- cvpr 2018----------this paper proposes an architecture with internal dynamic attention masks to reduce computation at inference.----------(2) multigrid neural architectures----- tsung-wei ke, michael maire, stella x. yu----- cvpr 2017----------this paper present a multiscale network design that processes all scales simultaneously (an alternative to the proposed coarse-to-fine approach), yielding improved results on imagenet.",0.1300813008130081,0.0165289256198347,0.0975609756097561,0.0975609756097561,0.1616161616161616,0.0204081632653061,0.0808080808080808,0.0808080808080808,0.0769230769230769,0.03125,0.0615384615384615,0.0615384615384615,0.0392156862745098,0.0,0.0392156862745098,0.0392156862745098,0.0769230769230769,0.03125,0.0615384615384615,0.0615384615384615,0.1616161616161616,0.0204081632653061,0.0808080808080808,0.0808080808080808,0.0530973451327433,0.0,0.0530973451327433,0.0530973451327433,0.1616161616161616,0.0204081632653061,0.0808080808080808,0.0808080808080808,9.074655532836914,9.741802215576172,12.059764862060549,9.074655532836914,7.984194755554199,9.741802215576172,9.074655532836914,5.333911895751953,0.356916295358021,0.4764333371223408,0.8887140903463947,0.786183220017593,0.8870689639500305,0.9045172288257287,0.9842192235686192,0.9796603001321235,0.9074557592186314,0.976784326604223,0.9766925162201384,0.9047950165113371,0.9842192235686192,0.9796603001321235,0.9074555601151253,0.786183220017593,0.8870689639500305,0.9045171433114838,0.12928539162400868,0.2894354664000836,0.9164304584250398,0.786183220017593,0.8870689639500305,0.9045171106632258
202,https://openreview.net/forum?id=rkeMHjR9Ym,"this paper studies the ability of sgd to learn dynamics of a linear system + non-linear activation. that is, in the standard lti setting, the dynamics of a system evolve according to----------------h_{t+1} = ah_t + bu_t,----------------on input u_t.----------------in addition, this paper considers the setting where the evolution is:----------------h_{t+1} = \phi(ah_t + bu_t)----------------for \phi a non-linear activation function. ----------------this is a difficult problem. though system identification was for many decades a large and active area in the control community, the understanding of system identification from a modern statistical perspective (understanding sample complexity and computational complexity simultaneously) is surprisingly lacking. this is evidenced by the fact that the first results along these lines for the simplest possible (siso, lti) system, came only recently (hardt, ma, recht 16). ----------------this paper attacks a more general setting, due to the presence of the nonlinearity.----------------however, the present setting is significantly limited in another sense: the authors assume that the state is observed directly. this is in contrast to the typical situation where we observe only a projection of the state, or possibly even a noisy such projection. indeed, this is one of the critical complications in the work of hardt, ma and recht. without it, i.e., under the assumption that the entire state trajectory can be directly observed, much more is possible, and indeed much more has been done. for example, work by bento, ibrahimi and montanari 10, solves a more difficult problem in that they estimate sparse dynamics (in appropriate sample complexity). jalali and sanghavi 11 generalized the work of bento et al., to the setting where some of the components of the state are not all observed, but rather some are latent.----------------the motivating application for this work is estimating rnns. in this case, the state variable represents the critical information that is carried from one time to the next in the rnn. presumably the setting here is to show that if indeed data are generated by an rnn, then we can compute this using sgd and backprop. towards this, the assumption of having access to the internal state is a difficult one. on the one hand, this is a hard and important problem. on the other, we really wont have access to such an internal state. there are of course other problematic aspects, such as robustness, the inability to use relu (defn 3.1). but the observation model seems important. again, i believe this is especially so, because the considerable complications present in hardt, ma, rect 16 specifically seemed to be a consequence of the observation model being partial.----------------the inability to use relu at first look does not seem like a great limitation. but then one problematic aspect here seems that the proof concept and direction critically rely on this, as they basically reduce to the setting of linear activations  something which, presumably, is impossible for something like relu. so it is not only the results, but also the developed machinery, that seem to be inherently limited.----------------this is, overall, an interesting paper, attacking an important and also very challenging area. as with all papers in this vein, we are left with having to make a judgement call on whether this simplified scenario is indeed a good first step towards solving the problems we are hoping to solve. is it developing the right insight, right tools, etc. while i find there is a lot of interesting and good work in this paper, i am not completely convinced about this last point. the paper studies discrete time dynamical systems with a non-linear state equation. they assume the non-linear function is assumed to be \beta-increasing like leaky relu. under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running sgd on a fixed length trajectory gives logarithmic convergence.----------------the paper is well-written and proves strong convergence properties. the deterministic result does not seem very novel and uses the idea of one-point strong convexity which has been studied in various prior works. however the bounding of the condition number of the data matrix is interesting and guarantees are near-optimal. the faster convergence for odd activations is a good observation. overall, i think the paper is good. i do list some concerns:--------questions/concerns:--------- the deterministic theorem (theorem 4.1) seems similar to theorem 3 in [1] with sgd instead of gd. also under the distribution being symmetric, it can be derived from [2] with . --------- can the ideas be extended to other commonly used activations such as relus/sigmoids? sigmoids have exponentially small slope near origin.--------- the proof seems to rely on the fact that due to the gaussian input added each time step and stable system assumption after a sufficient number of time steps, the input-output pairs will not be highly correlated. so the data is sufficiently uncorrelated taking enough data. what happens if this data at each step is not gaussian?--------- in the unstable setting, the solution proposed just samples from different trajectories which by default are independent hence correlation is not an issue, this seems a bit like cheating. --------- in rnns, the motivation of the work, the hidden vectors are not observed, thus this setting seems a bit restrictive.--------- if sgd was performed on only one truncated series, do the results still hold?----------------other comments:--------- there has been previous work on generalized linear models which work in more general settings like glmtron [3]. the authors should update prior work on generalized linear models as well as neural networks.--------- typo on page 2 y_t = h_{t+1} not y_t = h_t.----------------[1] dylan j. foster, ayush sekhari, and karthik sridharan. uniform convergence of gradients for non-convex learning and optimization. nips 2018.--------[2] surbhi goel, adam klivans, and raghu meka. learning one convolutional layer with overlapping patches. icml 2018.--------[3] sham m. kakade et al. efficient learning of generalized linear and single index models with isotonic regression. nips 2011.----------------------------------------------i would be maintaining the same score. i agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to relu and other activations to take it a notch higher.","this paper shows convergence of stochastic gradient descent for the problem of learning weight matrices for a linear dynamical system with non-linear activation. reviewers agree that the problem considered is both interesting and challenging. however the paper makes many simplifying assumptions - 1) both input and hidden state are observed, a very non standard assumption, 2) analysis requires increasing activation functions, cannot handle relu functions. i agree with r2 and think these assumptions make the results significantly weaker. r1 and r3 are more optimistic, but authors response does not give an insight into how one might extend this analysis to the setting where hidden state is not observed. relaxing these assumptions will make the paper more interesting.","the authors should update prior work on generalized linear models as well as neural networks.--------- typo on page 2 y_t = h_{t+1} not y_t = h_t.----------------[1] dylan j. foster, ayush sekhari, and karthik sridharan.","--------- in rnns, the motivation of the work, the hidden vectors are not observed, thus this setting seems a bit restrictive.--------- if sgd was performed on only one truncated series, do the results still hold?----------------other comments:--------- there has been previous work on generalized linear models which work in more general settings like glmtron [3].","----------------this paper attacks a more general setting, due to the presence of the nonlinearity.----------------however, the present setting is significantly limited in another sense: the authors assume that the state is observed directly.",this paper studies the ability of sgd to learn dynamics of a linear system + non-linear activation.,"sigmoids have exponentially small slope near origin.--------- the proof seems to rely on the fact that due to the gaussian input added each time step and stable system assumption after a sufficient number of time steps, the input-output pairs will not be highly correlated.","under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running sgd on a fixed length trajectory gives logarithmic convergence.----------------the paper is well-written and proves strong convergence properties.",--------- can the ideas be extended to other commonly used activations such as relus/sigmoids?,"--------- in rnns, the motivation of the work, the hidden vectors are not observed, thus this setting seems a bit restrictive.--------- if sgd was performed on only one truncated series, do the results still hold?----------------other comments:--------- there has been previous work on generalized linear models which work in more general settings like glmtron [3].",0.0903225806451612,0.0,0.0516129032258064,0.0516129032258064,0.1871345029239766,0.0236686390532544,0.1403508771929824,0.1403508771929824,0.2666666666666666,0.0675675675675675,0.1733333333333333,0.1733333333333333,0.208955223880597,0.0606060606060606,0.1641791044776119,0.1641791044776119,0.1728395061728395,0.0125,0.0987654320987654,0.0987654320987654,0.2375,0.0379746835443038,0.1375,0.1375,0.0763358778625954,0.0,0.0458015267175572,0.0458015267175572,0.1871345029239766,0.0236686390532544,0.1403508771929824,0.1403508771929824,10.747783660888672,6.855705261230469,15.847160339355469,11.981337547302246,12.42852783203125,9.631519317626951,11.981337547302246,9.562541961669922,0.4191146496181251,0.5191317614375635,0.840674579083969,0.7675744671479048,0.8223175865640716,0.08576275497211057,0.9877287185687358,0.9848736052465505,0.947724168650754,0.9741712706405358,0.9699401465627988,0.8799515195352587,0.17008703467398742,0.33909999807470537,0.754553591579199,0.6802529825010334,0.5953607296486151,0.9334491207011186,0.7528571831760199,0.821458758325409,0.8835790881018121,0.7675744671479048,0.8223175865640716,0.08576275497211057
203,https://openreview.net/forum?id=rkeSiiA5Fm,"deep learning 3d shapes using alt-az anisotropic 2-sphere convolution----------------this paper presents a polar anisotropic convolution scheme on a unit sphere. the known non-shift-invariance problems of current manifold neural nets are avoided by replacing filter translation with filter rotation on a sphere. spherical convolution are thus enabled and are rotation invariant compared to manifold convolutions. this shift also enables a proposed angular max-pooling scheme. results are presented on mesh projections, shape classification and shape retrieval. ----------------the paper generally reads well. tackling the learning problem on a unit sphere has high potential, however, the proposed paper seems to be highly constrained by heuristics on a 2-sphere, such as constraining filters on a reduced rotation group to 2 rotations. this could be fine for many 3d application, but results may lack an exhaustive comparison with other spherical and manifold-based methods on the proposed experiments. currently, several variants of data augmentations are used, and discussion may lack an explicit comparison with the state-of-the-art of spherical and spectral methods. this may impair understanding in which context the proposed method would work best.------------------------other comments, possible clarification and improvements:----------------[method]--------- can this be extended to unit 2-balls?--------- isn't the ""alt-az rotation group"" the same as so(3)? if orientation is removed, what quotient group would this be?--------- what is the benefit of containing a filter on this quotient group rather than using convolution filters within the full rotation group? could a simple experiment convince the reader that the proposed approach is better than using convolutions in so(3)?--------- is there a dependence created by the spherical parameterization strategy?--------- how robust is the convolution scheme to topological defects, such as holes, noise?--------- spherical images may induce parameterization distorsion if using a lat-lon grid, which would require complex variable filters on the spherical image. are these variable filters burdening the computational complexity?--------- how to handle the distortion induced by the spherization process?--------- how to handle discontinuities around the sphere poles?--------- computing geodesic may be costly - how does impact performance?--------- does this rely on data augmentation to cover rotation invariance of filters?--------- now icosahedrons are used - could the convolution work on an arbitrary mesh discretization, ranging from an ideal isoparametric sphere to a highly irregularly-triangulated mesh?--------- the remeshing strategy to a sphere also looses information from the original mesh connectivity - for instance, links between mesh nodes on the original surface may convey important information (e.g., brain connectivity in neuroscience), remising to a sphere would loose such connectivity information.----------------[results]--------- the experiments shows the proposed method with several augmented approaches - how exactly are data augmented?--------- comparison with other spherical methods (cohen et al 2018), or manifold-based methods (monti et al 2018)? illustrating the pros and cons with these respective state-of-the-art?--------- improvements could be better emphasized in fig 6, table 3 - how is the method better than others? # weaknesses--------applications are a bit unclear.--------it would be nice to see a better case made for spherical convolutions within the experimental section. the experiments on shrec17 show all three spherical methods under-performing other approaches. it leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods. is there a task that this representation significantly outperforms other spherical methods and non-spherical methods? or a specific useful application where spherical methods in general outperform other approaches? ----------------# strengths:--------the method is well developed and explained. --------ability to implement in a straight-forward manner on gpu.","strengths: well written paper on a new kind of spherical convolution for use in spherical cnns. evaluated on rigid and non-rigid 3d shape recognition and retrieval problems. paper provides solid strategy for efficient gpu implementation. weaknesses: there was some misunderstanding about the properties of the alt-az convolution detected by one of the reviewers along with some points needing clarifications. however, discussion of these issues appears to have led to a resolution of the issues. contention: the weaknesses above were discussed in some detail, but the procedure was not particularly contentious and the discussion unfolded well. all reviewers rate the paper as accept, the paper clearly provides value to the community and therefore should be accepted.","are these variable filters burdening the computational complexity?--------- how to handle the distortion induced by the spherization process?--------- how to handle discontinuities around the sphere poles?--------- computing geodesic may be costly - how does impact performance?--------- does this rely on data augmentation to cover rotation invariance of filters?--------- now icosahedrons are used - could the convolution work on an arbitrary mesh discretization, ranging from an ideal isoparametric sphere to a highly irregularly-triangulated mesh?--------- the remeshing strategy to a sphere also looses information from the original mesh connectivity - for instance, links between mesh nodes on the original surface may convey important information (e.g., brain connectivity in neuroscience), remising to a sphere would loose such connectivity information.----------------[results]--------- the experiments shows the proposed method with several augmented approaches - how exactly are data augmented?--------- comparison with other spherical methods (cohen et al 2018), or manifold-based methods (monti et al 2018)?","are these variable filters burdening the computational complexity?--------- how to handle the distortion induced by the spherization process?--------- how to handle discontinuities around the sphere poles?--------- computing geodesic may be costly - how does impact performance?--------- does this rely on data augmentation to cover rotation invariance of filters?--------- now icosahedrons are used - could the convolution work on an arbitrary mesh discretization, ranging from an ideal isoparametric sphere to a highly irregularly-triangulated mesh?--------- the remeshing strategy to a sphere also looses information from the original mesh connectivity - for instance, links between mesh nodes on the original surface may convey important information (e.g., brain connectivity in neuroscience), remising to a sphere would loose such connectivity information.----------------[results]--------- the experiments shows the proposed method with several augmented approaches - how exactly are data augmented?--------- comparison with other spherical methods (cohen et al 2018), or manifold-based methods (monti et al 2018)?",or a specific useful application where spherical methods in general outperform other approaches?,deep learning 3d shapes using alt-az anisotropic 2-sphere convolution----------------this paper presents a polar anisotropic convolution scheme on a unit sphere.,"are these variable filters burdening the computational complexity?--------- how to handle the distortion induced by the spherization process?--------- how to handle discontinuities around the sphere poles?--------- computing geodesic may be costly - how does impact performance?--------- does this rely on data augmentation to cover rotation invariance of filters?--------- now icosahedrons are used - could the convolution work on an arbitrary mesh discretization, ranging from an ideal isoparametric sphere to a highly irregularly-triangulated mesh?--------- the remeshing strategy to a sphere also looses information from the original mesh connectivity - for instance, links between mesh nodes on the original surface may convey important information (e.g., brain connectivity in neuroscience), remising to a sphere would loose such connectivity information.----------------[results]--------- the experiments shows the proposed method with several augmented approaches - how exactly are data augmented?--------- comparison with other spherical methods (cohen et al 2018), or manifold-based methods (monti et al 2018)?","could a simple experiment convince the reader that the proposed approach is better than using convolutions in so(3)?--------- is there a dependence created by the spherical parameterization strategy?--------- how robust is the convolution scheme to topological defects, such as holes, noise?--------- spherical images may induce parameterization distorsion if using a lat-lon grid, which would require complex variable filters on the spherical image.","this could be fine for many 3d application, but results may lack an exhaustive comparison with other spherical and manifold-based methods on the proposed experiments.","are these variable filters burdening the computational complexity?--------- how to handle the distortion induced by the spherization process?--------- how to handle discontinuities around the sphere poles?--------- computing geodesic may be costly - how does impact performance?--------- does this rely on data augmentation to cover rotation invariance of filters?--------- now icosahedrons are used - could the convolution work on an arbitrary mesh discretization, ranging from an ideal isoparametric sphere to a highly irregularly-triangulated mesh?--------- the remeshing strategy to a sphere also looses information from the original mesh connectivity - for instance, links between mesh nodes on the original surface may convey important information (e.g., brain connectivity in neuroscience), remising to a sphere would loose such connectivity information.----------------[results]--------- the experiments shows the proposed method with several augmented approaches - how exactly are data augmented?--------- comparison with other spherical methods (cohen et al 2018), or manifold-based methods (monti et al 2018)?",0.2113207547169811,0.0076045627376425,0.0981132075471698,0.0981132075471698,0.2113207547169811,0.0076045627376425,0.0981132075471698,0.0981132075471698,0.0615384615384615,0.0,0.0615384615384615,0.0615384615384615,0.1571428571428571,0.0434782608695652,0.0857142857142857,0.0857142857142857,0.2113207547169811,0.0076045627376425,0.0981132075471698,0.0981132075471698,0.2099447513812155,0.0,0.1104972375690607,0.1104972375690607,0.1258741258741259,0.0,0.0699300699300699,0.0699300699300699,0.2113207547169811,0.0076045627376425,0.0981132075471698,0.0981132075471698,9.22502899169922,9.437572479248049,14.971792221069336,9.437572479248049,9.437572479248049,9.3375244140625,9.437572479248049,8.81322956085205,0.9514437956731034,0.9556919984325265,0.8912894634816866,0.9514437956731034,0.9556919984325265,0.8912894634816866,0.9172905713560299,0.9294835773026947,0.29342665539153284,0.9681353393845796,0.9718908528544048,0.40217287701417864,0.9514437956731034,0.9556919984325265,0.8912893773134487,0.9720722221664979,0.9709969666743966,0.9173791781302923,0.9597247396100372,0.9587357845535346,0.6244579097123684,0.9514437956731034,0.9556919984325265,0.8912894634816866
204,https://openreview.net/forum?id=rkeZ9a4Fwr,"this paper examines adversarial attacks to a vae. it is known that by small norm perturbations on the conditioning input x of a vae can dramatically change the generated output. this paper-----empirically illustrates that alternative objectives can improve robustness, in the sense of previously proposed adversarial attacks such as (tabacof et al. (2016); gondim-ribeiro et al. (2018); kos et al.(2018)). this paper is concerned with the stability of reconstructions and their quality, and proposes seatbelt-vae to remedy some of the shortcomings in the original vae objective. ----------the key idea of the seatbelt-vae is introducing a conditionally gaussian chains (of length l) in the encoder and decoder distributions of a vae. this is a plausible and sensible idea. then the authors evaluate the robustness of reconstructions under various output attacks.----------the methodological part of the paper is quite well written and easy to follow, despite the fact that it is somewhat overloaded with too many abbreviations. the experimental section is harder to read as the motivations and its organization is not clearly stated. overall, this section feels as if it is too hastily written, many results put into appendix without much discussion. the organization can be much more improved.----------the disentanglement achieved by this novel representation is characterized only anecdotally and by contrasting the resulting objectives to a beta-vae. it would have been much more informative to illustrate and discuss further the representations learned by such a conditionally gaussian architecture. figure 6 and 7 partially try to achieve this by showing the interplay of depth l and the inverse-dispersion parameter beta but i found it hard to interpret this results, for which an entire page is devoted.----------in the experiments, the elbo is reported for various methods. i would argue that the elbo is not a very representative proxy for robustness. for example vae elbo and beta-vae elbo are both lower bounds of the true marginal likelihood and it is possible that beta-vae is much lower while attaining a higher robustness in the sense of being resilient to suitably defined attacks.----------the authors claim that there are no clear classification tasks for the datasets -- but this is not accurate as both celeb-a has clear classification tasks in the form of predicting attributes. it would have been really quite informative if adversarial accuracy on downstream tasks would have been reported. relying on qualitative results in figure 1 is only providing partial evidence about the approach.----------robustness to independent noise, as the authors have, is a good experiment to have -- however typical adversarial examples may be quite structured and such a randomized strategy may not give an accurate indication about the nature of the representation.----------overall, the paper is quite promising but i feel that one more iteration maybe needed. the authors of this paper propose a new vae model called seatbelt-vae and investigate its robustness to output and latent adversarial attacks. inspired by beta-tcvae, dlgm, and biva, seatbelt-vae allows multiple latent layers, enforce disentanglement via weighted total correlation on the top latent layer, and conditions the likelihood on all latent layers. robustness to adversarial attacks is the focus in experiments. visual and quantitative comparisons show that seatbelt-vae is more robust for latent attack than benchmarks. specifically i have the following three concerns:----------1. defining elbo using samples from the entire dataset may bring in some benefits, but it complicates the calculation of elbo and related distributions when minibatch or single sample are used in learning and inference. please explicitly discuss this issue.----------2. i would like to see how seatbelt-vae performs in sampling and generating new samples, instead of just reconstruction. ----------3. similarly, it would be beneficial to investigate disentanglement, that is the interpretation of the top latent factors in seatbelt-vae. ----------minors: -----factor analysis -> factor analysis-----section x -> section x-----figure x -> figure x i am a bit confused about the model. while i understand the generative model in fig 2a, i am a bit puzzled about the choice of proposal distribution eq(7)/fig 2b for the seatbelt-vae. the key claim of the paper is that an attacker has to attack all layers at the same time to attack the reconstruction in eq (12). however, figure 2b and eq(7) claim that all deeper layers only depend on the previous layer in the approximate posterior. since in (12) we rely on the posterior for the attack, a successful attack on layer m < l should immediately generate the correct values from q_phi(z_{i+1}|z_i) i=m,...l-1. so from that point of view it is not a seatbelt, as since in fig 2b if the attacker manages to control z^1, he has immediate control of z_2 and therefore the now attacker controlled z_2 will directly feed into the generated target. what might be is that the optimization problem (12) becomes harder to solve because of the increased model-complexity. ----------i am not too impressed by the attacks presented in fig1 as well as the appendix. one of the key points of the old adversarial attacks was that the attack-image was indistinguishable from the true image by a human. however, the adversarial inputs, even for vaes are clearly not part of the distribution and the errors reported for eq (12) are very large to the point where attacking via the target image would probably be harder to spot. if we for example look at page 24, second row: there is no way, that the adversarial image has a likelihood similar to the target. this looks more like the algorithm did not manage to find a suitable direction.----------i am therefore not sure whether the evaluation is correct: if we did not manage to find an attack image, does it proof there is none? and is it meaningful to report their error values if we did not manage an attack?-----btw: did the optimization of (12) begin with d=x_t-x or d=0? maybe starting with d=x_t-x would be more meaningful because it would make it easiest for an attacker to ensure the correct reconstruction.----------given the quality of the attack images, the error of eq(12) should be reported when choosing d=x_t-x as a baseline in fig 5. it would also be good to see the actual vae values.----------unfortunately, the reconstructions on dsprites are bad. but an important experiment could be to check whether you can attack the orientation of an object. orientation is difficult to regularize via tc, since the parameterisation is inherently circular. thus tc might make it difficult to encode orientation in higher layers and it should be easier to attack.","this work a ""seatbelt-vae"" algorithm to improve the robustness of vae against adversarial attacks. the proposed method is promising but the paper appears to be hastily written and leave many places to improve and clarify. this paper can be turned into an excellent paper with another round of throughout modification.","relying on qualitative results in figure 1 is only providing partial evidence about the approach.----------robustness to independent noise, as the authors have, is a good experiment to have -- however typical adversarial examples may be quite structured and such a randomized strategy may not give an accurate indication about the nature of the representation.----------overall, the paper is quite promising but i feel that one more iteration maybe needed.",the key claim of the paper is that an attacker has to attack all layers at the same time to attack the reconstruction in eq (12).,the authors of this paper propose a new vae model called seatbelt-vae and investigate its robustness to output and latent adversarial attacks.,this paper examines adversarial attacks to a vae.,the authors of this paper propose a new vae model called seatbelt-vae and investigate its robustness to output and latent adversarial attacks.,the authors of this paper propose a new vae model called seatbelt-vae and investigate its robustness to output and latent adversarial attacks.,please explicitly discuss this issue.----------2.,the key claim of the paper is that an attacker has to attack all layers at the same time to attack the reconstruction in eq (12).,0.2666666666666666,0.0338983050847457,0.15,0.15,0.2597402597402597,0.0533333333333333,0.1558441558441558,0.1558441558441558,0.4054054054054054,0.0833333333333333,0.1891891891891892,0.1891891891891892,0.2372881355932203,0.0701754385964912,0.135593220338983,0.135593220338983,0.4054054054054054,0.0833333333333333,0.1891891891891892,0.1891891891891892,0.4054054054054054,0.0833333333333333,0.1891891891891892,0.1891891891891892,0.0350877192982456,0.0,0.0350877192982456,0.0350877192982456,0.2597402597402597,0.0533333333333333,0.1558441558441558,0.1558441558441558,12.44789695739746,12.44789695739746,16.854312896728516,9.03674030303955,7.540163993835449,12.44789695739746,9.036741256713867,5.17137622833252,0.9235951134952581,0.9556580067324262,0.9407940470364643,0.23845877192165482,0.4429295358380934,0.8320994018119332,0.9697399359036771,0.9649432424751245,0.8684117622786774,0.9491857317525882,0.9500726410990209,0.36808698131795714,0.9697399359036771,0.9649432424751245,0.868411689731635,0.9697399359036771,0.9649432424751245,0.8684115187561446,0.9519115997136177,0.9502816416163881,0.2298375767530231,0.23845877192165482,0.4429295358380934,0.8320994018119332
205,https://openreview.net/forum?id=rkeZRGbRW,"the authors provided empirical analysis of different variants of gans and proposed a regularization scheme to combat the vanishing gradient when the discriminator is well trained. ----------------more specifically, the authors demonstrated the importance of intra-class variance in the discriminators output. methods whose discriminators tend to map inputs of a class to single real values are unable to provide a reliable learning signal for the generator, such as the standard gan and least squares gan. variance in the discriminators output is essential to allow the generator to learn in the presence of a well-trained discriminator. to ensure the discriminators output follows the mixture of two univariate gaussians, the authors proposed to add two additional discriminators which are trained in a similar was as the original gan formulation. the technique is related to linear discriminant analysis. from a broader perspective, the new meta-adversarial learning can be applied to ensure various desirable properties in gans.----------------the performance of variance regularization scheme was evaluated on the cifar-10 and celeba data.----------------summary:----------------i think the paper discusses a very interesting topic and presents an interesting direction for training the gans. a few points are missing which would provide significantly more value to readers. see comments below for details and other points.----------------comments:----------------1. why would a bi-modal distribution be meaningful? deep nets implicitly transform the data which is probably much more effective than using complex bi-modal gaussian distribution; the bi-modal concept can likely be captured using classical techniques.----------------2. on page 4, in eq. (8) and (9), it remains unclear what and really are beyond two-layer mlps; are the results of those two-layer mlps used as the mean of a gaussian distribution, i.e., and ?----------------3. regarding the description above eq. (12), what is really used in practice, i.e., in the experiments? the paper omits many details that seem important for understanding. could the authors provide more details on choosing the generator loss function and why eq. (12) provides satisfying results in practice? ----------------minor comments:----------------1. in sec 2.1, the sentence needs to be corrected: as shown in arjovsky & bottou (2017), the js divergence will be flat everywhere important if p and q both lie on low-dimensional manifolds (as is likely the case with real data) and do not prefectly align.----------------2. last sentence in conclusion: which can be applied to ensure enforce various desirable properties in gans. please remove either ensure or enforce. the paper proposes variance regularizing adversarial learning (vral), a new method for training gans.----------------the motivation is to ensure that the gradient for the generator does not vanish. the authors propose to use a discriminator whose output targets a mixture of two gaussians (one component each for real and fake data). the means and variances are fixed so that the discriminator does not overfit, which ensures that the generator learning is not hindered. ----------------the discriminator itself is trained through two additional meta-discriminators (!) are the meta-discriminators really necessary? have you tried matching moments or using other methods for comparing the distributions?----------------it would be useful to write down the actual loss function so that it's easier to compare with other gan variants. in particular, i'm curious to understand the difference between vral and fisher-gan. the authors discuss this in the end of section 3, but a more careful comparison is needed.----------------the experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.----------------overall, i think that the idea is interesting, but the paper needs more work and does not meet the iclr acceptance bar.----------------fyi, another concurrent submission showed that gradient penalties stabilize training of gans:--------many paths to equilibrium: gans do not need to decrease a divergence at every step--------https://openreview.net/pdf?id=byqpn1za- this paper studies how the variance of the discriminator affect the gradient signal provided to the generator and therefore how it might limit its ability to learn the true data distribution.----------------the approach suggested in this paper models the output of the discriminator using a mixture of two gaussians (one for fake and the other for not fake). this seems like a rather crude approximation as the distribution of each class is likely to be multimodal. can the authors comment on this? could they extend their approach to use a mixture of multimodal distributions?----------------the paper mentions that fixing the means of the distribution can be problematic during optimization as the discriminators goal is to maximize the difference between these two means.. this relates to my previous comment where the distribution might not be unimodal. in this case, shifting the mean doesnt seem to be a good solution and might just yield to oscillations between different modes. can you please comment on this?----------------mode collapse: can you comment on the behavior of your approach w.r.t. to mode collapse?----------------implementation details: how is the mean of the two gaussians initialized? ----------------relation to instance noise and regularization techniques: instance noise is a common trick being used to train gans, see e.g. http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/--------this also relates to some regularization techniques, e.g. roth et al., 2017 that provides a regularizer that amounts to convolving the densities with white gaussian noise. can you please elaborate on the potential advantages of the proposed solution over these existing techniques?----------------comparison to existing baselines: given that the paper addresses the stability problem, i would expect some empirical comparison to at least one or two of the stability methods cited in the introduction, e.g. gulrajani et al., 2017 or roth et al., 2017.----------------relation to kernel mmd: can the authors elaborate on how their method relates to approaches that replace the discriminator with mmd nets. e.g.--------- training generative neural networks via maximum mean discrepancy optimization, dziugaite et al--------- generative models and model criticism via optimized maximum mean discrepancy, sutherland et al--------more explicitly, the variance in these methods can be controlled via the bandwidth of the kernel and i therefore wonder what would one use a simple mixture of gaussians instead?","the reviewers found a number of short-comings in this work that would prevent it from being accepted at iclr in its current form, both in terms of writing (not specifying the loss function), experiments that are too limited, and inconclusive comparisons with existing regularization techniques. i recommend the authors take into account the feedback from reviewers in any follow-up submissions.","----------------minor comments:----------------1. in sec 2.1, the sentence needs to be corrected: as shown in arjovsky & bottou (2017), the js divergence will be flat everywhere important if p and q both lie on low-dimensional manifolds (as is likely the case with real data) and do not prefectly align.----------------2.","the authors discuss this in the end of section 3, but a more careful comparison is needed.----------------the experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.----------------overall, i think that the idea is interesting, but the paper needs more work and does not meet the iclr acceptance bar.----------------fyi, another concurrent submission showed that gradient penalties stabilize training of gans:--------many paths to equilibrium: gans do not need to decrease a divergence at every step--------https://openreview.net/pdf?id=byqpn1za- this paper studies how the variance of the discriminator affect the gradient signal provided to the generator and therefore how it might limit its ability to learn the true data distribution.----------------the approach suggested in this paper models the output of the discriminator using a mixture of two gaussians (one for fake and the other for not fake).","the paper proposes variance regularizing adversarial learning (vral), a new method for training gans.----------------the motivation is to ensure that the gradient for the generator does not vanish.",the authors provided empirical analysis of different variants of gans and proposed a regularization scheme to combat the vanishing gradient when the discriminator is well trained.,"e.g.--------- training generative neural networks via maximum mean discrepancy optimization, dziugaite et al--------- generative models and model criticism via optimized maximum mean discrepancy, sutherland et al--------more explicitly, the variance in these methods can be controlled via the bandwidth of the kernel and i therefore wonder what would one use a simple mixture of gaussians instead?","the authors discuss this in the end of section 3, but a more careful comparison is needed.----------------the experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.----------------overall, i think that the idea is interesting, but the paper needs more work and does not meet the iclr acceptance bar.----------------fyi, another concurrent submission showed that gradient penalties stabilize training of gans:--------many paths to equilibrium: gans do not need to decrease a divergence at every step--------https://openreview.net/pdf?id=byqpn1za- this paper studies how the variance of the discriminator affect the gradient signal provided to the generator and therefore how it might limit its ability to learn the true data distribution.----------------the approach suggested in this paper models the output of the discriminator using a mixture of two gaussians (one for fake and the other for not fake).",the paper omits many details that seem important for understanding.,"the authors discuss this in the end of section 3, but a more careful comparison is needed.----------------the experimental results are pretty limited and lack detailed quantitative evaluation, which makes it harder to compare the performance of the proposed variant to existing algorithms.----------------overall, i think that the idea is interesting, but the paper needs more work and does not meet the iclr acceptance bar.----------------fyi, another concurrent submission showed that gradient penalties stabilize training of gans:--------many paths to equilibrium: gans do not need to decrease a divergence at every step--------https://openreview.net/pdf?id=byqpn1za- this paper studies how the variance of the discriminator affect the gradient signal provided to the generator and therefore how it might limit its ability to learn the true data distribution.----------------the approach suggested in this paper models the output of the discriminator using a mixture of two gaussians (one for fake and the other for not fake).",0.1769911504424778,0.0,0.1061946902654867,0.1061946902654867,0.2477064220183486,0.0277777777777777,0.1192660550458715,0.1192660550458715,0.1777777777777777,0.0,0.1333333333333333,0.1333333333333333,0.2045454545454545,0.0232558139534883,0.1590909090909091,0.1590909090909091,0.1848739495798319,0.0,0.1176470588235294,0.1176470588235294,0.2477064220183486,0.0277777777777777,0.1192660550458715,0.1192660550458715,0.0555555555555555,0.0,0.0555555555555555,0.0555555555555555,0.2477064220183486,0.0277777777777777,0.1192660550458715,0.1192660550458715,7.214409351348877,8.132594108581543,15.986213684082031,7.214409351348877,6.143488883972168,13.804681777954102,7.214409351348877,2.202463865280152,0.9427361159202929,0.9368768872500377,0.039692457013853856,0.11603642170353005,0.2881092754291818,0.7169195500136798,0.9459129126661575,0.9499033417146341,0.8943085310538978,0.9394527238728614,0.9365411313299135,0.8760945711761212,0.21876303981072376,0.2952831167967263,0.5673657357358995,0.11603642170353005,0.2881092754291818,0.7169192688515509,0.8727275479958296,0.8940343895338543,0.9272181368667557,0.11603642170353005,0.2881092754291818,0.7169195500136798
206,https://openreview.net/forum?id=rkgAb1Btvr,"the paper presents a method to detect out-of-distribution or anomalous data points. they argue that fourier networks have lower confidence and thus better estimates of uncertainty in areas far away from training data. they also argue for using large initializations in the first layers and sin(x) as the activation function for the final hidden layer.----------the paper does not seem to have any significant logical reasoning on why their specific architecture works, but ""describes"" what they did. it is not clear what the novelty is, besides that they found an architecture that seems to work. additionally while fourier networks have lower confidence, that does not necessarily mean they are more accurate estimates of uncertainty. however the reviewer does acknowledge that the estimates are mostly likely better than relu networks that are well known for having terrible estimates of uncertainty. i have read the reviews and the comments. overall i am still positive about the paper and i have confirmed the rating.----------======================----------this paper proposes a method for the uncertainty estimates for neural network classifiers, specifically out-of-distribution detection. previous methods use an ensemble of independently trained networks and average the softmax outputs. the authors investigate this method (ensembles of relu networks) and observe three fundamental limitations:-----unreasonable extrapolation, unreasonable agreement between the networks in an ensemble, and the filtering out of features that distinguish the training distribution from some outofdistribution inputs, but do not contribute to the classification (constant functions on the training manifold).----------to mitigate these problems the authors proposed the following:----------- changing the activation function of the last hidden layer to the sin(x) function, and they claimed that this is going to guard against overgeneralization.------ use larger than usual initialization to increase the chances of obtaining more diverse networks for an ensemble.------ they claimed that this combines the out-of-distribution behavior from nearest neighbor methods with the generalization capabilities of neural networks, and achieves greatly improved out-of-distribution detection on standard data sets.----------the paper addresses an important problem, out of distribution detection, by proposing a fourier network which is somewhere between a relu network (small initialization) and a nearest neighbor classifier (large initialization). the authors claimed that this leads to an out-of-distribution detection which is better than either of them.----------the paper is well written and easy to follow. the authors did an interesting and precise investigation in how to force the confidence score to decay like a gauss function by proposing to use the fourier transform of such a gauss function. by doing so they get the advantage of relu (ability to generalize) and prevent the network to become arbitrarily certain of its classification for all points. however, the authors claimed that when they switch the activation function to sin(x) the increase of |x| will usually stop at the first maximum or minimum of sin which is (around \pi/2). however, the authors did not explain how they get this result (i.e the value \pi/2). it would be interesting if the authors could show results for the case greater than or less than (\pi/2) to show the difference.-----in addition, figure 3 shows that the ensemble of relu networks is overconfident in most of the area, whereas the ensemble of fourier networks is only confident close to the input, and in the discussion of constant function of their training manifold the authors discuss some example.----------i would like to ask the authors: ----------did the fourier networks learn the input distribution?-----how are defined: usual initialization, small initialization and large initialization?----------the experiment section is adequate. however, it would strengthen the paper if the authors compared against other approaches such as: ------ predictive uncertainty estimation via prior networks, neurips 2018.------ generative probabilistic novelty detection with adversarial autoencoders, neurips 2018. ** updates after rebuttal **----------i thank the authors for the response, though i am still skeptical about the evaluation of the method, which might be a result of heavy tuning and overfit to the chosen test sets. the proposed approach also requires more theoretical justification.--------------------------------------------------------------------i'm not an expert in this area but i do find this paper interesting. though the name ""fourier networks"" is a bit arbitrary because the proposed approach also applies to multi-layer networks where only the last hidden layer has the proposed change.----------the extrapolation problem of relu networks is an interesting point. i don't know previous works that point out this for out-of-distribution detection but it's worth figuring out if this observation has been made in the adversarial robustness community.----------i do have several concerns, summarized below:-----* on page 3 the fourier transform is motivated by that rbf networks ""do not generalize as well as relu networks"". i doubt if there is any evidence for this argument.-----* i have some issue understanding proposition 1: what is w_i'? only w_i is mentioned before.-----* the ""fourier network"" is not defined explicitly in the paper, which makes it hard to understand the architecture/algorithm details. if i understand it correctly, it is only about changing the activation function of the last hidden layer and large initialization, with everything else the same as the relu networks?-----* how does the magic number ""\sigma_1 = 0.75"" and ""\sigma_2 = 0.0002"" come from? did you search it by looking at the test performance? is the performance sensitive w.r.t. the two parameters?----------i'm willing to increase my score if the authors addressed my concerns.","this paper presents a new method for detecting out-of-distribution (ood) samples.----------a reviewer pointed out that the paper discovers an interesting finding and the addressed problem is important. on the other hand, other reviewers pointed out theoretical/empirical justifications are limited. ----------in particular, i think that experimental supports why the proposed method is superior beyond the existing ones are limited. i encourages the authors to consider more scenarios of ood detection (e.g., datasets and architectures) and more baselines as the problem of measuring the confidence of neural networks or detecting outliers have rich literature. this would guide more comprehensive understandings on the proposed method.----------hence, i recommend rejection.","they also argue for using large initializations in the first layers and sin(x) as the activation function for the final hidden layer.----------the paper does not seem to have any significant logical reasoning on why their specific architecture works, but ""describes"" what they did.","the authors investigate this method (ensembles of relu networks) and observe three fundamental limitations:-----unreasonable extrapolation, unreasonable agreement between the networks in an ensemble, and the filtering out of features that distinguish the training distribution from some outofdistribution inputs, but do not contribute to the classification (constant functions on the training manifold).----------to mitigate these problems the authors proposed the following:----------- changing the activation function of the last hidden layer to the sin(x) function, and they claimed that this is going to guard against overgeneralization.------ use larger than usual initialization to increase the chances of obtaining more diverse networks for an ensemble.------ they claimed that this combines the out-of-distribution behavior from nearest neighbor methods with the generalization capabilities of neural networks, and achieves greatly improved out-of-distribution detection on standard data sets.----------the paper addresses an important problem, out of distribution detection, by proposing a fourier network which is somewhere between a relu network (small initialization) and a nearest neighbor classifier (large initialization).","the authors investigate this method (ensembles of relu networks) and observe three fundamental limitations:-----unreasonable extrapolation, unreasonable agreement between the networks in an ensemble, and the filtering out of features that distinguish the training distribution from some outofdistribution inputs, but do not contribute to the classification (constant functions on the training manifold).----------to mitigate these problems the authors proposed the following:----------- changing the activation function of the last hidden layer to the sin(x) function, and they claimed that this is going to guard against overgeneralization.------ use larger than usual initialization to increase the chances of obtaining more diverse networks for an ensemble.------ they claimed that this combines the out-of-distribution behavior from nearest neighbor methods with the generalization capabilities of neural networks, and achieves greatly improved out-of-distribution detection on standard data sets.----------the paper addresses an important problem, out of distribution detection, by proposing a fourier network which is somewhere between a relu network (small initialization) and a nearest neighbor classifier (large initialization).",the paper presents a method to detect out-of-distribution or anomalous data points.,"overall i am still positive about the paper and i have confirmed the rating.----------======================----------this paper proposes a method for the uncertainty estimates for neural network classifiers, specifically out-of-distribution detection.","it would be interesting if the authors could show results for the case greater than or less than (\pi/2) to show the difference.-----in addition, figure 3 shows that the ensemble of relu networks is overconfident in most of the area, whereas the ensemble of fourier networks is only confident close to the input, and in the discussion of constant function of their training manifold the authors discuss some example.----------i would like to ask the authors: ----------did the fourier networks learn the input distribution?-----how are defined: usual initialization, small initialization and large initialization?----------the experiment section is adequate.","it would be interesting if the authors could show results for the case greater than or less than (\pi/2) to show the difference.-----in addition, figure 3 shows that the ensemble of relu networks is overconfident in most of the area, whereas the ensemble of fourier networks is only confident close to the input, and in the discussion of constant function of their training manifold the authors discuss some example.----------i would like to ask the authors: ----------did the fourier networks learn the input distribution?-----how are defined: usual initialization, small initialization and large initialization?----------the experiment section is adequate.","the authors investigate this method (ensembles of relu networks) and observe three fundamental limitations:-----unreasonable extrapolation, unreasonable agreement between the networks in an ensemble, and the filtering out of features that distinguish the training distribution from some outofdistribution inputs, but do not contribute to the classification (constant functions on the training manifold).----------to mitigate these problems the authors proposed the following:----------- changing the activation function of the last hidden layer to the sin(x) function, and they claimed that this is going to guard against overgeneralization.------ use larger than usual initialization to increase the chances of obtaining more diverse networks for an ensemble.------ they claimed that this combines the out-of-distribution behavior from nearest neighbor methods with the generalization capabilities of neural networks, and achieves greatly improved out-of-distribution detection on standard data sets.----------the paper addresses an important problem, out of distribution detection, by proposing a fourier network which is somewhere between a relu network (small initialization) and a nearest neighbor classifier (large initialization).",0.1794871794871794,0.0259740259740259,0.1153846153846153,0.1153846153846153,0.3597122302158273,0.0579710144927536,0.1870503597122302,0.1870503597122302,0.3597122302158273,0.0579710144927536,0.1870503597122302,0.1870503597122302,0.192,0.0975609756097561,0.144,0.144,0.2797202797202797,0.0851063829787234,0.1538461538461538,0.1538461538461538,0.2830188679245283,0.019047619047619,0.1509433962264151,0.1509433962264151,0.2830188679245283,0.019047619047619,0.1509433962264151,0.1509433962264151,0.3597122302158273,0.0579710144927536,0.1870503597122302,0.1870503597122302,7.917960166931152,10.8829984664917,9.677642822265623,8.712471961975098,10.721216201782228,8.712471961975098,8.712471961975098,7.917960166931152,0.9691271220063751,0.9596532654166751,0.9439013864379604,0.9908498231129628,0.9893242464907986,0.8833995426943926,0.9908498231129628,0.9893242464907986,0.8833996220465222,0.9694048712562907,0.9737264916498668,0.898310981972265,0.977806948033362,0.9779948525871841,0.8336250292403726,0.8805637827489076,0.9172624928439354,0.9025060406544302,0.8805637827489076,0.9172624928439354,0.9025059348924949,0.9908498231129628,0.9893242464907986,0.8833995426943926
207,https://openreview.net/forum?id=rkgqCiRqKQ,"not all examples in the introduction are necessarily biases but can be modeled with reward functions, where reward is given to specific states other than finishing work by the deadline. it would be helpful for the reader to get examples that correspond to the investigated biases. ----------------it would be good if the authors could at least mention that boltzmann rational is a specific model of systematic bias for which much experimental support eith humans and animals exists. ----------------the authors are strongly encouraged to review the literature on irl, which includes other examples of modeling explicitly suboptimal agents, e.g.:--------- rothkopf, c. a., & dimitrakakis, c. (2011). preference elicitation and inverse reinforcement learning. ecml.--------similarly, the idea to learn an agents reward functions across multiple tasks has also appeared in the literature before, e.g.:--------- dimitrakakis, c., & rothkopf, c. a. (2011). bayesian multitask inverse reinforcement learning. ewrl.--------- choi, j., & kim, k. e. (2012). nonparametric bayesian inverse reinforcement learning for multiple reward functions. nips----------------the authors state:--------the key idea behind our algorithms is to learn a model of how the demonstrator plans, and invert the models ""understanding"" using backpropagation to infer the reward from actions.--------it would be also important in this case to relate this to prior work, as several authors have proposed a very similar idea, in which a particular parameterization of the agents planning given the rewards and the transition function are learned, including ziebart et al. and dimitrakakis et al. this is also related to --------- neu, g., & szepesvri, c. (2007). apprenticeship learning using inverse reinforcement learning and gradient methods. uai.----------------it would be great if the authors could also discuss how assumption 3 is a necessary for accurately inferring reward functions and biases and how deviations from this assumption interfere with the goal of this inference. this seems to be a central and important point for the viability of the approach the authors take here.----------------currently, the evaluation of the proposed method is in terms of the loss incurred by a planner between the inferred reward function and the true reward function, figure 3. it would be important for the evaluation of the current manuscript to know what the inferred biases are. that using a wrong model of how actions are generated given values, e.g. myopic vs. boltzmann-rational, results in wrong inferences, should not be too surprising. therefore, the main question is: does the proposed algorithm recover the actual biases?----------------minor points:--------like they naive and sophisticated hyperbolic discounters this paper addresses the interesting and challenging problem of learning the reward function from demonstrators which have unknown biases. as this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. they propose algorithms for both cases and evaluate these in basic experiments.----------------the studied problem is relevant as many/most demonstrators have unknown biases and we still need methods to effectively learn from those.----------------as far as i am aware of the related literature, the problem has not been studied in that explicit form although there is related work which targets the problem of learning from suboptimal demonstrators or demonstrators that can fail, e.g. [1] (i suggest to discuss this and other relevant papers in a related work section).----------------the main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation:--------* for instance, the formalization of assumption 1 is unclear. in which sense does this cover similarity in planing? as far as i understand, the function d could still map any combination of world model and reward function to any arbitrary policy. what does it mean that the planning algorithm d is fixed and independent?--------* a crucial point requiring more investigation in my opinion is assumption 3 (well-suited inductive bias). empirically the chosen experimental setup yields expected results. however, to better understand the problem of learning with unknown biases it would be important to see how results change if the model for the planner changes. a small step in that direction would have been to provide results for value iteration networks with different number of iterations and number neurons, etc. --------* if you use the differentiable planner instead of the vin, how many iterations do you unroll?--------* is there any evidence that the proposed approach can work effectively in larger scale domains with more difficult biases? also in the case in which the biases are inconsistent among demonstrations?----------------further suggestions:--------* test how algorithm 1 performs if first initialized on simulated optimal demonstrations.--------* improve notation for the planning algorithm d by using brackets.----------------[1] shiarlis, k., messias, j., & whiteson, s. (2016, may). inverse reinforcement learning from failure. in proceedings of the 2016 international conference on autonomous agents & multiagent systems (pp. 1060-1068). international foundation for autonomous agents and multiagent systems. this paper has proposed algorithms for inferring reward functions from demonstrations with unknown biases. to achieve this, the authors have proposed to learn planners from demonstrations in multiple tasks via value iteration networks to learn the reward functions.----------------this paper has provided an excellent motivation of their work in sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions. the paper is well-written, up till section 4. ----------------on the flip side, there does not seem to be any significant technical challenges, perhaps due to some of the assumptions that they have made. like the authors have mentioned, i do find assumption 3 to be overly strong and restrictive, as empirically demonstrated in section 5.2. arguably, is it really weaker than that of noisy rationality? at this moment, it is difficult to overlook this, even though the authors have argued that it may not be as restrictive in the future when more sophisticated differentiable planners are developed.----------------the experimental results are not as convincing as i would have liked. in particular, algorithm 2 (learning a demonstrator's model) does not seem to outperform that of assuming an optimal demonstrator for the noiseless case and a boltzmann demonstrator for the noisy case (fig. 3). this was also highlighted by the authors as well: ""the learning methods tend to perform on par with the best of two choices."" it begs the question whether accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias when we know a priori whether the demonstrations are noisy or not.--------------------------------other detailed comments are provided below:----------------i would have preferred that the authors present their technical formulations in section 4 using the demonstrator's trajectories instead of policies.----------------the authors say that ""in some cases, like they naive and sophisticated hyperbolic discounters, especially the noisy ones, the learning methods outperform both optimal and boltzmann assumptions."" but, fig. 3 shows that algorithm 2 does not perform better than either that of the optimal or boltzmann demonstrator.----------------in section 5.2, the authors have empirically demonstrated the poor approximate planning performance of vin, as compared to an exact model the demonstrator. what then would its implications be on the adaptivity of algorithms 1 and 2 to biases?----------------the following references on irl with noisy demonstration trajectories would be relevant:----------------benjamin burchfiel, carlo tomasi, and ronald parr. distance minimization for reward learning from scored trajectories. in proc. aaai, 2016.----------------j. zheng, s. liu, and l. m. ni. robust bayesian inverse reinforcement learning with sparse behavior noise. in proc. aaai, 2014.--------------------------------minor issues:--------on page 4, the expression d : w  r -> s -> a -> [0, 1] can be more easily understood with the use of parentheses.----------------for assumption 2b, you can italicize ""some"".----------------in the first paragraph of section 4.1, what are you summing over?----------------line 3 of algorithm 1: pi_w?----------------page 7: for the learning the bias setting?----------------page 7: figure 3 shows?----------------page 7: they naive?----------------page 7: so as long as?----------------page 8: adaption?----------------page 8: predicated?----------------page 8: figure 4 shows?","the authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias. to achieve this, the authors learn the planners and the reward functions from demonstrations. as this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. they propose algorithms for both cases and evaluate these in basic experiments. the problem considered is important and challenging. one issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well-suited inductive bias). it is not clear if the assumptions made are reasonable. experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. overall, there is consensus among the reviewers that the paper is interesting but not ready for publication.","nips----------------the authors state:--------the key idea behind our algorithms is to learn a model of how the demonstrator plans, and invert the models ""understanding"" using backpropagation to infer the reward from actions.--------it would be also important in this case to relate this to prior work, as several authors have proposed a very similar idea, in which a particular parameterization of the agents planning given the rewards and the transition function are learned, including ziebart et al. and dimitrakakis et al. this is also related to --------- neu, g., & szepesvri, c. (2007).","nips----------------the authors state:--------the key idea behind our algorithms is to learn a model of how the demonstrator plans, and invert the models ""understanding"" using backpropagation to infer the reward from actions.--------it would be also important in this case to relate this to prior work, as several authors have proposed a very similar idea, in which a particular parameterization of the agents planning given the rewards and the transition function are learned, including ziebart et al. and dimitrakakis et al. this is also related to --------- neu, g., & szepesvri, c. (2007).","it begs the question whether accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias when we know a priori whether the demonstrations are noisy or not.--------------------------------other detailed comments are provided below:----------------i would have preferred that the authors present their technical formulations in section 4 using the demonstrator's trajectories instead of policies.----------------the authors say that ""in some cases, like they naive and sophisticated hyperbolic discounters, especially the noisy ones, the learning methods outperform both optimal and boltzmann assumptions.""","not all examples in the introduction are necessarily biases but can be modeled with reward functions, where reward is given to specific states other than finishing work by the deadline.","to achieve this, the authors have proposed to learn planners from demonstrations in multiple tasks via value iteration networks to learn the reward functions.----------------this paper has provided an excellent motivation of their work in sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions.","it begs the question whether accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias when we know a priori whether the demonstrations are noisy or not.--------------------------------other detailed comments are provided below:----------------i would have preferred that the authors present their technical formulations in section 4 using the demonstrator's trajectories instead of policies.----------------the authors say that ""in some cases, like they naive and sophisticated hyperbolic discounters, especially the noisy ones, the learning methods outperform both optimal and boltzmann assumptions.""","3 shows that algorithm 2 does not perform better than either that of the optimal or boltzmann demonstrator.----------------in section 5.2, the authors have empirically demonstrated the poor approximate planning performance of vin, as compared to an exact model the demonstrator.","nips----------------the authors state:--------the key idea behind our algorithms is to learn a model of how the demonstrator plans, and invert the models ""understanding"" using backpropagation to infer the reward from actions.--------it would be also important in this case to relate this to prior work, as several authors have proposed a very similar idea, in which a particular parameterization of the agents planning given the rewards and the transition function are learned, including ziebart et al. and dimitrakakis et al. this is also related to --------- neu, g., & szepesvri, c. (2007).",0.356060606060606,0.0839694656488549,0.1818181818181818,0.1818181818181818,0.356060606060606,0.0839694656488549,0.1818181818181818,0.1818181818181818,0.2857142857142857,0.0389105058365758,0.138996138996139,0.138996138996139,0.1584158415841584,0.01,0.0792079207920792,0.0792079207920792,0.2857142857142857,0.0762711864406779,0.1848739495798319,0.1848739495798319,0.2857142857142857,0.0389105058365758,0.138996138996139,0.138996138996139,0.1962616822429906,0.0094339622641509,0.102803738317757,0.102803738317757,0.356060606060606,0.0839694656488549,0.1818181818181818,0.1818181818181818,13.362611770629885,9.711499214172363,11.399957656860352,9.345335960388184,9.345335960388184,13.362611770629885,9.345335960388184,9.541298866271973,0.9744617947282368,0.7457467503750879,0.6090950547415436,0.9744617947282368,0.7457467503750879,0.6090950144827229,0.47337627473756444,0.7251222387154628,0.4885155251245659,0.9582743851474861,0.9575642100976616,0.8523698628020165,0.5390321871131142,0.5368137432172975,0.9237528589415437,0.47337627473756444,0.7251222387154628,0.48851309651125685,0.17555531703174593,0.3285642604991567,0.027275602783080513,0.9744617947282368,0.7457467503750879,0.6090950411479545
208,https://openreview.net/forum?id=rklhb2R9Y7,"the draft proposes a heuristic combining environment rewards with an irl-style rewards recovered from expert demonstrations, seeking to extend the gail approach to irl to the case of mismatching action spaces between the expert and the learner. the interesting contribution is, in my opinion, the self-exploration parameter that reduces the reliance of learning on demonstrations once they have been learned sufficiently well.----------------questions:----------------- in general, it's known that behavioral cloning, of which this work seem to be an example in so much it learns state distributions that are indistinguishable from the expert ones, can fail spectacularly because of the distribution shift (kaariainen@alw06, ross&bagnell@aistats10, ross&bagnell@aistats11). can you comment if gan-based methods are immune or susceptible to this?-------- --------- would this work for tasks where the state-space has to be learned together with the policy? e.g. image captioning tasks or atari games.----------------- is it possible to quantify the ease of learning or the frequency of use of the ""new"" actions, i.e. ?. won't learning these actions effectively be as difficult as rl with sparse rewards? say, in a grid world where 4-way diagonal moves allow reaching the goal faster, learner is king 8-way, demonstrations come from a 4-way expert, rewards are sparse and each step receives a -1 reward and the final goal is large positive -- does the learner's final policy actually use the diagonals and when?----------------related work:-------- --------- is it possible to make a connection to (data or policy) aggregation methods in il. such methods (e.g. chang et al.@icml15) can also sometimes learn policies better than the expert.----------------experiments:--------- why gail wasn't evaluated in fig. 3 and fig. 4?----------------minor:--------- what's bce in algorithm 1? --------- fig.1: ""the the""--------- sec 3.2: but avoid -> but avoids--------- sec 3.2: be to considers -> be to consider--------- sec 3.2: any hyperparameter -> any hyperparameters--------- colors in fig 2 are indistinguishable--------- table 1: headers saying which method is prior work and which is contribution would be helpful--------- fig. 3: if possible try to find a way of communicating the relation of action spaces between expert and learner (e.g. a subset of/superset of). using the same figure to depict self-exploration make it complicated to analyse.--------- sec 3.2: wording in the last paragraph on p.4 (positive scaling won't _make_ anything positive if it wasn't before) the paper proposes to combine expert demonstration together with reinforcement learning to speed up learning of control policies. to do so, the authors modify the gail algorithm and create a composite reward function as a linear combination of the extrinsic reward and the imitation reward. they test their approach on several toy problems (small grid worlds). ----------------the idea of combining gail reward and extrinsic reward is not really new and quite straight forward so i wouldn't consider this as a contribution. also, using state only demonstration in the framework of gail is not new as the authors also acknowledge in the paper. finally, i don't think the experiments are convincing since the chosen problems are rather simple. ----------------but my main concern is that the major claim of the authors is that they don't use expert actions as input to their algorithm, but only sequences of states. yet they test their algorithm on deterministic environments. in such a case, two consecutive states kind of encode the action and all the information is there. even if the action sets are different in some of the experiments, they are still very close to each other and the encoding of the expert actions in the state sequence is probably helping a lot. so i would like to see how this method works in stochastic environments. this paper proposes some new angles to the problem of imitation learning from state only observations (not state-action pairs which are more expensive). --------specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the mdp itself in a gradual manner, guided by the rate of learning.--------it also proposes a couple of variants of imitation rewards, rtgd and atd inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (csd, ssd), which are based on either consecutive or single states, which constitute the baseline methods for comparison.--------the authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines. --------some moderately interesting observations are reported, which largely confirm one's intuition about when these methods may perform relatively well. --------there is not very much theoretical support for the proposed methods per se, the paper is mostly an empirical study on these competing reward schemes for imitation learning.--------the empirical evaluation is done in a single domain/problem, and in that sense it is questionable how far the observed trends on the relative performance of the competing methods generalizes to other problems and domains. --------also the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research.","this paper proposes to combine rewards obtained through irl from rewards coming from the environment, and evaluate the algorithm on grid world environments. the problem setting is important and of interest to the iclr community. while the revised paper addresses the concerns about the lack of a stochastic environment problem, the reviewers still have major concerns regarding the novelty and significance of the algorithmic contribution, as well as the limited complexity of the experimental domains. as such, the paper does not meet the bar for publication at iclr.","--------also the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research.","--------specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the mdp itself in a gradual manner, guided by the rate of learning.--------it also proposes a couple of variants of imitation rewards, rtgd and atd inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (csd, ssd), which are based on either consecutive or single states, which constitute the baseline methods for comparison.--------the authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines.","--------specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the mdp itself in a gradual manner, guided by the rate of learning.--------it also proposes a couple of variants of imitation rewards, rtgd and atd inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (csd, ssd), which are based on either consecutive or single states, which constitute the baseline methods for comparison.--------the authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines.","the draft proposes a heuristic combining environment rewards with an irl-style rewards recovered from expert demonstrations, seeking to extend the gail approach to irl to the case of mismatching action spaces between the expert and the learner.",using the same figure to depict self-exploration make it complicated to analyse.--------- sec 3.2: wording in the last paragraph on p.4 (positive scaling won't _make_ anything positive if it wasn't before) the paper proposes to combine expert demonstration together with reinforcement learning to speed up learning of control policies.,"--------specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the mdp itself in a gradual manner, guided by the rate of learning.--------it also proposes a couple of variants of imitation rewards, rtgd and atd inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (csd, ssd), which are based on either consecutive or single states, which constitute the baseline methods for comparison.--------the authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines.","--------- fig.1: ""the the""--------- sec 3.2: but avoid -> but avoids--------- sec 3.2: be to considers -> be to consider--------- sec 3.2: any hyperparameter -> any hyperparameters--------- colors in fig 2 are indistinguishable--------- table 1: headers saying which method is prior work and which is contribution would be helpful--------- fig.","--------specifically, the paper proposes ""self exploration"", in which it mixes the imitation reward with environment reward from the mdp itself in a gradual manner, guided by the rate of learning.--------it also proposes a couple of variants of imitation rewards, rtgd and atd inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (csd, ssd), which are based on either consecutive or single states, which constitute the baseline methods for comparison.--------the authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines.",0.2459016393442623,0.0333333333333333,0.1639344262295081,0.1639344262295081,0.3274336283185841,0.0625,0.1946902654867256,0.1946902654867256,0.3274336283185841,0.0625,0.1946902654867256,0.1946902654867256,0.2698412698412698,0.0161290322580645,0.2063492063492063,0.2063492063492063,0.1408450704225352,0.0571428571428571,0.0845070422535211,0.0845070422535211,0.3274336283185841,0.0625,0.1946902654867256,0.1946902654867256,0.1014492753623188,0.0,0.072463768115942,0.072463768115942,0.3274336283185841,0.0625,0.1946902654867256,0.1946902654867256,10.410076141357422,7.648043632507324,13.587713241577148,10.410076141357422,9.754098892211914,10.410076141357422,10.410076141357422,6.647818088531494,0.5909211489858932,0.7181708051680274,0.9218351595804852,0.9541885661388243,0.6147983825871648,0.4572286406262118,0.9541885661388243,0.6147983825871648,0.457228512564429,0.9628479094355482,0.9563785581368667,0.9167730040445731,0.9443515701046743,0.9529675188717048,0.07221707784566375,0.9541885661388243,0.6147983825871648,0.4572285256006435,0.9839753404529343,0.9836544129112482,0.0946526084804425,0.9541885661388243,0.6147983825871648,0.4572286406262118
209,https://openreview.net/forum?id=rkxciiC9tm,"the authors introduce a novel on-policy temporally consistent exploration strategy, named neural adaptivedropout policy exploration (nadpex), for deep reinforcement learning agents. the main idea is to sample from a distribution of plausible subnetworks modeling the temporally consistent exploration. for this, the authors use the ideas of the standard dropout for deep networks. using the proposed dropout transformation that is differentiable, the authors show that the kl regularizers on policy-space play an important role in stabilizing its learning. the experimental validation is performed on continuous control learning tasks, showing the benefits of the proposed. ----------------this paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works. this poses a challenge in evaluating this paper. nevertheless, this paper clearly explores and offers a novel approach for more efficient on-policy exploration which allows for more stable learning compared to traditional approaches. ----------------even though the authors answer positively to each of their four questions in the experiments section, it would like that the authors provide more intuition why these improvements occur and also outline the limitations of their approach. the authors propose a new on-policy exploration strategy by using a policy with a hierarchy of stochasticity. the authors use a two-level hierarchical distribution as a policy, where the global variable is used for dropout. this work is interesting since the authors use dropout for policy learning and exploration. the authors show that parameter noise exploration is a particular case of the proposed policy. the main concern is the gap between the problem formulation and the actual optimization problem in eq 12. i am very happy to give a higher rating if the authors address the following points. ----------------detailed comments --------(1) the authors give the derivation for eq 10. however, it is not obvious that how to move from line 3 to line 4 at eq 15.--------minor: since the action is denoted by ""a"", it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at eq 10 and 15.----------------(2) due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at eq 12. does such approximation guarantee the policy improvement? any justification?----------------(3) instead of using the mean policy approximation in eq 12, the authors should consider existing monte carlo techniques to reduce the variance of the gradient estimation. for example, [1] could be used to reduce the variance of gradient w.r.t. \phi. note that the gradient is biased if the mean policy approximation is used.----------------(4) are \theta and \phi jointly and simultaneously optimized at eq 12? the authors should clarify this point. ----------------(5) due to the mean policy approximation, does the mean policy depend on \phi? the authors should clearly explain how to update \phi when optimizing eq 12. ----------------(6) if the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z) is missing in eq 12 while a regularization term about \pi_{\theta|z} does appear in eq 12? ----------------(7) the authors give the derivations about \theta such as the gradient and the regularization term about \theta (see, eq 18-19). however, the derivations about \phi are missing. for example, how to compute the gradient w.r.t. \phi? since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi. --------minor, 1/2 is missing in the last line of eq 19.----------------reference:--------[1] aueb, michalis titsias rc, and miguel lzaro-gredilla. ""local expectation gradients for black box variational inference."" in advances in neural information processing systems, pp. 2638-2646. 2015. this paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration. the dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration. the paper shows that with small amount of gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments. and it can significantly outperform vanilla ppo for environments with sparse rewards.----------------the paper is clearly written. the introduced technique is interesting. i wonder except for the difference of memory consumption, how different it is compared to parameter space exploration. i feel that it is a straightforward extension/generalization of the parameter space exploration. but the stochastic alignment and policy space constraint seem novel and important.----------------the motivation of this paper is mostly about learning with sparse reward. i am curious whether the paper has other good side effects. for example, will the dropout cause the policy to be more robust? furthermore, if i deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores. in addition, i would like to see some discussions whether this technique could be applied to off-policy learning as well.----------------overall, i like this paper. it is well written. the method seems technically sound and achieves good results. for this reason, i would recommend accepting this paper.","the authors have proposed a new method for exploration that is related to parameter noise, but instead uses gaussian dropout across entire episodes, thus allowing for temporally consistent exploration. the method is evaluated in sparsely rewarded continuous control domains such as half-cheetah and humanoid, and compared against ppo and other variants. the method is novel and does seem to work stably across the tested tasks, and simple exploration methods are important for the rl field. however, the paper is poorly and confusingly written and really really needs to be thoroughly edited before the camera ready deadline. there are many approaches which are referred to without any summary or description, which makes it difficult to read the paper. the three reviewers all had low confidence in their understanding of the paper, which makes this a very borderline submission even though the reviewers gave relatively high scores.","using the proposed dropout transformation that is differentiable, the authors show that the kl regularizers on policy-space play an important role in stabilizing its learning.","----------------detailed comments --------(1) the authors give the derivation for eq 10. however, it is not obvious that how to move from line 3 to line 4 at eq 15.--------minor: since the action is denoted by ""a"", it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at eq 10 and 15.----------------(2) due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at eq 12. does such approximation guarantee the policy improvement?",this work is interesting since the authors use dropout for policy learning and exploration.,"the authors introduce a novel on-policy temporally consistent exploration strategy, named neural adaptivedropout policy exploration (nadpex), for deep reinforcement learning agents.","the authors introduce a novel on-policy temporally consistent exploration strategy, named neural adaptivedropout policy exploration (nadpex), for deep reinforcement learning agents.","----------------detailed comments --------(1) the authors give the derivation for eq 10. however, it is not obvious that how to move from line 3 to line 4 at eq 15.--------minor: since the action is denoted by ""a"", it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at eq 10 and 15.----------------(2) due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at eq 12. does such approximation guarantee the policy improvement?",the method seems technically sound and achieves good results.,"----------------detailed comments --------(1) the authors give the derivation for eq 10. however, it is not obvious that how to move from line 3 to line 4 at eq 15.--------minor: since the action is denoted by ""a"", it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at eq 10 and 15.----------------(2) due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at eq 12. does such approximation guarantee the policy improvement?",0.127906976744186,0.0235294117647058,0.0930232558139534,0.0930232558139534,0.2605042016806722,0.0169491525423728,0.1428571428571428,0.1428571428571428,0.125,0.0126582278481012,0.0875,0.0875,0.1071428571428571,0.036144578313253,0.0952380952380952,0.0952380952380952,0.1071428571428571,0.036144578313253,0.0952380952380952,0.0952380952380952,0.2605042016806722,0.0169491525423728,0.1428571428571428,0.1428571428571428,0.0516129032258064,0.0130718954248366,0.0516129032258064,0.0516129032258064,0.2605042016806722,0.0169491525423728,0.1428571428571428,0.1428571428571428,2.83349084854126,13.217552185058594,13.217552185058594,2.83349084854126,10.663579940795898,12.490612983703612,2.83349084854126,8.227855682373047,0.9037069791269438,0.9251110865620932,0.7450186076590972,0.9768482874061365,0.9731321288193848,0.8756141552273097,0.9619885388860612,0.9668181360375822,0.8921022085458097,0.9658264415025903,0.9655225051054174,0.87443729673304,0.9658264415025903,0.9655225051054174,0.8744373693702253,0.9768482874061365,0.9731321288193848,0.8756144663006371,0.49425748453505997,0.6626505758383057,0.7534246132372496,0.9768482874061365,0.9731321288193848,0.8756141552273097
210,https://openreview.net/forum?id=ryOG3fWCW,"the authors review and evaluate several empirical methods to create faster versions of big neural nets for vision without sacrificing accuracy. they show using the resnet architecture that combining distillation, pruning, and cascades are complementary and can yield pretty nice speedups.----------------this is a great idea and could be a strong paper, but it's really hard to glean useful recommendations from this for several reasons:----------------- the writing of the paper makes it hard to understand exactly what's being compared and evaluated. for a paper like this it's really crucial to be precise. when the authors say ""specialization"" or ""specialized model"", they sometimes mean distillation, sometimes filter pruning, and sometimes cascades. the distinction of ""task-aware"" also seems arbitrary to me and obfuscates the contribution of the paper as well. as far as i can tell, the technique is exactly the same, all that's changing is a slight modification. it's not like any of the intuitions or objectives are changing, so adding this new terminology just complicates things. for example, just say ""we distill a parent model to a child model with a subset of the labels."" ----------------- in terms of substance, the experiments don't really add much value in terms of general lessons. for example, the cat/dog from imagenet distillation only works if the target labels are exactly a subset of the original. obviously if the parent model was overcomplete before, it is certainly overcomplete now. the proposed cascade method is also fairly trivial -- a cheap distilled model backs off to the reference model. why not train the whole cascade end-to-end? what about multiple levels of cascades? the only useful conclusion i can draw from the experiments is that (1) distillation still works (2) cascades also still work (3) pruning doesn't seem that useful in comparison. training a cascade also involves a bunch of non-trivial design choices which are largely ignored -- how to set pass through, how to train the model, etc. etc.----------------- nit: where are the blue squares in figure 4? (distill only) shouldn't those be the fastest methods (aside from pruning)? ----------------an ideal story would for a paper like this would be: here are some complementary ideas that we can combine in non-obvious ways for superlinear benefits, e.g. it turns out that by distilling into a cascade in some end-to-end fashion, you can get much better accuracy vs. speed trade-offs. instead this paper is a grab-back of tricks. such a paper can also provide value, but to do that right, the tricks need to be obvious *in retrospect only* and/or the experiments need to show a lot of precise practical lessons. all in all this paper reads like a tech report but not a conference publication. this paper presents three different techniques for model specialization, i.e. adapting a pretrained network to a more specific task and reduce its computational cost while maintaining the performance. the three techniques are distillation, weight pruning and cascades. evaluation compares how effective each technique is and how they interact with each other. in certain settings the obtained speed-up reaches 5x without loss of accuracy.----------------pros:--------- the idea of reducing the computational cost of specialized models makes sense.--------- in some setting the speed-up can reach more than 5x, which is quite relevant.----------------cons:--------- the fact that the models are specialized to simpler tasks is not explicitly used in the approach. the authors should test what would happen when using their cascade for classification on all classes of imagenet for instance. would it be the gain in speed much lower?--------- it is not clear if the distillation on smaller networks is really improving the models accuracy. the authors compared the distilled models with models trained from scratch. there should be an additional experiment with the small models trained on imagenet first and then fine-tuned to the task. if in that case there is non gain, then, what is the advantage of distilling in these settings? imagenet annotations need to be used anyway to train the teacher network.--------- in section 3.2 it seems that the filters of a cnn are globally ranked based on their average activation values. those with the lowest average activation will be removed. however, in my understanding, the ranking can work better if performed layer specific and not globally.--------- in section 3.4, the title says ""end-to-end specialization pipeline"", but actually, the specialization is done in 3 steps, therefore in my understanding it is not end-to-end.--------- there are some spelling errors, for instance in the beginning of section 4-------- pruning does not seem to produce much speed-up.--------- the experimental part is difficult to read. in particular fig. 4 should be better explained. there are some symbols in the legend that do not appear in the graph, and others (baselines only) that appear multiple times, but it is not clear what they represent. also, at the end of the explanation of fig. 4 the authors mention a gain of 8%, which in my understanding is not really relevant compared with the total speed-up, which can be in the order of 500%----------------overall, the idea of model specialization seem interesting. however, in my understanding the main source of speed-up is a cascade approach with a reduced model, in which is not clear how much speed-up is actually due to the specialized task. the paper presents an approach to do task aware distillation, task-specific pruning and specialized cascades. the main result is that such methods can yield smaller, efficient and sometimes more accurate models.----------------the proposed approach is simple and easy to understand. the task aware distillation relies on the availability of data that is target specific. in practice, i believe this is not an unreasonable requirement.----------------the speedups and accuracy gains of this paper are impressive. the fact that the proposed technique is simple yet yields such speedups is encouraging. however, evaluating on simple datasets like kaggle cat/dog and oxford flowers diminishes the value of the paper. i would strongly encourage the authors to try harder datasets such as coco, voc etc. this will make the paper more valuable to the community.----------------missing citations--------do deep nets really need to be deep? - ba & caruana 2014","this paper does not meet the bar for iclr - neither in terms of the quality of the write-up, nor in experimental design. the two confident reviewers agree to reject the paper, the weak accept comes from a less confident reviewer who did not write a good review at all. the rebuttal does not change this assessment.","----------------an ideal story would for a paper like this would be: here are some complementary ideas that we can combine in non-obvious ways for superlinear benefits, e.g. it turns out that by distilling into a cascade in some end-to-end fashion, you can get much better accuracy vs. speed trade-offs.","they show using the resnet architecture that combining distillation, pruning, and cascades are complementary and can yield pretty nice speedups.----------------this is a great idea and could be a strong paper, but it's really hard to glean useful recommendations from this for several reasons:----------------- the writing of the paper makes it hard to understand exactly what's being compared and evaluated.","when the authors say ""specialization"" or ""specialized model"", they sometimes mean distillation, sometimes filter pruning, and sometimes cascades.",the authors review and evaluate several empirical methods to create faster versions of big neural nets for vision without sacrificing accuracy.,"in certain settings the obtained speed-up reaches 5x without loss of accuracy.----------------pros:--------- the idea of reducing the computational cost of specialized models makes sense.--------- in some setting the speed-up can reach more than 5x, which is quite relevant.----------------cons:--------- the fact that the models are specialized to simpler tasks is not explicitly used in the approach.","the paper presents an approach to do task aware distillation, task-specific pruning and specialized cascades.",- ba & caruana 2014,"when the authors say ""specialization"" or ""specialized model"", they sometimes mean distillation, sometimes filter pruning, and sometimes cascades.",0.1441441441441441,0.0,0.072072072072072,0.072072072072072,0.2352941176470588,0.0512820512820512,0.1176470588235294,0.1176470588235294,0.0266666666666666,0.0,0.0266666666666666,0.0266666666666666,0.1282051282051282,0.0,0.0769230769230769,0.0769230769230769,0.2413793103448275,0.0,0.1896551724137931,0.1896551724137931,0.0821917808219178,0.028169014084507,0.0547945205479452,0.0547945205479452,0.0,0.0,0.0,0.0,0.0266666666666666,0.0,0.0266666666666666,0.0266666666666666,8.510204315185547,12.260239601135254,15.819191932678224,13.71973991394043,8.111041069030762,7.795182228088379,7.795182228088379,3.0665149688720703,0.9809474796488418,0.9640283335235323,0.7020073260221209,0.979027901053536,0.9826201439487474,0.88854441505004,0.9591283842091798,0.964552467446709,0.953000714909443,0.9666822546104917,0.9655713148256225,0.8151454720043184,0.9811946412699484,0.981327851283708,0.8799944351921168,0.9223654096304247,0.9264112116552128,0.8586680256191229,0.5903297963975191,0.7023648183369947,0.009924177896141162,0.9591283842091798,0.964552467446709,0.9530007397610089
211,https://openreview.net/forum?id=ryeFY0EFwS,"summary-----the surprising generalization properties of neural networks trained with stochastic gradient descent are still poorly understood. the present work suggests that they can be explained at least partly by the fact that patterns shared across many data points will lead to gradients pointing in similar directions, thus reinforcing each other. artefacts specific to small numbers of data points however will not have this property and thus have a substantially smaller impact on the learning. numerical experiments on mnist with label-noise indeed show that even though the neural network is able to perfectly fit even the flipped labels, the ""pristine"" labels are fittet much earlier during training. the authors also experiment with explicitly clipping ""outlier gradients"" and show that the resulting algorithm drastically reduces overfitting, thus further supporting the coherent gradient hypothesis.----------decision-----the present work proposes a plausible, simple mechanism that might be contributing to the generalization of neural networks trained with gradient descent. parts of the discussion stay informal as the authors themselves admit, but i appreciate that rather than providing mathematical decoration the authors focus on well-designed experiments that support their claims. overall, the paper is of high quality and provides an interesting perspective on an important topic, which is why i think it should be accepted.----------questions for the authors-----the coherent gradient hypothesis seems equally valid in the absence of stochasticity. however, the latter is often seen as an explanation of the generalization performance of sgd. my understanding is that you are also using minibatched gradient descent. would you expect your experiments to still be valid when using deterministic gradient descent (full batch)? did you study the effects of large batch sizes on the experiments? this paper posits that similar input examples will have similar gradients, leading to a gradient ""coherence"" phenomenon. a simple argument then suggests that the loss should decrease much more rapidly when gradients cohere than when they do not. this hypothesis and analysis is supported with clever experiments that confirm some of the predictions of this theory. furthermore, since, as the authors emphasize, their hypothesis is prescriptive, they are able to suggest a novel regularization technique and show that it is effective in a simple setting.----------i find the coherent gradient hypothesis to be simple and reasonable. furthermore, the paper is written very clearly, and as far as i know the main idea is original (although since it is a rather simple phenomenon, it's possible something similar could have appeared elsewhere in the literature). perhaps more importantly, the associated experiments are very cleverly designed and are very supportive of the hypothesis. for instance, figure 1 provides compelling evidence for the coherent gradient hypothesis and in particular motivates the way phenomenon of early stopping arises a natural consequence. overall, the paper is of very high quality, and i recommend its acceptance.----------one criticism perhaps is whether these results are sufficiently significant. on the one hand, most of the experiments were done on small network and dataset combinations -- and the proposed regularization scheme as is will not scale to practical problems of interest. on the other hand, i really feel like i learned something interesting about gradient descent from reading this paper and absorbing the experimental results -- which is often not something i can say given the large array of reported experimental results in this field. it's clear that the authors themselves are aware that it's of interest to extend their results to more realistic settings, and regardless i think that this paper stands alone as is and should be accepted to iclr.","the paper proposes an intuitive causal explanation for the generalization properties of gd methods. the reviewers appreciated the insights, with one reviewer claiming that there was significant overlap with existing work.----------i ultimately decided to accept this paper as i believe intuitive explanations are critical to the propagation of ideas. that being said, there is a tendency in this community to erase past, especially theoretical, work, for that very reason that theoretical work is less popular.----------hence, i want to make it clear that the acceptance of this paper is based on the premise that the authors will incorporate all of reviewer 3's comments and give enough credit to all relevant work (namely, all the papers cited by the reviewer) with a proper discussion on the link between these.","overall, the paper is of high quality and provides an interesting perspective on an important topic, which is why i think it should be accepted.----------questions for the authors-----the coherent gradient hypothesis seems equally valid in the absence of stochasticity.","the authors also experiment with explicitly clipping ""outlier gradients"" and show that the resulting algorithm drastically reduces overfitting, thus further supporting the coherent gradient hypothesis.----------decision-----the present work proposes a plausible, simple mechanism that might be contributing to the generalization of neural networks trained with gradient descent.","furthermore, since, as the authors emphasize, their hypothesis is prescriptive, they are able to suggest a novel regularization technique and show that it is effective in a simple setting.----------i find the coherent gradient hypothesis to be simple and reasonable.",summary-----the surprising generalization properties of neural networks trained with stochastic gradient descent are still poorly understood.,summary-----the surprising generalization properties of neural networks trained with stochastic gradient descent are still poorly understood.,"the authors also experiment with explicitly clipping ""outlier gradients"" and show that the resulting algorithm drastically reduces overfitting, thus further supporting the coherent gradient hypothesis.----------decision-----the present work proposes a plausible, simple mechanism that might be contributing to the generalization of neural networks trained with gradient descent.","on the one hand, most of the experiments were done on small network and dataset combinations -- and the proposed regularization scheme as is will not scale to practical problems of interest.","the authors also experiment with explicitly clipping ""outlier gradients"" and show that the resulting algorithm drastically reduces overfitting, thus further supporting the coherent gradient hypothesis.----------decision-----the present work proposes a plausible, simple mechanism that might be contributing to the generalization of neural networks trained with gradient descent.",0.2222222222222222,0.0473372781065088,0.1286549707602339,0.1286549707602339,0.2022471910112359,0.0454545454545454,0.1235955056179775,0.1235955056179775,0.2117647058823529,0.0119047619047619,0.1058823529411764,0.1058823529411764,0.0816326530612244,0.0275862068965517,0.0816326530612244,0.0816326530612244,0.0816326530612244,0.0275862068965517,0.0816326530612244,0.0816326530612244,0.2022471910112359,0.0454545454545454,0.1235955056179775,0.1235955056179775,0.1739130434782608,0.0125786163522012,0.0993788819875776,0.0993788819875776,0.2022471910112359,0.0454545454545454,0.1235955056179775,0.1235955056179775,10.354665756225586,15.544240951538086,15.54423713684082,10.354665756225586,6.947423934936523,8.051559448242188,10.354665756225586,5.8806023597717285,0.9699802565334312,0.972221910123996,0.9477199258530693,0.9713207685667066,0.966052322871297,0.8333227332104037,0.9676861822120382,0.9583783819748144,0.9222299486146474,0.9766545268715034,0.9725254540455522,0.9464596227630057,0.9766545268715034,0.9725254540455522,0.9464597208182447,0.9713207685667066,0.966052322871297,0.8333228151705873,0.9670899589892819,0.9638635250928047,0.7902971742302618,0.9713207685667066,0.966052322871297,0.8333227332104037
212,https://openreview.net/forum?id=ryex8CEKPr,"this paper proposes a way of employing modern generative models like vaes and gplvms within the recently popular knockoff framework for variable/feature selection. roughly speaking, the main idea of knockoffs is to construct a duplicate (knockoff) of each regression variable which matches its distributional properties but is independent of the response variable when conditioning on the true input variable the knockoff is based on. under certain assumptions, fitting a model on top of both original and knocked-off features and observing the difference between the fitted coefficients for the true and the knockoff features can then be used for variable selection with guaranteed false discovery rate. the authors of this paper suggest that many of the current methods for construction of knockoffs may not be appropriate for image, text, and other types of datasets commonly used in modern ml, and suggest a heuristic way of employing vaes and gplvms for this purpose. the paper concludes with an empirical study which shows that their algorithm is competitive with existing feature selections methods in terms of post-selection accuracy of the fitted model.----------i am currently leaning towards recommending rejection. while the paper is nicely written and does a good job of reviewing knockoffs, i see two main issues: (1) i am not sure about applications for the proposed algorithm; in particular, the authors allude to use on devices with limited memory and computational power, but do not discuss why the johnson-lindenstrauss transform or some of the many low-precision implementations of neural networks (binarised neural networks, xnor-nets, ) cannot be used; furthermore, in the case of images, i am not sure why there is no comparison to simple downsampling to a smaller resolution; (2) the reported results do not show a reliable improvements over existing methods.---------------major comments:----------- i am not entirely sure lemma 1 is correct. in particular, mu_{z | x} tends to be a function of the whole vector x including x_n. for example, if (a, b) are jointly distributed according to a bivariate normal, then e (b | a) = e(b) + sigma_{ab} sigma_{aa}^{-1} (a - e(a)) where sigma is the corresponding covariance matrix. hence claiming that the marginal distribution z | x_{-n} is n ( mu_{z|x} , sigma_{z | x} ) seems wrong as mu_{z | x} will generally depend on x_n (similarly to how e(b | a) depends on a above) which cannot be the case when x_n is marginalised. if z depends linearly on x, there is a standard expression for the distribution you seek. however, with the non-linear dependence employed, e.g., within vae, working out a closed form expression may be quite a challenge. can you please clarify or drop this result from your paper if it indeed turns out incorrect?----------- can you please clarify why dont you benchmark against the cited algorithm proposed in lu et al. (2018)?----------- can you please explain how was the latent dimensionality for the vaes (5) and gplvms (10) selected? also, for the algorithms where random seed plays a role (e.g., vaes), how many random seeds were used (are the numbers reported in fig.2 a result of averaging over multiple seeds)? relatedly, have you tested whether starting from different seeds results in the same subset of variables selected (e.g., when using vaes)?---------------minor comments:----------- in the 1st sentence of the introduction, prevalence can be high or low, increase or decrease, etc. but becoming increasingly pervasive does not sound right. please consider rewording.----------- in fig.1e, do you know why gplvm leads to such a significant mode collapse?----------- just after the 1st display on p.5, since -> since this paper presents an interesting use of a knockoff framework similar to that of model-x knockoffs (candes et al., 2018) in generative models like vae and gplvm for feature selection in supervised learning. ----------on the flip side, it is really hard to tell from the experimental results (fig. 2) what performance benefits their proposed designs bring over the state of the art. i would encourage the authors to provide a more detailed analysis of the conditions under which their proposed designs would outperform the state of the art (or not). the author should consider presenting their results through other means that can better highlight the performance benefits of their proposed designs.----------besides applying their knockoff designs to a softmax classifier, can the authors provide examples of applications to other supervised learning models?----- ----------proof of lemma 1: without further assumptions from that stated in the lemma, can the authors provide a derivation (or result that they've used) and explanation for the mean vector and covariance matrix of the joint probability p(z,x_n|x_{n}) = p(z|x)p(x_n|x_{n})? in particular, why are z and x_n conditionally independent given x_{-n}, as reflected in their zero covariance value?---------------page 2: the authors say that ""they must be independent from the labels to be predicted"". shouldn't it be conditionally independent (2nd paragraph, page 3) instead?--------------------minor issues-----page 5: the notation of tilde{bold{x}}_n is ambiguous since the subscript is used to index the data sample (page 2). is it intended to be without a bold?----------step 3 of algorithm 1: i would have preferred that it is stated consistently with that in the main text (step i in 2nd paragraph of section 3).----------page 5: this sentence is difficult to parse: ""the training dataset for the generative models used to compute new knockoffs sample values is trained over the original data samples"".----------page 5: since?----------page 6: i cannot find figure 1(g) and figure 1(h).","this manuscript proposes feature selection inspired by knockoffs, where the generative models are implemented using modern deep generative techniques. the resulting procedure is evaluated in a variety of empirical settings and shown to improve performance.----------the reviewers and ac agree that the problem studied is timely and interesting, as knockoffs combined with generative models have recently shown promise for inferential problems. however, the reviewers were unconvinced about the motivation of the work, and the strength of the empirical evaluation results. in the option of the ac, this work might be improved by focusing (both conceptually and empirically) on applications where inferential variable selection is most relevant e.g. causal settings, healthcare applications, and so on.",the paper concludes with an empirical study which shows that their algorithm is competitive with existing feature selections methods in terms of post-selection accuracy of the fitted model.----------i am currently leaning towards recommending rejection.,"the author should consider presenting their results through other means that can better highlight the performance benefits of their proposed designs.----------besides applying their knockoff designs to a softmax classifier, can the authors provide examples of applications to other supervised learning models?----- ----------proof of lemma 1: without further assumptions from that stated in the lemma, can the authors provide a derivation (or result that they've used) and explanation for the mean vector and covariance matrix of the joint probability p(z,x_n|x_{n}) = p(z|x)p(x_n|x_{n})?","the author should consider presenting their results through other means that can better highlight the performance benefits of their proposed designs.----------besides applying their knockoff designs to a softmax classifier, can the authors provide examples of applications to other supervised learning models?----- ----------proof of lemma 1: without further assumptions from that stated in the lemma, can the authors provide a derivation (or result that they've used) and explanation for the mean vector and covariance matrix of the joint probability p(z,x_n|x_{n}) = p(z|x)p(x_n|x_{n})?",this paper proposes a way of employing modern generative models like vaes and gplvms within the recently popular knockoff framework for variable/feature selection.,"is it intended to be without a bold?----------step 3 of algorithm 1: i would have preferred that it is stated consistently with that in the main text (step i in 2nd paragraph of section 3).----------page 5: this sentence is difficult to parse: ""the training dataset for the generative models used to compute new knockoffs sample values is trained over the original data samples"".----------page 5: since?----------page 6: i cannot find figure 1(g) and figure 1(h).","the author should consider presenting their results through other means that can better highlight the performance benefits of their proposed designs.----------besides applying their knockoff designs to a softmax classifier, can the authors provide examples of applications to other supervised learning models?----- ----------proof of lemma 1: without further assumptions from that stated in the lemma, can the authors provide a derivation (or result that they've used) and explanation for the mean vector and covariance matrix of the joint probability p(z,x_n|x_{n}) = p(z|x)p(x_n|x_{n})?","is it intended to be without a bold?----------step 3 of algorithm 1: i would have preferred that it is stated consistently with that in the main text (step i in 2nd paragraph of section 3).----------page 5: this sentence is difficult to parse: ""the training dataset for the generative models used to compute new knockoffs sample values is trained over the original data samples"".----------page 5: since?----------page 6: i cannot find figure 1(g) and figure 1(h).","the author should consider presenting their results through other means that can better highlight the performance benefits of their proposed designs.----------besides applying their knockoff designs to a softmax classifier, can the authors provide examples of applications to other supervised learning models?----- ----------proof of lemma 1: without further assumptions from that stated in the lemma, can the authors provide a derivation (or result that they've used) and explanation for the mean vector and covariance matrix of the joint probability p(z,x_n|x_{n}) = p(z|x)p(x_n|x_{n})?",0.1854304635761589,0.0268456375838926,0.1059602649006622,0.1059602649006622,0.2488038277511962,0.0193236714975845,0.1244019138755981,0.1244019138755981,0.2488038277511962,0.0193236714975845,0.1244019138755981,0.1244019138755981,0.2158273381294964,0.0291970802919708,0.1438848920863309,0.1438848920863309,0.2564102564102564,0.0310880829015544,0.1333333333333333,0.1333333333333333,0.2488038277511962,0.0193236714975845,0.1244019138755981,0.1244019138755981,0.2564102564102564,0.0310880829015544,0.1333333333333333,0.1333333333333333,0.2488038277511962,0.0193236714975845,0.1244019138755981,0.1244019138755981,7.904822826385498,6.730318069458008,11.05605411529541,7.904822826385498,8.988737106323242,7.904822826385498,7.904822826385498,6.730318069458008,0.9732103675054293,0.9755878702288989,0.752384394568844,0.5143419867236081,0.6256397555734274,0.9230123318186271,0.5143419867236081,0.6256397555734274,0.9230122974334969,0.9762872242061555,0.9671220573205443,0.9440875838494722,0.31947794621316034,0.347131754995665,0.33478373446262755,0.5143419867236081,0.6256397555734274,0.9230122974334969,0.31947794621316034,0.347131754995665,0.33478373446262755,0.5143419867236081,0.6256397555734274,0.9230123318186271
213,https://openreview.net/forum?id=rygixkHKDH,"this paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.----------for the over-complete dictionary setting, given the measurements of the form , where and denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an ------norm maximization approach to recover the dictionary . this corresponds to maximizing ----- over . interestingly, the paper shows that when is unit norm tight frame and incoherent the optimization landscape of the aforementioned non-convex objective has strict saddle points that can be escaped by along negative curvature. furthermore, all local minimizers are globally optimal which are close to one of the columns of . this shows that any descent method that can escape the saddle points will (approximately) recover one of the columns of . ----------for convolution dictionaries, the paper shows that when the underlying filters are incoherent a suitably modified ------norms based objective has only strict saddles over a sub-level set. furthermore, all local optimizers within this sub-level set are close to one of the convolution filters. ----------the reviewer believes that this paper presents many interesting and novel results that extend our understanding of provable methods for dictionary learning. as claimed in the paper, this the first global characterization for the non-convex optimization landscape for over-complete dictionary learning. besides, the paper provides the first provable guarantees for convolution dictionary learning. overall, the paper is very well written and the key ideas used in the paper are nicely explained in the main body of the paper. the experimental results in the paper also corroborate the theoretical findings of the paper.----------minor comments:----------1. in page 2, ""....can be simply summarized by the following slogan."" ---> ""....can be simply summarized by the following statement.""?----------2. in page 7, replace ""cook up"" with ""propose""?---------------------------------------------after rebuttal----------thank you for the response. releasing the code for reproducibility purposes is certainly a great idea. [summary]-----this paper studies the problem of non-convex optimization for dictionary learning (dl) in the situation when the underlying dictionary is over-complete (more basis vectors m than the dimension n). the paper proves that the l4 maximization formulation has a nice global landscape and can be efficiently minimized by (riemannian) gradient descent, when the over-complete ratio m/n is less than an absolute constant. a similar result is proved for convolutional dictionary learning.----------[pros]-----the theoretical results in this paper provides a solid improvement over the prior understandings on overcomplete dl, a setting that is practically important yet theoretically more challenging than standard orthogonal/complete dl. ----------specifically, the prior work of (ge & ma 2017) shows only a nice local optimization landscape when m > n^{1+\eps} and hypothesizes that the global landscape is bad in the same setting (there exists bad local minima out of a certain sub-level set). in comparison, this work proves that at least for m/n <= 3 (roughly), the landscape is globally benign (has the strict saddle property), therefore providing a new understanding that the benign landscape is still preserved from the other side where m/n grows mildly above 1. the analysis contains novel technicalities and can be of general interest for understanding the landscape of non-convex problems.----------the paper also provides experimental evidence that gradient descent converges globally up until m = o(n^2), a broader regime than suggested by the theory (m <= 3n). (though when m >= n^{1+\eps}, the reason of global convergence from random init may be far from the present theory, in that there can be potentially exponentially many bad local min yet gradient descent wont get trapped.)----------[cons]-----it is still a bit disturbing to see that m/n needs to be bounded by a fixed absolute constant, rather than *any* constant, for the theory to work. from the proofs it seems like this constant (3) may have the potential to be improved, but it is not quite easy to completely get rid of it?","this paper investigates the use non-convex optimization for two dictionary learning problems, i.e., over-complete dictionary learning and convolutional dictionary learning. the paper provides theoretical results, associated with empirical experiments, about the fact that, that when formulating the problem as an l4 optimization, gives rise to a landscape with strict saddle points and as such, they can be escaped with negative curvature. as a result, descent methods can be used for learning with provable guarantees. all reviews found the work extremely interesting, highlighting the importance of the results that constitute ""a solid improvement over the prior understandings on over-complete dl"" and ""extends our understanding of provable methods for dictionary learning"". this is an interesting submission on non-convex optimization, and as such of interest to the ml community of iclr . i'm recommending this work for acceptance.","interestingly, the paper shows that when is unit norm tight frame and incoherent the optimization landscape of the aforementioned non-convex objective has strict saddle points that can be escaped by along negative curvature.","this paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.----------for the over-complete dictionary setting, given the measurements of the form , where and denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an ------norm maximization approach to recover the dictionary .","the paper proves that the l4 maximization formulation has a nice global landscape and can be efficiently minimized by (riemannian) gradient descent, when the over-complete ratio m/n is less than an absolute constant.","this paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.----------for the over-complete dictionary setting, given the measurements of the form , where and denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an ------norm maximization approach to recover the dictionary .","this paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.----------for the over-complete dictionary setting, given the measurements of the form , where and denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an ------norm maximization approach to recover the dictionary .","this paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.----------for the over-complete dictionary setting, given the measurements of the form , where and denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an ------norm maximization approach to recover the dictionary .","as claimed in the paper, this the first global characterization for the non-convex optimization landscape for over-complete dictionary learning.","this paper studies the dictionary learning problem for two popular settings involving sparsely used over-complete dictionaries and convolutional dictionaries.----------for the over-complete dictionary setting, given the measurements of the form , where and denote the over-complete dictionary and the sparse coefficients, respectively, the paper explores an ------norm maximization approach to recover the dictionary .",0.2528735632183908,0.1046511627906976,0.1724137931034483,0.1724137931034483,0.3384615384615385,0.1139896373056994,0.2461538461538461,0.2461538461538461,0.2171428571428571,0.0346820809248554,0.1828571428571428,0.1828571428571428,0.3384615384615385,0.1139896373056994,0.2461538461538461,0.2461538461538461,0.3384615384615385,0.1139896373056994,0.2461538461538461,0.2461538461538461,0.3384615384615385,0.1139896373056994,0.2461538461538461,0.2461538461538461,0.1987577639751552,0.0754716981132075,0.1366459627329192,0.1366459627329192,0.3384615384615385,0.1139896373056994,0.2461538461538461,0.2461538461538461,17.230749130249023,17.230749130249023,17.230749130249023,17.230749130249023,6.154319286346436,7.349646091461182,17.230749130249023,15.682701110839844,0.9314339621904881,0.9353736407369467,0.00428308427139884,0.923106968757901,0.9212800708921589,0.931407056387842,0.9532952367827932,0.9474613569890024,0.9258294572525231,0.923106968757901,0.9212800708921589,0.9314070216897817,0.923106968757901,0.9212800708921589,0.9314069302374125,0.923106968757901,0.9212800708921589,0.931407056387842,0.9442342720838116,0.9394836909263891,0.9026685240331018,0.923106968757901,0.9212800708921589,0.931407056387842
214,https://openreview.net/forum?id=ryl0cAVtPH,"the author proposes different approaches to the problem of ""warm-started"" neural networks. models trained from scratch on the whole dataset have better performance than ""warm-started"" models, which are trained with weights initialized from training using part of the available data. the authors change hyperparameters like batch size and learning rate and demonstrate a tradeoff between generalization performance of the model and time, required for its training. we can also see that the choice of hyperparameters, necessary for the best performance, levels benefit in time from ""warm starting.""----------the core idea of the paper is the investigation of various possible causes of difficulty of ""warm start"" to reduce training time without damaging a generalization performance. the authors investigate the dependence of this effect with gradient values, regularization, part of ""warm started"" layers, noising weights, catastrophic forgetting, and so on. ----------the authors describe different problems where this research can be useful and tries to shed light on the causes of this problem, but the solution is not found. this article can be helpful for future researchers as they continue research in this direction from a warm start. however, it is hard to judge how valuable this contribution is.----------also, see a few minor comments:-----1. maybe it should be useful to include in table 1 results for models trained using only 50% of data.-----2. typo: nvida -> nvidia (p. 6)-----3. it seems that the problem lies in the area of the complex learning landscape for optimization of neural networks as we end up in the worse local optimum if we use a warm start. maybe one should attack the problem with this direction, as there are several papers that investigate how the loss function landscape looks like e.g. [1] -----4. it seems that the behavior becomes worse if we increase the complexity of the model. investigation of the dependence of severity of warm start effect on e.g. number of layers in the network can be useful. also, it can be possible to gain some theoretical insights and provable results while dealing with simpler models. [1.] h. li, zh. xu et. al. visua-----5. it might be useful to research the dependence of ""warm start"" effect from the portion of data on which the model was pre-trained. as i understand situations, where 80-90% of data are used in the first round of training, are more common.-----6. the authors provide us with graphics of accuracy on training dataset for ""warm started"" and trained from the scratch models. we can see that both models reach 100% train accuracy, and the authors state that it is impossible to spot the ""warm start"" problem on the training dataset because the metrics are equal. maybe it could be useful to show graphics of loss function. despite of similar 100% train accuracy, final losses might be different(""warm start"" loss > from scratch loss). it may mean that in case of warm start we reach a local minimum, but not the best one. -----7. it might be a good idea to add to table 2 results for not-regularized model in order to compare results with and without regularization and figure out what effect on ""warm started"" models regularization have. this paper conducted an empirical study on why training with warm starting has worse generalization ability than learning from scratch. the paper is interesting, however, it has something unclear to me, as explained below.---------------1) the scale and diversity of the study can be improved. only three models and three datasets were examined, which might not be representative enough. for example, the popular transformer model, the large-scale datasets like imagenet, the language understanding and machine translation tasks, etc. were not included in the study. this may make the study less relevant to many important tasks and domains.----------2) the interesting and highlight part of the paper is that it studies many different factors and aspects, including the influence of batch size, learning rate, regularization, moment, denoising, etc. however, i kind of feel that the experimental setting has some fatal problems, which makes the experimental results not convincing. the authors partitioned the dataset into two halves, using the first half for pre-training, and then use the whole datasets for continued training. although the partitioning is random, given the limited size of the datasets, such a treatment will change the underlying distribution (frequency of the samples during training). the first half of the dataset plays a more important role in training: it was used during pre-training, and also used in training. so somehow the first half was used twice, or at least used more than once, depending on the numbers of epochs in pre-training and training. this distribution change may make the training a little biased, and at least it is not a fair comparison with learning from scratch (the latter will not have such bias in data). so for a fair comparison, one needs to add some baselines to understand the influence of data frequency change. without the understanding from this angle, the study may be mis-leading.---------------**i read the author rebuttal, however, i still think the experiments are not comprehensive and the use of data partitions in the experiments are problematic (nothing to do with realistic or not, just for fair comparison with learning from scratch). so i would not change my assessment. this paper examines the problem of warm-starting the training of neural networks. in particular, a generalization gap arises when the network is trained on the full training set from the start versus being warm-started, where the network is initially (partially) trained on a subset of the training set, then switched to the full training set. this problem is practical, as it is often preferable to train online while data is collected to make up-to-date predictions for tasks (such as in online advertising or recommendation systems), but it has been found that retraining is necessary in order to obtain optimal performance. the paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems.----------the paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters. ----------strengths:----------i believe very strongly in the practical impact of the problems presented in this paper. these indeed are challenging problems that are relevant to industry that have not been given sufficient attention in the academic literature. i appreciate the initial experimentation on this subject, and the clear demonstration of the problem through simple experiments. the paper is also well-written.----------weaknesses:----------some questions i had include:----------- why is the pearson correlation between parameters of the neural network a good way of measuring the correlation to their initialization?----------- why is it surprising that the magnitude of the gradients of the ""new"" data is higher than at a random initialization?----------- why does this phenomena occur even though the data is sampled from the same distribution? ----------- does this work have any relationship with work on generalization such as:-----[1] recht, benjamin, et al. ""do cifar-10 classifiers generalize to cifar-10?."" arxiv preprint arxiv:1806.00451 (2018).-----[2] recht, benjamin, et al. ""do imagenet classifiers generalize to imagenet?."" arxiv preprint arxiv:1902.10811 (2019).-----etc.----------although i like the topic of this paper, the investigation seems too preliminary at this point. there is no clear hypothesis towards answering the problems proposed in the paper. there is also no analysis, which places the burden on the numerical experiments to demonstrate something interesting, and the experiments seem sparse and small-scale. for these reasons, i am inclined to reject this paper at this time, but i strongly encourage further exploration into the topic. ----------some potential questions or directions could include:----------1. what if only a single epoch of training is used on 50% of the data? does the gap appear in that setting? i ask because one would expect that a single epoch of training on 50% of the data, then training on new data would be equivalent to training on the full dataset from the start.----------2. how does this gap change with respect to the (relative) amount of new data introduced into the problem? for example, if one were to only add a single datapoint to the training set, would one still observe this behavior? could one potentially add data more incrementally (rather than half of the training set) and potentially mitigate this drastic change in the problem?----------3. there are optimization algorithms specifically designed for stochastic optimization (with a fixed distribution) versus for online optimization (online gradient, adagrad). is the online optimization framework perhaps more ""realistic"" than the stochastic optimization framework in these streaming/warm-starting settings?","the paper addresses the question of why warm starting could result in worse generalization ability than training from scratch. the reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources. however, the reviewers were unanimous in their opinion that the paper is not suitable for publication at iclr in its current form. concerns included that the analysis was not sufficiently focused and the experiments too small scale. as the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.",typo: nvida -> nvidia (p. 6)-----3. it seems that the problem lies in the area of the complex learning landscape for optimization of neural networks as we end up in the worse local optimum if we use a warm start.,"the paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems.----------the paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters.","models trained from scratch on the whole dataset have better performance than ""warm-started"" models, which are trained with weights initialized from training using part of the available data.","the author proposes different approaches to the problem of ""warm-started"" neural networks.","in particular, a generalization gap arises when the network is trained on the full training set from the start versus being warm-started, where the network is initially (partially) trained on a subset of the training set, then switched to the full training set.","the paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems.----------the paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters.","the first half of the dataset plays a more important role in training: it was used during pre-training, and also used in training.","the paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems.----------the paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters.",0.1866666666666666,0.0405405405405405,0.1066666666666666,0.1066666666666666,0.2626262626262626,0.0510204081632653,0.1313131313131313,0.1313131313131313,0.1726618705035971,0.072992700729927,0.1294964028776978,0.1294964028776978,0.0975609756097561,0.0165289256198347,0.08130081300813,0.08130081300813,0.2727272727272727,0.0526315789473684,0.1688311688311688,0.1688311688311688,0.2626262626262626,0.0510204081632653,0.1313131313131313,0.1313131313131313,0.1343283582089552,0.0151515151515151,0.1044776119402984,0.1044776119402984,0.2626262626262626,0.0510204081632653,0.1313131313131313,0.1313131313131313,10.523066520690918,7.586555480957031,14.473114013671877,10.523066520690918,6.888950824737549,14.0948486328125,10.523066520690918,9.427873611450195,0.9527953549726189,0.9612556262134616,0.4752017194253387,0.09701363644474821,0.08461309437771411,0.9032758173956649,0.9692382503061239,0.9684997432558743,0.9383391731068723,0.9717837770635382,0.9722389789319348,0.9381297299362547,0.651227369164687,0.8348144016070762,0.8812824526349551,0.09701363644474821,0.08461309437771411,0.9032758443157727,0.6255729551262502,0.7861302667604854,0.9138347853892902,0.09701363644474821,0.08461309437771411,0.903275797205585
215,https://openreview.net/forum?id=ryl260NYDr,"the title of this paper is rather misleading that i thought it was an empirical study of learned priors in deep latent variable models. but in fact this is a methodology paper that proposes a new learning objective for these models. the objective is new in that i haven't seen it anywhere else but the contribution is quite incremental given a closely-related objective is studied in wasserstein autoencoders. despite that it might look surprising to people familiar with variational autoencoders, removing the kl regularization term and match aggregated posterior and prior is already justified in the wasserstein ae paper.----------more empirical evidences are needed to show the strengths of the proposed method over methods with similar motivations, such as wae, resampled prior vae (baur & mnih, 2018), 2-stage vae (dai & wipf).----------more concerns are summarized below-----* there are many claims in the main text that is unclear to me. for example, on page 4, ""in doing so it is easy to see that the issue of posterior overlap can be mitigated"", how?-----* fid is not a meaningful metric for generalization. there is no evaluation of test log likelihoods.-----* the learned deep generative model may generate pretty images. but i suspect it will be outperformed by waes in terms of representation learning (because no regularization ever happens in the objective).-----* did you apply vgg19 perceptual loss to all models or only models using the proposed method? in this paper, the authors study vaes which are trained with a loss that does not contain the typical kl divergence term. additionally, the authors focus on the case where the prior (latent) distribution is learned via a normalizing flow. the authors empirically show that higher quality samples can be generated by vaes without the kl divergence (than in comparison to those with the term) in this context; that the linear separability of the latent representations increases without the kl term, and that greater diversity can be obtained by conditional sampling. ----------the observation that the authors report is intriguing and definitely worth further inquiry, but i feel that the paper in its current form does not present a coherent picture that other researchers can easily grasp and build upon. the quality of the writing is a significant barrier toward understanding the deeper meaning of the paper. as a result, i recommend declining this paper for publication in iclr. ----------(1) the paper needs a much clearer focus. for example, the title of ""empirical observations pertaining to ..."" should be replaced by something related to the actual claim of the paper-----(2) the abstract is too vague for experts or non-experts to get a clear indication of what is in the paper-----(3) where does this work leave the relative performance levels of vaes versus other generative models? -----(4) how broadly should the results be interpreted for vaes? does it only apply to flow-based priors? or is it making a claim for a broader set of learnable priors? this will also affect what is in the title and abstract. is it only being claimed for l2 loss (which then admits a 2-wasserstein distance problem) or is it being claimed about a broader category of losses? if so, what is the principle under which removing the kl terms is reasonable when it doesn't lead to a 2-wasserstein problem?-----(5) individual sentences of the paper are adequately written, but the writing of the paper currently feels like a long list of facts and details, which makes extracting the deep takeaway much harder. it is unclear after reading this paper what i would tell another researcher or a student, though i feel that with a substantial revision there could be some very interesting things that could be told based a continuation by the authors of this work. the paper explores the idea of removing the prior regularization, such as the kl regularization in variational autoencoders. in order to allow sampling from the generative model, the prior distribution is parameterized by a normalizing flow that is fit to minimize the cross-entropy between the marginal posterior distribution and the prior. i find the idea to be interesting. however, the paper, especially the experimental section, is in a very rough state, making assessing the contribution hard. i recommend rejection of the paper in the current state, but encourage the authors to finalize the paper.----------pros:-----1. the paper proposes a fairly radical change to the vae setup.-----2. the results look sensible and in some aspects better than the results of vaes and gans.----------cons:-----1. as mentioned above, the paper is clearly unfinished. there are many formatting issues (too small figures; \citet and \citep being used incorrectly). most importantly, the text is unnecessarily verbose. as an example, autoregressive flows are discussed in detail, yet not used in the experiments.-----2. despite the verbosity, there is no discussion of the downsides of the removal of the regularization. for instance, i would expect the log-likelihood of the proposed model to be arbitrarily poor. furthermore, it seems that the optimal encoder distribution in this setup is a delta-function, which makes the aggregate posterior a mixture of delta-functions and can lead to issues when fitting a prior distribution.-----3. ive found the disentanglement results confusing. beta-vae usually requires beta > 1 for disentanglement, but this paper achieves better disentanglement for beta = 0 than for beta = 1","this paper presents an adaptive computation time method for reducing the average-case inference time of a transformer sequence-to-sequence model. ----------the reviewers reached a rough consensus: this paper makes a proposes a novel method for an important problem, and offers reasonably compelling evidence for that method. however, the experiments aren't *quite* sufficient to isolate the cause of the observed improvements, and the discussion of related work could be clearer.----------i acknowledge that this paper is borderline (and thank r3 for an extremely thorough discussion, both in public and privately), but i lean toward acceptance: the paper doesn't have any fatal flaws, and it brings some fresh ideas to an area where further work would be valuable.","----------the observation that the authors report is intriguing and definitely worth further inquiry, but i feel that the paper in its current form does not present a coherent picture that other researchers can easily grasp and build upon.","despite that it might look surprising to people familiar with variational autoencoders, removing the kl regularization term and match aggregated posterior and prior is already justified in the wasserstein ae paper.----------more empirical evidences are needed to show the strengths of the proposed method over methods with similar motivations, such as wae, resampled prior vae (baur & mnih, 2018), 2-stage vae (dai & wipf).----------more concerns are summarized below-----* there are many claims in the main text that is unclear to me.",but i suspect it will be outperformed by waes in terms of representation learning (because no regularization ever happens in the objective).-----* did you apply vgg19 perceptual loss to all models or only models using the proposed method?,the title of this paper is rather misleading that i thought it was an empirical study of learned priors in deep latent variable models.,"despite that it might look surprising to people familiar with variational autoencoders, removing the kl regularization term and match aggregated posterior and prior is already justified in the wasserstein ae paper.----------more empirical evidences are needed to show the strengths of the proposed method over methods with similar motivations, such as wae, resampled prior vae (baur & mnih, 2018), 2-stage vae (dai & wipf).----------more concerns are summarized below-----* there are many claims in the main text that is unclear to me.","despite that it might look surprising to people familiar with variational autoencoders, removing the kl regularization term and match aggregated posterior and prior is already justified in the wasserstein ae paper.----------more empirical evidences are needed to show the strengths of the proposed method over methods with similar motivations, such as wae, resampled prior vae (baur & mnih, 2018), 2-stage vae (dai & wipf).----------more concerns are summarized below-----* there are many claims in the main text that is unclear to me.",this will also affect what is in the title and abstract.,"despite that it might look surprising to people familiar with variational autoencoders, removing the kl regularization term and match aggregated posterior and prior is already justified in the wasserstein ae paper.----------more empirical evidences are needed to show the strengths of the proposed method over methods with similar motivations, such as wae, resampled prior vae (baur & mnih, 2018), 2-stage vae (dai & wipf).----------more concerns are summarized below-----* there are many claims in the main text that is unclear to me.",0.2025316455696202,0.0384615384615384,0.1265822784810126,0.1265822784810126,0.208955223880597,0.0100502512562814,0.1194029850746268,0.1194029850746268,0.1518987341772152,0.0128205128205128,0.0759493670886076,0.0759493670886076,0.1666666666666666,0.028169014084507,0.1111111111111111,0.1111111111111111,0.208955223880597,0.0100502512562814,0.1194029850746268,0.1194029850746268,0.208955223880597,0.0100502512562814,0.1194029850746268,0.1194029850746268,0.0763358778625954,0.0,0.0763358778625954,0.0763358778625954,0.208955223880597,0.0100502512562814,0.1194029850746268,0.1194029850746268,9.751441955566406,9.751441955566406,14.097309112548828,9.751441955566406,6.440437316894531,7.732620716094971,9.751441955566406,6.986627578735352,0.9773841156609683,0.9745125174739858,0.9527493096753302,0.9677937634391025,0.9752141912045786,0.9069953006306176,0.9139881836791386,0.9334961743972958,0.8647941809545325,0.9675552501356093,0.9645406994880116,0.9041579180758247,0.9677937634391025,0.9752141912045786,0.9069953006306176,0.9677937634391025,0.9752141912045786,0.9069952600844047,0.9464001309817466,0.955689334702044,0.9016884534897759,0.9677937634391025,0.9752141912045786,0.9069953006306176
216,https://openreview.net/forum?id=ryl4-pEKvB,"the paper proposes an backpropagation-free algorithm for training in deep neural networks.-----the algorithm called deepagrel does not use the chain rule for computing derivatives and instead is based on direct reinforcement. -----authors conduct experiments on mnist and cifar where they find their method to reach performance similar to bp.----------novelty: the method is built upon existing ideas, however, the exact algorithm seems to be novel.----------clarity: although it is possible to understand the algorithm based on the provided textual description, it would not hurt to provide a more formal presentation of the main update equation (1). ----------it is not clear why authors provide two different expressions for updates in each layer (in terms of fb^{l} and fb^{l+1}).----------i would also appreciate a clear discussion on the theoretical properties of the algorithm, for example, is it guaranteed to converge to a critical point of some loss function? can we derive the update rule from some known optimisation procedure, e.g. reinforce?----------quality: unfortunately i have a number of major issues with this respect.----------1. authors do not clearly pose what are the properties of back propagation that they find biologically non-plausible and how exactly their algorithm is making progress on them. there seems to be some sort of consensus in the literature about these properties and if authors accept it, then it looks like deepagrel is not very plausible too, because it does not address the weight-transport issue at all.-----2. many recent results are not even mentioned in the paper, for example, (bartunov et al, 2019). from not so recent  the whole branch of research around target propagation algorithm (lee et al, 2014).-----3. the experiments are not convincing to me, as the considered network architectures are quite shallow and thus the ability to perform credit assignment can not be demonstrated. besides that, the locally-connected architecture has only the first layer locally-connected and it does not make much sense to me, as it does not factor out the impact of weight sharing. ----------overall, i think the paper is not ready for publication at iclr. summary:----------this paper generalizes the agrel biologically plausible learning algorithm to deeper networks. it presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance.----------major comments:----------this is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks. the proposed mapping to the biology is of a different flavor from many other recently proposed approaches. ----------it is striking that the cifar10 network trains about as fast as the ebp network, while the cifar100 network trains much more slowly. this is presumably because randomly guessing the right answer out of 100 possibilities is the bottleneck. the paper could be strengthened by studying the speed of learning as a function of the number of output classes. for this approach to scale to imagenet, with a 1000-way classification (or to our human visual recognition abilities with far more classes), this is an important scaling dimension to consider. ----------it should be noted that other biologically plausible schemes like feedback alignment were able to solve cifar and other smaller image classification tasks, but struggled when applied to the larger scale imagenet problem. the paper could be improved by pointing to this limitation of the present work, the possibility that performance could change on larger tasks, and the need to conduct larger experiments in future work.----------personally i think the statement at the beginning of the introduction that only rl occurs in animals and humans is overstated. unsupervised learning occurs in some form in critical period plasticity, and in various unrewarded statistical learning paradigms, at a minimum.----------i would also caution against categorical statements of biological plausibility, for instance, in saying that shared weights in convolutions are biologically implausible (bottom of pg 6). the same criticism has been leveled at gradient descent learning, and as the present submission shows, these intuitive judgements can be misleading.----------the distinction between rl and supervised learning is a bit blurred in the present submission because it is considering a classification setting in which exactly one out of a number of possible outputs is rewarded on each trial. this looks very similar to the supervised learning scenario, and relies on stochastic softmax-like competition to select a single output. this approach is very reasonable for mutually exclusive classification tasks, but this is a small subset of the tasks that full ebp could be applied to. ----------the paper is fairly clear but the explanation of the algorithm could be further streamlined and condensed. is deepagrel equivalent to stochastically selecting a single unit in a softmax layer and then back propagating only its error? it seems so from what i understand, and this may be a straightforward way to explain the algorithm. ----------typos:----------the text on page 7 describes mnist performance as 99.17% but the table has 99.16%. this paper presents deepagrel, a framework for biologically plausible deep learning that is modified to use reinforcement learning as a training mechanism. this framework is shown to perform similarly to error-backpropagation on a set of architectures. the idea relies on feedback mechanism that can resemble local connections between real neurons.----------this paper is an interesting approach to provide a reinforcement learning paradigm for training deep networks, it is well written and the experiments are convincing, although more explanation about why these specific architectures were tested would be more convincing. i also think the assumptions about feedback connections in real neurons should be visited and more neuroscientific evidence from the literature should be included in the paper. do we expect feedback to happen at each level of a neuron-neuron interaction and between each pair of connected neurons? is there a possibility that feedback is more general to sets of neurons, or skips entire layers of neurons? i think more neuroscience background would help this paper (and others on the topic). nonetheless, i think the paper does offer an interesting proposal of a more biologically plausible form of deep learning.","the paper proposes an rl-based algorithm for training neural networks that is able to match the performance of backprop on cifar and mnist datasets.----------the reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of ""biologically plausible"" used by the authors. one reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow.----------for this type of paper, clarity and precision of exposition is crucial in my opinion, and so i recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue.","----------it should be noted that other biologically plausible schemes like feedback alignment were able to solve cifar and other smaller image classification tasks, but struggled when applied to the larger scale imagenet problem.",it presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance.----------major comments:----------this is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks.,"this paper presents deepagrel, a framework for biologically plausible deep learning that is modified to use reinforcement learning as a training mechanism.",the paper proposes an backpropagation-free algorithm for training in deep neural networks.-----the algorithm called deepagrel does not use the chain rule for computing derivatives and instead is based on direct reinforcement.,it presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance.----------major comments:----------this is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks.,it presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance.----------major comments:----------this is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks.,----------typos:----------the text on page 7 describes mnist performance as 99.17% but the table has 99.16%.,it presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance.----------major comments:----------this is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks.,0.1621621621621621,0.0410958904109589,0.1081081081081081,0.1081081081081081,0.2386363636363636,0.0114942528735632,0.0909090909090909,0.0909090909090909,0.1605839416058394,0.0296296296296296,0.1021897810218978,0.1021897810218978,0.2567567567567567,0.0958904109589041,0.2162162162162162,0.2162162162162162,0.2386363636363636,0.0114942528735632,0.0909090909090909,0.0909090909090909,0.2386363636363636,0.0114942528735632,0.0909090909090909,0.0909090909090909,0.0902255639097744,0.0,0.075187969924812,0.075187969924812,0.2386363636363636,0.0114942528735632,0.0909090909090909,0.0909090909090909,8.824731826782227,8.824731826782227,15.43030834197998,8.824731826782227,8.253788948059082,12.803549766540527,8.824731826782227,5.191961288452148,0.970462124269381,0.9627150331637068,0.9292338247301605,0.9396632911294415,0.9517906174093711,0.8894950342903293,0.9511653954097121,0.9528351206077516,0.9387651467502094,0.9851410970834752,0.9826418157378026,0.9466687801740589,0.9396632911294415,0.9517906174093711,0.8894950342903293,0.9396632911294415,0.9517906174093711,0.8894944912506653,0.6695930940684383,0.6772794724395237,0.7509964187487511,0.9396632911294415,0.9517906174093711,0.8894950342903293
217,https://openreview.net/forum?id=rylNH20qFQ,"this paper introduces a high-level semantic description for 3d shapes. the description is given by the so-called shapeprogram, each shape program consists of several program statements. a program statement can be either draw, which describes a shape primitive as well as its geometric and semantic attributes, or for, which contains a sub-program and parameters specifying how the sub-program should be repeatedly executed. the shapeprogram is connected with an input through two networks, the program generator (encoder) and a neural program executor (decoder). both encoder/decoder are implemented using lstm. the key ml contribution is on the decoder, which leverages a parametrization to make the decoder differentiable. the major advantage of the proposed technique is that it does not need to specify the shapeprogram in advance. in the same spriit of training an auto-encoder. it can be learned in a semi-supervised manner. however, in practice, one has to start with a reasonably good initial program. in the paper, this initial program was learned from synthetic data. ----------------the paper presents many experimental results, including evaluation on synthetic datasets, guided adaptation on shapenet, analysis of stability, connectivity measurement, and generalization, and application in shape completion. the presented evaluations, from the perspective of proposed experiments, is satisfactory. ----------------on the downside, this paper does not present any baseline evaluation, party due to the fact that the proposed problem is new. in fact, existing inverse procedural modeling techniques require the users to specify the program. however, the proposed approach could be even more convincing if it evaluates the performance of semantic understanding. for example, would it be possible to evaluate the performance on shape segmentation?----------------additional comments:--------1. how important is the initial program? ----------------2. the interactions among shape parts usually form a graph, not necessarily hierarchical. this should be discussed.----------------3. what is the difference between 3d shapes and 3d scenes? does this approach require a front/up-right orientation?----------------4. it would be interesting to visualize/analyze the intermediate representations of the neural shape generator. does it encode meaningful distributions among shape parts?----------------overall, it is a good paper, and i would like to see it at iclr 2019. this paper presents a methodology to infer shape programs that can describe 3d objects. the key intuition of the shape programs is to integrate bottom-up low-level feature recognition with symbolic high-level program structure, which allows the shape programs to capture both high-level structure and the low-level geometry of the shapes. the paper proposes a domain-specific language for 3d shapes that consists of for loops for capturing high-level regularity, and associates objects with both their geometric and semantic attributes. it then proposes an end-to-end differentiable architecture to learn such 3d programs from shapes using an interesting self-supervised mechanism. the neural program generator proposes a program in the dsl that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution. the technique is evaluated on both synthetic and shapenet tasks, and leads to significant improvements compared to tulsiani et al. that embed a prior structure on learning shape representations as a composition of primitive abstractions. in addition, the technique is also paired with marrnet to allow for a better 3d reconstruction from 2d images.----------------overall, this paper presents an elegant idea to describe 3d shapes as a dsl program that captures both geometric and spatial abstractions, and at the same time captures regularities using loops. csgnet [sharma et al. 2018] also uses programs to describe 2d and 3d shapes, but the dsl used here is richer as it captures more high-level regularities using loops and also semantic relationships such as top, support etc. the idea of training a neural program executor and using it for self-supervised training is quite elegant. i also liked the idea of guided adaption to make the program generator generalize beyond the synthetic template programs. finally, the results show impressive improvements and generalization capability of the model.----------------can the authors comment on some notion of completeness of the proposed dsl? in other words, is this the only set of operators, shapes, and semantics needed to represent all of shapenet objects? also, it might be interesting to comment more on how this particular dsl was derived. some of the semantics operator such as support, locker, etc. look overly specific to chair and tables. is there a way to possibly learn such abstractions automatically?----------------what is the total search space of programs in this dsl? how would a naive random search perform in this synthesis task?----------------i also particularly liked the decomposition of programs into draw and compound statements, and the corresponding program generator decomposition into 2 steps blocklstm and steplstm. at inference time, does the model use some form of beam search to sample block programs or are the results corresponding to top-1 prediction?----------------would it be possible to compare the results to the technique presented in csgnet [sharma et al. 2018]? there are some key differences in terms of using lower-level dsl primitives and using reinforce for training the program generator, but it would be good to measure how well having higher-level primitives improve the results.----------------i presume the neural program executor module was trained using a manually-written shape program interpreter. how difficult is it to write such an interpreter? also, how easy/difficult is to extend the dsl with new semantics operator and then write the corresponding interpreter extension?----------------minor typos:--------page 3: consists a variable  consists of a variable--------page 5: we executes  we execute--------page 6: synthetica dataset  synthetic dataset","this paper presents a method whereby a model learns to describe 3d shapes as programs which generate said shapes. beyond introducing some new techniques in neural program synthesis through the use of loops, this method also produces disentangled representations of the shapes by deconstructing them into the program that produced them, thereby introducing an interesting and useful level of abstraction that could be exploited by models, agents, and other learning algorithms. despite some slightly aggressive anonymous comments by a third party, the reviewers agree that this paper is solid and publishable, and i have no qualms in recommending it from inclusion in the proceedings.","the technique is evaluated on both synthetic and shapenet tasks, and leads to significant improvements compared to tulsiani et al. that embed a prior structure on learning shape representations as a composition of primitive abstractions.","the neural program generator proposes a program in the dsl that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution.","does it encode meaningful distributions among shape parts?----------------overall, it is a good paper, and i would like to see it at iclr 2019. this paper presents a methodology to infer shape programs that can describe 3d objects.",this paper introduces a high-level semantic description for 3d shapes.,"also, how easy/difficult is to extend the dsl with new semantics operator and then write the corresponding interpreter extension?----------------minor typos:--------page 3: consists a variable  consists of a variable--------page 5: we executes  we execute--------page 6: synthetica dataset  synthetic dataset","the neural program generator proposes a program in the dsl that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution.",both encoder/decoder are implemented using lstm.,"the neural program generator proposes a program in the dsl that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution.",0.2158273381294963,0.0,0.0863309352517985,0.0863309352517985,0.2818791946308724,0.0544217687074829,0.1610738255033557,0.1610738255033557,0.2394366197183098,0.0857142857142857,0.1126760563380281,0.1126760563380281,0.1217391304347826,0.0353982300884955,0.0869565217391304,0.0869565217391304,0.1360544217687074,0.0,0.0680272108843537,0.0680272108843537,0.2818791946308724,0.0544217687074829,0.1610738255033557,0.1610738255033557,0.018018018018018,0.0,0.018018018018018,0.018018018018018,0.2818791946308724,0.0544217687074829,0.1610738255033557,0.1610738255033557,9.41708755493164,5.748015880584717,14.800482749938965,9.41708755493164,8.417951583862305,11.144613265991213,9.417088508605955,9.609588623046877,0.9448411773365064,0.7909386989985521,0.4005040155852104,0.9507967057575594,0.9533245078037339,0.8861052904995164,0.9657466294147272,0.9662962925254576,0.7181920116191856,0.9672905166331265,0.9660594257217371,0.9343417822845547,0.48921955381157056,0.6303291035149846,0.05767339272760725,0.9507967057575594,0.9533245078037339,0.8861051910949719,0.946861445266189,0.9593676578318022,0.9386481537353177,0.9507967057575594,0.9533245078037339,0.8861052904995164
218,https://openreview.net/forum?id=rylkma4twr,"this paper considers zeroth-order method for min-max optimization (zo-min-max) in two cases: one-sided black box (for outer minimization) and two-sided black box (for both inner maximization and outer minimization). convergence analysis is carefully provided to show that zo-min-max converges to a neighborhood of stationary points. then, the authors empirically compare several methods on -----1) adversarial attack on imagenet with deep networks, and -----2) black-box poisoning attack on logistic regression. the results show that zo-min-max can provide satisfactory performance on these tasks.----------in general a good paper with dense content, clear organization and writing. however, the experiment part does not seem truly convincing. -----1. what is the relationship between eqn.(13) and the proposed zo-min-max? it seem that in the experiment you compare using this loss ( eqn.(13) ) against finite-sum loss, but both with zo-min-max algorithm? in figure 1 and 2, i dont see a competing method. so the point here is that the loss eqn.(13) is better, but not the proposed algorithm? i think you should compare different optimization algorithm under same loss, e.g. something like eqn.(13)+zo-min-max vs. eqn.(13)+fo-min-max. this is not evident to show that zo-min-max is better than other zero-th order methods.-----2. i would suggest comparing to more zeroth-order methods in the experiment.----------from the experiments i cannot tell whether zo-min-max is good enough compared with other methods the paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence. the algorithm is evaluated for the min-max problems that arise in the context of adversarial attacks. the presented algorithm is a natural application of a zeroth-order gradient estimator and the authors also prove that the algorithm has a sublinear convergence rate (in a specific sense). ----------considering that the algorithm merely applies the zeroth-order gradient estimator to min-max problems, the algorithm itself only makes up a somewhat novel contribution. however, to the best of my knowledge, it has not been used in this context before and personally i find the algorithm quite appealing. in fact, due to its simplicity it is essentially something that anyone could implement from scratch. ----------perhaps a more important contribution is that the authors provide a fairly extensive convergence analysis, which is an important tool in analysing the algorithm and its properties. unfortunately, it is not trivial to understand the presented convergence results and their practical implications (if any). for instance, equation (10), which is arguably one of the key equations in the paper, contains variables zeta, nu and p1, all of which depend on a number of other variables in a fairly complicated manner. the expression in (10) also contains terms that do not depend on t and it is not obvious how large these terms might be in practice (in the event that the assumptions are at least approximately true in a local region). even though i am somewhat sceptical to the practical relevance of this convergence analysis, i recognise that it is an interesting and fascinating achievement that the authors have managed to provide a convergence analysis of an algorithm which is based on black-box min-max optimisation.","this paper proposes convergence results for zeroth-order optimization.----------one of the main complaints was that zo has limited use in ml. i appreciate the authors' response that there are cases where gradients are not easily available, especially for black-box attacks.----------however, i find the limited applicability an issue for iclr and i encourage the authors to find a conference that is more suited to that work.","the results show that zo-min-max can provide satisfactory performance on these tasks.----------in general a good paper with dense content, clear organization and writing.",i would suggest comparing to more zeroth-order methods in the experiment.----------from the experiments i cannot tell whether zo-min-max is good enough compared with other methods the paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence.,i would suggest comparing to more zeroth-order methods in the experiment.----------from the experiments i cannot tell whether zo-min-max is good enough compared with other methods the paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence.,this paper considers zeroth-order method for min-max optimization (zo-min-max) in two cases: one-sided black box (for outer minimization) and two-sided black box (for both inner maximization and outer minimization).,i would suggest comparing to more zeroth-order methods in the experiment.----------from the experiments i cannot tell whether zo-min-max is good enough compared with other methods the paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence.,"even though i am somewhat sceptical to the practical relevance of this convergence analysis, i recognise that it is an interesting and fascinating achievement that the authors have managed to provide a convergence analysis of an algorithm which is based on black-box min-max optimisation.",i would suggest comparing to more zeroth-order methods in the experiment.----------from the experiments i cannot tell whether zo-min-max is good enough compared with other methods the paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence.,i would suggest comparing to more zeroth-order methods in the experiment.----------from the experiments i cannot tell whether zo-min-max is good enough compared with other methods the paper presents an algorithm for performing min-max optimisation without gradients and analyses its convergence.,0.1702127659574468,0.0217391304347826,0.1063829787234042,0.1063829787234042,0.3185840707964602,0.018018018018018,0.1592920353982301,0.1592920353982301,0.3185840707964602,0.018018018018018,0.1592920353982301,0.1592920353982301,0.2912621359223301,0.0594059405940594,0.2524271844660194,0.2524271844660194,0.3185840707964602,0.018018018018018,0.1592920353982301,0.1592920353982301,0.3157894736842105,0.0357142857142857,0.1929824561403509,0.1929824561403509,0.3185840707964602,0.018018018018018,0.1592920353982301,0.1592920353982301,0.3185840707964602,0.018018018018018,0.1592920353982301,0.1592920353982301,7.869861602783203,9.83311653137207,10.861898422241213,9.83311653137207,10.75013542175293,9.83311653137207,9.833115577697754,9.83311653137207,0.9748181725431944,0.9772775205059829,0.893593656774047,0.9773459395318758,0.9795280517039505,0.07428860532298613,0.9773459395318758,0.9795280517039505,0.07428860532298613,0.9702237097489568,0.9715995809128809,0.9594794986025843,0.9773459395318758,0.9795280517039505,0.07428860532298613,0.967258713732854,0.9661248006680522,0.962749319628566,0.9773459395318758,0.9795280517039505,0.07428865556213055,0.9773459395318758,0.9795280517039505,0.07428860532298613
219,https://openreview.net/forum?id=ryup8-WCW,"this paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem. taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace. performance on this subspace is then evaluated relative to that over the full parameter space (the baseline). as an empirical standard, the authors focus on the subspace dimension that achieves a performance of 90% of the baseline. the authors then test out their measure of intrinsic dimensionality for fully-connected networks and convolutional networks, for several well-known datasets, and draw some interesting conclusions.----------------pros:----------------* this paper continues the recent research trend towards a better characterization of neural networks and their performance. the authors show a good awareness of the recent literature, and to the best of my knowledge, their empirical characterization of the number of latent parameters is original. ----------------* the characterization of the number of latent variables is an important one, and their measure does perform in a way that one would intuitively expect. for example, as reported by the authors, when training a fully-connected network on the mnist image dataset, shuffling pixels does not result in a change in their intrinsic dimensionality. for a convolutional network the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet.----------------* the proposed measures seem very practical - training on random projections uses far fewer parameters than in the original space (the baseline), and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training.----------------* except for the occasional typo or grammatical error, the paper is well-written and organized. the issues are clearly identified, for the most part (but see below...).----------------cons:----------------* in the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections. variance should be taken into account explicitly, in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself. how often does a random projection lead to a high-quality solution, and how often does it not?----------------* the authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region (which in general cannot be guaranteed). in their experiments (fc networks of varying depths and layer widths for the mnist dataset), between projected and original solutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed. this calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network.----------------* the authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension. however, i don't think that they make a convincing case for this approach. again, variation is the difficulty: two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality. how then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality, even when its dimensionality greatly exceeds the intrinsic dimension?----------------* the authors argue for a relationship between intrinsic dimensionality and the minimum description length (mdl) of their solution, in that the intrinsic dimensionality should serve as an upper bound on the mdl. however they don't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model - it varies from setting to setting, with some parameters potentially requiring many more bits than others. and given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality, it is hard to accept that ""there is some rigor behind"" their conclusion that lenet is better than fc networks for classification on mnist because its empirical intrinsic dimensionality score is lower.----------------* the experimental validation of their measure of intrinsic dimension could be made more extensive. in the main paper, they use three image datasets - mnist, cifar-10 and imagenet. in the supplemental information, they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets.----------------overall, i think that this characterization does have the potential to give insights into the performance of neural networks, provided that variation across projections is properly taken into account. for now, more work is needed.----------------====================================================================================================--------addendum:----------------the authors have revised their paper to take into account the effect of variation across projections, with results that greatly strengthen their results and provide a much better justification of their approach. i'm satisfied too with their explanations, and how they incorporated them into their revised version. i've adjusted my rating of the paper accordingly.----------------one point, however: the revisions seem somewhat rushed, due to the many typos and grammatical errors in the updated sections. i would like to encourage the authors to check their manuscript once more, very carefully, before finalizing the paper.--------==================================================================================================== while deep learning usually involves estimating a large number of variable, this paper suggests to reduce its number by assuming that these variable lie in a low-dimensional subspace. in practice, this subspace is chosen randomly. simulations show the promise of the proposed method. in particular, figure 2 shows that the number of parameters could be greatly reduced while keeping 90% of the performance; and figure 4 shows that this method outperforms the standard method. the method is clearly written and the idea looks original. ----------------a con that i have is about the comparison in figure 4. while the proposed subspace method might have the same number of parameters as the direct method, i wonder if it is a fair comparison since the subspace method could still be more computational expensive, due to larger number of latent variables. [ =============================== revision =========================================================]--------my questions are answered, paper undergone some revision to clarify the presentation. i still maintain that it is a good paper and argue for acceptance - it provides a witty way of checking whether the network is overparameterized. mnist with shuffled labels is a great example that demonstrates the value of the approach, i would though have moved the results of it into the main paper, instead of supplemental materials--------[ ======================== end of revision =========================================================]----------------authors introduce ransom subspace training (random subspace neural nets) where for a fixed architecture, only a subset of the parameters is trained, and the update for all the parameters is derived via random projection which is fixed for the duration of the training. using this type of a network, authors introduce a notion of intrinsic dimension of optimization problems - it is minimal dimension of a subset, for which random subset neural net already reaches best (or comparable) performance.--------authors mention that this can be used for compressing networks - one would need to store the seed for the random matrix and the # of params equal to the intrinsic dimension of the net. --------they then demonstrate that the intrinsic dimension for the same problem stays the same when different architectures are chosen. finally they mention neural nets with comparable number of params to intrinsic dimension but that dont use random subspace trick dont achieve comparable performance. this does not always hold for cnns--------model with smaller intrinsic dimension is suggested to be better . they also suggest that intrinsic dimension might be a good approximation to minimum description length metric----------------my main concern is computational efficiency. they state that if used for compressing, their method is different from post-train compression, and the authors state that they train once end-to-end. it is indeed the case that once they found model that performs well, it is easy to compress, however they do train a number of models (up to a number of intrinsic dimension) until they get to this admissible model, which i envision would be computationally very expensive.----------------questions:--------- are covets always better on mnist: didnt understand when authors said that intrinsic dimension of fc on shuffled data stayed the same (why) and then say that it becomes 190k - which one is correct?--------- mnist - state the input dimension size, not clear how you got to that number of parameters overall","the authors make an empirical study of the ""dimension"" of a neural net optimization problem, where the ""dimension"" is defined by the minimal random linear parameter subspace dimension where a (near) solution to the problem is likely to be found. i agree with reviewers that in light of the authors' revisions, the results are interesting enough to be presented at the conference.","in their experiments (fc networks of varying depths and layer widths for the mnist dataset), between projected and original solutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed.","taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace.","taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace.",this paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem.,"taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace.","taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace.","how often does a random projection lead to a high-quality solution, and how often does it not?----------------* the authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region (which in general cannot be guaranteed).","taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace.",0.18,0.0,0.12,0.12,0.3636363636363636,0.0504201680672268,0.1983471074380165,0.1983471074380165,0.3636363636363636,0.0504201680672268,0.1983471074380165,0.1983471074380165,0.2077922077922078,0.1066666666666666,0.2077922077922078,0.2077922077922078,0.3636363636363636,0.0504201680672268,0.1983471074380165,0.1983471074380165,0.3636363636363636,0.0504201680672268,0.1983471074380165,0.1983471074380165,0.3418803418803419,0.017391304347826,0.188034188034188,0.188034188034188,0.3636363636363636,0.0504201680672268,0.1983471074380165,0.1983471074380165,11.530590057373049,11.530590057373049,12.388309478759766,11.530590057373049,6.968923568725586,11.530590057373049,11.530590057373049,7.737693786621094,0.9328728595298472,0.9407111936683924,0.8935642979362671,0.9614975475625536,0.9651900095328327,0.5523125167160421,0.9614975475625536,0.9651900095328327,0.5523129072798528,0.9588316212095627,0.9659906437356238,0.9483148736087917,0.9614975475625536,0.9651900095328327,0.5523125167160421,0.9614975475625536,0.9651900095328327,0.5523129072798528,0.9630454714070548,0.9651735686666657,0.908371433276224,0.9614975475625536,0.9651900095328327,0.5523125167160421
220,https://openreview.net/forum?id=ryxF80NYwS,"in this paper, the authors consider the neural amortized inference for clustering processes, in which the number of cluster can be automatically adapted based on the observed samples. the proposed algorithm largely follows the standard variational auto-encoder. the major contribution of the paper is the design of the posterior parametrization so that the posterior satisfies the permutation invariant within a cluster, between clusters, and unassigned data, based on the deepset method. the model can be incorporated into random communities models. finally, the authors apply the algorithm for neural spike sorting problem. ---------------the paper is well-organized and easy to follow. however, there are two issues should be addressed:----------1, the novelty of the proposed algorithm might not enough. the two major components in this paper, i.e., vae and deepset, are all carefully investigated before. this paper applies the deepset parameterization in the vae framework.----------2, the details of the amortized inference training is not clearly explained. it is well-known that gradient through the discrete random variable is quite difficult. how the gradient for the parameter of the proposed model is calculated should be carefully discussed. the reinforce gradient in this model, whose support of c can be as large as the number of samples, can be quite huge. ----------3, in the empirical evaluation, i was curious why the mean-field and mcmc have not been considered in the spike sorting problem.----------i am expecting the authors can address my concerns during rebuttal. ----------=======================================================================----------thanks for the responses to clarify my concerns. ----------the learning procedure and experiments are clear now. indeed, as a purely variational inference paper, the discrete variable problem is absent, since the model is always *fixed* and not updated. ----------the major contribution of the paper becomes the design of the posterior parametrization by deepsets, which i think still not enough. ----------i will keep my score. the paper presents a neural network based clustering process where the number of clusters is not known a-priori. proposed approach requires conjecturing a generative process (where number of clusters/classes is a random variable) and the model learns to uncover the posterior distribution over clusters given the observed samples.-----overall i think it is a valuable contribution, well written paper with good results. -----specific comments:-----a) even though the model allows for variable number of clusters, i feel there may be a strong dependence between the number of clusters the model can hypothesize and the number of clusters in the training data. it will be useful to give further insights into this. for instance, if an mnist model is trained with only digits 0-5 training data, how well would it perform in detecting all 10 clusters at test time? understanding models biases based on training data is one area i feel is important and the paper could add to.-----b) the neural clustering process could potentially be viewed as a transductive inference model for classification of test data. typically at test time classification is done for each test sample independently, and the clustering process allows one to bring in other similar test samples to help with classification. have the authors considered this and have any comments on potential value / feasibility of this?-----c) in the examples presented in section 2.3, please clarify how training & testing was done. specifically what training data was used (all of mnist training data?), and the test time clustering was done on a subset of mnist test data?-----d) use of q for a neural-network and q_\theta for posterior distribution is a little confusing, will be better to have different notation for these. summary:-----this paper introduces a novel deep learning architecture for efficient amortized bayesian inference over mixture models. unlike previous approaches to amortized clustering, the proposed method allows us to treat local discrete labels of data points and infer the unbounded number of mixture components, making it more flexible as in the case of bayesian nonparametrics. it is shown that the resulting algorithm can be parallelized and applied to both conjugate and non-conjugate models. the authors also suggest an extension to models of random communities and a novel approach to neural spike sorting for high-density multielectrode arrays based on the proposed method.----------strengths:-----the paper is generally well written and the relationship to previous works is well described. empirical results seem quite convincing, for example, the clustering results presented in fig. 2 and fig. 3 clearly show not only the inferred number of clusters, but also the posterior probability which indicates that reasonable samples are assigned higher probability.----------weaknesses:------ overall, the idea looks very original and promising, but i find some technical details are not easy to understand under the current form, especially for non-experts in this domain. i would recommend the authors to elaborate a bit more on the proposed architecture and the variable-input soft-max function in sect. 2.1.------ on page 8, the authors mention that the ncp is much more efficient compared with mcmc, for example, in the gaussian 2d example. however, regarding the dpmm clustering model, it is known that mcmc methods are generally slower compared with variational inference, which is computationally faster. i think it would be interesting to add a discussion or comparison with variational inference in terms of computational efficiency.------ if i understand correctly, the ncp is essentially based on a sequential sampling procedure. the authors claim that the proposed method is easily parallelized using a gpu, but there does not seem to be sufficient details on the gpu-parallelization of sequential sampling.----------minor comments:-----the size of some figures appears too small, for example fig. 6 and fig. 10, which may hinder readability.","this paper uses neural amortized inference for clustering processes to automatically tune the number of clusters based on the observed data. the main contribution of the paper is the design of the posterior parametrization based on the deepset method. the reviewers feel that the paper has limited novelty since it mainly follows from existing methodologies. also, experiments are limited and not all comparisons are made.","3 clearly show not only the inferred number of clusters, but also the posterior probability which indicates that reasonable samples are assigned higher probability.----------weaknesses:------ overall, the idea looks very original and promising, but i find some technical details are not easy to understand under the current form, especially for non-experts in this domain.","-----specific comments:-----a) even though the model allows for variable number of clusters, i feel there may be a strong dependence between the number of clusters the model can hypothesize and the number of clusters in the training data.","in this paper, the authors consider the neural amortized inference for clustering processes, in which the number of cluster can be automatically adapted based on the observed samples.","in this paper, the authors consider the neural amortized inference for clustering processes, in which the number of cluster can be automatically adapted based on the observed samples.",summary:-----this paper introduces a novel deep learning architecture for efficient amortized bayesian inference over mixture models.,understanding models biases based on training data is one area i feel is important and the paper could add to.-----b) the neural clustering process could potentially be viewed as a transductive inference model for classification of test data.,"----------the major contribution of the paper becomes the design of the posterior parametrization by deepsets, which i think still not enough.","-----specific comments:-----a) even though the model allows for variable number of clusters, i feel there may be a strong dependence between the number of clusters the model can hypothesize and the number of clusters in the training data.",0.3,0.0508474576271186,0.1666666666666666,0.1666666666666666,0.2884615384615385,0.0588235294117647,0.1923076923076923,0.1923076923076923,0.4086021505376344,0.2637362637362637,0.3440860215053763,0.3440860215053763,0.4086021505376344,0.2637362637362637,0.3440860215053763,0.3440860215053763,0.1219512195121951,0.025,0.0975609756097561,0.0975609756097561,0.3076923076923077,0.0588235294117647,0.1346153846153846,0.1346153846153846,0.3023255813953489,0.1904761904761904,0.3023255813953489,0.3023255813953489,0.2884615384615385,0.0588235294117647,0.1923076923076923,0.1923076923076923,7.813612937927246,9.930581092834473,12.796916961669922,7.555329322814941,7.943521022796631,12.796916961669922,7.555330753326416,8.50680923461914,0.05765990732892569,0.14253288917333937,0.7794265639809657,0.9741179835807439,0.9666996075400056,0.9547505316308778,0.9664015307975258,0.967238404983228,0.9455812039729683,0.9664015307975258,0.967238404983228,0.9455812145407525,0.9832157693437397,0.9566507152321668,0.9504140648187268,0.9622177874886683,0.9634549443994879,0.6138634712486531,0.9772808123558854,0.9625112354609097,0.8525935255293418,0.9741179835807439,0.9666996075400056,0.9547505316308778
221,https://openreview.net/forum?id=ryxIZR4tvS,"the paper proposes models for link prediction task in a generalized knowledge graph setting that can contain n-ary-----relationships. the paper explains the problem of using only binary relationships to model real-world scenarios, typical workaround for converting n-ary relationships to binary relationships and their-----problems. the work then extends one model for link prediction in a binary setting to n-ary setting. work also proposes a novel model using positional convolutional filters for entity embeddings to model n-ary relationships and use that for link prediction. work also establishes baselines for the new problem and publishes two-----datasets (subsets of freebase) for the same. experimental results show the proposed models, outperform the baselines by good margin (with some exceptions). ----------this is a very obvious generalization from binary to n-array relations, but the work is very incremental and does not provide any good motivations. it is well known that hypergraphs can be approximated with graphs using clique and star expansions but i do not see any discussion regarding this. frankly, i do not see any good motivation to consider this generalization. ---------- comments: ----- 1. table 3: for arity 2, r-simply is performing better, but hsimplie is marked best. ideally, for arity 2 both should-----have the same result right?----- 2. for arity 6, there is a sudden drop in performance for both the proposed models. but the baseline model mtransh holds very tight. any reason or explanation for this massive drop and large gap in performance.----- 3. have you conducted experiments for higher arity? on the continuation of the above point, the performance drop consistently for higher arity than 5? '----- this paper proposes two new embedding strategies for the task of knowledge graph completion, with special attention on generalizations that support hypergraphs. the first method, hsimple, learns an embedding for entities that directly contains multiple positional representations; these are shifted depending on the relation they're used in. the second method, hype, disentangles entity embeddings and positional convolutional filters, allowing stronger positional generalization. experiments on standard benchmarks demonstrate that the approaches work well.----------i think this paper should be accepted. while the ideas are so simple that they border on being trivial generalizations of previous work, the paper is well written, and the results seem solid. i think this is work that needs to be done, so i favor accepting it.----------on the positive side:----------* the ideas underlying hsimple and hype are natural and clear - we basically want to learn better embeddings for hypergraphs, and there are a couple of obvious ways to do that. both of these seem clear.----------* the experiments are nicely done, and show a consistent (if unsurprising) benefit to the approach.----------* the paper is well written and well situated in the literature.----------* i expect that other researchers in this area will be able to reproduce and build upon this work without any difficulty.----------on the negative side:----------* the ideas are obvious; the results are unsurprising; the paper lacks ""deep insight"". it is a contribution in the sense that someone needed to do this work, and i'm glad that it's been done (and done well), but it's not earth-shattering.----------* it doesn't seem like this is quite the best version of this idea. i really like the idea of hype, but it seems strange that entity embeddings are modulated based on position *only*, without regard to relation -- that is, it seems like a given entity in position #2 might need to be represented in very different ways depending on which relation is being used. contributions:-----1. this paper extends simple, a previous embedding model, from modeling binary knowledge graphs to knowledge hypergraphs, where n-ary relations may show up.-----2. the paper designs two different architectures, hype and hsimple, to achieve knowledge hypergraph embedding.-----3. empirical results comparing with existing methods are proposed.----------the major contribution of this paper is to propose two new architectures for hypergraph embedding. however, i still have some concerns after reading this paper.----------1. the paper proposes to use 1d convolutions to separate entity positions and entity embeddings (hype) and claim this strategy is better than just rotating the entity embeddings (hsimple) because one can introduce additional parameters through the convolution filters. i'm expecting to see more evidence justifying this strategy. for example, in table 2, simple is better than hype in most cases. so probably hype is not the most efficient way to use parameters. besides, i'm curious about how performance will be when you use different n,l,d,s, since some of them are set to be very small according to 6.2.----------2. the second thing that i feel confused about is the fair comparison with other methods to handle hypergraphs. for example, t-simple performs badly on all metrics, including hit@t and mrr in table 1. this could result from a sub-optimal adaptation/reification of simple to hypergraphs. however, in table 3 it does much better than the rest methods for binary relations (0.478 vs others). this makes me confused and i don't understand why that could happen. moreover, have you tried another adaptation method? for example, by converting non-binary relations into cliques?----------i think the paper makes some contribution to the existing literature, but it should at least clarify its contribution with more evidence and better comparison criterions.","the paper proposes two methods for link prediction in knowledge hypergraphs. the first method concatenates the embedding of all entities and relations in a hyperedge. the second method combines an entity embedding, a relation embedding, and a weighted convolution of positions. the authors demonstrate on two datasets (derived by the authors from freebase), that the proposed methods work well compared to baselines. the paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions.----------the authors should be commended for providing the source code for reproducibility. one of the reviewers (who was unfortunately also the most negative), was time pressed. unfortunately, the discussion period was not used by the reviewers to respond to the authors' rebuttal of their concerns.----------even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to iclr, it unfortunately falls below the acceptance threshold in its current form.","i think this is work that needs to be done, so i favor accepting it.----------on the positive side:----------* the ideas underlying hsimple and hype are natural and clear - we basically want to learn better embeddings for hypergraphs, and there are a couple of obvious ways to do that.","contributions:-----1. this paper extends simple, a previous embedding model, from modeling binary knowledge graphs to knowledge hypergraphs, where n-ary relations may show up.-----2.","contributions:-----1. this paper extends simple, a previous embedding model, from modeling binary knowledge graphs to knowledge hypergraphs, where n-ary relations may show up.-----2.",the paper proposes models for link prediction task in a generalized knowledge graph setting that can contain n-ary-----relationships.,"contributions:-----1. this paper extends simple, a previous embedding model, from modeling binary knowledge graphs to knowledge hypergraphs, where n-ary relations may show up.-----2.",work also proposes a novel model using positional convolutional filters for entity embeddings to model n-ary relationships and use that for link prediction.,"besides, i'm curious about how performance will be when you use different n,l,d,s, since some of them are set to be very small according to 6.2.----------2.","contributions:-----1. this paper extends simple, a previous embedding model, from modeling binary knowledge graphs to knowledge hypergraphs, where n-ary relations may show up.-----2.",0.2037037037037037,0.0093457943925233,0.0833333333333333,0.0833333333333333,0.1139896373056994,0.031413612565445,0.0725388601036269,0.0725388601036269,0.1139896373056994,0.031413612565445,0.0725388601036269,0.0725388601036269,0.1283422459893048,0.0648648648648648,0.1176470588235294,0.1176470588235294,0.1139896373056994,0.031413612565445,0.0725388601036269,0.0725388601036269,0.1675392670157068,0.0317460317460317,0.0732984293193717,0.0732984293193717,0.050251256281407,0.0,0.0402010050251256,0.0402010050251256,0.1139896373056994,0.031413612565445,0.0725388601036269,0.0725388601036269,13.685221672058104,10.958898544311523,15.68739414215088,10.958900451660156,4.70554780960083,10.958900451660156,10.958898544311523,2.72411584854126,0.971470111223873,0.9693975294769963,0.9446824837675889,0.9726325637217735,0.9753225451682743,0.09430898780546401,0.9726325637217735,0.9753225451682743,0.09430882923376309,0.9787918309752811,0.9788391676155631,0.902915292787342,0.9726325637217735,0.9753225451682743,0.09430898780546401,0.9679665350999197,0.9693541388182766,0.2050985207719693,0.12782440204207807,0.17007935454309375,0.9042561974527226,0.9726325637217735,0.9753225451682743,0.09430898780546401
222,https://openreview.net/forum?id=ryxMW6EtPB,"this paper proposed to use the duality gap sup_f v(f, g*)  inf_g v(f*, g) as a metric for gan training. it proves that this metric is an upper bound of f-distance. it also proves a generalization bound for this metric. simulation resultson mnist, cifar10, etc. are reported.---------- the contribution of this paper is incremental due to the following reasons.---------- 1) the duality gap is only an upper bound of the f-distance. this means that if the duality gap is zero then the learned distribution is the true distribution. however, the converse is not necessarily true: even if the algorithm starts with the true distribution, the duality gap may not be zero. thus the metric is not a proper metric.----- the proof of the upper bound is straightforward.---------- 2) another issue is the gap between the min-max formulation and the real training algorithm. as for gan, due to the inexact update, it is not really solving the min-max problem. for the proposed metric, it is also impossible to solve sup_f v(f, g*) and inf_g v(f*, g) to reasonable accuracy. thus what the algorithm is really doing, perhaps, is to optimizing a new loss which is the sum of the original loss and and an extra term. viewing it as a duality gap seems to be far from the practical training. this discrepancy exists for gans, but it is a bigger issue for the duality gap interpretation. ---------- 3) the simulation is not convincing. the reported fid for cifar10 using wgan-gp is 54.4, which seems to be a bit high. im not sure whether it is due to parameter choice or due to weak d/g networks used in the simulation. if the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like dcgan. or at least report the parameter tuning effort made for getting the results. i vote to reject the paper at this stage, mainly because of the following three points:----------1) the motivation is unclear and overall structure of the paper is confusing. it should be better motivated why one should use the duality gap as an upper bound for the ""f-distance"". minimizing the f-distance as is usually done seems like the more direct and simple approach. since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial. ----------2) the presentation is not professional, hard to follow and the submission overall looks very rushed:------ in equations, please use \inf, \sup, and \text{...} for text such as distance, data, ... ------ i have trouble understanding the overall idea behind algorithm 1 and eq. (22). what is the definition of f^* and g^* in eq. (22)? some explanatory text would be valuable.------ the set f in definition 3.5 looks odd, as it appears to be recursive and might not be unique. ------ the writing looks very rushed, and should be improved. for example, i have trouble understanding the sentence ""so the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction. ------ the aspect ratio in fig. 5 should be fixed.----------3) the experiments are completely preliminary and not reasonable:------ the wgan-gp baseline is very weak, i.e. does not show any reasonable generated images (fig. 9). there are countless open pytorch implementations on github which out-of-the-box produce much better results. ------ the shown inception scores are far from state-of-the-art. it is unclear, why one should use the proposed duality gap gan.","this paper proposes looking at the duality gap to measure performance. however, the metric is just an upperbound on the true metric of interest, and therefore its value can be ambiguous. ----------the reviewers found the paper to be in an unacceptable form and was clearly hastily prepared. they were also skeptical about the novelty of the result as well as the comprehensiveness of the experiments.----------this paper would require extensive revisions before any potential acceptance. reject","----------2) the presentation is not professional, hard to follow and the submission overall looks very rushed:------ in equations, please use \inf, \sup, and \text{...} for text such as distance, data, ... ------ i have trouble understanding the overall idea behind algorithm 1 and eq.",are reported.---------- the contribution of this paper is incremental due to the following reasons.---------- 1) the duality gap is only an upper bound of the f-distance.,are reported.---------- the contribution of this paper is incremental due to the following reasons.---------- 1) the duality gap is only an upper bound of the f-distance.,"this paper proposed to use the duality gap sup_f v(f, g*)  inf_g v(f*, g) as a metric for gan training.","it is unclear, why one should use the proposed duality gap gan.",are reported.---------- the contribution of this paper is incremental due to the following reasons.---------- 1) the duality gap is only an upper bound of the f-distance.,"as for gan, due to the inexact update, it is not really solving the min-max problem.",are reported.---------- the contribution of this paper is incremental due to the following reasons.---------- 1) the duality gap is only an upper bound of the f-distance.,0.1694915254237288,0.0,0.135593220338983,0.135593220338983,0.2524271844660194,0.0792079207920792,0.174757281553398,0.174757281553398,0.2524271844660194,0.0792079207920792,0.174757281553398,0.174757281553398,0.18,0.0816326530612244,0.1399999999999999,0.1399999999999999,0.1136363636363636,0.0232558139534883,0.0681818181818181,0.0681818181818181,0.2524271844660194,0.0792079207920792,0.174757281553398,0.174757281553398,0.1075268817204301,0.0,0.086021505376344,0.086021505376344,0.2524271844660194,0.0792079207920792,0.174757281553398,0.174757281553398,12.063201904296877,14.988506317138672,16.4383602142334,12.06319808959961,7.259979724884033,12.06319808959961,12.063202857971191,7.282916069030762,0.9744643233092654,0.9730551229938469,0.7066793504042039,0.6395060767341583,0.8324726845546726,0.01746507784162572,0.6395060767341583,0.8324726845546726,0.01746511078384803,0.9643169227149715,0.9678058000357518,0.8788671240151008,0.9710901443811271,0.966222028400282,0.008787078746114457,0.6395060767341583,0.8324726845546726,0.01746511078384803,0.9363856853902224,0.9437233927286294,0.8721494540422001,0.6395060767341583,0.8324723741230864,0.01746508699224325
223,https://openreview.net/forum?id=ryxyHnR5tX,"clarity: paper is generally well written; however, certain theoretical statements (e.g. theorem 1) are not very precise.----------------originality: contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.----------------significance: unclear whether the techniques significantly advance the state of the art.----------------quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.----------------------------summary: --------the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, cosamp and iht, but presumably other similar methods can also be used here). the key idea is that if the measurement matrix is normalized, then the k-sparse thresholding of the gradient update can be viewed as solving a k-nearest neighbor problem. therefore, one can presumably use fast k-nn methods instead of exact nn methods. specifically the authors propose to use the prioritized dci method of li and malik.----------------pros: --------reasonable idea to use fast (sublinear) nn techniques in the k-sparse projection step.----------------cons: --------* it appears that the running time improvement over the baseline iht (which has otilde(mn) complexity) heavily depends on the intrinsic dimensionality of a. however, the authors do not characterize this.--------* the authors neglect to mention in the paper that prioritized dci has a pre-processing time of o(mn), so the final algorithm isn't really asymptotically faster.--------* i cannot parse theorem 1 (especially, the second sentence). is epsilon the failure probability of dci?--------* experimental results are far too synthetic. in real-life problems k itself is big, so there may be other bottlenecks (least squares, gradient updates, etc) and not necessarily the hard thresholding step. the paper is very well-written, readable, with the ideas and derivations clearly explained. ----------------the literature review is comprehensive and informative. i do feel however that the review could be improved, for example, by discussing the recent papers by chinmay hegde and piotr indyk on ""head"" and ""tail"" approximate projections to speed up recovery algorithms. the problem under study is indeed important and the contribution is interesting. ----------------my biggest concern is that the technical contribution is too modest. theorem 1 serves more as a decorative technical result (the assumption ""and for any vector v..."" seems out of the blue and too convenient) and the paper does not answer the many questions that come to mind here. for example, what is the intrinsic dimension of common random measurement matrices? or how do any wrongly detected nearest neighbours propagate through the iterations of the algorithm? how does the measurement noise change the intrinsic dimension? we should intuitively lose stability in return for faster recovery. how would this be quantified in what you've proposed. the paper proposes a greedy-like algorithm for sparse recovery that uses nearest neighbors algorithms to efficiently identify candidates for the support estimates obtained at each iteration of a greedy algorithm. it assumes that the norms of the columns of the matrix a are one to be able to change the project-and-sort step into a nearest neighbors search.----------------it is not clear what the value of fact 1 is, given that none of the sparse recovery algorithms discussed here actually performs ell0 norm minimization. additionally, it is common in theoretical analysis of sparse recovery to assume that the columns of the matrix a have unit norm. in fact, the rip implies that the columns of the matrix must have norm within delta of 1. nonetheless, it would be useful to have a discussion of the effect that having non-unit column norms would have on the proposed approach.----------------similarly, fact 2 is almost self-evident; i suggest to discard the proof.----------------the equivalence of definition 1 and the statement involving ps and qs needs to be shown more clearly. the statement in definition 1 is given in terms of distances (ball radiuses), not counts of neighbors.----------------i suggest swapping the use of cosamp and aiht - the theoretical results of the paper refer to aiht, so it is not clear why the algorithm itself is relegated to the supplementary material.----------------it is not clear how d0 is to be computed to implement accelerated aiht.----------------for theorem 1, the authors should comment on when the assumption ""xtilde(t) converges linearly to a k-sparse signal with rate c"".----------------in figures 1 and 2, does ""residual"" refer to the difference between x and xtilde, or b and axtilde? ----------------minor comments:--------typo in page 5 """"--------grammar error in page 6 ""characterizing of the difficulty"".","the main idea of this paper is to use nearest neighbor search to to accelerate iterative thresholding based sparse recovery algorithms. all reviewers were underwhelmed by somewhat straightforward combination of existing results in sparse recovery and nearest-neighbor search. while the proposed method seems effective in practice, the paper has the feel of not being a fully publishable unit yet. several technical questions were asked but no author feedback was provided to potentially lift this paper up.","i do feel however that the review could be improved, for example, by discussing the recent papers by chinmay hegde and piotr indyk on ""head"" and ""tail"" approximate projections to speed up recovery algorithms.","clarity: paper is generally well written; however, certain theoretical statements (e.g. theorem 1) are not very precise.----------------originality: contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.----------------significance: unclear whether the techniques significantly advance the state of the art.----------------quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.----------------------------summary: --------the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, cosamp and iht, but presumably other similar methods can also be used here).","it assumes that the norms of the columns of the matrix a are one to be able to change the project-and-sort step into a nearest neighbors search.----------------it is not clear what the value of fact 1 is, given that none of the sparse recovery algorithms discussed here actually performs ell0 norm minimization.","clarity: paper is generally well written; however, certain theoretical statements (e.g. theorem 1) are not very precise.----------------originality: contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.----------------significance: unclear whether the techniques significantly advance the state of the art.----------------quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.----------------------------summary: --------the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, cosamp and iht, but presumably other similar methods can also be used here).","clarity: paper is generally well written; however, certain theoretical statements (e.g. theorem 1) are not very precise.----------------originality: contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.----------------significance: unclear whether the techniques significantly advance the state of the art.----------------quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.----------------------------summary: --------the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, cosamp and iht, but presumably other similar methods can also be used here).","clarity: paper is generally well written; however, certain theoretical statements (e.g. theorem 1) are not very precise.----------------originality: contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.----------------significance: unclear whether the techniques significantly advance the state of the art.----------------quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.----------------------------summary: --------the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, cosamp and iht, but presumably other similar methods can also be used here).",the problem under study is indeed important and the contribution is interesting.,"clarity: paper is generally well written; however, certain theoretical statements (e.g. theorem 1) are not very precise.----------------originality: contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.----------------significance: unclear whether the techniques significantly advance the state of the art.----------------quality: overall, i think this is a promising direction but the idea might not have fully fleshed out.----------------------------summary: --------the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, cosamp and iht, but presumably other similar methods can also be used here).",0.2162162162162162,0.018348623853211,0.1261261261261261,0.1261261261261261,0.4043715846994535,0.1767955801104972,0.2295081967213114,0.2295081967213114,0.303030303030303,0.0615384615384615,0.1515151515151515,0.1515151515151515,0.4043715846994535,0.1767955801104972,0.2295081967213114,0.2295081967213114,0.4043715846994535,0.1767955801104972,0.2295081967213114,0.2295081967213114,0.4043715846994535,0.1767955801104972,0.2295081967213114,0.2295081967213114,0.0898876404494382,0.0,0.0898876404494382,0.0898876404494382,0.4043715846994535,0.1767955801104972,0.2295081967213114,0.2295081967213114,16.996807098388672,16.996807098388672,16.996807098388672,16.996807098388672,7.530219078063965,6.368307590484619,16.996807098388672,8.913859367370605,0.909754417741925,0.9222769354069275,0.927129239657916,0.986527643254412,0.9586206949861364,0.027771941999844355,0.9448768111375013,0.9500892054751474,0.01719596541775471,0.986527643254412,0.958620661455702,0.02777187035404028,0.986527643254412,0.95862068446253,0.027771982793813155,0.986527643254412,0.9586207074707562,0.027771944225365566,0.9458115453126469,0.9542113002604852,0.8939230297961513,0.986527643254412,0.9586206949861364,0.027771900240077717
224,https://openreview.net/forum?id=uSYfytRBh-f,"summary: in this work, the authors seek to leverage external sources of data to improve the generalization of segmentation models. in particular, they seek to identify images which generate discordance among models, hypothesizing that they would be well-suited to improve model performance. once selected, they leverage human annotators to first filter this image set and then segment the images, which are then used to retrain the model. they demonstrate improved performance relative to a batch of competing models which are not updated using this procedure.-----recommendation: given the lack of a competitive baseline, opaqueness around the impact of the algorithm's hyperparameters, and what's likely to be noisy estimates of improvement, i cannot recommend this paper for acceptance as is. see below for greater detail.-----positives:-----the authors recognize that not all images are created equal when looking to improve a model and attempt to tackle this challenging problem. this is particularly relevant in high-stakes environments when failure in rare cases can have a disproportionate impact (e.g., autonomous vehicles, healthcare, etc.).-----the authors identify that scaling human annotation, in particular for image segmentation, can be cost prohibitive and propose a method to optimize this process. if successful, such a method could have significant implications for industries where the cost of annotation is high (e.g., healthcare where highly paid experts are required).-----concerns:-----the authors propose a fairly complex (and costly) pipeline for improving generalization to the unseen dataset. however, they fail to compare against even a simple baseline such as random selection of images from this dataset. comparing against models which are not updated, in particular when it's clear that none of them generalize, is a weak baseline that could likely be outperformed by far simpler uses of the external data.-----t^(i+1) = 30 is quite small, especially given the number of classes. while it is understandable that such a sample cannot be extraordinarily large, by leveraging such a small value there's significant noise in the evaluation criteria. given the previous concern, this likely would not affect the reported results in the paper. however, with a more competitive baseline, such noise may make it challenging to identify improvements in the selection of image for finetuning.-----there are a few magic constants throughout the paper. while conducting an ablation study may be cost-prohibitive given the sequential dependencies of the algorithm, it would be helpful to the reader to provide some means of estimating the impact of and sensitivity to these parameters.-----nits:-----the comment on maximum test set size on page 1 is a bit strong. test set sizes are limited by financial incentives. for potentially lucrative endeavors, it would not be surprising to find a test set larger than 10k images.-----there is a typo on page 2: ""not be[en] spotted beforehand""-----it would be nice to quantify the computational cost of constructing m in each iteration.-----consider moving the superscript 4 after the period to make it clear that it is a footnote and not exponentiation.-----there is a typo on page 4:(arc) -> (acr)-----the f in ""failure"" is erroneously capitalized in the first paragraph of section 3----you may wish to consider citing the field of computer-assisted annotation as relevant work. one such paper (among others) and open-source implementation include: ** efficient interactive annotation of segmentation datasets with polygon-rnn++ (acuna et al) ** https://developer.nvidia.com/blog/annotation-transfer-learning-clara-train/ this work used a variety of existing segmentation algorithms to discover most ""controversial"" samples from massive online unlabeled images. those representative controversial samples were believed to have the best chance to confuse the algorithm being trained and to expose its weakness. they are rated by annotators on a spectrum from bad to excellent, and segmentation masks are collected from human annotators for the worst images. several clever measures were taken to reduce human labor.-----the paper addressed an important and somewhat overlooked problem for segmentation and deep learning in the general. leveraging those counterexamples to improve the segmentation models' generalization performance on unseen images seems to be novel in this field. this looks like a special case of finding natural adversarial examples from unlabeled data. the proposed solution is intuitive and logical, with lots of practical considerations made for its feasibility. the manuscript is also very well written, and the literature review is especially comprehensive.-----this work extends from mad, iclr 2020 (https://openreview.net/forum?id=rjehnt4ypr); but it also presents with two nontrivial and interesting innovations: (1) generalizing it to a dense prediction task, which requires revising the human labelling strategy in subjective experiments. weakly-supervised labeling is more practical for segmentation; and (2) extending to active training/tuning, leveraging the selected hard examples to improve the segmentation model for multiple rounds. the tuned models were shown to improve their robustness remarkably on spotted catastrophic mistakes, while preserving their performance on canonical testing sets.-----this approach is also an instance of the basic active learning paradigm that iterates between spotting hard examples, labeling them, and tuning the model. the method of finding hard examples by competing with a human oracle seems to be novel though.-----i noticed in the appendix, the authors also compared their method with entropy-based active learning  a vanilla baseline needing no competing model. their model seems to achieve a good miou advantage, showing the new proposed way has better performance on open-world images than simple entropy-based fine-tuning. that is perhaps understandable, since the proposed new method also costs more human inspection efforts. however, id be interested to see the authors to compare the hard examples selected by their method and by entropy-based one: are there any distinct pattern or notable trend? is there anything that the entropy-based method can clear miss but your method can pick up?-----besides, this paper uses different deep networks trained as competing models. why wouldnt those models more tend to make similar mistakes, due to same training dataset and model type? what if using learning-based but non-deep segmentation models to compete? what if using segmentation models trained on other datasets to compete?","this paper studies how to efficiently expose failures of ""top-performing"" segmentation models in the real world and how to leverage such counterexamples to rectify the models. the key idea is to discover most ""controversial"" samples from massive online unlabeled images. the approach is sound, well grounded, and quite logical. results demonstrate the effectiveness.-----however, there exists some limitations coming from r2 and r3, for example, 1) segmentation benchmarks may not require pixel-level dense annotation. there are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans. 2) it is much harder for segmentation data to be class-balanced in the pixel level, making highly skewed class distributions common for this particular task. 3) citing the field of computer-assisted annotation as relevant work.-----in the end, i think that this paper may not be ready for publication at iclr, but the next version must be a strong paper if above limitations can be well addressed.","the manuscript is also very well written, and the literature review is especially comprehensive.-----this work extends from mad, iclr 2020 (https://openreview.net/forum?id=rjehnt4ypr); but it also presents with two nontrivial and interesting innovations: (1) generalizing it to a dense prediction task, which requires revising the human labelling strategy in subjective experiments.","the method of finding hard examples by competing with a human oracle seems to be novel though.-----i noticed in the appendix, the authors also compared their method with entropy-based active learning  a vanilla baseline needing no competing model.","the method of finding hard examples by competing with a human oracle seems to be novel though.-----i noticed in the appendix, the authors also compared their method with entropy-based active learning  a vanilla baseline needing no competing model.","summary: in this work, the authors seek to leverage external sources of data to improve the generalization of segmentation models.","weakly-supervised labeling is more practical for segmentation; and (2) extending to active training/tuning, leveraging the selected hard examples to improve the segmentation model for multiple rounds.","the method of finding hard examples by competing with a human oracle seems to be novel though.-----i noticed in the appendix, the authors also compared their method with entropy-based active learning  a vanilla baseline needing no competing model.","their model seems to achieve a good miou advantage, showing the new proposed way has better performance on open-world images than simple entropy-based fine-tuning.","once selected, they leverage human annotators to first filter this image set and then segment the images, which are then used to retrain the model.",0.2129629629629629,0.0,0.1203703703703703,0.1203703703703703,0.1393034825870646,0.0201005025125628,0.099502487562189,0.099502487562189,0.1393034825870646,0.0201005025125628,0.099502487562189,0.099502487562189,0.143646408839779,0.0335195530726257,0.0994475138121546,0.0994475138121546,0.1481481481481481,0.0213903743315508,0.0952380952380952,0.0952380952380952,0.1393034825870646,0.0201005025125628,0.099502487562189,0.099502487562189,0.0744680851063829,0.0,0.0425531914893617,0.0425531914893617,0.1397849462365591,0.0108695652173913,0.086021505376344,0.086021505376344,7.401414394378662,8.851738929748535,13.709890365600586,7.401414394378662,7.091116428375244,7.401414394378662,12.615640640258787,9.028038024902344,0.2932009674942393,0.36133245516622414,0.848999754930354,0.3896459602601558,0.47741315847056554,0.47591869462488917,0.3896459602601558,0.4774128760135873,0.47591830574510857,0.9025352951181413,0.8462981079223236,0.9342221279722448,0.12313918662011417,0.2812361042994445,0.023657042054567857,0.3896459602601558,0.4774128760135873,0.47591871024429944,0.04045254875497336,0.07281198702780481,0.5039081752497424,0.9496124726257148,0.9530842173857513,0.23897015723550058
225,https://openreview.net/forum?id=x9C7Nlwgydy,"this paper studies the effect of combining ensemble learning approaches with deep clustering. the paper wants to show that ensemble learning methods, in particular consensus clustering, can improve the clustering accuracy when combined with general representation learning/clustering blocks. however, i am not sure that the results presented in the paper are enough to support the claims.-----the paper's pros are: (+) it is the first to combine ensemble methods with deep clustering models. although ensemble methods have been widely applied, studying consensus clustering in the current problem setting is novel. (+) it is the first to be able to have an ensemble deep clustering algorithm that gains empirically over other state-of-the-art models, showing the ideas to be potentially effective. (+) the writing is in general clear and undestandable.-----the paper's cons are: (-) the wording in the abstract is a bit confusing in the sense that after reading it one might think the algorithm does consensus clustering first and uses the clustering to learn better representations of the input data. although this is clarified later in the main body. (-) the description of the main algorithm seems to be more intuitive than innovative. some algorimic design choices are not very convincing. for example, the choice of using random projections on embedding to produce different clusterings, although an interesting idea, makes me wonder why it is necessarily a good way to introduce randomness into the whole framework. the authors can expand their discussion on this. i also remain dubious about why different representation instead of different clustering methods also, since the authors meant the idea to be applicable to general representation learning/deep clustering blocks, i'm not sure the current experimental data in the paper can lead to that conclusion. (-) using the performance metrics provided by the authors, i find it a bit hard to conclude that the proposed algorithm has a significant advantage over state of the art methods, especially pica. also, the fluctuation in performance metrics caused by different parameter settings seems to be, in magnitude, at least comparable to the margin of concurl over the baselines.-----overall, i think this paper contains interesting seed ideas such as combining consensus clustering with representation learning and making use of the learned representation to generate multiple clusterings. these seed ideas could be good for this venue. however, the work is still premature and flawed by crude algorithm/experiment design. the quality can be significantly improved if the authors can give a more general algorithmic framework (since the authors meant the ideas to be applicable to general representation learning/clustering algorithms), equipped with more thorough experimental investigation to support the applicability and superiority of the current approach. the authors propose a learning-based approach for image clustering. in particular, similarly to recent algorithms fro unsupervised representation learning, such a deepcluster, they propose to iterate between clustering the images in the feature space of the network and updating the network weights to respect the clusters. two main differences with respect to deepcluster-like algorithms is that they target the task of clustering itself, and do not evaluate the generalizability of the leaned representations for other tasks, and that they propose to use cluster ensembles to improve training robustness. in particular, they generate 2 clusterings of the images at every iteration by applying different sets of data augmentations and feature transformations to the input images. the objective is then not only to respect these clusterings but also to enforce consistency between them over time, thus improving representation invariance to irrelevant image details. in an experimental evaluation on a set of standard image clustering benchmarks they outperform prior work in most scenarios.-----the idea is reasonable and the method seems sound, however a similar approach has been proposed in zhuang et al., iccv'19 (local aggregation for unsupervised learning of visual embeddings). in contrast to this work, the authors of zhuang et al., used different runs of the clustering algorithm to obtain diverse clusterings, instead of transforming the images, but the overall approach is very similar. the authors seem to be not aware of that work.-----in the current from it is not clear whether the proposed approach has any advantages over zhuang et al. in the rebuttal the authors need to provide a discussion of their novelty with respect to that method as well as an experimental comparison on imagenet.-----i appreciate the authors' efforts to fairly compare to zhuang et al. in the rebuttal, and i do find the preliminary evidence sufficient to establish that their approach outperforms la on clustering metrics. however, the authors seem to miss the the point that la is not just another consensus clustering approach which they forgot to include into literature review since it does not report clustering metrics. the main contribution of their work is combining consensus clustering with representation learning, which is exactly what the authors of la had done before. it does seems that the particular approach proposed in this paper results in a better clustering performance, so the submission contains a valid contribution, but the relationship between the two methods needs to be discussed in a lot more detail, and they have to be throughly compared experimentally. i encourage the authors to improve the manuscript in this direction and resubmit to a different venue.","this paper proposes a model for learning using ensemble clustering. the reviewers found the general idea promising. however, while promising, all reviewers noted that in its curent form the paper is not fit for publication. the reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work. because of all these reasons, this paper does not meet the bar of acceptance. i recommend the authors take into account the feedback provided in the reviews and discussion and resubmit to another venue.","in an experimental evaluation on a set of standard image clustering benchmarks they outperform prior work in most scenarios.-----the idea is reasonable and the method seems sound, however a similar approach has been proposed in zhuang et al., iccv'19 (local aggregation for unsupervised learning of visual embeddings).","the authors seem to be not aware of that work.-----in the current from it is not clear whether the proposed approach has any advantages over zhuang et al. in the rebuttal the authors need to provide a discussion of their novelty with respect to that method as well as an experimental comparison on imagenet.-----i appreciate the authors' efforts to fairly compare to zhuang et al. in the rebuttal, and i do find the preliminary evidence sufficient to establish that their approach outperforms la on clustering metrics.","the paper wants to show that ensemble learning methods, in particular consensus clustering, can improve the clustering accuracy when combined with general representation learning/clustering blocks.",this paper studies the effect of combining ensemble learning approaches with deep clustering.,"also, the fluctuation in performance metrics caused by different parameter settings seems to be, in magnitude, at least comparable to the margin of concurl over the baselines.-----overall, i think this paper contains interesting seed ideas such as combining consensus clustering with representation learning and making use of the learned representation to generate multiple clusterings.","the paper wants to show that ensemble learning methods, in particular consensus clustering, can improve the clustering accuracy when combined with general representation learning/clustering blocks.",the authors can expand their discussion on this.,"i also remain dubious about why different representation instead of different clustering methods also, since the authors meant the idea to be applicable to general representation learning/deep clustering blocks, i'm not sure the current experimental data in the paper can lead to that conclusion.",0.2384105960264901,0.0,0.1059602649006622,0.1059602649006622,0.3157894736842105,0.0425531914893617,0.2,0.2,0.171875,0.0158730158730158,0.078125,0.078125,0.1391304347826087,0.0176991150442477,0.0695652173913043,0.0695652173913043,0.267515923566879,0.0258064516129032,0.1401273885350318,0.1401273885350318,0.171875,0.0158730158730158,0.078125,0.078125,0.0727272727272727,0.0185185185185185,0.0545454545454545,0.0545454545454545,0.2297297297297297,0.0547945205479452,0.1351351351351351,0.1351351351351351,15.962124824523926,8.947391510009766,16.901582717895508,7.23566722869873,7.279719352722168,15.962124824523926,8.7628812789917,6.160569190979004,0.8167655292545366,0.9531266096278223,0.3686631155436881,0.0974571493907559,0.49301370054390165,0.2945298226622693,0.9716876844338709,0.9736570631057198,0.9275135549812822,0.9681070404849964,0.9670561906598657,0.9396133345079586,0.9243987810227162,0.9257081793407291,0.9162348219548063,0.9716876844338709,0.9736570631057198,0.9275135022863177,0.9158906531663806,0.9515395615996712,0.5429552365385415,0.9320167660196708,0.9121393852380338,0.1187533756409946
