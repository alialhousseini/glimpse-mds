id,text,gold
https://openreview.net/forum?id=ByYPLJA6W,"This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions. A well-written manuscript, though the introduction could have motivated the problem a little better (i.e. why would we want to do this). The novelty in the paper is implementing such a regression in a layered network. The paper shows how the densities at each nodes are computed (and normalised). Optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow. The paper uses three problems to illustrate the idea -- a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices.  --------My only two reservations of this paper is the illustration on the stock index data -- it seems to me, returns on individual constituent stocks of an index are used as samples of the return on the index itself.  But this cannot be true when the index is a weighted sum of the constituent assets.  Secondly, it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into 100 bins for numerical reasons -- does the smoothing that results from this give any advantage over a histogram of the returns in 100 bins?-------- ","The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method."
https://openreview.net/forum?id=ByYPLJA6W,"Summary:----------------This paper presents a new network architecture for learning a regression of probability distributions.----------------The distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes. The conditional probability function is an unnormalized distribution with the same form as the Boltzman distribution, and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bins. By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level.----------------Under these dynamics and discretization, the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribution. These gradients are in turn used to calculate updates for the network parameters with respect to the Jensen Shannon divergence between the predicted distribution and a target distribution.----------------The approach is evaluated on three tasks, two synthetic and one real world. The baselines are the state of the art triple basis estimator (3BE) or a standard MLP that represents the output distribution using a softmax over quantiles. On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance. On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines. However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing.----------------Notes to authors:----------------I'm not familiar with 3BE but the fact that it is used outside of its intended use case for the stock data is worrying. How does 3BE perform at predicting the FTSE distribution at time t + k from the FTSE distribution at time t only? Do the multiple input distributions actually help?----------------You use a kernel density estimate with a Gaussian kernel function to estimate the stock market pdf, but then you apply your network directly to this estimate. What would happen if you built more complex networks using the kernel values themselves as inputs?----------------Could you also run experiments on the real-world datasets used by the 3BE paper?----------------What is the structure of the DRN that uses > 10^3 parameters (from Fig. 4)? The width of the network is bounded by the two input distributions, so is this network just incredibly deep? Also, is it reasonable to assume that both the DRN and MLP are overfitting the toy task when they have access to an order of magnitude more parameters than datapoints.----------------It would be nice if section 2.4 was expanded to actually define the cost gradients for the network parameters, either in line or in an appendix.","The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method."
https://openreview.net/forum?id=ByYPLJA6W,"The paper considers distribution to distribution regression with MLPs.  The authors use an energy function based approach.  They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters.----------------This seems to be a nice treatment of distribution to distribution regression with neural networks. The approach is methodological similar to using expected likelihood kernels.  While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters.  That’s what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation.  In the discussion, it says --------“For future work, a possible study is to investigate what classes of problems DRN can solve.”  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve.  Its practical utility is questionable.  It’s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.  In the introduction, it would also improve the paper to outline clear points of methodological novelty.  ","The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method."
https://openreview.net/forum?id=HkOhuyA6-,"The paper presents a novel representation of graphs as multi-channel image-like structures. These structures are extrapolated  by --------1) mapping the graph nodes into an embedding using an algorithm like node2vec--------2) compressing the embedding space using pca--------3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.--------he resulting multi-channel image-like structures are then feed into vanilla 2D CNN.--------  --------The papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures. Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures. The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.","The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns."
https://openreview.net/forum?id=HkOhuyA6-,"The paper introduces a method for learning graph representations (i.e., vector representations for graphs). An existing node embedding method is used to learn vector representations for the nodes. The node embeddings are then projected into a 2-dimensional space by PCA. The 2-dimensional space is binned using an imposed grid structure. The value for a bin is the (normalized) number of nodes falling into the corresponding region. ----------------The idea is simple and easily explained in a few minutes. That is an advantage. Also, the experimental results look quite promising. It seems that the methods outperforms existing methods for learning graph representations. ----------------The problem with the approach is that it is very ad-hoc. There are several (existing) ideas of how to combine node representations into a representation for the entire graph. For instance, averaging the node embeddings is something that has shown promising results in previous work. Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing, it is especially important to compare your method more thoroughly to simpler methods. Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives. ----------------The experimental results are also not explained thoroughly enough. For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps. How many times did you run node2vec on each graph? ","The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns."
https://openreview.net/forum?id=HkOhuyA6-,"The authors propose to use 2D CNNs for graph classification by transforming graphs to an image-like representation from its node embedding. The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms. Essentially the authors propose an approach to use a node embedding to achieve graph classification.----------------In my opinion there are several weak points:----------------1) The approach to obtain the image-like representation is not well motivated. Other approaches how to  aggregate the set of node embeddings for graph classification are known, see, e.g., ""Representation Learning on Graphs: Methods and Applications"", William L. Hamilton, Rex Ying, Jure Leskovec, 2017. The authors should compare to such methods as a baseline.----------------2) The experimental evaluation is not convincing:--------- the selection of competing methods is not sufficient. I would like to suggest to add an approach similar to Duvenaud et al., ""Convolutional networks on graphs for learning molecular fingerprints"", NIPS 2015.--------- the accuracy results are taken from other publications and it is not clear that this is an authoritative comparison; the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method, cf., e.g., Kriege et al., ""On Valid Optimal Assignment Kernels and Applications to Graph Classification"", NIPS 2016.--------- it would be interesting to apply the approach to graphs with discrete and continuous labels.----------------3) The authors argue that their method is preferable to graph kernels in terms of time complexity. This argument is questionable. Most graph kernels compute explicit feature maps and can therefore be used with efficient linear SVMs (unfortunately most publications use a kernelized SVM). Moreover, the running of computing the node embedding must be emphasized: On page 2 the authors claim a ""constant time complexity at the instance level"", which is not true when also considering the running time of node2vec. Moreover, I do not think that node2vec is more efficient than, e.g., Weisfeiler-Lehman refinement used by graph kernels.----------------In summary: Since the technical contribution is limited, the approach needs to be justified by an authoritative experimental comparison. This is not yet achieved with the results presented in the submitted paper. Therefore, it should not be accepted in its current form.","The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns."
https://openreview.net/forum?id=ryjw_eAaZ,"Authors propose a deep architecture learning algorithm in an unsupervised fashion. By finding conditional in-dependencies in input as a Bayesian network and using a stochastic inverse mechanism that preserves the conditional dependencies, they suggest an optimal structure of fully connected hidden layers (depth, number of groups and connectivity). Their algorithm can be applied recursively, resulting in multiple layers of connectivity. The width of each layer (determined by number of neurons in each group) is still tuned as a hyper-parameter.----------------Pros:--------- Sound derivation for the method.--------- Unsupervised and fast algorithm. --------Cons:--------- Poor writing, close to a first draft. --------- Vague claims of the gain in replacing FC with these structures, lack of comparison with methods targeting that claim.-------- - If the boldest claim is to have a smaller network, compare results with other compression methods.-------- - If it is the gain in accuracy compare with other learn to learn methods and show that you achieve same or higher accuracy. The NAS algorithm achieves 3.65% test error. With a smaller network than the proposed learned structure (4.2M vs 6M) here they achieve slightly worse (5.5% vs 4.58%) but with a slightly larger (7.1M vs 6M) they achieve slightly better results (4.47% vs 4.58%). The winner will not be clear unless the experiments fixes one of variables or wins at both of them simultaneously.----------------Detailed comments:----------------- Results in Table 4 mainly shows that replacing fully connected layer with the learned structures leads to a much sparser connectivity (smaller number of parameters) without any loss of accuracy. Fewer number of parameters usually is appealing either because of better generalizability or less computation cost. In terms of generalizability, on most of the datasets the accuracy gain from the replacement is not statistically significant. Specially without reporting the standard deviation. Also the generalizability impact of this method on the state-of-the-art is not clear due to the fact that the vanilla networks used in the experiments are generally not the state-of-the-art networks. Therefore, it would be beneficial if the authors could show the speed impact of replacing FC layers with the learned structures. Are they faster to compute or slower?--------- The purpose of section 5.1 is written as number of layers and number of parameters. But it compares with an FC network which has same number of neurons-per-layer. The rest of the paper is also about number of parameters. Therefore, the experiments in this section should be in terms of number of parameters as well. Also most of the numbers in table 1 are not significantly different. ----------------Suggestions for increasing the impact:----------------This method is easily adaptable for convolutional layers as well. Each convolutional kernel is a fully connected layer on top of a patch of an image. Therefore, the input data rather than being the whole image would be all patches of all images. This method could be used to learn a new structure to replace the KxK fully connected transformation in the convolutional layer. ----------------The fact that this is an unsupervised algorithm and it is suitable for replacing FC layers suggests experimentation on semi-supervised tasks or tasks that current state-of-the-art relies more on FC layers than image classification. However, the experiments in this paper are on fully-labeled image classification datasets which is possibly not a good candidate to verify the full potential of this algorithm.","The updated draft has helped to address some of the issues that the reviewers had, however the reviewers believe there are still outstanding issues. With regard to the technical flaw, one reviewer has pointed out that the update changes the story of the paper by breaking the connection between the generative and discriminative model in terms of preserving or ignoring conditional dependencies.  In terms of the experiments, the paper has been improved by the reporting of standard deviation, and comparison to other works. However it is recommended that the authors compare to NAS by fixing the number of parameters and reporting the results to facilitate an apples-to-apples comparison. Another reviewer also recommends comparing to other architectures for a fixed number of neurons."
https://openreview.net/forum?id=ryjw_eAaZ,"This paper tackles the important problem of structure learning by introducing an unsupervised algorithm, which encodes a hierarchy of independencies in the input distribution and allows introducing skip connections among neurons in different layers. The quality of the learnt structure is evaluated in the context of image classification, analyzing the impact of the number of parameters and layers on the performance.----------------The presentation of the paper could be improved. Moreover, the paper largely exceeds the recommended page limit (11 pages without references).----------------My main comments are related to the experimental section:----------------- Section 5 highlights that experiments were repeated 5 times; however, the standard deviation of the results is only reported for some cases. It would be beneficial to include the standard deviations of all experiments in the tables summarizing the obtained results.----------------- Are the differences among results presented in table 1 (MNIST) and table 2 (CIFAR10) statistically significant?----------------- It is not clear how the numbers of table 4 were computed (size replaced, size total, t-size, replaced-size). Would it be possible to provide the number of parameters of the vanilla model, the pre-trained feature extractor and the learned structure separately?----------------- In section 5.2., there is only one sentence mentioning comparisons to alternative approaches. It might be worth expanding this and including numerical comparisons.----------------- It seems that the main focus of the experiments is to highlight the parameter reduction achieved by the proposed algorithm. There is a vast literature on model compression, which might be worth reviewing, especially given that all the experiments are performed on standard image classification tasks.","The updated draft has helped to address some of the issues that the reviewers had, however the reviewers believe there are still outstanding issues. With regard to the technical flaw, one reviewer has pointed out that the update changes the story of the paper by breaking the connection between the generative and discriminative model in terms of preserving or ignoring conditional dependencies.  In terms of the experiments, the paper has been improved by the reporting of standard deviation, and comparison to other works. However it is recommended that the authors compare to NAS by fixing the number of parameters and reporting the results to facilitate an apples-to-apples comparison. Another reviewer also recommends comparing to other architectures for a fixed number of neurons."
https://openreview.net/forum?id=ryjw_eAaZ,"The paper proposes an unsupervised structure learning method for deep neural networks. It first constructs a fully visible DAG by learning from data, and decomposes variables into autonomous sets. Then latent variables are introduced and stochastic inverse is generated. Later a deep neural network structure is constructed based on the discriminative graph. Both the problem considered in the paper and the proposed method look interesting. The resulting structure seems nice.----------------However, the reviewer indeed finds a major technical flaw in the paper. The foundation of the proposed method is on preserving the conditional dependencies in graph G. And each step mentioned in the paper, as it claims, can preserve all the conditional dependencies. However, in section 2.2, it seems that the stochastic inverse cannot. In Fig. 3(b), A and B are no longer dependent conditioned on {C,D,E} due to the v-structure induced in node H_A and H_B. Also in Fig. 3(c), if the reviewer understands correctly, the bidirectional edge between H_A and H_B is equivalent to H_A <- h -> H_B, which also induces a v-structure, blocking the dependency between A and B. Therefore, the very foundation of the proposed method is shattered. And the reviewer requests an explicit explanation of this issue.----------------Besides that, the reviewer also finds unfair comparisons in the experiments.----------------1. In section 5.1, although the authors show that the learned structure achieves 99.04%-99.07% compared with 98.4%-98.75% for fully connected layers, the comparisons are made by keeping the number of parameters similar in both cases. The comparisons are reasonable but not very convincing. Observing that the learned structures would be much sparser than the fully connected ones, it means that the number of neurons in the fully connected network is significantly smaller. Did the authors compare with fully connected network with similar number of neurons? In such case, which one is better? (Having fewer parameters is a plus, but in terms of accuracy the number of neurons really matters for fair comparison. In practice, we definitely would not use that small number of neurons in fully connected layers.)----------------2. In section 5.2, it is interesting to observe that using features from conv10 is better than that from last dense layer. But it is not a fair comparison with vanilla network. In vanilla VGG-16-D, there are 3 more conv layers and 3 more fully connected layers. If you find that taking features from conv10 is good for the learned structure, then maybe it will also be good by taking features from conv10 and then apply 2-3 fully-connected layers directly (The proposed structure learning is not comparable to convolutional layers, and what it should really compare to is fully-connected layers.) In such case, which one is better? --------Secondly, VGG-16 is a large network designed for ImageNet data. For small dataset such as CIFAR10 and CIFAR100, it is really overkilled. That's maybe the reason why taking the output of shallow layers could achieve pretty good results.----------------3. In Fig. 6, again, comparing the learned structure with fully-connected network by keeping parameters to be similar and resulting in large difference of the number of neurons is unfair from my point of view.----------------Furthermore, all the comparisons are made with respect to fully-connected network or vanilla CNNs. No other structure learning methods are compared with. Reasonable baseline methods should be included.----------------In conclusion, due to the above issues both in method and experiments, the reviewer thinks that this paper is not ready for publication.","The updated draft has helped to address some of the issues that the reviewers had, however the reviewers believe there are still outstanding issues. With regard to the technical flaw, one reviewer has pointed out that the update changes the story of the paper by breaking the connection between the generative and discriminative model in terms of preserving or ignoring conditional dependencies.  In terms of the experiments, the paper has been improved by the reporting of standard deviation, and comparison to other works. However it is recommended that the authors compare to NAS by fixing the number of parameters and reporting the results to facilitate an apples-to-apples comparison. Another reviewer also recommends comparing to other architectures for a fixed number of neurons."
https://openreview.net/forum?id=rkmu5b0a-,"MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen-Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state-of-art results.----------------The paper is easy to follow.----------------Comment:----------------1. Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various.--------2. Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing.","If each generator models a different region then when a generator is collapsed does it mean that the corresponding region is missing in the model distribution? In your CIFAR10 example, some classes seem missing in the generated samples.   What is your opinion about this very simple approach to forcing each generator to model a different region of the target distribution: 1. Divide the training data into  subsets 2. Train a generator on each subset"
https://openreview.net/forum?id=rkmu5b0a-,"Summary:----------------The paper proposes a mixture of  generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added.------------------------Quality/clarity:----------------The paper is well written and easy to follow.----------------clarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text.----------------Originality:---------------- Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator.----------------General review:----------------- when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z!----------------- Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps?  -------- --------- in the tied weight case, in the synthetic example, can you show what each ""generator"" of the mixture learn? are they really learning modes of the data? ----------------- the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior.  would be good to have some experiments and  see how much we loose for example in term of inception scores, between tied and untied weights of generators.","If each generator models a different region then when a generator is collapsed does it mean that the corresponding region is missing in the model distribution? In your CIFAR10 example, some classes seem missing in the generated samples.   What is your opinion about this very simple approach to forcing each generator to model a different region of the target distribution: 1. Divide the training data into  subsets 2. Train a generator on each subset"
https://openreview.net/forum?id=rkmu5b0a-,"The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components.----------------All told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before. The Inception scores are good but it's widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception network's p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score.----------------The mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious.----------------Finally, their own qualitative results indicate that they've simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing.----------------Uncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if I'm not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted.----------------Other notes:--------- The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice.--------- ""As a result, the optimization order in 1 can be reversed"" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) ""On distinguishability criteria..."".--------- Section 3: the second last sentence of the third paragraph is vague and doesn't really say anything. Of course parameter sharing leverages common information. How does this help to train the model effectively?--------- Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it's not clear what the steps are; it looks like a regular KL divergence to me.--------- Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter.--------- The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi).--------- ""... which further minimizes the objective value"" -- it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it.--------- There's no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which I'm not sure was quite right in the first place)--------- Section 4: does the DAE introduced in DFM really introduce that much of a computational burden? --------- ""Symmetric Kullback Liebler divergence"" is not a well-known measure. The standard KL is asymmetric. Please define it.--------- Figure 2 is illegible in grayscale.--------- Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. It's fine to include it but indicate it as such.----------------Update: many of my concerns were adequately addressed, however I still feel that calling this an avenue to ""overcome mode collapse"" is misleading. This seems aimed at improving coverage of the support of the data distribution; test log likelihood bounds via AIS (there are GAN baselines for MNIST in the Wu et al manuscript I mentioned) would have been more compelling quantitative evidence. I've raised my score to a 5.","If each generator models a different region then when a generator is collapsed does it mean that the corresponding region is missing in the model distribution? In your CIFAR10 example, some classes seem missing in the generated samples.   What is your opinion about this very simple approach to forcing each generator to model a different region of the target distribution: 1. Divide the training data into  subsets 2. Train a generator on each subset"
https://openreview.net/forum?id=B1twdMCab,"The quality of this paper is good. The presentation is clear but I find lack of description of a key topic. The proposed model is not very innovative but works fine for the DQA task. For the TE task, the proposed method does not perform better than the state-of-the-art systems. ----------------- As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM.--------- The reference of ESIM is not correct.--------- Figure 1 is hard to understand. What do you indicate with the box and arrow? Arrows seem to have some different meanings. --------- What corpus did you use to pre-train word vectors? --------- As the proposed method was successful for the QA task, you need to explain QA data sets and how the questions are solved.--------- I also expect performance and  error analysis of the task results.  --------- To claim ""task-agnostic"", you need to try to apply your method to other NLP tasks as well.--------- Page 3. \Sigma is not defined.","Pros: + The paper is very clearly written. + The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures.  Cons: - A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors).  This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced."
https://openreview.net/forum?id=B1twdMCab,"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format. However, in this paper, the way it is added is simply by updating word representations based on this extra text. This seems too simple to really be the right way to add background knowledge. ----------------In practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing; the paper never says exactly how, not even if you read the supplementary material). This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning).----------------pp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as ""reading"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%). A note on the QA results: The QA results are certainly good enough to be in the range of ""good systems"", but none of the results really push the SOTA. The best SQuAD (devset) results are shown as several percent below the SOTA. In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission, but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD. Similar remarks perhaps apply to the NLI results.----------------p.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge.----------------Biggest question:-------- - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge?----------------Minor notes:-------- - The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used ➔ and can be used; that rely on ➔ that relies on.-------- - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?-------- - On p.3 above sec 3.1: What is u? Was that meant to be z?-------- - On p.8, I'm a bit suspicious of the ""Is additional knowledge used?"" experiment which trains with knowledge and then tests without knowledge. It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone.-------- - In the supplementary material the paper notes that the numbers are from the best result from 3 runs. This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.","Pros: + The paper is very clearly written. + The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures.  Cons: - A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors).  This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced."
https://openreview.net/forum?id=B1twdMCab,"This paper proposes a model for adding background knowledge to natural language understanding tasks. The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction. The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion).----------------I think the paper does a fairly good job at doing what it does, it is just hard to get excited by it. --------Here are my major comments:----------------* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic. But then they just concept net to augment text. This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation. As is, I don't really see how this motivation has anything to do with getting things out of a KB. A KB is usually a pretty static entity, and things are added to it at a slow pace.----------------* The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI. Specifically they take text and add common sense knowledge from concept net. The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis. However, is this statement enough to cross the acceptance threshold of ICLR? Seems a bit marginal to me.----------------* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them. To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way. For example, have another RNN read the assertions and somehow integrate that. The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated. There are no comparisons to other possibilities. As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful. As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution.----------------So to conclude, the paper is well-written, clear, and has nice results and analysis. The conclusion is that reading background knowledge from concept net boost performance using some architecture. This is nice to know but I think does not cross the acceptance threshold.","Pros: + The paper is very clearly written. + The proposed re-embedding approach is easily implemented and can be integrated into fancier architectures.  Cons: - A lot of the gains reported come from lemmatization, and the gains from background knowledge become marginal when used on a stronger baseline (e.g., ESIM with full training data and full word vectors).  This paper is rather close to the decision boundary. The authors had reasonable answers for some of the reviewers' concerns, but in the end the reviewers were not completely convinced."
https://openreview.net/forum?id=H1O0KGC6b,"This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. However, the results do not convince this reviewer to switch to using 'post-training'.----------------Learning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago. However, freezing the layers and continue to train the last layer is of a minor novelty. The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \lambda term which is added for post-training but not added for the baseline. c.f. Eq 3 and Eq 4.--------Therefore, it's unclear whether the gain in generalization is due to an additional \lambda term or from the post-training training itself.----------------A way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.----------------Other notes, ----------------Remark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments.----------------For table 1, please use decimal points instead of commas.",* the proposed fine-tuning of only the last layer is not novel enough * experiments are not sufficient to isolate the differences to support the benefit of post-training
https://openreview.net/forum?id=H1O0KGC6b,"This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model).----------------Summary of evaluation----------------There is not much novelty in this idea (of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step), which dates back at least a decade, so the only real contribution would be in the experiments. However the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method.----------------More details----------------Previous work on the same idea: at least a decade old, e.g., Huang and LeCun 2006. See a review of such work in 'Deep Learning using Linear Support Vector Machines' more recently.----------------Experiments----------------You should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training). Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct.",* the proposed fine-tuning of only the last layer is not novel enough * experiments are not sufficient to isolate the differences to support the benefit of post-training
https://openreview.net/forum?id=H1O0KGC6b,"Summary: --------Based on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network.--------This additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments).----------------According to the authors, the contributions are the following:--------1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to ""make sure"" that the representation learned is used in the most efficient way.--------2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression).--------3. Experimental results.----------------Clarity:--------The paper is well-written, the main ideas well-clarified. ----------------Importance:--------While the majority of papers nowadays focuses on the representation part (i.e., how we get to \Phi_{L-1}(x)), this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm. This by itself is not enough to boost the performance universally (e.g., if \Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures. From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference.----------------On the other hand, to my humble opinion, the experimental results do not show a significant gain in the performances of all networks (esp. Figure 3 and Table 1 are within the range of statistical error). In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better.----------------Originality:--------It would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer.----------------Comments:--------1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer? One reason is convexity in W of the problem (2). Any other? ----------------2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN? The authors state this question in the future direction, but it would make the paper more complete to consider it here.",* the proposed fine-tuning of only the last layer is not novel enough * experiments are not sufficient to isolate the differences to support the benefit of post-training
https://openreview.net/forum?id=HklpCzC6-,"This paper proposes an iterative procedure on top of a standard image semantic segmentation networks. ----------------The submission proposes a change to the training procedure of stacking a denoising auto-encoder for image segmentation. The technical contribution of this paper is small. The paper aims to answer a single question: When using a DAE network on top of a segmentation network output, should one condition on the predicted, or the ground truth segmentation? (why not on both?) The answer is conditioning on the predicted image for a second round of inference is a bit better. The method also performs a bit better (no statistical significance tests) than other post-processing methods (Dense-CRF, CRF-RNNs)----------------Experimental results are available only on a small dataset and for two different networks. This may be sufficient for a first proof-of-concept but a comparison against standard benchmark methods and datasets for semantic segmentation is missing. It is unlikely that in the current state of this submission is a contribution to image segmentation, evidence is weak and several improvements are suggested.----------------- The experimental evidence is insufficient. The improvements are small, statistical tests are not available. The CamVid dataset is the smallest of the image segmentation datasets used these days, more compelling would be MSCOCO or Cityscapes, better most of them. The question whether this network effect is tied to small-dataset and low-resolution is not answered. Will a similar effect be observed when compared to networks trained on way more data (e.g., CityScapes)? --------- The most important baseline is missing: auto-context [Tu08]. Training the same network the DAE uses in an auto-context way. That is, take the output of the first model, then train another network using both input and prediction again for semantic segmentation (and not Eq.3). This is easy to do, practically almost always achieves better performance and I would assume the resulting network is faster and performs similar to the method presented in this submission on (guessing, I have not tried). In any case, to me this is the most obvious baseline. --------- I am in favour of probabilistic methods, but the availability of an approximation of p(y) (or the nearest mode) is not used (as is most often the case).--------- Runtimes are absent. This is a practical consideration which is important especially if there is little technological improvement. The DAE model of this submission compares to simple filtering methods as Krähenbühl&Koltun DenseCRF which are fast and performance results are comparable. The question wether this is practically relevant is missing, judging from the construction I guess this does not fare well. Also training time is significantly more, please comment.--------- The related work is very well written, thanks. This proposal is conceptually very similar to auto-context [Tu08] and this reference missing (this is also the most important baseline)----------------[Tu08] Tu, “Auto-context and its application to high-level vision tasks”, CVPR 2008",The experimental work was seen as one of the main weaknesses.
https://openreview.net/forum?id=HklpCzC6-,"I am a returning reviewer for this paper, from a previous conference. Much of the paper remains unchanged from the time of my previous review. I have revised my review according to the updates in the paper:----------------Summary of the paper:--------This work proposes a neural network based alternative to standard CRF post-processing techniques that are generally used on top semantic segmentation CNNs. As an alternative to CRF, this work proposes to iteratively refine the predicted segmentation with a denoising auto encoder (DAE). Results on CamVid semantic segmentation dataset showed better improvements over base CNN predictions in comparison to popular DenseCRF technique.------------------------Paper Strengths:--------- A neat technique for incorporating CRF-like pixel label relations into semantic segmentation via neural networks (auto encoders).--------- Promising results on CamVid segmentation dataset with reliable improvements over baseline techniques and minor improvements when used in conjunction with recent models.------------------------Major Weaknesses:--------I have two main concerns for this work:--------- One is related to the novelty as the existing work of Xie et al. ECCV'16 also proposed similar technique with very similar aim.  I think, conceptual or empirical comparisons are required to assess the importance of the proposed approach with respect to existing ones. Mere citation and short discussion is not enough. Moreover, Xie et al. seem to have demonstrated their technique on two different tasks and on three different datasets.--------- Another concern is related to experiments. Authors experimented with only one dataset and with one problem. But, I would either expect some demonstration of generality (more datasets or tasks) or strong empirical performance (state-of-the-art on CamVid) to assess the empirical usefulness with respect to existing techniques. Both of these aspects are missing in experiments. ------------------------Minor Weaknesses:--------- Negligible improvements with respect to CRF techniques on modern deep architectures.--------- Runtime comparison is missing with respect to baseline techniques. Applying the proposed DAE 40-50 times seems very time consuming for each image.--------- By back-propagating through CRF-like techniques [Zheng et al. ICCV'15, Gadde et al. ECCV'16, Chandra et al. ECCV'16 etc.], one could refine the base segmentation CNN as well. It seems this is also possible with the proposed architecture. Is that correct? Or, are there any problems with the end-to-end fine-tuning as the input distribution to DAE constantly changes? Did authors try this?------------------------Suggestions:--------- Only Gaussian noise corruption is used for training DAE. Did authors experiment with any other noise types? Probably, more structured noise would help in learning better contextual relations across pixel labels?----------------Clarifications:--------What is the motivation to add Euclidean loss to the standard cross-entropy loss for segmentation in Eq-3?----------------Review summary:--------The use of denoising auto encoders (DAEs) for capturing pixel label relations and then using them to iteratively refine the segmentation predictions is interesting. But, incomplete comparisons with similar existing work and limited experiments makes this a weak paper.",The experimental work was seen as one of the main weaknesses.
https://openreview.net/forum?id=HklpCzC6-,"The paper proposes an image segmentation method which iteratively refines the semantic segmentation mask obtained from a deep net. To this end the authors investigate a denoising auto-encoder (DAE). Its purpose is to provide a semantic segmentation which improves upon its input in terms of the log-likelihood.----------------More specifically, the authors `propose to condition the autoencoder with an additional input’ (page 1). To this end they use features obtained from the deep net. Instead of training the DAE with ground truth y, the authors found usage of the deep net prediction to yield better results.----------------The proposed approach is evaluated on the CamVid dataset.----------------Summary:--------——--------I think the paper discusses a very interesting topic and presents an elegant approach. A few points are missing which would provide significantly more value to a reader. Specifically, an evaluation on the classical Pascal VOC dataset, details regarding the training protocol of the baseline (which are omitted right now), an assessment regarding stability of the proposed approach (not discussed right now), and a clear focus of the paper on segmentation or conditioning. See comments below for details and other points.----------------Comments:--------——--------1. When training the DAE, a combination of squared loss and categorical cross-entropy loss is used. What’s the effect of the squared error loss and would the categorical cross-entropy on its own be sufficient? This question remains open when reading the submission.----------------2. The proposed approach is evaluated on the CamVid dataset which is used less compared to the standard and larger Pascal VOC dataset. I conjecture that the proposed approach wouldn’t work too well on Pascal VOC. On Pascal VOC, images are distinctly different from each other whereas subsequent frames are similar in CamVid, i.e., the road is always located at the bottom center of the image. The proposed architecture is able to take advantage of this dataset bias, but would fail to do so on Pascal VOC, which has a much more intricate bias. It would be great if the authors could check this hypothesis and report quantitative results similar to Tab. 1 and Fig. 4 for Pascal VOC.----------------3. The authors mention a grid-search for the stepsize and the number of iterations. What values were selected in the end on the CamVid and hopefully the Pascal VOC dataset?----------------4. Was the dense CRF applied out of the box, or were its parameters adjusted for good performance on the CamVid validation dataset? While parameters such as the number of iterations and epsilon are tuned for the proposed approach on the CamVid validation set, the submission doesn’t specify whether a similar procedure was performed for the CRF baseline.----------------5. Fig. 4 seems to indicate that the proposed approach doesn’t converge. Hence an appropriate stepsize and a reasonable number of iterations need to be chosen on a validation set. Choosing those parameters guarantees that the method performs well on average, but individual results could potentially be entirely wrong, particularly if large step sizes are chosen. I suspect this effect to be more pronounced on the Pascal VOC dataset (hence my conjecture in point 2). To further investigate this property, as a reader, I’d be curious to get to know the standard deviation/variance of the accuracy in addition to the mean IoU. Again, it would be great if the authors could check this hypothesis and report those results.----------------6. I find the experimental section to be slightly disconnected from the initial description. Specifically, the paper `proposes to condition the autoencoder with an additional input’ (page 1). No experiments are conducted to validate this proposal. Hence the main focus of the paper (image segmentation or DAE conditioning) remains vague. If the authors choose to focus on image segmentation, a comparison to state-of-the-art should be provided on classical datasets such as Pascal VOC, if DAE conditioning is the focus, some experiments in this direction should be included in addition to the Pascal VOC results.----------------Minor comment:--------——--------- I find it surprising that the authors choose not to cite some related work on combining deep nets with structured prediction.",The experimental work was seen as one of the main weaknesses.
https://openreview.net/forum?id=SkA-IE06W,"This paper considers the convergence of (stochastic) gradient descent for learning a convolutional filter with ReLU activations. It doesn't assume the input is Gaussian as in most previous work and shows that starting from random initialization, the (stochastic) gradient descent can learn the underlying convolutional filter in polynomial time. It is also shown that the convergence rate depends on the smoothness of the input distribution and the closeness of the patches. ----------------The main contribution and the most intriguing part is that the result doesn't require assuming the input is Gaussian. Also, the guarantee holds for random initialization. The analysis that achieves these results can potentially provide better techniques for analyzing more general deep learning optimizations. ----------------The main drawback is that the assumptions are somewhat difficult to interpret, though significantly more general than those made in previous work. It will be great if more explanations/comments are provided for these assumptions. It will be even better if one can get a simplified set of assumptions. ----------------The presentation is clear but can be improved. Especially, more remarks would help readers to understand the paper. ----------------minor:---------- Thm 2.1: what are w_1 w_2 here? ---------- Assumption 3.1: the statement seems incomplete. I guess it should be ""max_... \lambda_max(...) \geq \beta for some beta > 0""?---------- Just before Section 2.1: "" This is also consistent with empirical evidence in which more data are helpful for optimization."" --------I don't see any evidence that more data help the optimization by filling in the holds in the distribution; they may help for other reasons. This statement here is not rigorous. ","Dear authors,  The reviewers all appreciated your work and agree that this a very good first step in an interesting direction."
https://openreview.net/forum?id=SkA-IE06W,"(a) Significance--------This is an interesting theoretical deep learning paper, where the authors try to provide the theoretical insights why SGD can learn the neural network well. The motivation is well-justified and clearly presented in the introduction and related work section. And the major contribution of this work is the generalization to the non-Gaussian case, which is more in line with the real world settings. Indeed, this is the first work analyzing the input distribution beyond Gaussian, which might be an important work towards understanding the empirical success of deep learning. ----------------(b) Originality--------The division of the input space and the analytical formulation of the gradient are interesting, which are also essential for the convergence analysis. Also, the analysis framework relies on novel but reasonable distribution assumptions, and is different from the relevant literature, i.e., Li & Yuan 2017, Soltanolkotabi 2017, Zhong et al. 2017. I curious whether the angular smoothness assumptions can be applied to a more general network architecture, say two-layer neural network.----------------(c) Clarity & Quality --------Overall, this is a well-written paper. The theoretical results are well-presented and followed by insightful explanations or remarks. And the experiments are demonstrated to justify the theoretical findings as well. The authors did a really good job in explaining the intuitions behind the imposed assumptions and justifying them based on the theoretical and experimental results. I think the quality of this work is above the acceptance bar of ICLR and it should be published in ICLR 2018.----------------Minor comments: --------1. Figure 3 looks a little small. It is better to make them clearer.--------2. In the appendix, ZZ^{\top} and the indicator function are missing in the first equation of page 13.","Dear authors,  The reviewers all appreciated your work and agree that this a very good first step in an interesting direction."
https://openreview.net/forum?id=SkA-IE06W,"This paper studies the problem of learning a single convolutional filter using SGD. The main result is: if the ""patches"" of the convolution are sufficiently aligned with each other, then SGD with a random initialization can recover the ground-truth parameter of a convolutional filter (single filter, ReLU, average pooling). The convergence rate, and how ""sufficiently aligned"" depend on some quantities related to the underlying data distribution. A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.----------------Detailed comments:--------1. It would be good to clarify what the angle requirement means on page 2. It says the angle between Z_i, Z_j is at most \rho, is this for any i,j? From the later part it seems that each Z_i should be \rho close to the average, which would imply pairwise closeness (with some constant factor).--------2. The paper first proves result for a single neuron, which is a clean result. It would be interesting to see what are values of \gamma(\phi) and L(\phi) for some distributions (e.g. Gaussian, uniform in hypercube, etc.) to give more intuitions. --------3. The convergence rate depends on \gamma(\phi_0), from the initialization, \phi_0 is probably very close to \pi/2 (the closeness depend on dimension), which is  also likely to make \gamma(\phi_0) depend on dimension (this is especially true of Gaussian). --------4. More precisely, \gamma(\phi_0) needs to be at least 6L_{cross} for the result to work, and L_{cross} seems to be a problem dependent constant that is not related to the dimension of the data. Also \gamma(\phi_0) depends on \gamma_{avg}(\phi_0) and \rho, when \rho is reasonable (say a constant), \gamma(\phi_0) really needs to be a constant that is independent of dimension. On the other hand, in Theorem 3.4 we can see that the upperbound on \alpha (the quality of initialization) depends on the dimension. --------5. Even assuming \rho is a constant strictly smaller than \pi/2 seems a bit strong. It is certainly plausible that nearby patches are highly correlated, but what is required here is that all patches are close to the average. Given an image it is probably not too hard to find an almost all white patch and an almost all dark patch so that they cannot both be within a good angle to the average. ----------------Overall I feel the result is interesting but hard to interpret correctly. The details of the theorem do not really support the high level claims very strongly. The paper would be much better if it goes over several example distributions and show explicitly what are the guarantees. The reviewer tried to do that for Gaussian and as I mentioned above (esp. 4) the result does not seem very impressive, maybe there are other distributions where this result works better?----------------After reading the response, I feel the contribution for the single neuron case does not require too much assumptions and is itself a reasonable result. I am still not convinced by the convolution case (which is the main point of this paper), as even though it does not require Gaussian input (a major plus), it still seems very far from ""general distribution"". Overall this is a first step in an interesting direction, so even though it is currently a bit weak I think it is OK to be accepted. I hope the revised version will clearly discuss the limitations of the approach and potential future directions as the response did.","Dear authors,  The reviewers all appreciated your work and agree that this a very good first step in an interesting direction."
https://openreview.net/forum?id=HyI6s40a-,"This paper proposes an unsupervised method, called Parallel Checkpointing Learners (PCL), to detect and defend adversarial examples. The main idea is essentially learning the manifold of the data distribution and using Gaussian mixture models (GMMs) and dictionary learning to train a ""reformer"" (without seeing adversarial examples) to detect and correct adversarial examples. With PCL, one can use hypothesis testing framework to analyze the detection rate and false alarm of different neural networks against adversarial attacks. Although the motivation is well grounded, there are two major issues of this work: (i) limited  novelty - the idea of unsupervised manifold projection method has been proposed in the previous work; and (ii) insufficient attack evaluations - the defender performance is evaluated against weak attacks or attacks with improper parameters. The details are as follows.----------------1.  Limited novelty and performance comparison - the idea of unsupervised manifold projection method has been proposed and well-studied in ""MagNet: a Two-Pronged Defense against Adversarial Examples"", appeared in May 2017. Instead of GMMs and dictionary learning in PCL,  MagNet trains autoencoders for defense and provides sufficient experiments to claim its defense capability. On the other hand, the authors of this paper seem to be not aware of this pioneering work and claim ""To the best of our knowledge, our proposed PCL methodology is the first unsupervised countermeasure that is able to detect DL adversarial samples generated by the existing state-of-the-art attacks"", which is obviously not true. More importantly, MagNet is able to defend the adversarial examples very well (almost 100% success) no matter the adversarial examples are close to the information manifold or not. As a result, the resulting ROC and AUC score are expected be better than PCL. In addition, the authors of MagNet also compared their performance in white-box (attacker knowing the reformer), gray-box (having multiple independent reformers), and black-box (attacker not knowing the reformer) scenarios, whereas this paper only considers the last case.----------------2. Insufficient attack evaluations - the attacks used in this paper to evaluate the performance of PCL are either weak (no longer state-of-the-art) or incorrectly implemented. For FGSM, the iterative version proposed by (Kurakin, ICLR 2017) should be used. JSMA and deep fool are not considered strong attacks now (see Carlini's bypassing 10 detection methods paper). Carlini-Wagner attack is still strong, but the authors only use 40 iterations (should be at least 500) and setting the confidence=0, which is known to be producing non-transferable adversarial examples. In comparison, MagNet has shown to be effective against different confidence parameters. ----------------In summary, this paper has limited novelty, incremental contributions, and lacks convincing experimental results due to weak attack implementation.  -------- ","The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:  - It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples  - The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker  - It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)"
https://openreview.net/forum?id=HyI6s40a-,"Summary:-------- The paper presents an unsupervised method for detecting adversarial examples of neural networks. The method includes two independent components: an ‘input defender’ which tried to inspect the input, and a ‘latent defender’ trying to inspect a hidden representation. Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection. The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians. Experiments are presented on MInst, Cifar10, and ImageNet.-------- --------- Introduction: The motivation for detecting adversarial examples is not stated clearly enough. How can such examples be used by a malicious agent to cause damage to a system? Sketching some such scenarios would help the reader understand why the issue is practically important. I was not convinced it is. --------Page 4: --------- Step 3 of the algorithm is not clear:--------o How exactly does HDDA model the data (formally) and how does it estimate the parameters? In the current version, the paper does not explain the HDDA formalism and learning algorithm, which is a main building block in the proposed system (as it provides the density score used for adversarial examples detection). Hence the paper cannot be read as a standalone document. I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper.--------o What is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA? Are they completely different models? Or are the C^I used when learning the HDDA model (and how)? --------If these are separate models, how are they used in conjunction to give a final density score? If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the \phy(x) representation more class-seperable. Is that right?--------- Figure 4, b and c: it is not clear what the (x,y,z) measurements plotted in these 3D drawings are (what are the axis).--------Page 5:--------- Section 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5. However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)  - there is not generative model of p(x|W2)).  --------- The authors claim in the last sentence of the section that p(x|W2) is equivalent to 1-p(x|W1), but this is not true: these are two continuous densities, they do not sum to 1, and a model of p(x|W2) is not available (as far as I understand the method)--------Page 6:--------- How is equation 7) optimized?--------- Which patchs are extracted from images, for training and at inference time? Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps?--------- Its not clear enough what exactly is the ‘PSNR’ value which is used for the adversarial example detection, and what exactly is ‘profile the PSNR of legitimate samples within each class’. A formal definition of PSNR and’profiling’ is missing (does profiling simply mean finding a threshold for filtering?)--------Page 7:--------- Figure 7 is not very informative. Given the ROC curves in figure 8  and table 1 it is redundant. ----------------Page 8:--------- The results in general indicate that the method is much better than chance, but it is not clear if it is practical, because the false alarm rates for high detection are quite high. For example on ImageNet, 14.2% of the innocent images are mistakenly rejected as malicious to get 90% detection rate. I do not think this working point is useful for a real application--------- Given the high flares alarm rate, it is surprising that experiments with multiple checkpoints are not presented (specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the paper).  Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method--------- The experiments show that essentially, the latent defenders are stronger than the input defender in most cases. However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it? And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space? Of which dimension?----------------Overall:--------Pros:---------  A nice idea with some novelty,  based on a non-trivial observation--------- The experimental results how the idea holds some promise--------Cons--------- The method is not presented clearly enough: the main component modeling the network activity is not explained (the HDDA module used)--------- The results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)--------- Experimental results are partial: results are not presented for multiple defenders, no ablation experiments------------------------After revision:--------Some of my comments were addressed, and some were not.--------Specifically, results were presented for multiple defenders and some ablation experiments were highlihgted--------Things not addressed:-------- - The risk analysis is still not relevant. The authors removed a clearly flawed sentence, but the analysis still assumes that two densities (of 'good' and 'bad' examples) are modeled, while in the work presented only one of them is. Hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear. It would have been better to omit it and use the space to describe HDDA and the specific variant used in this work, as this is the main tool doing the distinction.----------------I believe the paper should be accepted.","The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:  - It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples  - The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker  - It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)"
https://openreview.net/forum?id=HyI6s40a-,"This paper present a method for detecting adversarial examples in a deep learning classification setting.  The idea is to characterize the latent feature space (a function of inputs) as observed vs unobserved, and use a module to fit a 'cluster-aware' loss that aims to cluster similar classes tighter in the latent space. ----------------Questions/Comments:----------------- How is the checkpointing module represented?  Which parameters are fit using the fine-tuning loss described on page 3? ----------------- What is the rationale for setting the gamma (concentration?) parameters to .01?  Is that a general suggestion or a data-set specific recommendation?----------------- Are the checkpointing modules designed to only detect adversarial examples?  Or is it designed to still classify adversarial examples in a robust way?----------------Clarity: I had trouble understanding some of this paper.  It would be nice to have a succinct summary of how all of the pieces presented fit together, e.g. the original victim network, fine-tuning loss, per-class dictionary learning w/ OMP.  ----------------Technical: It is hard to tell how some of the components of this approach are technically justified. ----------------Novel: I am not familiar enough with adversarial deep learning to assess novelty or impact. ","The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:  - It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples  - The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker  - It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)"
https://openreview.net/forum?id=SkRsFSRpb,"In this paper, the authors propose to integrate the Fisher information metric with the  Seq2Seq network, which abridges the gap between deep recurrent neural networks and information geometry. By considering of the information geometry of the latent embedding, the authors propose to encode the RNN feature as a Fisher kernel of a parametric Gaussian Mixture Model, which demonstrate an experimental improvements compared with the non-probabilistic embedding. ----------------The idea is interesting. However, the technical contribution is rather incremental. The authors seem to integrate some well-explored techniques, with little consideration of the specific challenges. Moreover, the experimental section is rather insufficient. The results on road network graph is not a strong support for the Seq2Seq model application. ",The reviewers found the paper meaningful but noted that they were not convinced by the experiments as they stand and the presentation was dense for them.
https://openreview.net/forum?id=SkRsFSRpb,"==== UPDATE AFTER REVIEWER RESPONSE----------------I apologize to the authors for my late response.----------------I appreciate the reviewer responses, and they are helpful on a number of--------fronts. Still, there are several problematic points.----------------First, as the authors anticipated, I question whether the geometric encoding--------operations can be included in an end-to-end learning setting. I can imagine--------several arguments why an end-to-end algorithm may not be preferred, but the--------authors do not offer any such arguments.----------------Second, I am still interested in more discussion of the empirical investigation--------into the behavior of the algorithm. For example, ""Shortest"" and ""Successful""--------in Table 1 still do not really capture how close ""successful but not shortest""--------paths are to optimal.----------------The authors have addressed a number of my concerns, but there--------are a few outstanding concerns. Also, other reviewers are much more familiar--------with the work than myself. I defer to their judgement after the updates.----------------==== Original review----------------In this work, the authors propose an approach to adapt latent representations to account for local geometry in the embedding space. They show modest improvement compared to reasonable baselines.----------------While I find the idea of incorporating information geometry into embeddings very promising, the current work omits a number of key details that would allow the reader to draw deeper connections between the two (specific comments below). Additionally, the experiments are not particularly insightful.----------------I believe a substantially revised version of the paper could address most of my concerns; still, I find the current version too preliminary for publication.----------------=== Major comments / questions----------------The transformation from context vectors into Fisher vectors is not clear. Presumably, shortest paths in the training data have different lengths, and thus produce different numbers of context vectors. Does the GMM treat all of these independently (regardless of sample)? or is a separate GMM somehow trained for each training sequence? The same question applies to the VLAD-based approach.----------------In a related vein, it is not clear to what extent this method depends on the sequential nature of the considered networks. In particular, could a similar approach be applied to latent space embeddings from non-sequential models?----------------It is not clear if the geometric encoding operations are differentiable, or more generally, the entire training algorithm is not clear.----------------The choice to limit the road network graph feels quite arbitrary. Why was this done?----------------Deep models are known to be sensitive to the choice of hyperparameters. How were these chosen? was a validation set used in addtion to the training and testing sets?----------------The target for training is very unclear. Throughout Sections 1 and 2, the aim of the paper appears to be to learn shortest paths; however, Section 3 states that the “network is capable of learning the adjacency matrix”, and the caption for Figure 2 suggests that “[t]he adjacency matrix is iteratively learnt (sic)....” However, calculating such training error for back-propagation/optimization would seem to rely on *already knowing* the adjacency matrix.----------------The performed experiments are minimal and offer very little insight into what is learned. For example, does the model predict “short” shortest paths better than longer ones? what do the “valid but not optimal” paths look like? are they close to optimal? what do the invalid paths look like? does it seem to learn parts of the road network better than others? sparse parts of the network? dense parts?----------------=== Minor comments / questions----------------The term “context vector” is not explicitly defined or described. Based on the second paragraph in the “Fisher encoding” section, I assume these are the latent states for each element in the shortest path sequences.----------------Is the graph directed? weighted? by Euclidean distance? (Roads are not necessarily straight, so the Euclidean distance from intersection to intersection may not accurately reflect the distance in some cases.)----------------Are the nodes sampled uniformly at random for creating the training data?----------------Is the choice to use a diagonal covariance matrix (as opposed to some more flexible one) a computational choice? or does the theory justify this choice?----------------Roughly, what are the computational resources required for training?----------------The discussion should explain “condition number” in more detail.----------------Do the “more precise” results for the Fisher encoding somehow rely on an infinite mixture? or, how much does using only a single component in the GMM affect the results?----------------It is not clear what “features” and “dictionary elements” are in the context of VLAD.----------------What value of k was used for K-means clustering for VLAD?----------------It is not possible to assess the statistical significance of the presented experimental results. More datasets (or different parts of the road network) or cross-validation should be used to provide an indication of the variance of each method.----------------=== Typos, etc.----------------The paper includes a number of runon sentences and other small grammatical mistakes. I have included some below.----------------The first paragraph in Section 2.2 in particular needs to be edited.----------------The references are inconsistently and improperly (e.g., “Turing” should be capitalized) formatted.----------------It seems like that  for the hard assignments in clustering.",The reviewers found the paper meaningful but noted that they were not convinced by the experiments as they stand and the presentation was dense for them.
https://openreview.net/forum?id=SkRsFSRpb,"The paper proposes a method for augmenting sequence-to-sequence (seq2seq) methods with Fisher vector encodings, allowing the decoder to better model the geometric structure of the embedding space. Experiments are performed on a shortest-route problem, where augmenting standard seq2seq architectures with Fisher vectors improves performance.----------------Pros:--------- Combining deep learning with methods from information geometry is an interesting direction for research--------- Method is a generic drop-in replacement for improving any seq2seq architecture--------- Experimental results show modest performance improvements over vanilla seq2seq methods----------------Cons:--------- Missing references for prior work combining information geometry and deep learning--------- Insufficient explanation of the method--------- Only experimental results are a nonstandard route-finding task--------- Missing references and baselines for prior work on deep learning on graphs----------------The general research direction of combining deep learning with methods from information geometry is an exciting and fertile area for interesting work. Unfortunately this paper fails to cite or discuss much recent work in this area; for example natural gradient methods in deep learning have recently been explored in [1, 2, 3]; more closely related to the topic of this paper, [4] and [5] have combined Fisher vector encodings and deep networks for image classification tasks. Although these prior methods do not consider the use of recurrent networks, the authors should discuss how their method compares to the approaches of [4] and [5].----------------The method is not described in sufficient detail. How exactly is the Fisher encoding combined with the recurrent neural network? In particular, how is GMM fitting interleaved with learning the RNN? Do you backpropagate through the GMM fitting procedure in order to jointly learn the RNN parameters and the GMM for computing Fisher encodings? Or is GMM fitting an offline step done once, after which the RNN decoder is learned on top of the Fisher encodings? The paper should clarify along these points. As a side note, it also feels a little disingenuous to describe the method in terms of GMMs, but to perform all experiments with K=1 mixture components; in this setting the GMM degrades to a simple Gaussian distribution.----------------The proposed method could in theory be used as a drop-in replacement for seq2seq on any task. Given its generality, I am surprised at the nonstandard choice of route-finding in a graph of Minnesota roads as the only task on which the method is tested; as a minimum the method should have been tested on more than one graph.----------------More generally, I would have liked to see the method evaluated on multiple tasks, and on more well-established seq2seq tasks so that the method could be more easily compared with previously published work. Strong results on machine translation would be particularly convincing; the authors might also consider algorithmic tasks such as copying, repeat copying, sorting, etc. similar to those on which Neural Turing Machines were evaluated.----------------I am not sure that seq2seq is the best approach for the route-finding task. In particular, since the input is encoded as a [source, destination] tuple it has a fixed length; this means that you could use a feedforward rather than recurrent encoder.----------------The paper also fails to cite or discuss recent work involving deep learning on graphs. For example Pointer Networks [6] use a seq2seq model with attention to solve convex hull, Delaunnay Triangulation, and traveling salesman problems; however Pointer Networks assume that the entire graph is provided as input to the model, while in this paper the network learns to specialize to a single graph. In that case, the authors might consider embedding the nodes of the graph using methods such as DeepWalk [7], LINE [8], or node2vec [9] as a preprocessing step rather than learning these embeddings from scratch.----------------From Table 1, seq2seq + VLAD significantly outperforms seq2seq + FV. Given these results, are there any reasons why one should use seq2seq + FV instead of seq2seq + VLAD?----------------Overall I think that this paper has some interesting ideas. However, due to a number of missing references, unclear description of the method, and limited experimental results I feel that the paper is not ready for publication in its current form.------------------------References----------------[1] Grosse and Salakhutdinov, “Scaling Up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix”, ICML 2015----------------[2] Grosse and Martens, “A Kronecker-factored approximate Fisher matrix for convolution layers”, ICML 2016----------------[3] Desjardins et al, “Natural Neural Networks”, NIPS 2015----------------[4] Simonyan et al, “Deep Fisher Networks for Large-Scale Image Classification”, NIPS 2013----------------[5] Sydorov et al, “Deep Fisher Kernels - End to End Learning of the Fisher Kernel GMM Parameters”, CVPR 2014----------------[6] Vinyals et al, “Pointer Networks”, NIPS 2015----------------[7] Perozzi et al, “DeepWalk: Online Learning of Social Representations”, KDD 2014----------------[8] Tang et al, “LINE: Large-scale Information Network Embedding”, WWW 2015----------------[9] Grover and Leskovec, “node2vec: Scalable Feature Learning for Networks”, KDD 2016",The reviewers found the paper meaningful but noted that they were not convinced by the experiments as they stand and the presentation was dense for them.
https://openreview.net/forum?id=S1xDcSR6W,"The authors present a method to embed graphs in hyperbolic space, and show that this approach yields stronger attribute predictions on a set of graph datasets. I am concerned by the strong similarity between this work and Poincaré Embeddings for Learning Hierarchical Representations (https://arxiv.org/abs/1705.08039). The latter has been public since May of this year, which leads me to doubt the novelty of this work.----------------I also find the organization of the paper to be poor.--------- There is a surprisingly high number of digressions.--------- For some reason, Eq 17 is not included in the main paper. I would argue that this equation is one of the most important equations in the paper, given that it is the one you are optimizing.--------- The font size in the main result figure is so small that one cannot hope to parse what the plots are illustrating.--------- I am not sure what insights the readers are supposed to gain from the visual comparisons between the Euclidean and Poincare embeddings. ----------------Due to the poor presentation, I actually have difficulty making sense of the evaluation in this paper (it would help if the text was legible). I think this paper requires significant work and it not suitable for publication in its current state.----------------As a kind of unrelated note. It occurs to me that papers on hyperbolic embeddings tend to evaluate evaluate on attribute or link prediction. It would be great if authors would also evaluate these pretrained embeddings on downstream applications such as relation extraction, knowledge base population etc.","This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
https://openreview.net/forum?id=S1xDcSR6W,"The authors present a neural embedding technique using a hyperbolic space.--------The idea of embedding data into a space that is not Euclidean is not new.--------There have been attempts to project onto (hyper)spheres.--------Also, the proposal bears some resemblance with what is done in t-SNE, where an (exponential) distortion of distances is induced. Discussing this potential similarity would certainly broaden the readership of the paper.----------------The organisation of the paper might be improved, with a clearer red line and fewer digressions.--------The call to the very small appendix via eq. 17 is an example.--------The position of Table in the paper is odd as well.--------The order of examples in Fig.5 differs from the order in the list.----------------The experiments are well illustrative but rather small sized.--------The qualitative assessment is always interesting and it is completed with some label prediction task.--------Due the geometrical consideretations developed in the paper, other quality criteria like e.g. how well neighbourhoods are preserved in the embeddings would give some more insights.----------------All in all the idea developed in the paper sounds interesting but the paper organisation seems a bit loose and additional aspects should be investigated. ","This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
https://openreview.net/forum?id=S1xDcSR6W,"This paper proposes tree vertex embeddings over hyperbolic space. The conditional predictive distribution is the softmax of <v1, v2>_H = ||v1|| ||v2|| cos(theta1-theta2), and v1, v2 are points  defined via polar coordinates (r1,theta1), and (r2,theta2).--------To evaluate, the authors show some qualitative embeddings of graph and 2-d projections, as well as F1 scores in identifying the biggest cluster associated with a class. ----------------The paper is well motivated, with an explanation of the technique as well as its applications in tree embedding in general. I also like the evaluations, and shows a clear benefit of this poincare embedding vs euclidean embedding.----------------However, graph embeddings are now a very well explored space, and this paper does not seem to mention or compare against other hyperbolic (or any noneuclidean) embedding techniques. From a 2 second google search, I found several sources with very similar sounding concepts:----------------Maximilian Nickel, Douwe Kiela, Poincaré Embeddings for Learning Hierarchical Representations----------------A Cvetkovski, M Crovella, Hyperbolic Embedding and Routing for Dynamic Graphs----------------Yuval Shavitt, Tomar Tankel, Hyperbolic Embedding of Internet Graph for Distance Estimation and Overlay Construction----------------Thomas Bläsius, Tobias Friedrich, Anton Krohmer, andSören Laue. Efficient Embedding of Scale-Free Graphs in the Hyperbolic Plane----------------I think this paper does have some novelty in applying it to the skip-gram model and using deep walk, but it should make more clear that using hyperbolic space embeddings for graphs is a popular and by now, intuitive construct. Along the same lines, the benefit of using the skip-gram and deep-walk techniques should be compared against some of the other graph embedding techniques out there, of which none are listed in the experiment section. ----------------Overall, a detailed comparison against 1 or 2 other hyperbolic graph embedding techniques would be sufficient for me to change my vote to accept. ","This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
https://openreview.net/forum?id=S1xDcSR6W,"== Preamble ==----------------As promised, I have read the updated paper from scratch and this is my revised review. My original review is kept below for reference. My original review had rating ""4: Ok but not good enough - rejection"".----------------== Updated review ==----------------The revised improves upon the original submission in several ways and, in particular, does a much better job at positioning itself within the existing body of literature. The new experiments also indicate that the proposed model offer some improvement over Nickel & Douwe, NIPS 2017).----------------I do have remaining concerns that unfortunately still prevent me from recommending acceptance:----------------- Throughout the paper it is argued that we should embed into a hyperbolic space. Such a space is characterized by its metric, but the proposed model do not use a hyperbolic metric. Rather it relies on a heuristic similarity measure that is inspired by the hyperbolic metric. I understand that this may be a practical choice, but then I find it misleading that the paper repeatedly states that points are embedded into a hyperbolic space (which is incorrect). This concern was also raised on this forum prior to the revision.----------------- The resulting optimization is one of the key selling points of the proposed method as it is unconstrained while Nickel & Douwe resort to a constrained optimization. Clearly unconstrained optimization is to be preferred. However, it is not entirely correct (from what I understand), that the resulting optimization is indeed unconstrained. Nickel & Douwe work under the constraint that |x| < 1, while the proposed model use polar coordinates (r, theta): r in (0, infinity) and theta in (0, 2 pi]. Note that theta parametrize a circle, and therefore wrapping may occur (this should really be mentioned in the paper). The constraints on theta are quite easy to cope with, so I agree with the authors that they have a more simple optimization problem. However, this is only true since points are embedded on the unit disk (2D). Should you want to embed into higher dimensional spaces, then theta need to be confined to live on the unit sphere, i.e. |theta| = 1 (the current setting is just a special-case of the unit sphere). While optimizing over the unit sphere is manageable it is most definitely a constrained optimization problem, and it is far from clear that it is much easier than working under the Nickel & Douwe constraint, |x| < 1.----------------Other comments:--------- The sentence ""even infinite trees have nearly isometric embeddings in hyperbolic space (Gromov, 2007)"" sounds cool (I mean, we all want to cite Gromov), but what does it really mean? An isometric embedding is merely one that preserves a metric, so this statement only makes sense if the space of infinite trees had a single meaningful metric in the first place (it doesn't; that's a design choice).----------------- In the ""Contribution"" and ""Conclusion"" sections it is claimed that the paper ""introduce the new concept of neural embeddings in hyperbolic space"". I thought that was what Nickel & Douwe did... I understand that the authors are frustrated by this parallel work, but at this stage, I don't think the present paper can make this ""introducing"" claim.----------------- The caption in Figure 2 miss some indication that ""a"" and ""b"" refer to subfigures. I recommend ""a"" --> ""a)"" and ""b"" --> ""b)"".----------------- On page 4 it is mentioned that under the heuristic similarity measure some properties of hyperbolic spaces are lost while other are retained. From what I can read, it is only claimed that key properties are kept; a more formal argument (even if trivial) would have been helpful.------------------------== Original Review ==----------------The paper considers embeddings of graph-structured data onto the hyperbolic Poincare ball. Focus is on word2vec style models but with hyperbolic embeddings. I am unable to determine how suitable an embedding space the Poincare ball really is, since I am not familiar enough with the type of data studied in the paper. I have a few minor comments/questions to the work, but my main concern is a seeming lack of novelty:--------The paper argues that the main contribution is that this is the first neural embedding onto a hyperbolic space. From what I can see, the paper----------------  Poincaré Embeddings for Learning Hierarchical Representations--------  https://arxiv.org/abs/1705.08039----------------consider an almost identical model to the one proposed here with an almost identical motivation and application set. Some technicalities appear different, but (to me) it seems like the main claimed novelties of the present paper has already been out for a while. If this analysis is incorrect, then I encourage the authors to provide very explicit arguments for this in the rebuttal phase.----------------Other comments:--------*) It seems to me that, by construction, most data will be pushed towards the boundary of the Poincare ball during the embedding. Is that a property you want?--------*) I found it rather surprising that the log-likelihood under consideration was pushed to an appendix of the paper, while its various derivatives are part of the main text. Given the not-so-tight page limits of ICLR, I'd recommend to provide the log-likelihood as part of the main text (it's rather difficult to evaluate the correctness of a derivative when its base function is not stated).--------*) In the introduction must energy is used on the importance of large data sets, but it appears that only fairly small-scale experiments are considered. I'd recommend a better synchronization.--------*) I find visual comparisons difficult on the Poincare ball as I am so trained at assuming Euclidean distances when making visual comparisons (I suspect most readers are as well). I think one needs to be very careful when making visual comparisons under non-trivial metrics.--------*) In the final experiment, a logistic regressor is fitted post hoc to the embedded points. Why not directly optimize a hyperbolic classifier?----------------Pros:--------+ well-written and (fairly) well-motivated.----------------Cons:--------- It appears that novelty is very limited as highly similar work (see above) has been out for a while.","This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
